<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Statistics with R</title>
  <meta name="description" content="Lecture notes providing an introduction to computational statistics using the R programming language.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Statistics with R" />
  
  <meta name="twitter:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

<meta name="author" content="Niels Richard Hansen">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="transformation-techniques.html">
<link rel="next" href="MCI.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro-smooth.html"><a href="intro-smooth.html"><i class="fa fa-check"></i><b>1.1</b> Smoothing</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro-smooth.html"><a href="intro-smooth.html#intro-angles"><i class="fa fa-check"></i><b>1.1.1</b> Angle distributions in proteins</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-smooth.html"><a href="intro-smooth.html#using-ggplot2"><i class="fa fa-check"></i><b>1.1.2</b> Using ggplot2</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-smooth.html"><a href="intro-smooth.html#changing-the-defaults"><i class="fa fa-check"></i><b>1.1.3</b> Changing the defaults</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-smooth.html"><a href="intro-smooth.html#large-scale-smoothing"><i class="fa fa-check"></i><b>1.1.4</b> Large scale smoothing</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo Methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#vM"><i class="fa fa-check"></i><b>1.2.1</b> Univariate von Mises distributions</a></li>
<li class="chapter" data-level="1.2.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#mixtures-of-von-mises-distributions"><i class="fa fa-check"></i><b>1.2.2</b> Mixtures of von Mises distributions</a></li>
<li class="chapter" data-level="1.2.3" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#large-scale-simulation"><i class="fa fa-check"></i><b>1.2.3</b> Large scale simulation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="optimization.html"><a href="optimization.html"><i class="fa fa-check"></i><b>1.3</b> Optimization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="optimization.html"><a href="optimization.html#the-em-algorithm"><i class="fa fa-check"></i><b>1.3.1</b> The EM-algorithm</a></li>
<li class="chapter" data-level="1.3.2" data-path="optimization.html"><a href="optimization.html#large-scale-optimization"><i class="fa fa-check"></i><b>1.3.2</b> Large scale optimization</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.4</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#r-training-exercises"><i class="fa fa-check"></i>R training exercises</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#histograms-with-non-equidistant-breaks"><i class="fa fa-check"></i>Histograms with non-equidistant breaks</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#functions-and-functional-programming"><i class="fa fa-check"></i>Functions and functional programming</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Smoothing</b></span></li>
<li class="chapter" data-level="2" data-path="density.html"><a href="density.html"><i class="fa fa-check"></i><b>2</b> Density estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="unidens.html"><a href="unidens.html"><i class="fa fa-check"></i><b>2.1</b> Univariate density estimation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="unidens.html"><a href="unidens.html#likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Likelihood considerations</a></li>
<li class="chapter" data-level="2.1.2" data-path="unidens.html"><a href="unidens.html#sieves"><i class="fa fa-check"></i><b>2.1.2</b> Method of sieves</a></li>
<li class="chapter" data-level="2.1.3" data-path="unidens.html"><a href="unidens.html#basis-density"><i class="fa fa-check"></i><b>2.1.3</b> Basis expansions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="kernel-density.html"><a href="kernel-density.html"><i class="fa fa-check"></i><b>2.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="kernel-density.html"><a href="kernel-density.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="kernel-density.html"><a href="kernel-density.html#benchmarking"><i class="fa fa-check"></i><b>2.2.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bandwidth.html"><a href="bandwidth.html"><i class="fa fa-check"></i><b>2.3</b> Bandwidth selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bandwidth.html"><a href="bandwidth.html#rectangular"><i class="fa fa-check"></i><b>2.3.1</b> Revisiting the rectangular kernel</a></li>
<li class="chapter" data-level="2.3.2" data-path="bandwidth.html"><a href="bandwidth.html#ise-mise-and-mse-for-kernel-estimators"><i class="fa fa-check"></i><b>2.3.2</b> ISE, MISE and MSE for kernel estimators</a></li>
<li class="chapter" data-level="2.3.3" data-path="bandwidth.html"><a href="bandwidth.html#plug-in-estimation-of-the-oracle-bandwidth"><i class="fa fa-check"></i><b>2.3.3</b> Plug-in estimation of the oracle bandwidth</a></li>
<li class="chapter" data-level="2.3.4" data-path="bandwidth.html"><a href="bandwidth.html#cross-validation"><i class="fa fa-check"></i><b>2.3.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multivariate-smoothing.html"><a href="multivariate-smoothing.html"><i class="fa fa-check"></i><b>2.4</b> Multivariate methods</a></li>
<li class="chapter" data-level="2.5" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#kernel-density-estimation"><i class="fa fa-check"></i>Kernel density estimation</a></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#benchmarking-1"><i class="fa fa-check"></i>Benchmarking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i><b>3</b> Bivariate smoothing</a><ul>
<li class="chapter" data-level="3.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html"><i class="fa fa-check"></i><b>3.1</b> Nearest neighbor smoothers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#linear-smoothers"><i class="fa fa-check"></i><b>3.1.1</b> Linear smoothers</a></li>
<li class="chapter" data-level="3.1.2" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#implementing-the-running-mean"><i class="fa fa-check"></i><b>3.1.2</b> Implementing the running mean</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="kernel-methods.html"><a href="kernel-methods.html"><i class="fa fa-check"></i><b>3.2</b> Kernel methods</a></li>
<li class="chapter" data-level="3.3" data-path="sparse-linear-algebra.html"><a href="sparse-linear-algebra.html"><i class="fa fa-check"></i><b>3.3</b> Sparse linear algebra</a></li>
<li class="chapter" data-level="3.4" data-path="orthogonal-basis-expansions.html"><a href="orthogonal-basis-expansions.html"><i class="fa fa-check"></i><b>3.4</b> Orthogonal basis expansions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="orthogonal-basis-expansions.html"><a href="orthogonal-basis-expansions.html#polynomial-expansions"><i class="fa fa-check"></i><b>3.4.1</b> Polynomial expansions</a></li>
<li class="chapter" data-level="3.4.2" data-path="orthogonal-basis-expansions.html"><a href="orthogonal-basis-expansions.html#fourier-expansions"><i class="fa fa-check"></i><b>3.4.2</b> Fourier expansions</a></li>
<li class="chapter" data-level="3.4.3" data-path="orthogonal-basis-expansions.html"><a href="orthogonal-basis-expansions.html#wavelets"><i class="fa fa-check"></i><b>3.4.3</b> Wavelets</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>3.5</b> Splines</a></li>
<li class="chapter" data-level="3.6" data-path="gaussian-processes.html"><a href="gaussian-processes.html"><i class="fa fa-check"></i><b>3.6</b> Gaussian processes</a></li>
<li class="chapter" data-level="3.7" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html"><i class="fa fa-check"></i><b>3.7</b> The Kalman filter</a><ul>
<li class="chapter" data-level="3.7.1" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#ar1-example"><i class="fa fa-check"></i><b>3.7.1</b> AR(1)-example</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-smoother"><i class="fa fa-check"></i><b>3.7.2</b> The Kalman smoother</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#implementation-1"><i class="fa fa-check"></i><b>3.7.3</b> Implementation</a></li>
<li class="chapter" data-level="3.7.4" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-filter-1"><i class="fa fa-check"></i><b>3.7.4</b> The Kalman filter</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Monte Carlo Methods</b></span></li>
<li class="chapter" data-level="4" data-path="univariate-random-variables.html"><a href="univariate-random-variables.html"><i class="fa fa-check"></i><b>4</b> Univariate random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="pseudo-random-numbers.html"><a href="pseudo-random-numbers.html"><i class="fa fa-check"></i><b>4.1</b> Pseudo random numbers</a></li>
<li class="chapter" data-level="4.2" data-path="transformation-techniques.html"><a href="transformation-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Transformation techniques</a><ul>
<li class="chapter" data-level="4.2.1" data-path="transformation-techniques.html"><a href="transformation-techniques.html#sampling-from-a-t-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Sampling from a <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="reject-samp.html"><a href="reject-samp.html"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a><ul>
<li class="chapter" data-level="4.3.1" data-path="reject-samp.html"><a href="reject-samp.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.1</b> Gamma distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="MCI.html"><a href="MCI.html"><i class="fa fa-check"></i><b>5</b> Monte Carlo integration</a><ul>
<li class="chapter" data-level="5.1" data-path="assessment.html"><a href="assessment.html"><i class="fa fa-check"></i><b>5.1</b> Assessment</a><ul>
<li class="chapter" data-level="5.1.1" data-path="assessment.html"><a href="assessment.html#using-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.1.1</b> Using the central limit theorem</a></li>
<li class="chapter" data-level="5.1.2" data-path="assessment.html"><a href="assessment.html#concentration-inequalities"><i class="fa fa-check"></i><b>5.1.2</b> Concentration inequalities</a></li>
<li class="chapter" data-level="5.1.3" data-path="assessment.html"><a href="assessment.html#exponential-tail-bound-for-gamma-distributed-variables"><i class="fa fa-check"></i><b>5.1.3</b> Exponential tail bound for Gamma distributed variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>5.2</b> Importance sampling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importance-sampling.html"><a href="importance-sampling.html#unknown-normalization-constants"><i class="fa fa-check"></i><b>5.2.1</b> Unknown normalization constants</a></li>
<li class="chapter" data-level="5.2.2" data-path="importance-sampling.html"><a href="importance-sampling.html#computing-a-high-dimensional-integral"><i class="fa fa-check"></i><b>5.2.2</b> Computing a high-dimensional integral</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="network-failure.html"><a href="network-failure.html"><i class="fa fa-check"></i><b>5.3</b> Network failure</a><ul>
<li class="chapter" data-level="5.3.1" data-path="network-failure.html"><a href="network-failure.html#object-oriented-implementation"><i class="fa fa-check"></i><b>5.3.1</b> Object oriented implementation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="design-of-experiments.html"><a href="design-of-experiments.html"><i class="fa fa-check"></i><b>5.4</b> Design of experiments</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-random-variables.html"><a href="multivariate-random-variables.html"><i class="fa fa-check"></i><b>6</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="6.1" data-path="sequential-simulation.html"><a href="sequential-simulation.html"><i class="fa fa-check"></i><b>6.1</b> Sequential simulation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="sequential-simulation.html"><a href="sequential-simulation.html#sequential-mc-for-the-ar1-process"><i class="fa fa-check"></i><b>6.1.1</b> Sequential MC for the AR(1)-process</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="gaussian-random-variables.html"><a href="gaussian-random-variables.html"><i class="fa fa-check"></i><b>6.2</b> Gaussian random variables</a></li>
</ul></li>
<li class="part"><span><b>Part III: Optimization</b></span></li>
<li class="chapter" data-level="7" data-path="four-examples.html"><a href="four-examples.html"><i class="fa fa-check"></i><b>7</b> Four Examples</a><ul>
<li class="chapter" data-level="7.1" data-path="multinomial-models.html"><a href="multinomial-models.html"><i class="fa fa-check"></i><b>7.1</b> Multinomial models</a><ul>
<li class="chapter" data-level="7.1.1" data-path="multinomial-models.html"><a href="multinomial-models.html#peppered-moths"><i class="fa fa-check"></i><b>7.1.1</b> Peppered Moths</a></li>
<li class="chapter" data-level="7.1.2" data-path="multinomial-models.html"><a href="multinomial-models.html#multinomial-cell-collapsing"><i class="fa fa-check"></i><b>7.1.2</b> Multinomial cell collapsing</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="mixtures.html"><a href="mixtures.html"><i class="fa fa-check"></i><b>7.2</b> Mixtures</a><ul>
<li class="chapter" data-level="7.2.1" data-path="mixtures.html"><a href="mixtures.html#gaussian-mixtures"><i class="fa fa-check"></i><b>7.2.1</b> Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html"><i class="fa fa-check"></i><b>7.3</b> Mixed effects models</a></li>
<li class="chapter" data-level="7.4" data-path="gaussian-state-space-models.html"><a href="gaussian-state-space-models.html"><i class="fa fa-check"></i><b>7.4</b> Gaussian state space models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="numerical-optimization.html"><a href="numerical-optimization.html"><i class="fa fa-check"></i><b>8</b> Numerical optimization</a><ul>
<li class="chapter" data-level="8.1" data-path="gradient-based-algorithms.html"><a href="gradient-based-algorithms.html"><i class="fa fa-check"></i><b>8.1</b> Gradient based algorithms</a><ul>
<li class="chapter" data-level="8.1.1" data-path="gradient-based-algorithms.html"><a href="gradient-based-algorithms.html#gradient-descent-and-conjugate-gradient"><i class="fa fa-check"></i><b>8.1.1</b> Gradient descent and conjugate gradient</a></li>
<li class="chapter" data-level="8.1.2" data-path="gradient-based-algorithms.html"><a href="gradient-based-algorithms.html#peppered-moths-1"><i class="fa fa-check"></i><b>8.1.2</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html"><i class="fa fa-check"></i><b>8.2</b> Newton-type algorithms</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="EM.html"><a href="EM.html"><i class="fa fa-check"></i><b>9</b> Expectation Maximization algorithms</a><ul>
<li class="chapter" data-level="9.1" data-path="basic-properties.html"><a href="basic-properties.html"><i class="fa fa-check"></i><b>9.1</b> Basic properties</a><ul>
<li class="chapter" data-level="9.1.1" data-path="basic-properties.html"><a href="basic-properties.html#incomplete-data-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Incomplete data likelihood</a></li>
<li class="chapter" data-level="9.1.2" data-path="basic-properties.html"><a href="basic-properties.html#the-em-algorithm-is-ascending"><i class="fa fa-check"></i><b>9.1.2</b> The EM-algorithm is ascending</a></li>
<li class="chapter" data-level="9.1.3" data-path="basic-properties.html"><a href="basic-properties.html#multinomial-cell-collapsing-1"><i class="fa fa-check"></i><b>9.1.3</b> Multinomial cell collapsing</a></li>
<li class="chapter" data-level="9.1.4" data-path="basic-properties.html"><a href="basic-properties.html#peppered-moths-e--and-m-steps"><i class="fa fa-check"></i><b>9.1.4</b> Peppered Moths E- and M-steps</a></li>
<li class="chapter" data-level="9.1.5" data-path="basic-properties.html"><a href="basic-properties.html#inside-the-em"><i class="fa fa-check"></i><b>9.1.5</b> Inside the EM</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="EM-exp.html"><a href="EM-exp.html"><i class="fa fa-check"></i><b>9.2</b> Exponential families</a></li>
<li class="chapter" data-level="9.3" data-path="fisher-information.html"><a href="fisher-information.html"><i class="fa fa-check"></i><b>9.3</b> Fisher information</a></li>
<li class="chapter" data-level="9.4" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html"><i class="fa fa-check"></i><b>9.4</b> Two examples revisited</a><ul>
<li class="chapter" data-level="9.4.1" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html#gaussian-mixtures-1"><i class="fa fa-check"></i><b>9.4.1</b> Gaussian mixtures</a></li>
<li class="chapter" data-level="9.4.2" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html#gaussian-state-space"><i class="fa fa-check"></i><b>9.4.2</b> Gaussian state space</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="stochastic-optimization.html"><a href="stochastic-optimization.html"><i class="fa fa-check"></i><b>10</b> Stochastic Optimization</a><ul>
<li class="chapter" data-level="10.1" data-path="stochastic-gradient.html"><a href="stochastic-gradient.html"><i class="fa fa-check"></i><b>10.1</b> Stochastic gradient</a></li>
<li class="chapter" data-level="10.2" data-path="stochastic-em.html"><a href="stochastic-em.html"><i class="fa fa-check"></i><b>10.2</b> Stochastic EM</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="reject-samp" class="section level2">
<h2><span class="header-section-number">4.3</span> Rejection sampling</h2>
<p>Let <span class="math inline">\(Y_1, Y_2, \ldots\)</span> be i.i.d. with density <span class="math inline">\(g\)</span> on <span class="math inline">\(\mathbb{R}\)</span> and <span class="math inline">\(U_1, U_2, \ldots\)</span> be i.i.d. uniformly distributed on <span class="math inline">\((0,1)\)</span> and independent of the <span class="math inline">\(Y_i\)</span>s. Define <span class="math display">\[T(\mathbf{Y}, \mathbf{U}) = Y_{\sigma}\]</span> with <span class="math display">\[\sigma = \inf\{n \geq 1 \mid U_n \leq \alpha f(Y_n) / g(Y_n)\},\]</span> for <span class="math inline">\(\alpha \in (0, 1]\)</span> and <span class="math inline">\(f\)</span> a density. Rejection sampling then consists of sampling independent pairs <span class="math inline">\((Y_n, U_n)\)</span> as long as we <em>reject</em> the proposals <span class="math inline">\(Y_n\)</span> sampled from <span class="math inline">\(g\)</span>, that is, as long as <span class="math display">\[U_n &gt; \alpha f(Y_n) / g(Y_n).\]</span> The first time we <em>accept</em> a proposal is <span class="math inline">\(\sigma\)</span>, and then we stop the sampling and return the proposal <span class="math inline">\(Y_{\sigma}\)</span>. The result is, indeed, a sample from the distribution with density <span class="math inline">\(f\)</span> as the following theorem states.</p>

<div class="theorem">
<span id="thm:reject" class="theorem"><strong>Theorem 4.1  </strong></span>If <span class="math inline">\(\alpha f(y) \leq g(y)\)</span> for all <span class="math inline">\(y \in \mathbb{R}\)</span> then the distribution of <span class="math inline">\(Y_{\sigma}\)</span> has density <span class="math inline">\(f\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> The formal proof decomposes the event <span class="math inline">\((Y_{\sigma} \leq y)\)</span> according to the value of <span class="math inline">\(\sigma\)</span> to reduce the computation to the one on page 155,</p>
<span class="math display">\[\begin{align}
P(Y_{\sigma} \leq y) &amp; = \sum_{n = 1}^{\infty} P(Y_{n} \leq y, \ \sigma = k) \\
&amp; = \sum_{n = 1}^{\infty} P(Y_{n} \leq y, \ U_n \leq \alpha f(Y_n) / g(Y_n)) P(\sigma &gt; n - 1) \\
&amp; = \ldots \\
&amp; = \int_{-\infty}^y f(z) \mathrm{d} z.
\end{align}\]</span>
</div>

<p>Note that if <span class="math inline">\(\alpha f \leq g\)</span> for <em>densities</em> <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>, then <span class="math display">\[\alpha = \int \alpha f(x) \mathrm{d}x \leq \int g(x) \mathrm{d}x = 1,\]</span> whence it follows automatically that <span class="math inline">\(\alpha \leq 1\)</span> whenever <span class="math inline">\(\alpha f\)</span> is dominated by <span class="math inline">\(g\)</span>. The function <span class="math inline">\(g/\alpha\)</span> is called the <em>envelope</em> of <span class="math inline">\(f\)</span>. The tighter the envelope, the smaller is the probability of rejecting a sample from <span class="math inline">\(g\)</span>, and this is quantified explicitly by <span class="math inline">\(\alpha\)</span> as <span class="math inline">\(1 - \alpha\)</span> is the rejection probability. Thus <span class="math inline">\(\alpha\)</span> should preferably be as close to one as possible.</p>
<p>If <span class="math inline">\(f(y) = c q(y)\)</span> and <span class="math inline">\(g(y) = d p(y)\)</span> for (unknown) normalizing constants <span class="math inline">\(c, d &gt; 0\)</span> and <span class="math inline">\(\alpha&#39; q \leq p\)</span> then <span class="math display">\[\underbrace{\left(\frac{\alpha&#39; d}{c}\right)}_{= \alpha} \ f \leq g.\]</span> The constant <span class="math inline">\(\alpha&#39;\)</span> may be larger than 1, but from the argument above we know that <span class="math inline">\(\alpha \leq 1\)</span>, and Theorem <a href="reject-samp.html#thm:reject">4.1</a> gives that <span class="math inline">\(Y_{\sigma}\)</span> has distribution with density <span class="math inline">\(f\)</span>. It appears that we need to compute the normalizing constants to implement rejection sampling. However, observe that <span class="math display">\[u \leq \frac{\alpha f(y)}{g(y)} \Leftrightarrow u \leq \frac{\alpha&#39; q(y)}{p(y)},\]</span> whence rejection sampling can actually be implemented with knowledge of the unnormalized densities and <span class="math inline">\(\alpha&#39;\)</span> only and without computing <span class="math inline">\(c\)</span> or <span class="math inline">\(d\)</span>. This is one great advantages of rejection sampling, though we should note that when we don’t know the normalizing constants, <span class="math inline">\(\alpha&#39;\)</span> does not tell us anything about how tight the envelope is, and thus how small the rejection probability is.</p>
<div id="gamma-distribution" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Gamma distribution</h3>
<p>It may be possible to find a suitable envelope of the density for the gamma distribution on <span class="math inline">\((0, \infty)\)</span>, but it turns out that there is a very efficient rejection sampler of a non-standard distribution that can be transformed it by a simple transformation.</p>
<p>Let <span class="math inline">\(t(y) = a(1 + by)^3\)</span> for <span class="math inline">\(y \in (-b^{-1}, \infty)\)</span>, then <span class="math inline">\(t(Y) \sim \Gamma(r,1)\)</span> if <span class="math inline">\(r \geq 1\)</span> and <span class="math inline">\(Y\)</span> has density <span class="math display">\[f(y) \propto t(y)^{r-1}t&#39;(y) e^{-t(y)} = e^{(r-1)\log t(y) + \log t&#39;(y) - t(y)}.\]</span></p>
<p>[Proof: Use univariate density transformation theorem.]</p>
<p>The density <span class="math inline">\(f\)</span> will be the <em>target density</em> for a rejection sampler.</p>
<p>With <span class="math display">\[f(y) \propto e^{(r-1)\log t(y) + \log t&#39;(y) - t(y)},\]</span> <span class="math inline">\(a = r - 1/3\)</span> and <span class="math inline">\(b = 1/(3 \sqrt{a})\)</span> <span class="math display">\[f(y) \propto e^{a \log t(y)/a - t(y) + a \log a} \propto \underbrace{e^{a \log t(y)/a - t(y) + a}}_{q(y)}.\]</span></p>
<p>An analysis of <span class="math inline">\(w(y) := - y^2/2 - \log q(y)\)</span> shows that it is convex on <span class="math inline">\((-b^{-1}, \infty)\)</span> and it attains its minimum in <span class="math inline">\(0\)</span> with <span class="math inline">\(w(0) = 0\)</span>, whence <span class="math display">\[q(y) \leq e^{-y^2/2}.\]</span> This gives us an envelope expressed in terms of unnormalized densities with <span class="math inline">\(\alpha&#39; = 1\)</span>.</p>
<p>The implementation of a rejection sampler based on this analysis is relatively straightforward. The rejection sampler will simulate from the distribution with density <span class="math inline">\(f\)</span> by simulating from the Gaussian distribution (the envelope). For the rejection step we need to implement <span class="math inline">\(q\)</span>. Finally, we also need to implement <span class="math inline">\(t\)</span> to transform the result from the rejection sampler to be gamma distributed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## r &gt;= 1 
tfun &lt;-<span class="st"> </span><span class="cf">function</span>(y, a) {
  b &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">3</span> <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(a))
  (y <span class="op">&gt;</span><span class="st"> </span><span class="op">-</span><span class="dv">1</span><span class="op">/</span>b) <span class="op">*</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>b <span class="op">*</span><span class="st"> </span>y)<span class="op">^</span><span class="dv">3</span>  ## 0 when y &lt;= -1/b
}

qfun &lt;-<span class="st"> </span><span class="cf">function</span>(y, r) {
  a &lt;-<span class="st"> </span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>
  tval &lt;-<span class="st"> </span><span class="kw">tfun</span>(y, a)
  <span class="kw">exp</span>(a <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(tval <span class="op">/</span><span class="st"> </span>a) <span class="op">-</span><span class="st"> </span>tval <span class="op">+</span><span class="st"> </span>a)
}

gammasim &lt;-<span class="st"> </span><span class="cf">function</span>(n, r) {
  y &lt;-<span class="st"> </span><span class="kw">numeric</span>(n)
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {
    ratio &lt;-<span class="st"> </span><span class="dv">0</span> 
    u &lt;-<span class="st"> </span><span class="dv">1</span>
    <span class="cf">while</span>(u <span class="op">&gt;</span><span class="st"> </span>ratio) {
      y0 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>)
      ratio &lt;-<span class="st"> </span><span class="kw">qfun</span>(y0, r) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(y0<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)
      u &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>)
    }
    y[i] &lt;-<span class="st"> </span>y0
  }
  <span class="kw">tfun</span>(y, r <span class="op">-</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)
}</code></pre></div>
<p>Even if the implementation is not optimized in any way, it can quickly simulate thousands of random variables. We test the implementation by simulating 10,000 values with parameters <span class="math inline">\(r = 8\)</span> as well as <span class="math inline">\(r = 1\)</span> and compare the resulting histograms to the respective theoretical densities.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system.time</span>(y &lt;-<span class="st"> </span><span class="kw">gammasim</span>(<span class="dv">10000</span>, <span class="dv">8</span>))</code></pre></div>
<pre><code>##    user  system elapsed 
##   0.057   0.003   0.060</code></pre>
<p><img src="CSwR_files/figure-html/gammaBench-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system.time</span>(y &lt;-<span class="st"> </span><span class="kw">gammasim</span>(<span class="dv">10000</span>, <span class="dv">1</span>))</code></pre></div>
<pre><code>##    user  system elapsed 
##   0.052   0.003   0.054</code></pre>
<p><img src="CSwR_files/figure-html/gammaBench2-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Though this is only a brief and informal test, it indicates that the implementation correctly simulates from the gamma distribution.</p>
<p>Rejection sampling can be computationally expensive if many samples are rejected. A very tight envelope will lead to fewer rejections, while a loose envelope will lead to many rejections. We modify the code above to estimate the rejection probability and thus quantify how tight the envelope is.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gammasim_trace &lt;-<span class="st"> </span><span class="cf">function</span>(n, r) {
  count &lt;-<span class="st"> </span><span class="dv">0</span>
  y &lt;-<span class="st"> </span><span class="kw">numeric</span>(n)
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {
    ratio &lt;-<span class="st"> </span><span class="dv">0</span> 
    u &lt;-<span class="st"> </span><span class="dv">1</span>
    <span class="cf">while</span>(u <span class="op">&gt;</span><span class="st"> </span>ratio) {
      count &lt;-<span class="st"> </span>count <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
      y0 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>)
      ratio &lt;-<span class="st"> </span><span class="kw">qfun</span>(y0, r) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(y0<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)
      u &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>)
    }
    y[i] &lt;-<span class="st"> </span>y0
  }
  <span class="kw">cat</span>((count <span class="op">-</span><span class="st"> </span>n) <span class="op">/</span><span class="st"> </span>count)  ## Rejection frequency
  <span class="kw">tfun</span>(y, r <span class="op">-</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span><span class="kw">gammasim_trace</span>(<span class="dv">100000</span>, <span class="dv">16</span>)</code></pre></div>
<pre><code>## 0.001876472</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span><span class="kw">gammasim_trace</span>(<span class="dv">100000</span>, <span class="dv">8</span>)</code></pre></div>
<pre><code>## 0.003438138</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span><span class="kw">gammasim_trace</span>(<span class="dv">100000</span>, <span class="dv">4</span>)</code></pre></div>
<pre><code>## 0.007975874</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span><span class="kw">gammasim_trace</span>(<span class="dv">100000</span>, <span class="dv">1</span>)</code></pre></div>
<pre><code>## 0.04891434</code></pre>
<p>We observe that the rejection frequencies are small with <span class="math inline">\(r = 1\)</span> being the worst case with around 5% rejections. For the other cases the rejection frequencies are all below 1%, thus rejection is rare.</p>
<p>A visual comparison of <span class="math inline">\(q\)</span> to the (unnormalized) Gaussian density also shows that the two (unnormalized) densities are very close except in the tails where there is very little probability mass.</p>
<p><img src="CSwR_files/figure-html/densComparison-1.png" width="100%" style="display: block; margin: auto;" /></p>

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="transformation-techniques.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="MCI.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

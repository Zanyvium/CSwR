<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 9 Stochastic Optimization | Computational Statistics with R</title>
<meta name="author" content="Niels Richard Hansen">
<meta name="description" content="Numerical optimization involves different tradeoffs such as an exploration-exploitation tradeoff. On the one hand, the objective function must be thoroughly explored to build an adequate model of...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 9 Stochastic Optimization | Computational Statistics with R">
<meta property="og:type" content="book">
<meta property="og:description" content="Numerical optimization involves different tradeoffs such as an exploration-exploitation tradeoff. On the one hand, the objective function must be thoroughly explored to build an adequate model of...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 9 Stochastic Optimization | Computational Statistics with R">
<meta name="twitter:description" content="Numerical optimization involves different tradeoffs such as an exploration-exploitation tradeoff. On the one hand, the objective function must be thoroughly explored to build an adequate model of...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/_Roboto%20Slab-0.4.0/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.3.0/transition.js"></script><script src="libs/bs3compat-0.3.0/tabs.js"></script><script src="libs/bs3compat-0.3.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Computational Statistics with R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li class="book-part">Part I: Smoothing</li>
<li><a class="" href="density.html"><span class="header-section-number">2</span> Density estimation</a></li>
<li><a class="" href="bivariate.html"><span class="header-section-number">3</span> Bivariate smoothing</a></li>
<li class="book-part">Part II: Monte Carlo Methods</li>
<li><a class="" href="univariate-random-variables.html"><span class="header-section-number">4</span> Univariate random variables</a></li>
<li><a class="" href="mci.html"><span class="header-section-number">5</span> Monte Carlo integration</a></li>
<li class="book-part">Part III: Optimization</li>
<li><a class="" href="four-examples.html"><span class="header-section-number">6</span> Four Examples</a></li>
<li><a class="" href="numopt.html"><span class="header-section-number">7</span> Numerical optimization</a></li>
<li><a class="" href="em.html"><span class="header-section-number">8</span> Expectation maximization algorithms</a></li>
<li><a class="active" href="StochOpt.html"><span class="header-section-number">9</span> Stochastic Optimization</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="app-R.html"><span class="header-section-number">A</span> R programming</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/nielsrhansen/CSwR">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="StochOpt" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Stochastic Optimization<a class="anchor" aria-label="anchor" href="#StochOpt"><i class="fas fa-link"></i></a>
</h1>
<p>Numerical optimization involves different tradeoffs such as
an <em>exploration-exploitation</em> tradeoff.
On the one hand, the objective function must be thoroughly explored
to build an adequate model of it. On the other hand, the model should be
exploited so as to find the minimum quickly. Another tradeoff is between
the accuracy of the model and the time it takes to compute with it.</p>
<p>The optimization algorithms considered in Chapters <a href="numopt.html#numopt">7</a> and <a href="em.html#em">8</a>
work on all available data and take deterministic steps in each iteration.
The models are based on accurate local computations of derivatives that can be
demanding to compute for large data sets. Moreover, the algorithms
greedily exploit the local model obtained from derivatives, but they do
little exploration.</p>
<p>By including randomness into optimization algorithms it is
possible to lower the computational costs and make the algorithms more
exploratory. This can be done in various ways. Examples of stochastic
optimization algorithms include simulated annealing and evolutionary algorithms that
incorporate randomness into the iterative steps with the purpose of exploring
the objective function better than a deterministic algorithm is able to. In
particular, to avoid getting stuck in saddle points and to escape local minima.
Stochastic gradient algorithms form another example, where descent
directions are approximated by gradients from random subsets of the data.</p>
<p>The literature on stochastic optimization is huge, and this chapter will
only cover some examples of particular relevance to statistics and machine
learning. The most prominent applications are to large scale optimization,
where stochastic gradient algorithms have
become the standard solution. When the dimension of the optimization problem becomes
very large, second order methods become prohibitively slow, and if the
number of observations is also large, even one computation of the gradient
for the entire data batch becomes time consuming. In those cases,
stochastic gradient algorithms, that originate from online learning, can make
progress more quickly while still using the entire batch of data.</p>
<div id="SG-alg" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> Stochastic gradient algorithms<a class="anchor" aria-label="anchor" href="#SG-alg"><i class="fas fa-link"></i></a>
</h2>
<p>The stochastic gradient algorithms have their origin in an online learning
framework, where the objective function is an expected loss and data arrives
sequentially as a stream of data points. <span class="citation"><a href="references.html#ref-Robbins:1951" role="doc-biblioref">Robbins and Monro</a> (<a href="references.html#ref-Robbins:1951" role="doc-biblioref">1951</a>)</span> introduced in their
seminal paper a variant of the online stochastic gradient algorithm that they
called the <em>stochastic approximation method</em>, and they established the first
convergence result for such algorithms. To understand what the stochastic gradient
algorithms generally are supposed to optimize, we will introduce the
general framework of a population model below and give conditions that ensure
that the basic online algorithm converges. Subsequently, the basic online algorithm
is turned into an algorithm for batch data, which is the algorithm of
primary interest. In the following sections, various beneficial extensions of
the basic batch algorithm are explored.</p>
<div id="Pop-model" class="section level3" number="9.1.1">
<h3>
<span class="header-section-number">9.1.1</span> Population models<a class="anchor" aria-label="anchor" href="#Pop-model"><i class="fas fa-link"></i></a>
</h3>
<p>We will consider observations from the sample
space <span class="math inline">\(\mathcal{X}\)</span>, and we will be interested in estimating parameters
from the parameter space <span class="math inline">\(\Theta\)</span>. A loss function
<span class="math display">\[L : \mathcal{X} \times \Theta \to \mathbb{R}\]</span>
is fixed throughout, and we will be interested in minimizing the expected
loss also known as the <em>risk</em>. That is, we want to minimize
<span class="math display">\[H(\theta) = E(L(X, \theta)) = \int L(x, \theta) \mathrm{\mu}(dx)\]</span>
with <span class="math inline">\(X \in \mathcal{X}\)</span> having distribution <span class="math inline">\(\mu\)</span>.
Of course, it is implicitly understood that the expectation has to be well
defined for all <span class="math inline">\(\theta\)</span>.</p>

<div class="example">
<p><span id="exm:least-squares" class="example"><strong>Example 9.1  </strong></span>Suppose that <span class="math inline">\(X = (Y, Z)\)</span> with <span class="math inline">\(Y\)</span> a real valued random variable, and
that <span class="math inline">\(\mu(z, \theta)\)</span> denotes a parametrized mean value conditionally on <span class="math inline">\(Z = z\)</span>.
Then the mean squared error is defined in terms of the squared error loss as
<span class="math display">\[\mathrm{MSE}(\theta) = \frac{1}{2} E (Y - \mu(Z, \theta))^2.\]</span>
That is, the loss function is the squared error loss
<span class="math inline">\(L((y,z), \theta) = \frac{1}{2} (y - \mu(z, \theta))^2\)</span>.
From the definition it follows that
<span class="math display">\[2 \mathrm{MSE}(\theta) =  E(Y - E(Y \mid Z))^2 + E (E(Y\mid Z) - \mu(Z, \theta))^2,\]</span>
where the first term does not depend upon <span class="math inline">\(\theta\)</span>. Thus minimizing the
mean squared error is the same as finding <span class="math inline">\(\theta_0\)</span> such that <span class="math inline">\(\mu(z, \theta_0)\)</span>
is the optimal approximation of <span class="math inline">\(E(Y \mid Z = z)\)</span> in a squared error sense.</p>
</div>
<p>Note how the link between the distribution of <span class="math inline">\(X\)</span> and the parameter is defined
in the example above by the choice of loss function. There is no upfront
assumption that <span class="math inline">\(E(Y\mid Z = z) = \mu(z, \theta_0)\)</span> for some “true” <span class="math inline">\(\theta_0\)</span>, but
if there is such a <span class="math inline">\(\theta_0\)</span>, it will clearly be a minimizer. In general, the
optimal <span class="math inline">\(\theta_0\)</span> is simply the <span class="math inline">\(\theta\)</span> that minimizes the risk.</p>
<p>The mean squared error is suitable when we model the conditional mean of <span class="math inline">\(Y\)</span>.
Other loss functions include the check loss functions used for
<a href="https://en.wikipedia.org/wiki/Quantile_regression#Quantiles">quantile regression</a>.
The absolute value is a special case suitable for modeling the conditional
median. The following example will introduce the log-likelihood loss that is
suitable for a parametrized family of distributions.</p>

<div class="example">
<p><span id="exm:log-likelihood" class="example"><strong>Example 9.2  </strong></span>Suppose that <span class="math inline">\(f_{\theta}\)</span> denotes a density on <span class="math inline">\(\mathcal{X}\)</span> parametrized by <span class="math inline">\(\theta\)</span>.
Then the cross-entropy is defined in terms of the log-likelihood loss as
<span class="math display">\[H(\theta) = - E \log(f_{\theta}(X)).\]</span>
Thus the loss function is <span class="math inline">\(L(x, \theta) = - \log f_{\theta}(x)\)</span>.
If the distribution of <span class="math inline">\(X\)</span> has density <span class="math inline">\(f^0\)</span> then
<span class="math display">\[\begin{align*}
H(\theta) &amp; = - E \log(f^0(X)) - E \log(f_{\theta}(X)/f^0(X)) \\
&amp; = H(f^0) + D(f_0 \ || \ f_{\theta})
\end{align*}\]</span>
where the first term is the entropy of <span class="math inline">\(f^0\)</span>, and the second is the
Kullback-Leibler divergence of <span class="math inline">\(f_{\theta}\)</span> from <span class="math inline">\(f^0\)</span>.
Note that the entropy does not depend upon <span class="math inline">\(\theta\)</span> and minimizing
the cross-entropy is thus the same as finding a <span class="math inline">\(\theta\)</span> with <span class="math inline">\(f_{\theta}\)</span>
being the optimal approximation of <span class="math inline">\(f^0\)</span> in a Kullback-Leibler sense.</p>
</div>
</div>
<div id="online-stochastic-gradient-algorithm" class="section level3" number="9.1.2">
<h3>
<span class="header-section-number">9.1.2</span> Online stochastic gradient algorithm<a class="anchor" aria-label="anchor" href="#online-stochastic-gradient-algorithm"><i class="fas fa-link"></i></a>
</h3>
<p>The classical stochastic gradient algorithm is an example of an online
learning algorithm.
It is based on the simple observation that if we can
interchange differentiation and expectation then
<span class="math display">\[\nabla H(\theta) = E \left( \nabla_{\theta} L(X, \theta) \right),\]</span>
thus if <span class="math inline">\(X_1, X_2, \ldots\)</span> form an i.i.d. sequence then <span class="math inline">\(\nabla_{\theta} L(X_i, \theta)\)</span>
is unbiased as an estimate of the gradient of <span class="math inline">\(H\)</span> for any <span class="math inline">\(\theta\)</span> and any <span class="math inline">\(i\)</span>.
With inspiration from gradient descent algorithms it is natural to suggest
stochastic parameter updates of the form
<span class="math display">\[\theta_{n + 1} = \theta_n - \gamma_n \nabla_{\theta} L(X_{n+1}, \theta_n)\]</span>
starting from some initial value <span class="math inline">\(\theta_0\)</span>. The sequence of
step size parameters <span class="math inline">\(\gamma_n \geq 0\)</span> are known collectively
as the <em>learning rate</em>. It can be a deterministic sequence, but <span class="math inline">\(\gamma_n\)</span> may
also be allowed to depend on <span class="math inline">\(X_1, \ldots, X_{n}\)</span> and <span class="math inline">\(\theta_0, \ldots, \theta_{n}\)</span>.</p>

<div class="theorem">
<span id="thm:SG-conv" class="theorem"><strong>Theorem 9.1  </strong></span>Suppose <span class="math inline">\(H\)</span> is strongly convex and
<span class="math inline">\(E(\|\nabla_{\theta} L(X, \theta))\|^2) \leq A + B \|\theta\|^2\)</span>. If
<span class="math inline">\(\theta^*\)</span> is the global minimizer of <span class="math inline">\(H\)</span> then <span class="math inline">\(\theta_n\)</span> converges almost surely
toward <span class="math inline">\(\theta^*\)</span> if
<span class="math display" id="eq:conv-cond">\[\begin{equation}
\sum_{n=1}^{\infty} \gamma_n^2 &lt; \infty \quad \text{and} \quad 
\sum_{n=1}^{\infty} \gamma_n = \infty. \tag{9.1}
\end{equation}\]</span>
</div>
<p>From the above result, convergence of the algorithm
is guaranteed if the learning rate, <span class="math inline">\(\gamma_n\)</span>, converges to 0 but
does so sufficiently slowly. Though formulated in a slightly different way,
<span class="citation"><a href="references.html#ref-Robbins:1951" role="doc-biblioref">Robbins and Monro</a> (<a href="references.html#ref-Robbins:1951" role="doc-biblioref">1951</a>)</span> were the first to demonstrate convergence of an online
learning algorithm under conditions as above on the learning rate.
Following their terminology, much has since been written on online
learning and adaptive control theory under the name <em>stochastic approximation</em>,
<span class="citation"><a href="references.html#ref-Lai:2003" role="doc-biblioref">Lai</a> (<a href="references.html#ref-Lai:2003" role="doc-biblioref">2003</a>)</span>.</p>
<p>The precise way that the learning
rate decays is known as the <em>decay schedule</em>, and a flexible
three-parameter power law family of decay schedules is given by
<span class="math display">\[\gamma_n = \frac{\gamma_0  K}{K + n^a} = \frac{\gamma_0 }{1 + K^{-1} n^{a}}\]</span>
for some initial learning rate <span class="math inline">\(\gamma_0 &gt; 0\)</span> and constants <span class="math inline">\(K, a &gt; 0\)</span>. If
<span class="math inline">\(a \in (0.5, 1]\)</span> the resulting learning rate satisfies the convergence conditions
<a href="StochOpt.html#eq:conv-cond">(9.1)</a>.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:decay-fig"></span>
<img src="CSwR_files/figure-html/decay-fig-1.png" alt="Power law decay schedules as a function of \(n\) for \(\gamma_0 = 1\) and for different choices of \(K\) and \(a.\) The left figure shows decay schedules with \(a\) chosen so that the convergence conditions are fulfilled, whereas the right figure shows decay schedules for which the convergence conditions are not fulfilled." width="100%"><p class="caption">
Figure 9.1: Power law decay schedules as a function of <span class="math inline">\(n\)</span> for <span class="math inline">\(\gamma_0 = 1\)</span> and for different choices of <span class="math inline">\(K\)</span> and <span class="math inline">\(a.\)</span> The left figure shows decay schedules with <span class="math inline">\(a\)</span> chosen so that the convergence conditions are fulfilled, whereas the right figure shows decay schedules for which the convergence conditions are not fulfilled.
</p>
</div>
<p>The parameter <span class="math inline">\(\gamma_0\)</span> determines the initial baseline rate, and Figure <a href="StochOpt.html#fig:decay-fig">9.1</a>
illustrates the effect of the parameters <span class="math inline">\(K\)</span> and <span class="math inline">\(a\)</span> on the decay. The parameter
<span class="math inline">\(a\)</span> is the asymptotic exponent of <span class="math inline">\(\gamma_n \sim \gamma_0 K n^{-a}\)</span>, and <span class="math inline">\(K\)</span>
determines how quickly the rate will turn into a pure power law decay. Moreover,
if we have a target rate, <span class="math inline">\(\gamma_{1}\)</span>, that we want to hit after <span class="math inline">\(n_{1}\)</span>
iterations, and we fix the exponent <span class="math inline">\(a\)</span>, we can also solve for <span class="math inline">\(K\)</span> to find
<span class="math display">\[K = \frac{n_1^a \gamma_1}{\gamma_0 - \gamma_1}.\]</span>
This gives us a decay schedule that interpolates between <span class="math inline">\(\gamma_0\)</span>
and <span class="math inline">\(\gamma_1\)</span> over the range <span class="math inline">\(0, \ldots, n_1\)</span> of iterations.</p>
<p>We implement <code>decay_scheduler()</code> as a function that returns a particular
decay schedule, with the possibility to determine <span class="math inline">\(K\)</span> automatically from a
target rate.</p>
<div class="sourceCode" id="cb387"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">decay_scheduler</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">gamma0</span> <span class="op">=</span> <span class="fl">1</span>, <span class="va">a</span> <span class="op">=</span> <span class="fl">1</span>, <span class="va">K</span> <span class="op">=</span> <span class="fl">1</span>, <span class="va">gamma1</span>, <span class="va">n1</span><span class="op">)</span> <span class="op">{</span>
  <span class="fu"><a href="https://rdrr.io/r/base/force.html">force</a></span><span class="op">(</span><span class="va">a</span><span class="op">)</span>
  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/missing.html">missing</a></span><span class="op">(</span><span class="va">gamma1</span><span class="op">)</span> <span class="op">&amp;&amp;</span> <span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/missing.html">missing</a></span><span class="op">(</span><span class="va">n1</span><span class="op">)</span><span class="op">)</span>
    <span class="va">K</span> <span class="op">&lt;-</span> <span class="va">n1</span><span class="op">^</span><span class="va">a</span> <span class="op">*</span> <span class="va">gamma1</span> <span class="op">/</span> <span class="op">(</span><span class="va">gamma0</span> <span class="op">-</span> <span class="va">gamma1</span><span class="op">)</span>
  <span class="va">b</span> <span class="op">&lt;-</span> <span class="va">gamma0</span> <span class="op">*</span> <span class="va">K</span>
  <span class="kw">function</span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="va">b</span> <span class="op">/</span> <span class="op">(</span><span class="va">K</span> <span class="op">+</span> <span class="va">n</span><span class="op">^</span><span class="va">a</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>The following example of online Poisson regression illustrates the general ideas.</p>

<div class="example">
<p><span id="exm:online-pois-SG" class="example"><strong>Example 9.3  </strong></span>In this example <span class="math inline">\(Y_i \mid Z_i = z_i \sim \mathrm{Pois(e^{\beta_0 + \beta_1 z_i})}\)</span>
for <span class="math inline">\(\beta = (\beta_0, \beta_1)^T\)</span> the parameter vector.
We let the <span class="math inline">\(Z_i\)</span>-s be uniformly distributed in <span class="math inline">\((-1, 1)\)</span>, but this choice is not
particularly important. The conditional mean of <span class="math inline">\(Y_i\)</span> given <span class="math inline">\(Z_i = z_i\)</span> is
<span class="math display">\[\mu(z_i, \beta) = e^{\beta_0 + \beta_1 z_i}\]</span>
and we will first consider the squared error loss. To this end, we observe that
<span class="math display">\[\nabla_{\beta}  \mu(z_i, \beta) =  \mu(z_i, \beta) 
\left( \begin{array}{c} 1 \\ z_i \end{array} \right),\]</span>
which for the squared error loss gives the gradient
<span class="math display">\[\nabla_{\beta} \frac{1}{2} (y_i - \mu(z_i, \beta) )^2 = 
  \mu(z_i, \beta) (\mu(z_i, \beta) - y_i) \left( \begin{array}{c} 1 \\ z_i \end{array} \right).\]</span></p>
</div>
<p>To clearly emulate the online nature of the algorithm, the implementation below
generates the observations sequentially in the loop.</p>
<div class="sourceCode" id="cb388"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">5000</span>
<span class="va">beta_true</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span>
<span class="va">mu</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">z</span>, <span class="va">beta</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">beta</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">beta</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="va">z</span><span class="op">)</span>
<span class="va">beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html">vector</a></span><span class="op">(</span><span class="st">"list"</span>, <span class="va">N</span><span class="op">)</span>

<span class="va">rate</span> <span class="op">&lt;-</span> <span class="fu">decay_scheduler</span><span class="op">(</span>gamma0 <span class="op">=</span> <span class="fl">0.0004</span>, K <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> 
<span class="va">beta</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>beta0 <span class="op">=</span> <span class="fl">1</span>, beta1 <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>

<span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span>
  <span class="co"># Simulating a new data point</span>
  <span class="va">z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>
  <span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fu">mu</span><span class="op">(</span><span class="va">z</span>, <span class="va">beta_true</span><span class="op">)</span><span class="op">)</span>
  <span class="co"># Update via squared error gradient </span>
  <span class="va">mu_old</span> <span class="op">&lt;-</span> <span class="fu">mu</span><span class="op">(</span><span class="va">z</span>, <span class="va">beta</span><span class="op">[[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>
  <span class="va">beta</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">beta</span><span class="op">[[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span><span class="op">]</span>  <span class="op">-</span> <span class="fu">rate</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">*</span> <span class="va">mu_old</span> <span class="op">*</span> <span class="op">(</span><span class="va">mu_old</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">z</span><span class="op">)</span>
<span class="op">}</span>
<span class="va">beta</span><span class="op">[[</span><span class="va">N</span><span class="op">]</span><span class="op">]</span>  <span class="co"># Compare this to beta_true</span></code></pre></div>
<pre><code>##    beta0    beta1 
## 2.037990 2.987941</code></pre>
<p>For the log-likelihood loss we instead find the gradient
<span class="math display">\[\nabla_{\beta} \big( \mu(z_i, \beta) - y_i \log(\mu(z_i, \beta)) \big) = 
  (\mu(z_i, \beta) - y_i) 
\left( \begin{array}{c} 1 \\ z_i \end{array} \right),\]</span>
which leads to a slightly different but equally valid algorithm. We see that the
gradient only differs from the squared error gradient by lacking the factor <span class="math inline">\(\mu(z_i, \beta)\)</span>.
The distribution of this factor has range between <span class="math inline">\(e^{-1} \simeq 0.3679\)</span>
and <span class="math inline">\(e^5 \simeq 148.4\)</span> and is right skewed, that is, it is concentrated toward the
smaller values but with a long right tail. Its median is <span class="math inline">\(e^2 \simeq 7.389\)</span>,
while its mean is <span class="math inline">\((e^5 - e) / 6 \simeq 24.67\)</span>. Due to the factor <span class="math inline">\(\mu(z_i, \beta)\)</span>,
the squared error gradient is typically longer than the log-likelihood
gradient — sometimes by a large factor. In the implementation below with the
gradient of the log-likelihood we therefore choose <span class="math inline">\(\gamma_0\)</span>
a factor 25 larger than the <span class="math inline">\(\gamma_0 = 0.0004\)</span> used with the squared error gradient.</p>
<div class="sourceCode" id="cb390"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">rate</span> <span class="op">&lt;-</span> <span class="fu">decay_scheduler</span><span class="op">(</span>gamma0 <span class="op">=</span> <span class="fl">0.01</span>, K <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> 
<span class="va">beta</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>beta0 <span class="op">=</span> <span class="fl">1</span>, beta1 <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>

<span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span>
  <span class="co"># Simulating a new data point</span>
  <span class="va">z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>
  <span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fu">mu</span><span class="op">(</span><span class="va">z</span>, <span class="va">beta_true</span><span class="op">)</span><span class="op">)</span>
  <span class="co"># Update via log-likelihood gradient </span>
  <span class="va">mu_old</span> <span class="op">&lt;-</span> <span class="fu">mu</span><span class="op">(</span><span class="va">z</span>, <span class="va">beta</span><span class="op">[[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>
  <span class="va">beta</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">beta</span><span class="op">[[</span><span class="va">i</span> <span class="op">-</span> <span class="fl">1</span><span class="op">]</span><span class="op">]</span>  <span class="op">-</span> <span class="fu">rate</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="va">mu_old</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">z</span><span class="op">)</span>
<span class="op">}</span>
<span class="va">beta</span><span class="op">[[</span><span class="va">N</span><span class="op">]</span><span class="op">]</span>  <span class="co"># Compare this to beta_true</span></code></pre></div>
<pre><code>##    beta0    beta1 
## 2.008452 2.987035</code></pre>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:pois-sgd"></span>
<img src="CSwR_files/figure-html/pois-sgd-1.png" alt="Estimated parameter values for the two parameters \(\beta_0\) (true value \(2\)) and \(\beta_1\) (true value \(3\)) in the Poisson regression model as a function of the number of data points in the online stochastic gradient algorithm." width="100%"><p class="caption">
Figure 9.2: Estimated parameter values for the two parameters <span class="math inline">\(\beta_0\)</span> (true value <span class="math inline">\(2\)</span>) and <span class="math inline">\(\beta_1\)</span> (true value <span class="math inline">\(3\)</span>) in the Poisson regression model as a function of the number of data points in the online stochastic gradient algorithm.
</p>
</div>
<p>Figure <a href="StochOpt.html#fig:pois-sgd">9.2</a> shows how the estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>
converge toward the true values for the two different choices of loss functions.
The gradient from the squared error loss resulted in slower convergence and
a more jagged sample path than the gradient from the log-likelihood. This is
explained by the random factor <span class="math inline">\(\mu(z_i, \beta)\)</span> for the squared error gradient,
which also makes the algorithm unstable if <span class="math inline">\(\gamma_0\)</span> is chosen much larger than in
the implementation. Thus
we cannot simply increase <span class="math inline">\(\gamma_0\)</span> to make the convergence faster when using
the squared error gradient. If we attempt to dampen the fluctuations by
lowering <span class="math inline">\(\gamma_0\)</span>, the convergence will be even slower.</p>
<p>The example illustrates that a suitable choice of decay schedule — and even just
the choice of the three parameters in our power law schedule — depends heavily
on the problem considered and the gradient used, and it is a
problem specific challenge to find a good schedule.</p>
</div>
<div id="batch-stochastic-gradient-algorithms" class="section level3" number="9.1.3">
<h3>
<span class="header-section-number">9.1.3</span> Batch stochastic gradient algorithms<a class="anchor" aria-label="anchor" href="#batch-stochastic-gradient-algorithms"><i class="fas fa-link"></i></a>
</h3>
<p>The online algorithm does not store data, and once a data point is used it is
forgotten. The online algorithm is working in a context where data arrives as
a stream of data points and the model is updated continually. In statistics,
we more frequently encounter batch algorithms, where an entire batch of
data points is stored and processed by the algorithm, and where each data point
in the batch can be accessed as many times as we like. However, when the batch
is sufficiently large, many standard batch algorithms are slow, and
some ideas from online algorithms can beneficially be transferred to batch
processing of data.</p>
<p>Within the population model framework in Section <a href="StochOpt.html#Pop-model">9.1.1</a>
the objective is to minimize a theoretical objective function defined
in terms of the probability distribution <span class="math inline">\(\mu\)</span>. For the online
algorithms we imagine an endless stream of data points from <span class="math inline">\(\mu\)</span>, which can be used to ultimately
minimize the expected loss <span class="math inline">\(H(\theta)\)</span>. Batch algorithms replace
the population quantity by an empirical surrogate — the average loss on the batch
<span class="math display">\[H_N(\theta) = \frac{1}{N} \sum_{i=1}^N L(x_i, \theta).\]</span>
Minimizing <span class="math inline">\(H_N\)</span> as a surrogate of minimizing <span class="math inline">\(H\)</span> is known as
<a href="https://en.wikipedia.org/wiki/Empirical_risk_minimization">empirical risk minimization</a>.</p>
<p>If data is i.i.d. the standard deviation of <span class="math inline">\(H_N(\theta)\)</span> decays as <span class="math inline">\(N^{-1/2}\)</span> with <span class="math inline">\(N\)</span>,
while the run time of its computation
increases as <span class="math inline">\(N\)</span>. Thus as we invest more computation time by using more
data we get diminishing returns; doubling the run time will only lower the
precision of <span class="math inline">\(H_N\)</span> as an approximation of <span class="math inline">\(H\)</span> by a factor <span class="math inline">\(1 / \sqrt{2} \simeq 0.7\)</span>.
The same is, of course,
true if we look at the gradient, <span class="math inline">\(\nabla H_N(\theta)\)</span>, or higher derivatives
of <span class="math inline">\(H_N\)</span>. Could we get away with using a fraction of data points each time we compute the
gradient, and then somehow cycle through the entire data set as the optimization
algorithm progresses? The answer is yes, and the stochastic gradient
algorithm gives a (randomized) procedure for “cycling through the data.”</p>
<p>With <span class="math inline">\(\hat{\mu}_N = \frac{1}{N}\sum_{i=1}^N \delta_{x_i}\)</span> denoting the
empirical distribution of the batch data set, we see that
<span class="math display">\[H_N(\theta) = \int L(x, \theta) \mathrm{\mu}_N(dx),\]</span>
that is, the empirical risk is simply the expected loss with respect to the
empirical distribution. Thus to minimize <span class="math inline">\(H_N\)</span> we can use the online
approach by sampling observations from <span class="math inline">\(\hat{\mu}_N\)</span>. This leads to the
following basic version of the stochastic gradient algorithm applied to
a data batch.</p>
<p>From an initial parameter value, <span class="math inline">\(\theta_0\)</span>, we iteratively compute
the parameters as follows: given <span class="math inline">\(\theta_{n}\)</span></p>
<ul>
<li>sample an index <span class="math inline">\(i\)</span> uniformly from <span class="math inline">\(\{1, \ldots, N\}\)</span>
</li>
<li>compute <span class="math inline">\(\rho_n = \nabla_{\theta} L(x_i, \theta_{n})\)</span>
</li>
<li>update the parameter <span class="math inline">\(\theta_{n+1} = \theta_{n} - \gamma_n \rho_n.\)</span>
</li>
</ul>
<p>Note that sampling the index <span class="math inline">\(i\)</span> is equivalent to sampling an observation
from <span class="math inline">\(\hat{\mu}_N\)</span>, which in turn is the same as nonparametric bootstrapping.
Just as in the online setting, the sequence of learning rates, <span class="math inline">\(\gamma_n\)</span>,
is a tuning parameter of the algorithm.</p>
<p>We implement the basic stochastic gradient algorithm below,
allowing for a user defined decay schedule of the learning rate. However,
instead of implementing one long loop, we divide the iterations into
<em>epochs</em> with each epoch consisting of <span class="math inline">\(N\)</span> iterations. In the implementation,
the maximal number of iterations is also given in terms of epochs, and
the decay schedule is applied on a per epoch basis.</p>
<p>We also introduce a small twist on the sampling from the empirical distribution;
instead of sampling with replacement (bootstrapping) we sample without
replacement. Sampling <span class="math inline">\(N\)</span> indices from <span class="math inline">\(\{1, \ldots, N\}\)</span> without replacement
is the same as sampling a permutation of the indices.</p>
<div class="sourceCode" id="cb392"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">SG</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span>
  <span class="va">par</span>, 
  <span class="va">grad</span>,              <span class="co"># Function of parameter and observation index</span>
  <span class="va">N</span>,                 <span class="co"># Sample size</span>
  <span class="va">gamma</span>,             <span class="co"># Decay schedule or a fixed learning rate</span>
  <span class="va">maxiter</span> <span class="op">=</span> <span class="fl">100</span>,     <span class="co"># Max epoch iterations</span>
  <span class="va">sampler</span> <span class="op">=</span> <span class="va">sample</span>,  <span class="co"># How data is resampled. Default is a random permutation</span>
  <span class="va">cb</span> <span class="op">=</span> <span class="cn">NULL</span>, 
  <span class="va">...</span>
<span class="op">)</span> <span class="op">{</span>
  <span class="va">gamma</span> <span class="op">&lt;-</span> <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/is.function.html">is.function</a></span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/Special.html">gamma</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">maxiter</span><span class="op">)</span> <span class="kw">else</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">gamma</span>, <span class="va">maxiter</span><span class="op">)</span> 
  <span class="kw">for</span><span class="op">(</span><span class="va">k</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">maxiter</span><span class="op">)</span> <span class="op">{</span>
    <span class="kw">if</span><span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html">is.null</a></span><span class="op">(</span><span class="va">cb</span><span class="op">)</span><span class="op">)</span> <span class="fu">cb</span><span class="op">(</span><span class="op">)</span>
    <span class="va">samp</span> <span class="op">&lt;-</span> <span class="fu">sampler</span><span class="op">(</span><span class="va">N</span><span class="op">)</span>   
    <span class="kw">for</span><span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span>
      <span class="va">i</span> <span class="op">&lt;-</span>  <span class="va">samp</span><span class="op">[</span><span class="va">j</span><span class="op">]</span>
      <span class="va">par</span> <span class="op">&lt;-</span> <span class="va">par</span> <span class="op">-</span> <span class="va">gamma</span><span class="op">[</span><span class="va">k</span><span class="op">]</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/pkg/numDeriv/man/grad.html">grad</a></span><span class="op">(</span><span class="va">par</span>, <span class="va">i</span>, <span class="va">...</span><span class="op">)</span>
    <span class="op">}</span>
  <span class="op">}</span>
  <span class="va">par</span>
<span class="op">}</span></code></pre></div>
<p>One epoch in the algorithm above is exactly one pass through the entire batch of
data points, but in a random order. Changing the default value of the <code>sampler</code>
argument to <code>function(N) sample(N, replace = TRUE)</code> would result in sampling with replacement,
in which case an epoch would be a pass through
<span class="math inline">\(N\)</span> data points sampled independently from the batch. Doing so, some data points would
be repeated while other data points from the
batch would be missing. Sampling with replacement will feed the stochastic gradient
algorithm with i.i.d. samples from the empirical distribution. Sampling without
replacement introduces some dependence. Curiously, sampling without
replacement has turned out to be empirically superior to sampling
with replacement, and recent theoretical
results, <span class="citation"><a href="references.html#ref-Gurbuzbalaban:2019" role="doc-biblioref">Gürbüzbalaban, Ozdaglar, and Parrilo</a> (<a href="references.html#ref-Gurbuzbalaban:2019" role="doc-biblioref">2019</a>)</span>, support that it leads to a faster rate of convergence.</p>
<p>We may ask if the sampling actually matters, and whether we could just leave out that
part of the algorithm? In practice, data sets may come in a “bad order,”
for instance in an unfortunate ordering according to
one or more of its variables, and cycling through the data points in such an
ordering can easily lead the algorithm astray. <em>It is therefore important to
always randomize the order of the data points somehow</em>. A minimal amount of randomization
in common use is to just do one initial random permutation, corresponding
to moving <code>samp &lt;- sampler(N)</code> outside of the outer for-loop above. This may
be enough randomization for the algorithm to work in practice, but the link
to the convergence result for the online algorithm is lost.</p>

<div class="example">
<p><span id="exm:batch-pois-SG" class="example"><strong>Example 9.4  </strong></span>As a continuation of Example <a href="StochOpt.html#exm:online-pois-SG">9.3</a> we consider the batch version
of Poisson regression. We will only use the log-likelihood gradient, and we
first simulate a small data set with <span class="math inline">\(N = 50\)</span> data points.</p>
</div>
<div class="sourceCode" id="cb393"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">50</span>
<span class="va">z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">N</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="va">N</span>, <span class="fu">mu</span><span class="op">(</span><span class="va">z</span>, <span class="va">beta_true</span><span class="op">)</span><span class="op">)</span>
<span class="va">grad_pois</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">par</span>, <span class="va">i</span><span class="op">)</span> <span class="op">(</span><span class="fu">mu</span><span class="op">(</span><span class="va">z</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="va">par</span><span class="op">)</span> <span class="op">-</span> <span class="va">y</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">z</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span></code></pre></div>
<p>Using the <code>grad_pois()</code> function above, we run the stochastic gradient
algorithm for 1000 epochs with a decay schedule that
interpolates between <span class="math inline">\(\gamma_0 = 0.02\)</span> and <span class="math inline">\(\gamma_1 = 0.001\)</span>.</p>
<div class="sourceCode" id="cb394"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pois_SG_tracer</span> <span class="op">&lt;-</span> <span class="fu">tracer</span><span class="op">(</span><span class="st">"par"</span>, N <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>
<span class="va">rate</span> <span class="op">&lt;-</span> <span class="fu">decay_scheduler</span><span class="op">(</span>gamma0 <span class="op">=</span> <span class="fl">0.02</span>, gamma1 <span class="op">=</span> <span class="fl">0.001</span>, n1 <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span> 
<span class="fu">SG</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, <span class="va">grad_pois</span>, N <span class="op">=</span> <span class="va">N</span>, gamma <span class="op">=</span> <span class="va">rate</span>, maxiter <span class="op">=</span> <span class="fl">1000</span>, cb <span class="op">=</span> <span class="va">pois_SG_tracer</span><span class="op">$</span><span class="va">tracer</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 1.905394 3.162559</code></pre>
<p>The resulting parameter estimate should be compared to the maximum-likelihood
estimate from an ordinary Poisson regression.</p>
<div class="sourceCode" id="cb396"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">beta_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coefficients</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">z</span>, family <span class="op">=</span> <span class="va">poisson</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>## beta_hat: 1.898305 3.15986</code></pre>
<p>The batch version of stochastic gradient
descent converges toward the minimizer, <span class="math inline">\((\hat{\beta}_0, \hat{\beta}_1)\)</span>,
of the empirical risk. This is contrary to
the online version that converges toward the minimizer of the
theoretical risk, which in this case is <span class="math inline">\((\beta_0, \beta_1) = (2, 3)\)</span>.
With a larger batch size, <span class="math inline">\((\hat{\beta}_0, \hat{\beta}_1)\)</span> will come closer
to <span class="math inline">\((2, 3)\)</span>. Figure <a href="StochOpt.html#fig:batch-pois-sgd-fig">9.3</a>
shows clearly how the algorithms converge toward a limit that depends on the
batch size, and for <span class="math inline">\(N = 500\)</span>, this limit is much closer to the theoretical
minimizer.</p>
<div class="sourceCode" id="cb398"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">500</span>
<span class="va">z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">N</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Poisson.html">rpois</a></span><span class="op">(</span><span class="va">N</span>, <span class="fu">mu</span><span class="op">(</span><span class="va">z</span>, <span class="va">beta_true</span><span class="op">)</span><span class="op">)</span>
<span class="va">pois_SG_tracer_2</span> <span class="op">&lt;-</span> <span class="fu">tracer</span><span class="op">(</span><span class="st">"par"</span>, N <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>
<span class="va">rate</span> <span class="op">&lt;-</span> <span class="fu">decay_scheduler</span><span class="op">(</span>gamma0 <span class="op">=</span> <span class="fl">0.02</span>, gamma1 <span class="op">=</span> <span class="fl">0.001</span>, n1 <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> 
<span class="fu">SG</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, <span class="va">grad_pois</span>, N <span class="op">=</span> <span class="va">N</span>, gamma <span class="op">=</span> <span class="va">rate</span>, cb <span class="op">=</span> <span class="va">pois_SG_tracer_2</span><span class="op">$</span><span class="va">tracer</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 1.994688 3.022137</code></pre>
<div class="sourceCode" id="cb400"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">beta_hat_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coefficients</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">z</span>, family <span class="op">=</span> <span class="va">poisson</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>## beta_hat_2: 1.98893 3.027715</code></pre>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:batch-pois-sgd-fig"></span>
<img src="CSwR_files/figure-html/batch-pois-sgd-fig-1.png" alt="Estimated parameter values for the two parameters \(\beta_0\) (true value \(2\)) and \(\beta_1\) (true value \(3\)) in the Poisson regression model as a function of the number of iterations in the stochastic gradient algorithm. For batch size \(N = 50\), the algorithm converges to a parameter clearly different from the theoretically optimal one (gray dashed lines), while for batch size \(N = 500\) the limit is closer to \((2, 3).\)" width="100%"><p class="caption">
Figure 9.3: Estimated parameter values for the two parameters <span class="math inline">\(\beta_0\)</span> (true value <span class="math inline">\(2\)</span>) and <span class="math inline">\(\beta_1\)</span> (true value <span class="math inline">\(3\)</span>) in the Poisson regression model as a function of the number of iterations in the stochastic gradient algorithm. For batch size <span class="math inline">\(N = 50\)</span>, the algorithm converges to a parameter clearly different from the theoretically optimal one (gray dashed lines), while for batch size <span class="math inline">\(N = 500\)</span> the limit is closer to <span class="math inline">\((2, 3).\)</span>
</p>
</div>
<p>Example <a href="StochOpt.html#exm:batch-pois-SG">9.4</a> and Figure <a href="StochOpt.html#fig:batch-pois-sgd-fig">9.3</a>, in
particular, illustrate that if the data set is relatively small, the algorithm quickly
attains a precision smaller than the statistical uncertainty, and further
optimization is therefore futile. However, for larger data sets, optimization
to a greater precision can be beneficial.</p>
</div>
<div id="news" class="section level3" number="9.1.4">
<h3>
<span class="header-section-number">9.1.4</span> Predicting news article sharing on social media<a class="anchor" aria-label="anchor" href="#news"><i class="fas fa-link"></i></a>
</h3>
<p>In this section we will illustrate the use of the basic stochastic gradient
algorithm for learning a model that predicts how
many times a news article is shared on social media. The data will be subjected to
transformations and normalizations to make the use of a linear model and
the squared error loss reasonable.</p>
<p>The basic stochastic gradient algorithm for the linear model was introduced
to the early machine learning community in 1960 via ADALINE (Adaptive Linear Neuron)
by Bernard Widrow and Ted Hoff. ADALINE was
<a href="https://www.youtube.com/watch?v=hc2Zj55j1zU">implemented as a physical device</a>
capable of learning patterns
via stochastic gradient updates. The math is the same today, though the implementation
has fortunately become somewhat easier.</p>
<p>With a linear model and the squared error loss,
<span class="math inline">\(L((y, x), \beta) = \frac{1}{2} (y - \beta^T x)^2\)</span>,
the gradient becomes
<span class="math display">\[\nabla_{\beta} L((y, x), \beta) = - x (y - \beta^T x) = x (\beta^T x - y),\]</span>
which results in updates of the form
<span class="math display">\[\beta_{n+1} = \beta_n - \gamma_n x_i (\beta_n^T x_i - y_i).\]</span>
That is, the parameter moves in the direction of <span class="math inline">\(x_i\)</span> if <span class="math inline">\(\beta_n^T x_i &lt; y_i\)</span>
and in the direction of <span class="math inline">\(-x_i\)</span> if <span class="math inline">\(\beta_n^T x_i &gt; y_i\)</span>. The amount by which
it moves is controlled partly by the learning rate, <span class="math inline">\(\gamma_n\)</span>, and partly by
the size of the residual, <span class="math inline">\(y_i - \beta_n^T x_i\)</span>. A larger residual gives
a larger move.</p>
<p>The following function factory for linear models takes a model
matrix and a response vector for the complete data batch as arguments and implements the squared error
loss function on the entire batch as well as the gradient in a single
observation. The returned list also contains a parameter vector of
the correct dimension, which can be used for initialization of optimization
algorithms.</p>
<div class="sourceCode" id="cb402"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ls_model</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>
  <span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/unname.html">unname</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="co"># Strips X of names</span>
  <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>
    <span class="co"># Initial parameter value</span>
    par0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">)</span>,
    <span class="co"># Objective function</span>
    H <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">beta</span><span class="op">)</span> 
      <span class="fu"><a href="https://rdrr.io/r/base/drop.html">drop</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/matrix-products.html">crossprod</a></span><span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="va">X</span> <span class="op">%*%</span> <span class="va">beta</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">N</span><span class="op">)</span>,
    <span class="co"># Gradient in a single observation</span>
    grad <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">beta</span>, <span class="va">i</span><span class="op">)</span> <span class="op">{</span>  
      <span class="va">xi</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span><span class="va">i</span>, <span class="op">]</span>
      <span class="va">xi</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/drop.html">drop</a></span><span class="op">(</span><span class="va">xi</span> <span class="op">%*%</span> <span class="va">beta</span> <span class="op">-</span> <span class="va">y</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span>
    <span class="op">}</span>
  <span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>The data, originally collected by <span class="citation"><a href="references.html#ref-Fernandes:2015" role="doc-biblioref">Fernandes, Vinagre, and Cortez</a> (<a href="references.html#ref-Fernandes:2015" role="doc-biblioref">2015</a>)</span>, was obtained from the
<a href="https://archive.ics.uci.edu/ml/datasets/online+news+popularity">UCI Machine Learning Repository</a>
and contains 39,644 observations on 61 variables. One variable is the integer
valued <code>shares</code>, which will be the target variable of our predictions.</p>
<div class="sourceCode" id="cb403"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">News</span> <span class="op">&lt;-</span> <span class="fu">readr</span><span class="fu">::</span><span class="fu"><a href="https://readr.tidyverse.org/reference/read_delim.html">read_csv</a></span><span class="op">(</span><span class="st">"data/OnlineNewsPopularity.csv"</span><span class="op">)</span></code></pre></div>
<p>Two of the variables, <code>timedelta</code> and <code>url</code>, are not relevant predictors, and
<code>is_weekend</code> is redundant given the other weekday variables, so we exclude
those three variables. Some of the predictors are also highly correlated,
and we exclude four additional predictors before the model matrix is constructed.
The <code>shares</code> target variable is, in addition, log-transformed.</p>
<div class="sourceCode" id="cb404"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">News</span> <span class="op">&lt;-</span> <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span>
  <span class="va">News</span>,
  <span class="op">-</span> <span class="va">url</span>, 
  <span class="op">-</span> <span class="va">timedelta</span>,
  <span class="op">-</span> <span class="va">is_weekend</span>,
  <span class="op">-</span> <span class="va">n_non_stop_words</span>,
  <span class="op">-</span> <span class="va">n_non_stop_unique_tokens</span>,
  <span class="op">-</span> <span class="va">self_reference_max_shares</span>,
  <span class="op">-</span> <span class="va">kw_min_max</span>
<span class="op">)</span>
<span class="co"># The model matrix without an explicit intercept is constructed using </span>
<span class="co"># all variables reamining in the data set but the target variable 'shares' </span>
<span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">shares</span> <span class="op">~</span> <span class="va">.</span> <span class="op">-</span> <span class="fl">1</span>, data <span class="op">=</span> <span class="va">News</span><span class="op">)</span>  
<span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">News</span><span class="op">$</span><span class="va">shares</span><span class="op">)</span></code></pre></div>
<p>This data set is by current standards not a large data set – the dense model
matrix takes only about 20 MB of memory – and the
linear model can easily be fitted by simply solving the least squares problem.
It takes about 0.2 seconds on a standard laptop to compute the solution.</p>
<div class="sourceCode" id="cb405"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lm_News</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lmfit.html">lm.fit</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">y</span><span class="op">)</span></code></pre></div>
<p>The residual plot in Figure <a href="StochOpt.html#fig:fig-res-lm-News">9.4</a> shows that the
model is actually not a poor fit to data, though there is a considerable
unexplained residual variance.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:fig-res-lm-News"></span>
<img src="CSwR_files/figure-html/fig-res-lm-News-1.png" alt="Residual plot for the linear model of the logarithm of news article shares." width="70%"><p class="caption">
Figure 9.4: Residual plot for the linear model of the logarithm of news article shares.
</p>
</div>
<p>Optimization of the squared error loss using this data set will be used to
illustrate a number of stochastic gradient algorithms even though we can
compute the optimizer easily by other means. The real practical benefit of
stochastic gradient algorithms comes when applied to large scale problems that are difficult to
treat as textbook examples. And using a toy problem makes it easier to
understand in detail how the different algorithms behave.</p>
<p>The very first thing we will do is to standardize all the columns of <span class="math inline">\(X\)</span> to have
the same norm. Specifically in this case to have non-central second moment 1.
This does not change the optimization problem, but corresponds to a
reparametrization where all parameters are rescaled. The rescaling brings
the parameters on a comparable scale, which is typically a good idea for
optimization algorithms based on gradients only. After rescaling we
initialize the linear model with a call to <code>ls_model()</code> and refit the
model using the new parametrization.</p>
<div class="sourceCode" id="cb406"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">X_raw</span> <span class="op">&lt;-</span> <span class="va">X</span>
<span class="co"># Standardization</span>
<span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="va">X</span>, center <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>
<span class="co"># The '%&lt;-%' destructure assignment operator is from the zeallot package</span>
<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">par0</span>, <span class="va">H</span>, <span class="va">grad_obs</span><span class="op">)</span> <span class="op">%&lt;-%</span> <span class="fu">ls_model</span><span class="op">(</span><span class="va">X</span>, <span class="va">y</span><span class="op">)</span>

<span class="va">lm_News</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lmfit.html">lm.fit</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">y</span><span class="op">)</span>
<span class="va">par_hat</span> <span class="op">&lt;-</span> <span class="va">lm_News</span><span class="op">$</span><span class="va">coefficients</span>  <span class="co"># Will be used below for comparisons</span></code></pre></div>
<p>We first run the stochastic gradient algorithm with a fixed learning rate of
<span class="math inline">\(\gamma = 10^{-5}\)</span> for 50 epochs with a tracer that computes and
stores the value of the objective function at each epoch.</p>
<div class="sourceCode" id="cb407"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">SG_tracer</span> <span class="op">&lt;-</span> <span class="fu">tracer</span><span class="op">(</span><span class="st">"value"</span>, expr <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/substitute.html">quote</a></span><span class="op">(</span><span class="va">value</span> <span class="op">&lt;-</span> <span class="fu">H</span><span class="op">(</span><span class="va">par</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="fu">SG</span><span class="op">(</span><span class="va">par0</span>, grad <span class="op">=</span> <span class="va">grad_obs</span>, N <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, gamma <span class="op">=</span> <span class="fl">1e-5</span>, maxiter <span class="op">=</span> <span class="fl">50</span>, cb <span class="op">=</span> <span class="va">SG_tracer</span><span class="op">$</span><span class="va">tracer</span><span class="op">)</span></code></pre></div>
<p>Using the trace from the last epochs, we can compare the objective function
values to the minimum found using <code><a href="https://rdrr.io/r/stats/lmfit.html">lm.fit()</a></code> above. The minimum was
not reached completely after the 50 epochs that took a little more than 17
seconds.</p>
<div class="sourceCode" id="cb408"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">SG_trace_low</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">SG_tracer</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">tail</a></span><span class="op">(</span><span class="va">SG_trace_low</span><span class="op">)</span></code></pre></div>
<pre><code>##        value    .time
## 45 0.4268483 17.30030
## 46 0.4262983 17.68812
## 47 0.4259705 18.01967
## 48 0.4251088 18.37306
## 49 0.4245424 18.72601
## 50 0.4240064 19.06375</code></pre>
<div class="sourceCode" id="cb410"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">H</span><span class="op">(</span><span class="va">par_hat</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 0.3781595</code></pre>
<p>We will use profiling to investigate what most of the 17 seconds were spent
on in the stochastic gradient algorithm.</p>
<div class="sourceCode" id="cb412"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/profvis/man/profvis.html">profvis</a></span><span class="op">(</span><span class="fu">SG</span><span class="op">(</span><span class="va">par0</span>, <span class="va">grad_obs</span>, N <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, gamma <span class="op">=</span> <span class="fl">1e-5</span>, maxiter <span class="op">=</span> <span class="fl">50</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="figures/SG_profile.png" width="90%" style="display: block; margin: auto;"></div>
<p>The <a href="figures/SG_profile.html">profiling result</a> shows, unsurprisingly, that the
computation of the gradient and the update of the parameter vector takes up most
of the run time. But if we look closer at the implementation of the gradient, we see
that the innocuously looking subsetting <code>X[i, ]</code> to the <span class="math inline">\(i\)</span>-th row is actually
responsible for about half of the run time. We also see a substantial
allocation and deallocation of memory associated with this line. It is a
bottleneck of the R implementation that slicing out a row from a bigger
matrix cannot be done without creating a copy of that row, and this is why
this particular line takes up so much time.</p>
<p>To further investigate the convergence of the basic stochastic gradient algorithm
we run it with a larger
learning rate of <span class="math inline">\(\gamma = 5 \times 10^{-5}\)</span> and then with a power law decay
schedule, which interpolates from an initial learning rate of <span class="math inline">\(10^{-3}\)</span> to
a learning rate of <span class="math inline">\(10^{-5}\)</span> after 50 epochs.</p>
<div class="sourceCode" id="cb413"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">SG_tracer</span><span class="op">$</span><span class="fu">clear</span><span class="op">(</span><span class="op">)</span>
<span class="fu">SG</span><span class="op">(</span><span class="va">par0</span>, <span class="va">grad_obs</span>, N <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, gamma <span class="op">=</span> <span class="fl">5e-5</span>, maxiter<span class="op">=</span> <span class="fl">50</span>, cb <span class="op">=</span> <span class="va">SG_tracer</span><span class="op">$</span><span class="va">tracer</span><span class="op">)</span>
<span class="va">SG_trace_high</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">SG_tracer</span><span class="op">)</span>
<span class="va">SG_tracer</span><span class="op">$</span><span class="fu">clear</span><span class="op">(</span><span class="op">)</span>
<span class="va">rate</span> <span class="op">&lt;-</span> <span class="fu">decay_scheduler</span><span class="op">(</span>gamma0 <span class="op">=</span> <span class="fl">1e-3</span>, gamma1 <span class="op">=</span> <span class="fl">1e-5</span>, a <span class="op">=</span> <span class="fl">0.6</span>, n1 <span class="op">=</span> <span class="fl">50</span><span class="op">)</span>
<span class="fu">SG</span><span class="op">(</span><span class="va">par0</span>, <span class="va">grad_obs</span>, N <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, gamma <span class="op">=</span> <span class="va">rate</span>, maxiter<span class="op">=</span> <span class="fl">50</span>, cb <span class="op">=</span> <span class="va">SG_tracer</span><span class="op">$</span><span class="va">tracer</span><span class="op">)</span>
<span class="va">SG_trace_decay</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">SG_tracer</span><span class="op">)</span></code></pre></div>
<p>We will compare the convergence of the three stochastic gradient algorithms with
the convergence of gradient descent with backtracking. For gradient descent we choose
<span class="math inline">\(\gamma = 8 \times 10^{-2}\)</span>, which results in only a few initial backtracking
steps and then all subsequent steps will use the step length
<span class="math inline">\(\gamma = 8 \times 10^{-2}\)</span>. Choosing a larger <span class="math inline">\(\gamma\)</span> for this particular
optimization resulted in backtracking until a step length around <span class="math inline">\(8 \times 10^{-2}\)</span> was
reached, thus this choice of <span class="math inline">\(\gamma\)</span> will use a minimal amount of time on
the backtracking step of the algorithm.</p>
<div class="sourceCode" id="cb414"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">grad_H</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">beta</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/matrix-products.html">crossprod</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">X</span> <span class="op">%*%</span> <span class="va">beta</span> <span class="op">-</span> <span class="va">y</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>
<span class="va">GD_tracer</span> <span class="op">&lt;-</span> <span class="fu">tracer</span><span class="op">(</span><span class="st">"value"</span>, N <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>
<span class="fu">GD</span><span class="op">(</span><span class="va">par0</span>, <span class="va">H</span>, <span class="va">grad_H</span>, gamma <span class="op">=</span> <span class="fl">8e-2</span>, maxiter <span class="op">=</span> <span class="fl">800</span>, cb <span class="op">=</span> <span class="va">GD_tracer</span><span class="op">$</span><span class="va">tracer</span><span class="op">)</span>
<span class="va">GD_trace</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">GD_tracer</span><span class="op">)</span></code></pre></div>
<p>Figure <a href="StochOpt.html#fig:news-trace-plot">9.5</a> shows how the four algorithms converge.
The stochastic gradient algorithm with the high learning rate
<span class="math inline">\(\gamma = 5 \times 10^{-5}\)</span> decays about as fast as the gradient descent
algorithm, and both are somewhat faster than stochastic gradient with
the low learning rate <span class="math inline">\(\gamma = 10^{-5}\)</span>. Using the power law decay schedule
is initially a little faster than the high learning
rate, but eventually the decay becomes slower. Even though there is some theoretical guarantee
of convergence with the power law decay schedule, the choice of a
suitable learning rate or decay schedule in practice is just as much
an empirical art.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:news-trace-plot"></span>
<img src="CSwR_files/figure-html/news-trace-plot-1.png" alt="Convergence of squared error loss on the news article data set for four algorithms: gradient descent (gd) and three basic stochastic gradient algorithms with a low learning rate, a high learning rate and a power law decay schedule." width="100%"><p class="caption">
Figure 9.5: Convergence of squared error loss on the news article data set for four algorithms: gradient descent (gd) and three basic stochastic gradient algorithms with a low learning rate, a high learning rate and a power law decay schedule.
</p>
</div>
<p>Comparisons of optimization algorithms should generally be done by
monitoring their convergence as a function of real time and not iterations,
but the comparisons in Figure <a href="StochOpt.html#fig:news-trace-plot">9.5</a> are arguably not
entirely fair to the stochastic gradient algorithm.
The figure indicates that stochastic gradient and gradient descent
are about equally fast, but when comparing algorithms in terms of
real time we are admittedly comparing
their specific implementations, and the R implementation of the
stochastic gradient algorithm has some shortcomings. One
epoch of the stochastic gradient algorithm should be about as computationally
demanding as one iteration of gradient descent as both will
compute the gradient in each data point exactly once and add them up.
The vectorized batch gradient computation is fairly efficient in R, but the iterative
looping over data in the stochastic gradient algorithm is not, and this
inefficiency is compounded by the inefficiency of matrix slicing
that results in data copying as the profiling revealed.
One way to circumvent these shortcomings of the implementation
is to rewrite the inner loop of an epoch using Rcpp.</p>
<p>In the subsequent section we will see alternatives to the
basic stochastic gradient algorithm, which will diminish the shortcomings
of a pure R implementation somewhat. Thus we will not pursue the Rcpp implementation
here but return to it in Section <a href="StochOpt.html#SG-Rcpp">9.3</a>.</p>
</div>
</div>
<div id="beyond-basic-stochastic-gradient-algorithms" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Beyond basic stochastic gradient algorithms<a class="anchor" aria-label="anchor" href="#beyond-basic-stochastic-gradient-algorithms"><i class="fas fa-link"></i></a>
</h2>
<p>The gradient <span class="math inline">\(\nabla_{\theta} L(x_i, \theta)\)</span> for a single random data point is quickly
computed, and though unbiased as an estimate of <span class="math inline">\(\nabla_{\theta} H_N(\theta)\)</span>
it has a large variance.
This affects the basic stochastic gradient algorithm negatively as the directions
suggested can oscillate quite wildly from iteration to iteration. This section
covers some techniques that yield a better tradeoff between
run time and variability.</p>
<p>The most obvious technique is to use more than one observation per computation
of the gradient, which gives us <em>mini-batch</em> stochastic gradient algorithms.
A second technique is to incorporate some memory about previous directions into
the movements of the algorithms — in the same spirit as how the conjugate gradient
algorithm uses the previous gradient to modify the descent direction.</p>
<p>The literature on deep learning has recently exploded with variations
on the stochastic gradient algorithm. Performance is mostly
studied empirically and applied in practice to the highly non-convex
optimization problem of learning a neural network. A comprehensive coverage
of all the different ideas will not be attempted, and only three of the most
solidified variations will be treated. The use of mini-batches is ubiquitous,
and momentum will be introduced to illustrate a variation with memory. Finally,
the Adam algorithm uses memory in combination with adaptive learning rates
to achieve both speed and robustness.</p>
<div id="mini-batches" class="section level3" number="9.2.1">
<h3>
<span class="header-section-number">9.2.1</span> Mini-batches<a class="anchor" aria-label="anchor" href="#mini-batches"><i class="fas fa-link"></i></a>
</h3>
<p>The three steps of the mini-batch stochastic gradient algorithm
with mini-batch size <span class="math inline">\(m\)</span> are: given <span class="math inline">\(\theta_{n}\)</span></p>
<ul>
<li>sample <span class="math inline">\(m\)</span> indices, <span class="math inline">\(I_n = \{i_1, \ldots, i_m\}\)</span>, from <span class="math inline">\(1, \ldots, N\)</span>
</li>
<li>compute <span class="math inline">\(\rho_n = \frac{1}{m} \sum_{i\in I_n} \nabla_{\theta} L(x_i, \theta_{n})\)</span>
</li>
<li>update the parameter <span class="math inline">\(\theta_{n+1} = \theta_{n} - \gamma_n \rho_n.\)</span>
</li>
</ul>
<p>Of course, the mini-batch algorithm with <span class="math inline">\(m = 1\)</span> is the basic stochastic gradient
algorithm. As for the basic algorithm, we implement the variation of the sampling,
where we sample a <em>partition</em>
<span class="math display">\[I_1 \cup \ldots \cup I_M \subseteq \{1, \ldots, N\}\]</span>
for <span class="math inline">\(M = \lfloor N / m \rfloor\)</span> and in one epoch loop through the mini-batches
<span class="math inline">\(I_1, \ldots, I_M\)</span>.</p>
<p>In the following sections we will develop a couple of modifications to the basic
stochastic gradient algorithm, and we will therefore implement a more generic
version of the algorithm. What is common to all the modifications is that they
differ in the details of the epoch loop, thus we take out that loop as a separate
function.</p>
<div class="sourceCode" id="cb415"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">SG</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span>
  <span class="va">par</span>, 
  <span class="va">N</span>,                 <span class="co"># Sample size</span>
  <span class="va">gamma</span>,             <span class="co"># Decay schedule or a fixed learning rate</span>
  <span class="va">epoch</span> <span class="op">=</span> <span class="va">batch</span>,     <span class="co"># Epoch update function</span>
  <span class="va">...</span>,               <span class="co"># Other arguments passed to epoch updates</span>
  <span class="va">maxiter</span> <span class="op">=</span> <span class="fl">100</span>,     <span class="co"># Max epoch iterations</span>
  <span class="va">sampler</span> <span class="op">=</span> <span class="va">sample</span>,  <span class="co"># How data is resampled. Default is a random permutation</span>
  <span class="va">cb</span> <span class="op">=</span> <span class="cn">NULL</span>
<span class="op">)</span> <span class="op">{</span>
  <span class="va">gamma</span> <span class="op">&lt;-</span> <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/is.function.html">is.function</a></span><span class="op">(</span><span class="va">gamma</span><span class="op">)</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/Special.html">gamma</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">maxiter</span><span class="op">)</span> <span class="kw">else</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">gamma</span>, <span class="va">maxiter</span><span class="op">)</span> 
  <span class="kw">for</span><span class="op">(</span><span class="va">k</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">maxiter</span><span class="op">)</span> <span class="op">{</span>
    <span class="kw">if</span><span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html">is.null</a></span><span class="op">(</span><span class="va">cb</span><span class="op">)</span><span class="op">)</span> <span class="fu">cb</span><span class="op">(</span><span class="op">)</span>
    <span class="va">samp</span> <span class="op">&lt;-</span> <span class="fu">sampler</span><span class="op">(</span><span class="va">N</span><span class="op">)</span>
    <span class="va">par</span> <span class="op">&lt;-</span> <span class="fu">epoch</span><span class="op">(</span><span class="va">par</span>, <span class="va">samp</span>, <span class="va">gamma</span><span class="op">[</span><span class="va">k</span><span class="op">]</span>, <span class="va">...</span><span class="op">)</span>
  <span class="op">}</span>
  <span class="va">par</span>
<span class="op">}</span></code></pre></div>
<p>The implementation uses <code>batch()</code> as the default update function,
and we implement this function below. It uses the
random permutation to generate the <span class="math inline">\(M\)</span> mini-batches, and looping through the
mini-batches it calls <code><a href="https://rdrr.io/pkg/numDeriv/man/grad.html">grad()</a></code> for the computation of the average
gradient for each mini-batch and then updates the parameter accordingly.
Note that the <code>grad</code> argument to <code>batch()</code> will be captured by and passed on from
a call of <code>SG()</code> via the <code>...</code> argument.</p>
<div class="sourceCode" id="cb416"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">batch</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span>
    <span class="va">par</span>, 
    <span class="va">samp</span>,
    <span class="va">gamma</span>,  
    <span class="va">grad</span>,              <span class="co"># Function of parameter and observation index</span>
    <span class="va">m</span> <span class="op">=</span> <span class="fl">50</span>,            <span class="co"># Mini-batch size</span>
    <span class="va">...</span>
  <span class="op">)</span> <span class="op">{</span>
    <span class="va">M</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">floor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">samp</span><span class="op">)</span> <span class="op">/</span> <span class="va">m</span><span class="op">)</span>
    <span class="kw">for</span><span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="va">M</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
      <span class="va">i</span> <span class="op">&lt;-</span> <span class="va">samp</span><span class="op">[</span><span class="op">(</span><span class="va">j</span> <span class="op">*</span> <span class="va">m</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="op">(</span><span class="va">j</span> <span class="op">*</span> <span class="va">m</span> <span class="op">+</span> <span class="va">m</span><span class="op">)</span><span class="op">]</span>
      <span class="va">par</span> <span class="op">&lt;-</span> <span class="va">par</span> <span class="op">-</span> <span class="va">gamma</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/pkg/numDeriv/man/grad.html">grad</a></span><span class="op">(</span><span class="va">par</span>, <span class="va">i</span>, <span class="va">...</span><span class="op">)</span>
    <span class="op">}</span>
    <span class="va">par</span>
<span class="op">}</span></code></pre></div>
<p>The <code><a href="https://rdrr.io/pkg/numDeriv/man/grad.html">grad()</a></code> function implemented in <code>ls_model()</code> above assumes that the
index argument <span class="math inline">\(i\)</span> is a single number and not a vector. As it is implemented,
it computes for a vector <span class="math inline">\(i\)</span> a matrix containing the
different gradients as columns. We therefore reimplement
<code>ls_model()</code> so that <code><a href="https://rdrr.io/pkg/numDeriv/man/grad.html">grad()</a></code> computes the average of the gradients
as it is supposed to for a mini-batch.</p>
<div class="sourceCode" id="cb417"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ls_model</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">N</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>
  <span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/unname.html">unname</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>
  <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>
    par0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">)</span>,
    <span class="co"># Objective function</span>
    H <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">beta</span><span class="op">)</span>
      <span class="fu"><a href="https://rdrr.io/r/base/drop.html">drop</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/matrix-products.html">crossprod</a></span><span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="va">X</span> <span class="op">%*%</span> <span class="va">beta</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">N</span><span class="op">)</span>,
    <span class="co"># Gradient </span>
    grad <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">beta</span>, <span class="va">i</span><span class="op">)</span> <span class="op">{</span>               
      <span class="va">xi</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span><span class="va">i</span>, , drop <span class="op">=</span> <span class="cn">FALSE</span><span class="op">]</span>
      <span class="fu"><a href="https://rdrr.io/r/base/drop.html">drop</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/matrix-products.html">crossprod</a></span><span class="op">(</span><span class="va">xi</span>, <span class="va">xi</span> <span class="op">%*%</span> <span class="va">beta</span> <span class="op">-</span> <span class="va">y</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span>
    <span class="op">}</span>
  <span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>We initialize the linear model using the new implementation of <code>ls_model()</code>.</p>
<div class="sourceCode" id="cb418"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">par0</span>, <span class="va">H</span>, <span class="va">grad_obs</span><span class="op">)</span> <span class="op">%&lt;-%</span> <span class="fu">ls_model</span><span class="op">(</span><span class="va">X</span>, <span class="va">y</span><span class="op">)</span></code></pre></div>
<p>With increased flexibility of the algorithms comes more tuning parameters,
and making a good choice of all of them becomes increasingly difficult. When
introducing mini-batches we need to choose the mini-batch size in
addition to the learning rate, and a good choice of learning rate or decay schedule
will depend on the size of the mini-batch. To simplify matters, a mini-batch
size of 1000 and a fixed learning rate are used in the subsequent applications.
The learning rate will generally be chosen as large as
possible without making the algorithms diverge, and with a mini-batch size of 1000
it is possible to run the algorithm with a learning rate of <span class="math inline">\(\gamma = 5 \times 10^{-2}\)</span>.</p>
<div class="sourceCode" id="cb419"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">SG_tracer</span><span class="op">$</span><span class="fu">clear</span><span class="op">(</span><span class="op">)</span>
<span class="fu">SG</span><span class="op">(</span>
  <span class="va">par0</span>, N <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, gamma <span class="op">=</span> <span class="fl">5e-2</span>, grad <span class="op">=</span> <span class="va">grad_obs</span>, 
  m <span class="op">=</span> <span class="fl">1000</span>, maxiter <span class="op">=</span> <span class="fl">200</span>, cb <span class="op">=</span> <span class="va">SG_tracer</span><span class="op">$</span><span class="va">tracer</span>
<span class="op">)</span>
<span class="va">SG_trace_mini</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">SG_tracer</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:news-trace-plot-2"></span>
<img src="CSwR_files/figure-html/news-trace-plot-2-1.png" alt="Convergence of squared error loss on the news article data set for three algorithms: gradient descent (gd), basic stochastic gradient with a power law decay schedule (decay), and mini-batch stochastic gradient with a batch size of 1000 and a fixed learning rate (mini)." width="100%"><p class="caption">
Figure 9.6: Convergence of squared error loss on the news article data set for three algorithms: gradient descent (gd), basic stochastic gradient with a power law decay schedule (decay), and mini-batch stochastic gradient with a batch size of 1000 and a fixed learning rate (mini).
</p>
</div>
<p>Figure <a href="StochOpt.html#fig:news-trace-plot-2">9.6</a> shows that this implementation of
the mini-batch stochastic gradient
algorithm converges faster than the basic stochastic gradient algorithm with
a power law decay schedule
as well as the gradient descent algorithm. Eventually, it begins to
fluctuate due to the fixed learning rate, but it quickly gets close to
the minimum.</p>
</div>
<div id="momentum" class="section level3" number="9.2.2">
<h3>
<span class="header-section-number">9.2.2</span> Momentum<a class="anchor" aria-label="anchor" href="#momentum"><i class="fas fa-link"></i></a>
</h3>
<p>Mini-batches stabilize the gradients, and so does momentum. Both techniques
can be used in combination, and the momentum update of a
mini-batch stochastic gradient algorithm is as follows: Given <span class="math inline">\(\theta_{n}\)</span>
and a batch <span class="math inline">\(I_n \subseteq \{1, \ldots, N\}\)</span> with <span class="math inline">\(|I_n| = m\)</span></p>
<ul>
<li>compute <span class="math inline">\(g_n = \frac{1}{m} \sum_{i\in I_n} \nabla_{\theta} L(x_i, \theta_{n})\)</span>
</li>
<li>compute <span class="math inline">\(\rho_n = \beta \rho_{n-1} + (1-\beta) g_n\)</span>
</li>
<li>update the parameter <span class="math inline">\(\theta_{n+1} = \theta_{n} - \gamma_n \rho_n.\)</span>
</li>
</ul>
<p>The memory of the algorithm is in the second step, where the direction, <span class="math inline">\(\rho_{n}\)</span>, is
updated using a convex combination of the previous direction, <span class="math inline">\(\rho_{n-1}\)</span>,
and the mini-batch gradient, <span class="math inline">\(g_n\)</span>. Usually, the initial direction is chosen
as <span class="math inline">\(\rho_0 = 0\)</span>. The parameter <span class="math inline">\(\beta \in [0,1)\)</span> is a tuning parameter determining
how long the memory is. A value like <span class="math inline">\(\beta = 0.9\)</span> or <span class="math inline">\(\beta = 0.95\)</span> is
often recommended – otherwise the memory in the algorithm will be rather short,
and the effect of using momentum will be small. A choice of <span class="math inline">\(\beta = 0\)</span>
corresponds to the mini-batch algorithm without memory.</p>
<p>Contrary to the batch epoch function, the momentum epoch function
needs to store the previous direction between updates. It is not immediately
clear how to achieve this between two epochs using the generic <code>SG()</code>
implementation, but by implementing momentum epochs using a function factory,
we can easily use an enclosing environment of the epoch function
for storage.</p>
<div class="sourceCode" id="cb420"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">momentum</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">rho</span> <span class="op">&lt;-</span> <span class="fl">0</span>
  <span class="kw">function</span><span class="op">(</span>
    <span class="va">par</span>, 
    <span class="va">samp</span>,
    <span class="va">gamma</span>,  
    <span class="va">grad</span>,
    <span class="va">m</span> <span class="op">=</span> <span class="fl">50</span>,             <span class="co"># Mini-batch size</span>
    <span class="va">beta</span> <span class="op">=</span> <span class="fl">0.95</span>,        <span class="co"># Momentum memory </span>
    <span class="va">...</span>
  <span class="op">)</span> <span class="op">{</span>
    <span class="va">M</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">floor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">samp</span><span class="op">)</span> <span class="op">/</span> <span class="va">m</span><span class="op">)</span>
    <span class="kw">for</span><span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="va">M</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
      <span class="va">i</span> <span class="op">&lt;-</span> <span class="va">samp</span><span class="op">[</span><span class="op">(</span><span class="va">j</span> <span class="op">*</span> <span class="va">m</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="op">(</span><span class="va">j</span> <span class="op">*</span> <span class="va">m</span> <span class="op">+</span> <span class="va">m</span><span class="op">)</span><span class="op">]</span>
      <span class="co"># Using '&lt;&lt;-' assigns the value to rho in the enclosing environment</span>
      <span class="va">rho</span> <span class="op">&lt;&lt;-</span> <span class="va">beta</span> <span class="op">*</span> <span class="va">rho</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">beta</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/pkg/numDeriv/man/grad.html">grad</a></span><span class="op">(</span><span class="va">par</span>, <span class="va">i</span>, <span class="va">...</span><span class="op">)</span>  
      <span class="va">par</span> <span class="op">&lt;-</span> <span class="va">par</span> <span class="op">-</span> <span class="va">gamma</span> <span class="op">*</span> <span class="va">rho</span>
    <span class="op">}</span>
    <span class="va">par</span>
  <span class="op">}</span>
<span class="op">}</span></code></pre></div>
<p>When calling <code>SG()</code> below with <code>epoch = momentum()</code>, the evaluation
of the function factory <code>momentum()</code> returns the momentum epoch function
with is own local environment used to store <span class="math inline">\(\rho\)</span>. With momentum, we can
increase the learning rate to <span class="math inline">\(\gamma = 7 \times 10^{-2}\)</span>.</p>
<div class="sourceCode" id="cb421"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">SG_trace_moment</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">SG_tracer</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:SG-fig-2"></span>
<img src="CSwR_files/figure-html/SG-fig-2-1.png" alt="Convergence of squared error loss on the news article data set for four algorithms: gradient descent (gd), basic stochastic gradient with a power law decay schedule (decay), and mini-batch stochastic gradient with a batch size of 1000 and a fixed learning rate either without momentum (mini) or with momentum (moment)." width="100%"><p class="caption">
Figure 9.7: Convergence of squared error loss on the news article data set for four algorithms: gradient descent (gd), basic stochastic gradient with a power law decay schedule (decay), and mini-batch stochastic gradient with a batch size of 1000 and a fixed learning rate either without momentum (mini) or with momentum (moment).
</p>
</div>
<p>Figure <a href="StochOpt.html#fig:SG-fig-2">9.7</a> shows that due to the increased learning rate of the momentum
algorithm it decays a little faster than the mini-batch algorithm without
momentum. With enough memory, momentum dampens rapid fluctuations,
which allows for a larger choice of learning rate and a speedier convergence.
However, too much memory results in low frequency oscillations and slow convergence.
The excellent article <a href="https://distill.pub/2017/momentum/">Why Momentum Really Works</a>
contains many more details about momentum algorithms and beautiful illustrations.</p>
</div>
<div id="adaptive-learning-rates" class="section level3" number="9.2.3">
<h3>
<span class="header-section-number">9.2.3</span> Adaptive learning rates<a class="anchor" aria-label="anchor" href="#adaptive-learning-rates"><i class="fas fa-link"></i></a>
</h3>
<p>One difficulty with optimization algorithms based only on gradients
is that gradients are not invariant to reparametrizations. In fact, using gradients
implicitly assumes that all parameters are on comparable scales. For our news article
example, we standardized the <span class="math inline">\(X\)</span>-matrix to achieve this, but for many other
optimization problems it is not so easy to choose a parametrization with all
parameters on comparable scales. And even when we can do so, the common scale
can change from problem to problem making it impossible to recommend a good
generic choice of a learning rate. The practical implication is that a considerable
amount of tuning is necessary, when the algorithms are applied in practice.</p>
<p>Algorithms that implement adaptive learning rates are alternatives to extensive
tuning. They include schemes for adjusting the learning rate to the specific
optimization problem. Adapting the learning rate is equivalent to scaling
the gradient adaptively, and to achieve a form of automatic standardization
of parameter scales, we will consider algorithms that adaptively scale each
coordinate of the gradient separately.</p>
<p>To gain some intuition on how to sensibly adapt the scales of the gradient,
we will analyze the typical scale of the mini-batch gradient for the linear
model. Introduce first the normalized squared column norms
<span class="math display">\[\zeta_j = \frac{1}{N} \sum_{i=1}^N x_{ij}^2 = \frac{1}{N} \| x_{\cdot j}\|^2_2\]</span>
and note that with a standardized <span class="math inline">\(X\)</span>, all the <span class="math inline">\(\zeta_j\)</span>-s are equal. With
<span class="math inline">\(\hat{\beta}\)</span> the least squares estimate we also have that
<span class="math display">\[\frac{1}{N} \sum_{i=1}^N x_{ij} (y_i - \hat{\beta}{}^Tx_i) = 0\]</span>
for <span class="math inline">\(j = 1, \ldots, p\)</span>. Thus if we sample a random index, <span class="math inline">\(\iota\)</span>, from
<span class="math inline">\(\{1, \ldots, N\}\)</span> it holds that
<span class="math inline">\(E(x_{\iota j} (y_{\iota} - \hat{\beta}{}^Tx_{\iota})) = 0\)</span>, where the expectation is w.r.t. <span class="math inline">\(\iota\)</span>.
With
<span class="math display">\[\hat{\sigma}^2 = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{\beta}{}^Tx_i)^2\]</span>
denoting the residual variance, we also have that</p>
<p><span class="math display">\[
V(x_{\iota j} (y_{\iota} - \hat{\beta}{}^Tx_{\iota})) = 
E(x_{\iota j}^2 (y_{\iota} - \hat{\beta}{}^Tx_{\iota})^2) 
\simeq \zeta_j \hat{\sigma}^2
\]</span></p>
<p>For the above approximation to hold, the squared residual, <span class="math inline">\((y_{\iota} - \hat{\beta}{}^Tx_{\iota})^2\)</span>,
must be roughly independent of <span class="math inline">\(x_{\iota}\)</span>, which is not guaranteed by the least
squares fit alone, but holds approximately if data is from a population with
homogeneous residual variance.</p>
<p>If <span class="math inline">\(I \subseteq \{1,\ldots,N\}\)</span> is a random subset of size <span class="math inline">\(m\)</span> sampled <em>with</em>
replacement, the averaged gradient
<span class="math display">\[g = - \frac{1}{m} \sum_{i \in I} x_{i} (y_i - \hat{\beta}{}^T x_i)\]</span>
is an average of <span class="math inline">\(m\)</span> i.i.d. random variables with mean 0, thus
<span class="math display">\[E(g_j^2) = V(g_j) \simeq \zeta_j \frac{\hat{\sigma}^2}{m}.\]</span>
If <span class="math inline">\(\odot\)</span> denotes the coordinate wise product of vectors (aka the Hadamard product),
this can also be written as
<span class="math display">\[E(g \odot g) \simeq  \zeta \frac{\hat{\sigma}^2}{m}.\]</span></p>
<p>The computations suggest that by estimating <span class="math inline">\(v = E(g \odot g)\)</span> for a mini-batch
gradient evaluated in <span class="math inline">\(\beta = \hat{\beta},\)</span> we are in fact estimating
<span class="math inline">\(\zeta\)</span> up to a scale factor. We extrapolate this insight to the general case
and standardize the <span class="math inline">\(j\)</span>-th coordinate of the descent direction by an
estimate of <span class="math inline">\(1 / \sqrt{v_j}\)</span>
to bring the coordinates on a (more) common scale. We will implement adaptive
estimation of <span class="math inline">\(v\)</span> using a similar update scheme as for momentum, where
the estimate in iteration <span class="math inline">\(n\)</span> is updated as a convex combination of the current
value of <span class="math inline">\(g_n \odot g_n\)</span> and the previous estimate of <span class="math inline">\(v\)</span>.</p>
<p>Given <span class="math inline">\(\theta_{n}\)</span> and a batch <span class="math inline">\(I_n \subseteq \{1, \ldots, N\}\)</span> with <span class="math inline">\(|I_n| = m\)</span>
the update consists of the following steps</p>
<ul>
<li>compute <span class="math inline">\(g_n = \frac{1}{m} \sum_{i\in I_n} \nabla_{\theta} L(x_i, \theta_{n})\)</span>
</li>
<li>compute <span class="math inline">\(\rho_n = \beta_1 \rho_{n-1} + (1-\beta_1) g_n\)</span>
</li>
<li>compute <span class="math inline">\(v_n = \beta_2 v_{n-1} + (1-\beta_2) g_n \odot g_n\)</span>
</li>
<li>update the parameter <span class="math inline">\(\theta_{n+1} = \theta_{n} - \gamma_n \rho_n / (\sqrt{v_n} + 10^{-8}).\)</span>
</li>
</ul>
<p>The vectors <span class="math inline">\(\rho_0\)</span> and <span class="math inline">\(v_0\)</span> are usually initialized to be <span class="math inline">\(0\)</span>. The tuning
parameters <span class="math inline">\(\beta_1, \beta_2 \in [0, 1)\)</span> control the memory of the first and
second moments, respectively. The <span class="math inline">\(\sqrt{v_n}\)</span> and the division in the last step
are coordinate wise. The constant <span class="math inline">\(10^{-8}\)</span> could, of course, be chosen differently,
but is just a safeguard against division-by-zero.</p>
<p>The algorithm above is known as <em>Adam</em> (adaptive moment estimation),
and was introduced and analyzed by <span class="citation"><a href="references.html#ref-kingma2014adam" role="doc-biblioref">Kingma and Ba</a> (<a href="references.html#ref-kingma2014adam" role="doc-biblioref">2014</a>)</span>. They include so-called
bias-correction steps that upscale <span class="math inline">\(\rho_n\)</span> and <span class="math inline">\(v_n\)</span> by the factors <span class="math inline">\(1 / (1 - \beta_1^n)\)</span>
and <span class="math inline">\(1 / (1 - \beta_2^n)\)</span>, respectively. These steps are not difficult
to implement but are left out in the implementation below for simplicity.
It is also possible to replace the <span class="math inline">\(\sqrt{v_n}\)</span> by other powers <span class="math inline">\(v_n^q\)</span>.
The choice of <span class="math inline">\(q = 1\)</span> instead of <span class="math inline">\(q = 1/2\)</span> makes the algorithm (more)
invariant to scale changes. Again, for simplicity we will only implement
the algorithm with <span class="math inline">\(q = 1/2\)</span>.</p>
<p>The <code>adam()</code> function below is a function factory just as <code>momentum()</code>, which
returns a function doing the Adam epoch update loop with an enclosing
environment used for storage of <code>rho</code> and <code>v</code>.</p>
<div class="sourceCode" id="cb422"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">adam</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">rho</span> <span class="op">&lt;-</span> <span class="va">v</span> <span class="op">&lt;-</span> <span class="fl">0</span>
  <span class="kw">function</span><span class="op">(</span>
    <span class="va">par</span>, 
    <span class="va">samp</span>,
    <span class="va">gamma</span>,   
    <span class="va">grad</span>,
    <span class="va">m</span> <span class="op">=</span> <span class="fl">50</span>,          <span class="co"># Mini-batch size</span>
    <span class="va">beta1</span> <span class="op">=</span> <span class="fl">0.9</span>,     <span class="co"># Momentum memory     </span>
    <span class="va">beta2</span> <span class="op">=</span> <span class="fl">0.9</span>,     <span class="co"># Second moment memory </span>
    <span class="va">...</span>
  <span class="op">)</span> <span class="op">{</span>
    <span class="va">M</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">floor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">samp</span><span class="op">)</span> <span class="op">/</span> <span class="va">m</span><span class="op">)</span>
   
    <span class="kw">for</span><span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="va">M</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
      <span class="va">i</span> <span class="op">&lt;-</span> <span class="va">samp</span><span class="op">[</span><span class="op">(</span><span class="va">j</span> <span class="op">*</span> <span class="va">m</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="op">(</span><span class="va">j</span> <span class="op">*</span> <span class="va">m</span> <span class="op">+</span> <span class="va">m</span><span class="op">)</span><span class="op">]</span>
      <span class="va">gr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/numDeriv/man/grad.html">grad</a></span><span class="op">(</span><span class="va">par</span>, <span class="va">i</span>, <span class="va">...</span><span class="op">)</span> 
      <span class="va">rho</span> <span class="op">&lt;&lt;-</span> <span class="va">beta1</span> <span class="op">*</span> <span class="va">rho</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">beta1</span><span class="op">)</span> <span class="op">*</span> <span class="va">gr</span> 
      <span class="va">v</span> <span class="op">&lt;&lt;-</span> <span class="va">beta2</span> <span class="op">*</span> <span class="va">v</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">beta2</span><span class="op">)</span> <span class="op">*</span> <span class="va">gr</span><span class="op">^</span><span class="fl">2</span> 
      <span class="va">par</span> <span class="op">&lt;-</span> <span class="va">par</span> <span class="op">-</span> <span class="va">gamma</span> <span class="op">*</span> <span class="op">(</span><span class="va">rho</span> <span class="op">/</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">v</span><span class="op">)</span> <span class="op">+</span> <span class="fl">1e-8</span><span class="op">)</span><span class="op">)</span>  
    <span class="op">}</span>
    <span class="va">par</span>
  <span class="op">}</span>
<span class="op">}</span></code></pre></div>
<p>We run the stochastic gradient algorithm with adam updates and mini-batches of
size 1000 with a fixed learning rate of <span class="math inline">\(10^{-2}\)</span> and a power law decay
schedule interpolating between a learning rate of <span class="math inline">\(0.5\)</span> and <span class="math inline">\(2 \times 10^{-3}\)</span>.
The theoretical results by <span class="citation"><a href="references.html#ref-kingma2014adam" role="doc-biblioref">Kingma and Ba</a> (<a href="references.html#ref-kingma2014adam" role="doc-biblioref">2014</a>)</span> support a decay schedule proportional
to <span class="math inline">\(1/\sqrt{n}\)</span>, thus we take <span class="math inline">\(a = 0.5\)</span> below.</p>
<div class="sourceCode" id="cb423"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">SG_tracer</span><span class="op">$</span><span class="fu">clear</span><span class="op">(</span><span class="op">)</span>
<span class="fu">SG</span><span class="op">(</span>
  <span class="va">par0</span>, N <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, gamma <span class="op">=</span> <span class="fl">1e-2</span>, epoch <span class="op">=</span> <span class="fu">adam</span><span class="op">(</span><span class="op">)</span>, grad <span class="op">=</span> <span class="va">grad_obs</span>,
  m <span class="op">=</span> <span class="fl">1000</span>, maxiter <span class="op">=</span> <span class="fl">150</span>, cb <span class="op">=</span> <span class="va">SG_tracer</span><span class="op">$</span><span class="va">tracer</span>
<span class="op">)</span>
<span class="va">SG_trace_adam</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">SG_tracer</span><span class="op">)</span>
<span class="va">rate</span> <span class="op">&lt;-</span> <span class="fu">decay_scheduler</span><span class="op">(</span>gamma0 <span class="op">=</span> <span class="fl">0.5</span>, gamma1 <span class="op">=</span> <span class="fl">2e-3</span>, a <span class="op">=</span> <span class="fl">0.5</span>, n1 <span class="op">=</span> <span class="fl">150</span><span class="op">)</span> 
<span class="va">SG_tracer</span><span class="op">$</span><span class="fu">clear</span><span class="op">(</span><span class="op">)</span>
<span class="fu">SG</span><span class="op">(</span>
  <span class="va">par0</span>, N <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, gamma <span class="op">=</span> <span class="va">rate</span>, epoch <span class="op">=</span> <span class="fu">adam</span><span class="op">(</span><span class="op">)</span>, grad <span class="op">=</span> <span class="va">grad_obs</span>,
  m <span class="op">=</span> <span class="fl">1000</span>, maxiter <span class="op">=</span> <span class="fl">150</span>, cb <span class="op">=</span> <span class="va">SG_tracer</span><span class="op">$</span><span class="va">tracer</span>
<span class="op">)</span>
<span class="va">SG_trace_adam_decay</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">SG_tracer</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:SG-fig-3"></span>
<img src="CSwR_files/figure-html/SG-fig-3-1.png" alt="Convergence of squared error loss on the news article data set for six algorithms: gradient descent (gd), basic stochastic gradient with a power law decay schedule (decay), mini-batch stochastic gradient with a batch size of 1000 and a fixed learning rate either without momentum (mini) or with momentum (moment), and the Adam algorithm either with a fixed learning rate (adam) or a power law decay schedule (adam\_decay)." width="100%"><p class="caption">
Figure 9.8: Convergence of squared error loss on the news article data set for six algorithms: gradient descent (gd), basic stochastic gradient with a power law decay schedule (decay), mini-batch stochastic gradient with a batch size of 1000 and a fixed learning rate either without momentum (mini) or with momentum (moment), and the Adam algorithm either with a fixed learning rate (adam) or a power law decay schedule (adam_decay).
</p>
</div>
<p>Figure <a href="StochOpt.html#fig:SG-fig-3">9.8</a> shows that both runs of the Adam implementation
decay faster initially than any of the other algorithms. Eventually they
both level off when the error is around <span class="math inline">\(10^{-3}\)</span> and from this point on
they fluctuate randomly. What is not shown is that Adam is also somewhat
more robust to changes of the learning rate and rescaling of the parameters.</p>
</div>
</div>
<div id="SG-Rcpp" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> Stochastic gradient algorithms with Rcpp<a class="anchor" aria-label="anchor" href="#SG-Rcpp"><i class="fas fa-link"></i></a>
</h2>
<p>As pointed out toward the end of Section <a href="StochOpt.html#news">9.1.4</a>, the implementations of
stochastic gradient algorithms in R suffer from some shortcomings. In this
section we will explore how either parts of the algorithms or entire
algorithms can be moved to C++ via Rcpp.</p>
<p>The modularity of the <code>SG()</code> implementation makes it easy to replace the
implementation of either the gradient computation or the entire epoch
loop in C++, while retaining the overall control of the algorithm and the
resampling in R. This is explored first and consists mostly of translating
the numerical linear algebra of the gradient computations into C++ code.
We can then easily test, compare and benchmark the implementations using the R
implementation as a reference.</p>
<p>In the second part of this section the entire mini-batch stochastic gradient
algorithm is translated into C++. This has a couple of notable consequences. First,
we need access to a sampler in C++ that can do the randomization. While
there are various C++ interfaces to an equivalent of <code><a href="https://rdrr.io/r/base/sample.html">sample()</a></code>, some
considerations need to go into an appropriate choice. Second, we have to
give up on tracing as otherwise implemented. Though it is possible to implement
callback of an R function from a C++ functions, a tracer will not have
the same access to the calling environment as in the R implementation.
Thus for performance assessment we will rely on benchmarking
of the entire algorithm.</p>
<p>For the C++ implementations we need to give up on some of the abstractions
that R provides, though we will benefit from Rcpp data types like
<code>NumericVector</code> and <code>NumericMatrix</code>. In a final implementation
we will use <a href="http://dirk.eddelbuettel.com/code/rcpp.armadillo.html">RcppArmadillo</a>
to regain an abstract approach to numerical linear
algebra via the C++ library <a href="http://arma.sourceforge.net/">Armadillo</a>.</p>
<div id="gradients-and-epochs-in-rcpp" class="section level3" number="9.3.1">
<h3>
<span class="header-section-number">9.3.1</span> Gradients and epochs in Rcpp<a class="anchor" aria-label="anchor" href="#gradients-and-epochs-in-rcpp"><i class="fas fa-link"></i></a>
</h3>
<div class="sourceCode" id="cb424"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb424-1"><a href="StochOpt.html#cb424-1" aria-hidden="true" tabindex="-1"></a><span class="co">// [[Rcpp::export]]</span></span>
<span id="cb424-2"><a href="StochOpt.html#cb424-2" aria-hidden="true" tabindex="-1"></a>NumericVector lin_grad(NumericVector beta, IntegerVector ii, </span>
<span id="cb424-3"><a href="StochOpt.html#cb424-3" aria-hidden="true" tabindex="-1"></a>                       NumericMatrix X, NumericVector y) {</span>
<span id="cb424-4"><a href="StochOpt.html#cb424-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> m = ii.length(), p = beta.length();</span>
<span id="cb424-5"><a href="StochOpt.html#cb424-5" aria-hidden="true" tabindex="-1"></a>  NumericVector grad(p), yhat(m);</span>
<span id="cb424-6"><a href="StochOpt.html#cb424-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">// Shift indices one down due to zero-indexing in C++</span></span>
<span id="cb424-7"><a href="StochOpt.html#cb424-7" aria-hidden="true" tabindex="-1"></a>  IntegerVector iii = clone(ii) - <span class="dv">1</span>;  </span>
<span id="cb424-8"><a href="StochOpt.html#cb424-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb424-9"><a href="StochOpt.html#cb424-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; m; ++i) {</span>
<span id="cb424-10"><a href="StochOpt.html#cb424-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(<span class="dt">int</span> j = <span class="dv">0</span>; j &lt; p; ++j) {</span>
<span id="cb424-11"><a href="StochOpt.html#cb424-11" aria-hidden="true" tabindex="-1"></a>      yhat[i] += X(iii[i], j) * beta[j];</span>
<span id="cb424-12"><a href="StochOpt.html#cb424-12" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb424-13"><a href="StochOpt.html#cb424-13" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb424-14"><a href="StochOpt.html#cb424-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; m; ++i) {</span>
<span id="cb424-15"><a href="StochOpt.html#cb424-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(<span class="dt">int</span> j = <span class="dv">0</span>; j &lt; p; ++j) {</span>
<span id="cb424-16"><a href="StochOpt.html#cb424-16" aria-hidden="true" tabindex="-1"></a>      grad[j] += X(iii[i], j) * (yhat[i]- y[iii[i]]);</span>
<span id="cb424-17"><a href="StochOpt.html#cb424-17" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb424-18"><a href="StochOpt.html#cb424-18" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb424-19"><a href="StochOpt.html#cb424-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> grad / m;</span>
<span id="cb424-20"><a href="StochOpt.html#cb424-20" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb425"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb425-1"><a href="StochOpt.html#cb425-1" aria-hidden="true" tabindex="-1"></a><span class="co">// [[Rcpp::export]]</span></span>
<span id="cb425-2"><a href="StochOpt.html#cb425-2" aria-hidden="true" tabindex="-1"></a>NumericVector lin_epoch(NumericVector par, IntegerVector ii, </span>
<span id="cb425-3"><a href="StochOpt.html#cb425-3" aria-hidden="true" tabindex="-1"></a>                        <span class="dt">double</span> gamma, NumericMatrix X, NumericVector y, <span class="dt">int</span> m = <span class="dv">50</span>) {</span>
<span id="cb425-4"><a href="StochOpt.html#cb425-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> p = par.length(), N = ii.length();</span>
<span id="cb425-5"><a href="StochOpt.html#cb425-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> M = floor(N / m);</span>
<span id="cb425-6"><a href="StochOpt.html#cb425-6" aria-hidden="true" tabindex="-1"></a>  NumericVector grad(p), yhat(N), beta = clone(par);</span>
<span id="cb425-7"><a href="StochOpt.html#cb425-7" aria-hidden="true" tabindex="-1"></a>  IntegerVector iii = clone(ii) - <span class="dv">1</span>;  </span>
<span id="cb425-8"><a href="StochOpt.html#cb425-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb425-9"><a href="StochOpt.html#cb425-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(<span class="dt">int</span> j = <span class="dv">0</span>; j &lt; M; ++j) {</span>
<span id="cb425-10"><a href="StochOpt.html#cb425-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(<span class="dt">int</span> i = j * m; i &lt; (j + <span class="dv">1</span>) * m; ++i) {</span>
<span id="cb425-11"><a href="StochOpt.html#cb425-11" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span>(<span class="dt">int</span> k = <span class="dv">0</span>; k &lt; p; ++k) {</span>
<span id="cb425-12"><a href="StochOpt.html#cb425-12" aria-hidden="true" tabindex="-1"></a>        yhat[i] += X(iii[i], k) * beta[k];</span>
<span id="cb425-13"><a href="StochOpt.html#cb425-13" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb425-14"><a href="StochOpt.html#cb425-14" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb425-15"><a href="StochOpt.html#cb425-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(<span class="dt">int</span> k = <span class="dv">0</span>; k &lt; p; ++k) {</span>
<span id="cb425-16"><a href="StochOpt.html#cb425-16" aria-hidden="true" tabindex="-1"></a>      grad[k] = <span class="dv">0</span>;</span>
<span id="cb425-17"><a href="StochOpt.html#cb425-17" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span>(<span class="dt">int</span> i = j * m; i &lt; (j + <span class="dv">1</span>) * m; ++i) {</span>
<span id="cb425-18"><a href="StochOpt.html#cb425-18" aria-hidden="true" tabindex="-1"></a>        grad[k] += X(iii[i], k) * (yhat[i] - y[iii[i]]);</span>
<span id="cb425-19"><a href="StochOpt.html#cb425-19" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb425-20"><a href="StochOpt.html#cb425-20" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb425-21"><a href="StochOpt.html#cb425-21" aria-hidden="true" tabindex="-1"></a>    beta = beta - gamma * grad / m;</span>
<span id="cb425-22"><a href="StochOpt.html#cb425-22" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb425-23"><a href="StochOpt.html#cb425-23" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> beta;</span>
<span id="cb425-24"><a href="StochOpt.html#cb425-24" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>A mini-batch run with Rcpp gradients</p>
<div class="sourceCode" id="cb426"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">SG_tracer</span><span class="op">$</span><span class="fu">clear</span><span class="op">(</span><span class="op">)</span>
<span class="fu">SG</span><span class="op">(</span>
  <span class="va">par0</span>, N <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, gamma <span class="op">=</span> <span class="fl">5e-5</span>, grad <span class="op">=</span> <span class="va">lin_grad</span>, 
  X <span class="op">=</span> <span class="va">X</span>, y <span class="op">=</span> <span class="va">y</span>, m <span class="op">=</span> <span class="fl">1</span>, maxiter <span class="op">=</span> <span class="fl">100</span>, cb <span class="op">=</span> <span class="va">SG_tracer</span><span class="op">$</span><span class="va">tracer</span>
<span class="op">)</span>
<span class="va">SG_trace_Rcpp_grad</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">SG_tracer</span><span class="op">)</span>
<span class="va">SG_tracer</span><span class="op">$</span><span class="fu">clear</span><span class="op">(</span><span class="op">)</span>
<span class="fu">SG</span><span class="op">(</span>
  <span class="va">par0</span>, N <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, gamma <span class="op">=</span> <span class="fl">5e-5</span>, epoch <span class="op">=</span> <span class="va">lin_epoch</span>,   
  X <span class="op">=</span> <span class="va">X</span>, y <span class="op">=</span> <span class="va">y</span>, m <span class="op">=</span> <span class="fl">1</span>, maxiter <span class="op">=</span> <span class="fl">200</span>, cb <span class="op">=</span> <span class="va">SG_tracer</span><span class="op">$</span><span class="va">tracer</span>
<span class="op">)</span>
<span class="va">SG_trace_Rcpp_epoch</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">SG_tracer</span><span class="op">)</span>
<span class="va">SG_tracer</span><span class="op">$</span><span class="fu">clear</span><span class="op">(</span><span class="op">)</span>
<span class="fu">SG</span><span class="op">(</span>
  <span class="va">par0</span>, N <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, gamma <span class="op">=</span> <span class="fl">5e-2</span>, epoch <span class="op">=</span> <span class="va">lin_epoch</span>,   
  X <span class="op">=</span> <span class="va">X</span>, y <span class="op">=</span> <span class="va">y</span>, m <span class="op">=</span> <span class="fl">1000</span>, maxiter <span class="op">=</span> <span class="fl">200</span>, cb <span class="op">=</span> <span class="va">SG_tracer</span><span class="op">$</span><span class="va">tracer</span>
<span class="op">)</span>
<span class="va">SG_trace_Rcpp_epoch_mini</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">SG_tracer</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:SG-fig-4"></span>
<img src="CSwR_files/figure-html/SG-fig-4-1.png" alt="Convergence of squared error loss on the news article data set for six algorithms: gradient descent (gd), basic stochastic gradient with a power law decay schedule (decay), mini-batch stochastic gradient with a batch size of 1000 and a fixed learning rate either without momentum (mini) or with momentum (moment), and the ADAM algorithm either with a fixed learning rate (adam) or a power law decay schedule (adam\_decay)." width="100%"><p class="caption">
Figure 9.9: Convergence of squared error loss on the news article data set for six algorithms: gradient descent (gd), basic stochastic gradient with a power law decay schedule (decay), mini-batch stochastic gradient with a batch size of 1000 and a fixed learning rate either without momentum (mini) or with momentum (moment), and the ADAM algorithm either with a fixed learning rate (adam) or a power law decay schedule (adam_decay).
</p>
</div>
</div>
<div id="full-rcpp-implementations" class="section level3" number="9.3.2">
<h3>
<span class="header-section-number">9.3.2</span> Full Rcpp implementations<a class="anchor" aria-label="anchor" href="#full-rcpp-implementations"><i class="fas fa-link"></i></a>
</h3>
<p>Here</p>
<div class="sourceCode" id="cb427"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb427-1"><a href="StochOpt.html#cb427-1" aria-hidden="true" tabindex="-1"></a><span class="co">// [[Rcpp::depends(dqrng)]]</span></span>
<span id="cb427-2"><a href="StochOpt.html#cb427-2" aria-hidden="true" tabindex="-1"></a><span class="co">// [[Rcpp::export]]</span></span>
<span id="cb427-3"><a href="StochOpt.html#cb427-3" aria-hidden="true" tabindex="-1"></a>NumericVector SG_Rcpp(NumericVector par, <span class="dt">int</span> N, NumericVector gamma,</span>
<span id="cb427-4"><a href="StochOpt.html#cb427-4" aria-hidden="true" tabindex="-1"></a>                      NumericMatrix X, NumericVector y,</span>
<span id="cb427-5"><a href="StochOpt.html#cb427-5" aria-hidden="true" tabindex="-1"></a>                      <span class="dt">int</span> m = <span class="dv">50</span>, <span class="dt">int</span> maxiter = <span class="dv">100</span>) {</span>
<span id="cb427-6"><a href="StochOpt.html#cb427-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> p = par.length(), M = floor(N / m);</span>
<span id="cb427-7"><a href="StochOpt.html#cb427-7" aria-hidden="true" tabindex="-1"></a>  NumericVector grad(p), yhat(N), beta = clone(par);</span>
<span id="cb427-8"><a href="StochOpt.html#cb427-8" aria-hidden="true" tabindex="-1"></a>  IntegerVector ii;</span>
<span id="cb427-9"><a href="StochOpt.html#cb427-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb427-10"><a href="StochOpt.html#cb427-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(<span class="dt">int</span> l = <span class="dv">0</span>; l &lt; maxiter; ++l) {</span>
<span id="cb427-11"><a href="StochOpt.html#cb427-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Note that dqsample_int samples from {0, 1, ..., N - 1}</span></span>
<span id="cb427-12"><a href="StochOpt.html#cb427-12" aria-hidden="true" tabindex="-1"></a>    ii = dqrng::dqsample_int(N, N); </span>
<span id="cb427-13"><a href="StochOpt.html#cb427-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(<span class="dt">int</span> j = <span class="dv">0</span>; j &lt; M; ++j) {</span>
<span id="cb427-14"><a href="StochOpt.html#cb427-14" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span>(<span class="dt">int</span> i = j * m; i &lt; (j + <span class="dv">1</span>) * m; ++i) {</span>
<span id="cb427-15"><a href="StochOpt.html#cb427-15" aria-hidden="true" tabindex="-1"></a>        yhat[i] = <span class="dv">0</span>;</span>
<span id="cb427-16"><a href="StochOpt.html#cb427-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span>(<span class="dt">int</span> k = <span class="dv">0</span>; k &lt; p; ++k) {</span>
<span id="cb427-17"><a href="StochOpt.html#cb427-17" aria-hidden="true" tabindex="-1"></a>          yhat[i] += X(ii[i], k) * beta[k];</span>
<span id="cb427-18"><a href="StochOpt.html#cb427-18" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb427-19"><a href="StochOpt.html#cb427-19" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb427-20"><a href="StochOpt.html#cb427-20" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span>(<span class="dt">int</span> k = <span class="dv">0</span>; k &lt; p; ++k) {</span>
<span id="cb427-21"><a href="StochOpt.html#cb427-21" aria-hidden="true" tabindex="-1"></a>        grad[k] = <span class="dv">0</span>;</span>
<span id="cb427-22"><a href="StochOpt.html#cb427-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span>(<span class="dt">int</span> i = j * m; i &lt; (j + <span class="dv">1</span>) * m; ++i) {</span>
<span id="cb427-23"><a href="StochOpt.html#cb427-23" aria-hidden="true" tabindex="-1"></a>          grad[k] += X(ii[i], k) * (yhat[i] - y[ii[i]]);</span>
<span id="cb427-24"><a href="StochOpt.html#cb427-24" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb427-25"><a href="StochOpt.html#cb427-25" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb427-26"><a href="StochOpt.html#cb427-26" aria-hidden="true" tabindex="-1"></a>      beta = beta - gamma[l] * grad / m;</span>
<span id="cb427-27"><a href="StochOpt.html#cb427-27" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb427-28"><a href="StochOpt.html#cb427-28" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb427-29"><a href="StochOpt.html#cb427-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> beta;</span>
<span id="cb427-30"><a href="StochOpt.html#cb427-30" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb428"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb428-1"><a href="StochOpt.html#cb428-1" aria-hidden="true" tabindex="-1"></a><span class="co">// [[Rcpp::depends(RcppArmadillo)]]</span></span>
<span id="cb428-2"><a href="StochOpt.html#cb428-2" aria-hidden="true" tabindex="-1"></a><span class="co">// [[Rcpp::export]]</span></span>
<span id="cb428-3"><a href="StochOpt.html#cb428-3" aria-hidden="true" tabindex="-1"></a>arma::colvec SG_arma(NumericVector par, <span class="dt">int</span> N, NumericVector gamma,</span>
<span id="cb428-4"><a href="StochOpt.html#cb428-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">const</span> arma::mat&amp; X, <span class="at">const</span> arma::colvec&amp; y, </span>
<span id="cb428-5"><a href="StochOpt.html#cb428-5" aria-hidden="true" tabindex="-1"></a>                      <span class="dt">int</span> m = <span class="dv">50</span>, <span class="dt">int</span> maxiter = <span class="dv">100</span>) {</span>
<span id="cb428-6"><a href="StochOpt.html#cb428-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> p = par.length(), M = floor(N / m);</span>
<span id="cb428-7"><a href="StochOpt.html#cb428-7" aria-hidden="true" tabindex="-1"></a>  arma::colvec grad(p), yhat(N), beta = clone(par);</span>
<span id="cb428-8"><a href="StochOpt.html#cb428-8" aria-hidden="true" tabindex="-1"></a>  uvec ii, iii;</span>
<span id="cb428-9"><a href="StochOpt.html#cb428-9" aria-hidden="true" tabindex="-1"></a>  gamma = gamma / m;</span>
<span id="cb428-10"><a href="StochOpt.html#cb428-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb428-11"><a href="StochOpt.html#cb428-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(<span class="dt">int</span> l = <span class="dv">0</span>; l &lt; maxiter; ++l) {</span>
<span id="cb428-12"><a href="StochOpt.html#cb428-12" aria-hidden="true" tabindex="-1"></a>    ii = as&lt;arma::uvec&gt;(dqrng::dqsample_int(N, N));</span>
<span id="cb428-13"><a href="StochOpt.html#cb428-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span>(<span class="dt">int</span> j = <span class="dv">0</span>; j &lt; M; ++j) {</span>
<span id="cb428-14"><a href="StochOpt.html#cb428-14" aria-hidden="true" tabindex="-1"></a>      iii = ii.subvec(j * m, (j + <span class="dv">1</span>) * m - <span class="dv">1</span>);</span>
<span id="cb428-15"><a href="StochOpt.html#cb428-15" aria-hidden="true" tabindex="-1"></a>      beta = beta - gamma[l] * X.rows(iii).t() * (X.rows(iii) * beta - y(iii)); </span>
<span id="cb428-16"><a href="StochOpt.html#cb428-16" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb428-17"><a href="StochOpt.html#cb428-17" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb428-18"><a href="StochOpt.html#cb428-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> beta;</span>
<span id="cb428-19"><a href="StochOpt.html#cb428-19" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>

</div>
</div>
</div>



<div class="chapter-nav">
<div class="prev"><a href="em.html"><span class="header-section-number">8</span> Expectation maximization algorithms</a></div>
<div class="next"><a href="app-R.html"><span class="header-section-number">A</span> R programming</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#StochOpt"><span class="header-section-number">9</span> Stochastic Optimization</a></li>
<li>
<a class="nav-link" href="#SG-alg"><span class="header-section-number">9.1</span> Stochastic gradient algorithms</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#Pop-model"><span class="header-section-number">9.1.1</span> Population models</a></li>
<li><a class="nav-link" href="#online-stochastic-gradient-algorithm"><span class="header-section-number">9.1.2</span> Online stochastic gradient algorithm</a></li>
<li><a class="nav-link" href="#batch-stochastic-gradient-algorithms"><span class="header-section-number">9.1.3</span> Batch stochastic gradient algorithms</a></li>
<li><a class="nav-link" href="#news"><span class="header-section-number">9.1.4</span> Predicting news article sharing on social media</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#beyond-basic-stochastic-gradient-algorithms"><span class="header-section-number">9.2</span> Beyond basic stochastic gradient algorithms</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#mini-batches"><span class="header-section-number">9.2.1</span> Mini-batches</a></li>
<li><a class="nav-link" href="#momentum"><span class="header-section-number">9.2.2</span> Momentum</a></li>
<li><a class="nav-link" href="#adaptive-learning-rates"><span class="header-section-number">9.2.3</span> Adaptive learning rates</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#SG-Rcpp"><span class="header-section-number">9.3</span> Stochastic gradient algorithms with Rcpp</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#gradients-and-epochs-in-rcpp"><span class="header-section-number">9.3.1</span> Gradients and epochs in Rcpp</a></li>
<li><a class="nav-link" href="#full-rcpp-implementations"><span class="header-section-number">9.3.2</span> Full Rcpp implementations</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/nielsrhansen/CSwR/blob/master/28-StochasticOptimization.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/nielsrhansen/CSwR/edit/master/28-StochasticOptimization.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>
</div>
  

  

</div>
 <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Computational Statistics with R</strong>" was written by Niels Richard Hansen. It was last built on 2021-10-12, Git version: 6b05821.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 1 Introduction | Computational Statistics with R</title>
<meta name="author" content="Niels Richard Hansen">
<meta name="description" content="Computational statistics is about turning theory and methods into algorithms and actual numerical computations with data. It is about solving real computational problems that arise when we...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 1 Introduction | Computational Statistics with R">
<meta property="og:type" content="book">
<meta property="og:description" content="Computational statistics is about turning theory and methods into algorithms and actual numerical computations with data. It is about solving real computational problems that arise when we...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 1 Introduction | Computational Statistics with R">
<meta name="twitter:description" content="Computational statistics is about turning theory and methods into algorithms and actual numerical computations with data. It is about solving real computational problems that arise when we...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/_Roboto%20Slab-0.4.0/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.3.0/transition.js"></script><script src="libs/bs3compat-0.3.0/tabs.js"></script><script src="libs/bs3compat-0.3.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Computational Statistics with R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="active" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li class="book-part">Part I: Smoothing</li>
<li><a class="" href="density.html"><span class="header-section-number">2</span> Density estimation</a></li>
<li><a class="" href="bivariate.html"><span class="header-section-number">3</span> Bivariate smoothing</a></li>
<li class="book-part">Part II: Monte Carlo Methods</li>
<li><a class="" href="univariate-random-variables.html"><span class="header-section-number">4</span> Univariate random variables</a></li>
<li><a class="" href="mci.html"><span class="header-section-number">5</span> Monte Carlo integration</a></li>
<li class="book-part">Part III: Optimization</li>
<li><a class="" href="four-examples.html"><span class="header-section-number">6</span> Four Examples</a></li>
<li><a class="" href="numopt.html"><span class="header-section-number">7</span> Numerical optimization</a></li>
<li><a class="" href="em.html"><span class="header-section-number">8</span> Expectation maximization algorithms</a></li>
<li><a class="" href="StochOpt.html"><span class="header-section-number">9</span> Stochastic Optimization</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="app-R.html"><span class="header-section-number">A</span> R programming</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/nielsrhansen/CSwR">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="intro" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Introduction<a class="anchor" aria-label="anchor" href="#intro"><i class="fas fa-link"></i></a>
</h1>
<p>Computational statistics is about turning theory and methods into algorithms and
actual numerical computations with data. It is about solving real computational
problems that arise when we visualize, analyze and model data.</p>
<p>Computational statistics is not a single coherent topic but a large
number of vaguely related computational techniques that we use in statistics.
This book is not attempting to be comprehensive.
Instead, a few selected statistical topics and methodologies are
treated in some detail with the intention that good computational practice
can be learned from these topics and transferred to other statistical
methodologies as needed. Though the topics are arguably fundamental, they reflect the
knowledge and interests of the author, and different topics could clearly have
been chosen.</p>
<p>The demarcation line between statistical methodology and computational statistics
is, moreover, blurred. Most methodology involves mathematical formulas and even
algorithms for computing estimates and other statistics of interest from data,
or for evaluating probabilities or integrals numerically or via simulations.
In this book, the transition from methodology to
computational statistics happens when the methodology is to be implemented.
That is, when formulas, algorithms and pseudo code are transformed into actual
code and statistical software. It is during this transition that a number of
practical challenges reveal themselves, such as actual run time and
memory usage, and the limitations of finite precision arithmetic. And
when dealing with actual implementations we learn to appreciate the
value of an approximate solution that may be theoretically suboptimal but
sufficiently accurate for any practical purpose.</p>
<p>Statistical software development also requires some basic software engineering
skills and knowledge of the most common programming paradigms. Implementing a
single algorithm for a specific problem is one thing, but developing a piece of
statistical software for others to use is something quite different. This book
is <em>not</em> an introduction to statistical software development as such, but the
process of developing good software plays a role throughout. Good implementations
are not presented as code that manifests itself from divine insight, but rather
as code that is derived through experimental and analytic cycles – somewhat
resembling how software development actually takes place.</p>
<p>There is a notable practical and experimental component to software
development. However important theoretical considerations are regarding
correctness and complexity of algorithms, say, the actual code has to strike
a balance between generality, readability, efficiency, accuracy, ease of usage
and ease of development among other things. Finding a good balance requires that
one is able to reason about benefits and deficiencies of different
implementations. It is an important point of this book that such reasoning should
rely on experiments and empirical facts and not speculations.</p>
<p>R and RStudio is used throughout, and the reader is expected to have some
basic knowledge of R programming. While RStudio is not a requirement for
most of the book, it is a recommendable IDE (integrated development environment)
for R, which offers a convenient framework for developing, benchmarking,
profiling, testing, documenting and experimenting with statistical software.
Appendix <a href="app-R.html#app-R">A</a> covers some basic and important aspects of R programming
and can serve as a survey and quick reference. For
an in-depth treatment of R as a programming language the reader is referred
to <a href="https://adv-r.hadley.nz/">Advanced R</a> by Hadley Wickham. In fact, direct
references to that book are given throughout for detailed explanations of
many R programming language concepts, while Appendix @ref{app-R} contains a
brief overview of core R concepts used.</p>
<p>This book is organized into three parts on <a href="intro.html#intro-smooth">smoothing</a>,
<a href="intro.html#intro-mc">Monte Carlo methods</a>
and <a href="intro.html#intro-op">optimization</a>. Each part is introduced in the following
three sections to give the reader an overview of the topics covered, how they
are related to each other and how they are related to some main trends and
challenges in contemporary computational statistics. In this introduction,
several R functions from various packages are used to illustrate how smoothing,
simulation of random variables and optimization play roles in statistics.
Thus the introduction relies largely on already implemented solutions, and some
data analysts will never want to move beyond that use of R. However, the
remaining part of the book is written for those who want to move on and learn
how to develop their own solutions and not just how to use interfaces to the plethora
of already existing implementations.</p>
<div id="intro-smooth" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> Smoothing<a class="anchor" aria-label="anchor" href="#intro-smooth"><i class="fas fa-link"></i></a>
</h2>
<p>Smoothing is a descriptive statistical tool for summarizing data, a practical
visualization technique, as well as a nonparametric estimation methodology.
The basic idea is that data is representative of an underlying distribution
with some smoothness properties, and we would like to approximate or
estimate this underlying distribution from data.</p>
<p>There are two related but slightly different approaches. Either we attempt to estimate
a smooth density of the observed variables, or we attempt to estimate a smooth
conditional density of one variable given others. The latter can in principle
be done by computing the conditional density from a smooth estimate of the
joint density. Thus it appears that we really just need a way of
computing smooth density estimates. In practice it may, however, be better
to solve the conditional smoothing problem directly instead of solving a
strictly more complicated problem. This is particularly so, if
the conditioning variables are fixed e.g. by a design, or if our main interest is
in the conditional mean or median, say, and not the entire conditional distribution.
Conditional smoothing is dealt with in Chapter <a href="bivariate.html#bivariate">3</a>.</p>
<p>In this introduction we focus on the univariate case, where there really only
is one problem: smooth density estimation. Moreover, this is a very
basic problem, and one viewpoint is that we simply need to “smooth out
the jumps of the histogram.” Indeed, it does not need to be made more
sophisticated than that! Humans are able to do this quite well using just
a pen and a printed histogram, but it is a bit more complicated to automatize
such a smoothing procedure. Moreover, an automatized procedure is likely
to need calibration to yield a good tradeoff between smoothness
and data fit. This is again something that humans can do quite well
by eyeballing visualizations, but that approach does not scale,
neither in terms of the number of density estimates we want to consider,
nor in terms of going from univariate to multivariate densities.</p>
<p>If we want to really discuss how a smoothing
procedure works not just as a heuristic but also as an estimator of an
underlying density, it is necessary to formalize how to quantify
the performance of the procedure. This increases the level of
mathematical sophistication, but it allows us to discuss optimality, and
it lets us develop fully automatized procedures that do not rely on human
calibration. While human inspection of visualizations is always a good
idea, computational statistics is also about offloading humans from all
computational tasks that can be automatized. This is true for smoothing
as well, hence the need for automatic and robust smoothing
procedures that produce well calibrated results with a minimum of human effort.</p>
<div id="intro-angles" class="section level3" number="1.1.1">
<h3>
<span class="header-section-number">1.1.1</span> Angle distributions in proteins<a class="anchor" aria-label="anchor" href="#intro-angles"><i class="fas fa-link"></i></a>
</h3>
<p>We will illustrate smoothing using a small data set on angles formed between
two subsequent peptide planes in 3D protein structures. This data set is selected
because the angle distributions are multimodal and slightly non-standard, and
these properties are well suited for illustrating fundamental considerations
regarding smooth density estimation in practice.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:phipsiangles"></span>
<img src="figures/PhiPsi_creative.jpg" alt="The 3D structure of proteins is largely given by the \(\phi\)- and \(\psi\)-angles of the peptide planes. (By Dcrjsr, CC BY 3.0 via Wikimedia Commons.)" width="40%"><p class="caption">
Figure 1.1: The 3D structure of proteins is largely given by the <span class="math inline">\(\phi\)</span>- and <span class="math inline">\(\psi\)</span>-angles of the peptide planes. (By <a href="https://commons.wikimedia.org/wiki/File%3APhiPsi_drawing_with_plane_and_labels.jpg">Dcrjsr</a>, <a href="http://creativecommons.org/licenses/by/3.0">CC BY 3.0</a> via Wikimedia Commons.)
</p>
</div>
<div class="inline-figure">A protein is a large molecule consisting of a backbone with carbon and nitrogen
atoms arranged sequentially:
<img src="figures/backbone.png" width="70%" style="display: block; margin: auto;">
</div>
<p>A hydrogen atom binds to each nitrogen (N) and an oxygen atom binds to each
carbon without the <span class="math inline">\(\alpha\)</span> subscript (C), see Figure <a href="intro.html#fig:phipsiangles">1.1</a>,
and such four atoms form together what is known as a peptide bond between two
alpha-carbon atoms (C<span class="math inline">\(_{\alpha}\)</span>). Each C<span class="math inline">\(_{\alpha}\)</span> atom binds a hydrogen
atom and an amino acid <em>side chain</em>. There are 20
naturally occurring amino acids in genetically encoded proteins,
each having a three letter code (such as Gly for Glycine, Pro for Proline, etc.).
The protein will typically form a complicated 3D structure determined by the
amino acids, which in turn determine the <span class="math inline">\(\phi\)</span>- and the <span class="math inline">\(\psi\)</span>-angles between
the peptide planes as shown on Figure <a href="intro.html#fig:phipsiangles">1.1</a>.</p>
<p>We will consider a small data set, <code>phipsi</code>, of experimentally determined angles from a
single protein, the human protein <a href="https://www.rcsb.org/structure/1HMP">1HMP</a>,
which is composed of two chains (denoted A and B). Figure <a href="intro.html#fig:1HMP">1.2</a> shows
the 3D structure of the protein.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">)</span></code></pre></div>
<pre><code>##   chain  AA pos        phi        psi
## 1     A Pro   5 -1.6218794  0.2258685
## 2     A Gly   6  1.1483709 -2.8314426
## 3     A Val   7 -1.4160220  2.1190570
## 4     A Val   8 -1.4926720  2.3941331
## 5     A Ile   9 -2.1814653  1.4877618
## 6     A Ser  10 -0.7525375  2.5676186</code></pre>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:1HMP"></span>
<img src="figures/1HMP.png" alt="The 3D structure of the atoms constituting the protein 1HMP. The colors indicate the two different chains." width="30%"><p class="caption">
Figure 1.2: The 3D structure of the atoms constituting the protein 1HMP. The colors indicate the two different chains.
</p>
</div>
<p>We can use base R functions such as <code>hist</code> and <code>density</code> to visualize the
marginal distributions of the two angles.</p>

<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">phi</span>, prob <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/rug.html">rug</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">phi</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">phi</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span>, prob <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/rug.html">rug</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:phipsiDens"></span>
<img src="CSwR_files/figure-html/phipsiDens-1.png" alt="Histograms equipped with a rug plot and smoothed density estimate (red line) of the distribution of \(\phi\)-angles (left) and \(\psi\)-angles (right)." width="49%"><img src="CSwR_files/figure-html/phipsiDens-2.png" alt="Histograms equipped with a rug plot and smoothed density estimate (red line) of the distribution of \(\phi\)-angles (left) and \(\psi\)-angles (right)." width="49%"><p class="caption">
Figure 1.3: Histograms equipped with a rug plot and smoothed density estimate (red line) of the distribution of <span class="math inline">\(\phi\)</span>-angles (left) and <span class="math inline">\(\psi\)</span>-angles (right).
</p>
</div>
<p>The smooth red density curve shown in Figure <a href="intro.html#fig:phipsiDens">1.3</a> can be thought
of as a smooth version of a histogram. It is surprisingly difficult to find
automatic smoothing procedures that perform uniformly
well – it is even quite difficult to automatically select the number and
positions of the breaks used for histograms. This is one of the important
points that is taken up in this book: how to implement good default choices
of various <em>tuning parameters</em> that are required by any smoothing procedure.</p>
</div>
<div id="using-ggplot2" class="section level3" number="1.1.2">
<h3>
<span class="header-section-number">1.1.2</span> Using ggplot2<a class="anchor" aria-label="anchor" href="#using-ggplot2"><i class="fas fa-link"></i></a>
</h3>
<p>It is also possible to use <code>ggplot2</code> to achieve similar results.</p>

<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">phipsi</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">phi</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">..density..</span><span class="op">)</span>, bins <span class="op">=</span> <span class="fl">13</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_density.html">geom_density</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"red"</span>, size <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_rug.html">geom_rug</a></span><span class="op">(</span><span class="op">)</span>

<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">phipsi</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">psi</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">..density..</span><span class="op">)</span>, bins <span class="op">=</span> <span class="fl">13</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_density.html">geom_density</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"red"</span>, size <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_rug.html">geom_rug</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:phipsiggplot"></span>
<img src="CSwR_files/figure-html/phipsiggplot-1.png" alt="Histograms and density estimates of \(\phi\)-angles (left) and \(\psi\)-angles (right) made with ggplot2." width="49%"><img src="CSwR_files/figure-html/phipsiggplot-2.png" alt="Histograms and density estimates of \(\phi\)-angles (left) and \(\psi\)-angles (right) made with ggplot2." width="49%"><p class="caption">
Figure 1.4: Histograms and density estimates of <span class="math inline">\(\phi\)</span>-angles (left) and <span class="math inline">\(\psi\)</span>-angles (right) made with ggplot2.
</p>
</div>
<p>Histograms produced by ggplot2 have a non adaptive default number of bins equal
to 30 (number of breaks equal to 31), which is different from <code>hist</code> that uses
<a href="https://en.wikipedia.org/wiki/Histogram#Sturges'_formula">Sturges’ formula</a>
<span class="math display">\[\text{number of breaks} = \lceil \log_2(n) + 1 \rceil\]</span>
with <span class="math inline">\(n\)</span> the number of observations in the data set. In addition, this number is
further modified by
the function <code>pretty</code> that generates “nice” breaks, which results in 14 breaks
for the angle data. For easier comparison, the number of bins used by
<code>geom_histogram</code> above is set to 13, though it should be
noticed that the breaks are not chosen in exactly the
same way by <code>geom_histogram</code> and <code>hist</code>. Automatic and data adaptive bin
selection is difficult, and <code>geom_histogram</code> implements a simple and fixed, but
likely suboptimal, default while notifying the user that this default choice
can be improved by setting <code>binwidth</code>.</p>
<p>For the density, <code>geom_density</code> actually relies on the <code>density</code> function and its
default choices of how and how much to smooth. Thus the figure
may have a slightly different appearance, but the estimated density obtained by
<code>geom_density</code> is identical to the one obtained by <code>density</code>.</p>
</div>
<div id="changing-the-defaults" class="section level3" number="1.1.3">
<h3>
<span class="header-section-number">1.1.3</span> Changing the defaults<a class="anchor" aria-label="anchor" href="#changing-the-defaults"><i class="fas fa-link"></i></a>
</h3>
<p>The range of the angle data is known to be <span class="math inline">\((-\pi, \pi]\)</span>, which neither
the histogram nor the density smoother take advantage of. The <code>pretty</code> function,
used by <code>hist</code> chooses, for instance, breaks in <span class="math inline">\(-3\)</span> and <span class="math inline">\(3\)</span>, which results in
the two extreme
bars in the histogram to be misleading. Note also that for the <span class="math inline">\(\psi\)</span>-angle
it appears that the defaults result in oversmoothing of the density
estimate. That is, the density is more
smoothed out than the data (and the histogram) appears to support.</p>
<p>To obtain different – and perhaps better – results, we can try to change some
of the defaults of the histogram and density functions. The two most important
defaults to consider are the <em>bandwidth</em> and the <em>kernel</em>.
Postponing the mathematics to Chapter <a href="density.html#density">2</a>, the kernel controls how
neighboring data points are weighted relatively to each other, and the
bandwidth controls the size of neighborhoods. A bandwidth can be specified
manually as a specific numerical value, but for a fully automatic procedure,
it is selected by a bandwidth selection algorithm. The <code>density</code> default
is a rather simplistic algorithm known as Silverman’s rule-of-thumb.</p>

<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span>, breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="va">pi</span>, <span class="va">pi</span>, length.out <span class="op">=</span> <span class="fl">15</span><span class="op">)</span>, prob <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/rug.html">rug</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span>, adjust <span class="op">=</span> <span class="fl">1</span>, cut <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span>, adjust <span class="op">=</span> <span class="fl">0.5</span>, cut <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"blue"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span>, adjust <span class="op">=</span> <span class="fl">2</span>, cut <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"purple"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span>, breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="va">pi</span>, <span class="va">pi</span>, length.out <span class="op">=</span> <span class="fl">15</span><span class="op">)</span>, prob <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/rug.html">rug</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span><span class="op">)</span>
<span class="co"># Default kernel is "gaussian"</span>
<span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span>, bw <span class="op">=</span> <span class="st">"SJ"</span>, cut <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span>, kernel <span class="op">=</span> <span class="st">"epanechnikov"</span>, bw <span class="op">=</span> <span class="st">"SJ"</span>, cut <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"blue"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span>, kernel <span class="op">=</span> <span class="st">"rectangular"</span>, bw <span class="op">=</span> <span class="st">"SJ"</span>, cut <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"purple"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:psiDens"></span>
<img src="CSwR_files/figure-html/psiDens-1.png" alt="Histograms and various density estimates for the \(\psi\)-angles. The colors indicate different choices of bandwidth adjustments using the otherwise default bandwidth selection (left) and different choices of kernels using Sheather-Jones bandwidth selection (right)." width="49%"><img src="CSwR_files/figure-html/psiDens-2.png" alt="Histograms and various density estimates for the \(\psi\)-angles. The colors indicate different choices of bandwidth adjustments using the otherwise default bandwidth selection (left) and different choices of kernels using Sheather-Jones bandwidth selection (right)." width="49%"><p class="caption">
Figure 1.5: Histograms and various density estimates for the <span class="math inline">\(\psi\)</span>-angles. The colors indicate different choices of bandwidth adjustments using the otherwise default bandwidth selection (left) and different choices of kernels using Sheather-Jones bandwidth selection (right).
</p>
</div>
<p>Figure
<a href="intro.html#fig:psiDens">1.5</a> shows examples of several different density estimates
that can be obtained by changing the defaults of <code>density</code>. The breaks for
the histogram have also been chosen manually to make sure that they match
the range of the data. Note, in particular, that Sheather-Jones bandwidth
selection appears to work better than the default bandwidth for this example.
This is generally the
case for multimodal distributions, where the default bandwidth tends to oversmooth.
Note also that the choice of bandwidth is far more consequential than
the choice of kernel, the latter mostly affecting how wiggly the density
estimate is locally.</p>
<p>It should be noted that defaults arise as
a combination of historically sensible choices and backward compatibility. Thought
should go into choosing a good, robust default, but once a default is chosen,
it should not be changed haphazardly, as this might break existing code. That is
why not all defaults used in R are by today’s standards the best known
choices. You see this argument made in the documentation of <code>density</code> regarding the
default for bandwidth selection, where Sheather-Jones is suggested as a
better default than the current, but for compatibility reasons Silverman’s
rule-of-thumb is the default and is likely to remain being so.</p>
</div>
<div id="multivariate-smoothing" class="section level3" number="1.1.4">
<h3>
<span class="header-section-number">1.1.4</span> Multivariate methods<a class="anchor" aria-label="anchor" href="#multivariate-smoothing"><i class="fas fa-link"></i></a>
</h3>
<p>This section provides a single illustration of how to use the
bivariate kernel smoother <code>kde2d</code> from the MASS package
for bivariate density estimation of the <span class="math inline">\((\phi, \psi)\)</span>-angle distribution.
A scatter plot of <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\psi\)</span> angles is known as a <a href="https://en.wikipedia.org/wiki/Ramachandran_plot">Ramachandran plot</a>, and it provides
a classical and important way of visualizing local structural
constraints of proteins in structural biochemistry. The density estimate
can be understood as an estimate of the distribution of <span class="math inline">\((\phi, \psi)\)</span>-angles
in naturally occuring proteins from the small sample of angles in our
data set.</p>
<p>We compute the density estimate in a grid of size 100 by 100 using a bandwidth
of 2 and using the <code>kde2d</code> function that uses a bivariate normal kernel.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">denshat</span> <span class="op">&lt;-</span> <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/kde2d.html">kde2d</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">phi</span>, <span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span>, h <span class="op">=</span> <span class="fl">2</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">denshat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>
  <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span>
    <span class="va">denshat</span><span class="op">$</span><span class="va">x</span>, 
    <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">denshat</span><span class="op">$</span><span class="va">y</span>, each <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">denshat</span><span class="op">$</span><span class="va">x</span><span class="op">)</span><span class="op">)</span>, 
    <span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="va">denshat</span><span class="op">$</span><span class="va">z</span><span class="op">)</span>
    <span class="op">)</span>
<span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">denshat</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"phi"</span>, <span class="st">"psi"</span>, <span class="st">"dens"</span><span class="op">)</span>
<span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">denshat</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span><span class="va">phi</span>, <span class="va">psi</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_tile.html">geom_tile</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>fill <span class="op">=</span> <span class="va">dens</span><span class="op">)</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_contour.html">geom_contour</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>z <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">dens</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">phipsi</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>fill <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_gradient.html">scale_fill_gradient</a></span><span class="op">(</span>low <span class="op">=</span> <span class="st">"white"</span>, high <span class="op">=</span> <span class="st">"darkblue"</span>, trans <span class="op">=</span> <span class="st">"sqrt"</span><span class="op">)</span></code></pre></div>
<p>We then recompute the density estimate in the same grid of size using
the smaller bandwidth of 0.5.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">denshat</span> <span class="op">&lt;-</span> <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/kde2d.html">kde2d</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">phi</span>, <span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span>, h <span class="op">=</span> <span class="fl">0.5</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:bidens2"></span>
<img src="CSwR_files/figure-html/bidens2-1.png" alt="Bivariate density estimates of protein backbone angles using a bivariate Gaussian kernel with bandwiths $2$ (left) and $0.5$ (right)." width="49%"><img src="CSwR_files/figure-html/bidens2-2.png" alt="Bivariate density estimates of protein backbone angles using a bivariate Gaussian kernel with bandwiths $2$ (left) and $0.5$ (right)." width="49%"><p class="caption">
Figure 1.6: Bivariate density estimates of protein backbone angles using a bivariate Gaussian kernel with bandwiths <span class="math inline">\(2\)</span> (left) and <span class="math inline">\(0.5\)</span> (right).
</p>
</div>
<p>The Ramachandran plot in Figure <a href="intro.html#fig:bidens2">1.6</a> shows how structural constraints
of a protein, such as steric effects, induce a non-standard bivariate distribution
of <span class="math inline">\((\phi, \psi)\)</span>-angles.</p>
</div>
<div id="large-scale-smoothing" class="section level3" number="1.1.5">
<h3>
<span class="header-section-number">1.1.5</span> Large scale smoothing<a class="anchor" aria-label="anchor" href="#large-scale-smoothing"><i class="fas fa-link"></i></a>
</h3>
<p>With small data sets of less than 10,000 data points, say, univariate
smooth density estimation requires a very modest amount of computation. That is true
even with rather naive implementations of the standard methods. The R function
<code>density</code> is implemented using a number of computational tricks like
binning and the fast Fourier transform, and it can compute density
estimates with a million data points (around 8 MB) within a fraction of a second.</p>
<p>It is
unclear if we ever need truly large scale <em>univariate</em> density
estimation with terabytes of data points, say. If we have that amount of
(heterogeneous) data it is likely that we are better off breaking the data down
into smaller and more homogeneous groups. That is, we should turn a big data computation
into a large number of small data computations. That does not remove the
computational challenge but it does diminish it somewhat e.g. by parallelization.</p>
<p>Deng and Wickham did a review in 2011 on <a href="http://www2.cs.uh.edu/~ceick/7362/T2-4.pdf">Density estimation in R</a>,
where they assessed the performance of a number of R packages including the
<code>density</code> function. The <a href="https://cran.r-project.org/web/packages/KernSmooth/index.html">KernSmooth</a>
package was singled out in terms of speed as well as accuracy
for computing <em>smooth</em> density estimates with <code>density</code> performing quite well too. (Histograms
are non-smooth density estimates and generally faster to compute).
The assessment was based on using defaults for the different packages, which is meaningful
in the sense of representing the
performance that the occasional user will experience. It is, however,
also an evaluation of the combination of default choices and the implementation,
and as different packages rely on e.g. different bandwidth selection algorithms,
this assessment is not the complete story. The <code>bkde</code> function from the KernSmooth
package, as well as <code>density</code>, are solid choices, but
the point is that performance assessment is a multifaceted problem.</p>
<p>To be a little more specific about the computational complexity of density
estimators, suppose that we have <span class="math inline">\(n\)</span> data points and want to evaluate the
density in <span class="math inline">\(m\)</span> points. A naive implementation of kernel smoothing,
Section <a href="density.html#kernel-density">2.2</a>, has <span class="math inline">\(O(mn)\)</span> time complexity, while
a naive implementation of the best bandwidth selection algorithms have
<span class="math inline">\(O(n^2)\)</span> time complexity. As a simple rule-of-thumb, anything beyond <span class="math inline">\(O(n)\)</span>
will not scale to very large data sets. A quadratic time complexity for bandwidth
selection will, in particular, be a serious bottleneck. Kernel smoothing
illustrates perfectly that a literal implementation of the mathematics behind
a statistical method may not always be computationally viable. Even
the <span class="math inline">\(O(mn)\)</span> time complexity may be quite a bottleneck as it reflects
<span class="math inline">\(mn\)</span> kernel evaluations, each being potentially a computationally
relatively expensive operation.</p>
<p>The binning trick, with the number of bins set to <span class="math inline">\(m\)</span>, is a grouping of the data
points into <span class="math inline">\(m\)</span> sets of neighbor points (bins) with each bin
representing the points in the bin via a single point and a weight. If <span class="math inline">\(m \ll n\)</span>,
this can reduce the time complexity substantially to <span class="math inline">\(O(m^2) + O(n)\)</span>. The fast
Fourier transform may reduce the <span class="math inline">\(O(m^2)\)</span> term even further to <span class="math inline">\(O(m\log(m))\)</span>.
Some approximations are involved, and it is of importance
to evaluate the tradeoff between time and memory complexity on one
side and accuracy on the other side.</p>
<p>Multivariate smoothing is a different story. While it is possible to
generalize the basic ideas of univariate density estimation to arbitrary dimensions, the
<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse-of-dimensionality</a>
hits unconstrained smoothing hard – statistically as well as
computationally. Multivariate smoothing is therefore still an active research
area developing computationally tractable and novel ways of fitting smooth
densities or conditional densities to multivariate
or even high-dimensional data. A key technique is to make structural
assumptions to alleviate the challenge of a large dimension, but there are many
different assumptions possible, which makes the body of methods and theory
richer and the practical choices much more difficult.</p>
</div>
</div>
<div id="intro-mc" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Monte Carlo methods<a class="anchor" aria-label="anchor" href="#intro-mc"><i class="fas fa-link"></i></a>
</h2>
<p>Broadly speaking, Monte Carlo methods are computations that rely on some form
of random input in order to carry out a computation. The actual
random input will be generated by a (pseudo)random number generator
according to some distributional specifications, and the precise value of the
computation will depend on the precise value of the random input but in a way
where we understand the magnitude of the dependence quite well. In most
cases we can make the dependence diminish by increasing the amount of random
input.</p>
<p>In statistics it is quite interesting in itself that we can make the computer
simulate data so that we can draw an example data set from a statistical
model. However, the real usage of such simulations is almost always as a
part of a Monte Carlo computation, where we repeat the simulation of
data sets a large number of times and compute distributional properties
of various statistics. This is just one of the most obvious applications of
Monte Carlo methods in statistics and there are many others. Disregarding
the specific computation of interest in applications, a core problem of
Monte Carlo methods is efficient simulation of random variables from a
given target distribution.</p>
<div id="vM" class="section level3" number="1.2.1">
<h3>
<span class="header-section-number">1.2.1</span> Univariate von Mises distributions<a class="anchor" aria-label="anchor" href="#vM"><i class="fas fa-link"></i></a>
</h3>
<p>We will exemplify Monte Carlo computations by considering angle distributions
just as in Section <a href="intro.html#intro-smooth">1.1</a>. The angles take values in the interval
<span class="math inline">\((-\pi, \pi]\)</span>, and we will consider models based on the <a href="https://en.wikipedia.org/wiki/Von_Mises_distribution#Moments">von Mises distribution</a>
on this interval, which has density</p>
<p><span class="math display">\[f(x) = \frac{1}{\varphi(\theta)} e^{\theta_1 \cos(x) + \theta_2 \sin(x)}\]</span></p>
<p>for <span class="math inline">\(\theta = (\theta_1, \theta_2)^T \in \mathbb{R}^2\)</span>. A common
alternative parametrization is obtained by introducing
<span class="math inline">\(\kappa = \|\theta\|_2 = \sqrt{\theta_1^2 + \theta_2^2}\)</span>, and
(whenever <span class="math inline">\(\kappa \neq 0\)</span>)
<span class="math inline">\(\nu = \theta / \kappa = (\cos(\mu), \sin(\mu))^T\)</span> for <span class="math inline">\(\mu \in (-\pi, \pi]\)</span>.
Using the <span class="math inline">\((\kappa, \mu)\)</span>-parametrization the density becomes<br><span class="math display">\[f(x) = \frac{1}{\varphi(\kappa \nu)} e^{\kappa \cos(x - \mu)}.\]</span>
The former parametrization in terms of
<span class="math inline">\(\theta\)</span> is, however, the canonical
parametrization of the family of distributions as an exponential family, which
is particularly useful for various likelihood estimation algorithms.
The normalization constant</p>
<p><span class="math display">\[\begin{align*}
\varphi(\kappa \nu) &amp; = \int_{-\pi}^\pi e^{\kappa \cos(x - \mu)}\mathrm{d} x \\
&amp; = 2 \pi \int_{0}^{1} e^{\kappa \cos(\pi x)}\mathrm{d} x = 2 \pi I_0(\kappa) 
\end{align*}\]</span></p>
<p>is given in terms of the <a href="http://mathworld.wolfram.com/ModifiedBesselFunctionoftheFirstKind.html">modified Bessel function</a>
<span class="math inline">\(I_0\)</span>. We can easily compute and plot the density using R’s <code>besselI</code> implementation of
the modified Bessel function.</p>

<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">phi</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">k</span><span class="op">)</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Bessel.html">besselI</a></span><span class="op">(</span><span class="va">k</span>, <span class="fl">0</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Trig.html">cos</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fu">phi</span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>, <span class="op">-</span><span class="va">pi</span>, <span class="va">pi</span>, lwd <span class="op">=</span> <span class="fl">2</span>, ylab <span class="op">=</span> <span class="st">"density"</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.52</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">cos</a></span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fu">phi</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>, col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span>, add <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fl">0.5</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">cos</a></span><span class="op">(</span><span class="va">x</span> <span class="op">+</span> <span class="fl">1.5</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fu">phi</span><span class="op">(</span><span class="fl">0.5</span><span class="op">)</span>, col <span class="op">=</span> <span class="st">"blue"</span>, lwd <span class="op">=</span> <span class="fl">2</span>, add <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:vonMisesDens"></span>
<img src="CSwR_files/figure-html/vonMisesDens-1.png" alt="Density for the von Mises distribution with parameters \(\kappa = 1\) and \(\nu = 0\) (black), \(\kappa = 2\) and \(\nu = 1\) (red), and \(\kappa = 0.5\) and \(\nu = - 1.5\) (blue)." width="70%"><p class="caption">
Figure 1.7: Density for the von Mises distribution with parameters <span class="math inline">\(\kappa = 1\)</span> and <span class="math inline">\(\nu = 0\)</span> (black), <span class="math inline">\(\kappa = 2\)</span> and <span class="math inline">\(\nu = 1\)</span> (red), and <span class="math inline">\(\kappa = 0.5\)</span> and <span class="math inline">\(\nu = - 1.5\)</span> (blue).
</p>
</div>
<p>It is not entirely obvious how we should go about simulating data points
from the von Mises distribution. It will be demonstrated in Section
<a href="univariate-random-variables.html#reject-samp">4.3</a> how to implement a <em>rejection sampler</em>, which is one
useful algorithm for simulating samples from a distribution with a density.</p>
<p>In this section we simply use the <code>rmovMF</code> function from the <code>movMF</code> package,
which implements a few functions for working with (finite mixtures of) von
Mises distributions, and even the general von <a href="https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution">Mises-Fisher distributions</a>
that are generalizations of the von Mises distribution to <span class="math inline">\(p\)</span>-dimensional
unit spheres.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"movMF"</span><span class="op">)</span>
<span class="va">xy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/movMF/man/movMF_distribution.html">rmovMF</a></span><span class="op">(</span><span class="fl">500</span>, <span class="fl">0.5</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Trig.html">cos</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1.5</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1.5</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="co"># rmovMF represents samples as elements on the unit circle</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">acos</a></span><span class="op">(</span><span class="va">xy</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sign.html">sign</a></span><span class="op">(</span><span class="va">xy</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span></code></pre></div>

<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">x</span>, breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="va">pi</span>, <span class="va">pi</span>, length.out <span class="op">=</span> <span class="fl">15</span><span class="op">)</span>, prob <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/rug.html">rug</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">x</span>, bw <span class="op">=</span> <span class="st">"SJ"</span>, cut <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fl">0.5</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">cos</a></span><span class="op">(</span><span class="va">x</span> <span class="op">+</span> <span class="fl">1.5</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fu">phi</span><span class="op">(</span><span class="fl">0.5</span><span class="op">)</span>, col <span class="op">=</span> <span class="st">"blue"</span>, lwd <span class="op">=</span> <span class="fl">2</span>, add <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:vMdata"></span>
<img src="CSwR_files/figure-html/vMdata-1.png" alt="Histogram of 500 simulated data points from a von Mises distribution with parameters \(\kappa = 0.5\) and \(\nu = - 1.5\). A smoothed density estimate (red) and the true density (blue) are added to the plot." width="70%"><p class="caption">
Figure 1.8: Histogram of 500 simulated data points from a von Mises distribution with parameters <span class="math inline">\(\kappa = 0.5\)</span> and <span class="math inline">\(\nu = - 1.5\)</span>. A smoothed density estimate (red) and the true density (blue) are added to the plot.
</p>
</div>
</div>
<div id="mixtures-of-von-mises-distributions" class="section level3" number="1.2.2">
<h3>
<span class="header-section-number">1.2.2</span> Mixtures of von Mises distributions<a class="anchor" aria-label="anchor" href="#mixtures-of-von-mises-distributions"><i class="fas fa-link"></i></a>
</h3>
<p>The von Mises distributions are unimodal distributions on <span class="math inline">\((-\pi, \pi]\)</span>. Thus to
find a good model of the bimodal angle data, say, we have do move beyond these
distributions. A standard approach for constructing multimodal distributions
is as <em>mixtures</em> of unimodal distributions. A mixture of two von Mises distributions
can be constructed by flipping a (biased) coin to decide which of the two
distributions to sample from. We will use the exponential family parametrization
in the following.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">thetaA</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3.5</span>, <span class="op">-</span><span class="fl">2</span><span class="op">)</span>
<span class="va">thetaB</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">4.5</span>, <span class="fl">4</span><span class="op">)</span>
<span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">0.55</span> <span class="co"># Probability of von Mises distribution A</span>
<span class="co"># The sample function implements the "coin flips"</span>
<span class="va">u</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, <span class="fl">500</span>, replace <span class="op">=</span> <span class="cn">TRUE</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">alpha</span>, <span class="fl">1</span> <span class="op">-</span> <span class="va">alpha</span><span class="op">)</span><span class="op">)</span>
<span class="va">xy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/movMF/man/movMF_distribution.html">rmovMF</a></span><span class="op">(</span><span class="fl">500</span>, <span class="va">thetaA</span><span class="op">)</span> <span class="op">*</span> <span class="va">u</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/pkg/movMF/man/movMF_distribution.html">rmovMF</a></span><span class="op">(</span><span class="fl">500</span>, <span class="va">thetaB</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">u</span><span class="op">)</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">acos</a></span><span class="op">(</span><span class="va">xy</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sign.html">sign</a></span><span class="op">(</span><span class="va">xy</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span></code></pre></div>
<p>The <code>rmovMF</code> actually implements simulation from a mixture distribution
directly, thus there is no need to construct the “coin flips” explicitly.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">thetaA</span>, <span class="va">thetaB</span><span class="op">)</span>
<span class="va">xy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/movMF/man/movMF_distribution.html">rmovMF</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="va">theta</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">alpha</span>, <span class="fl">1</span> <span class="op">-</span> <span class="va">alpha</span><span class="op">)</span><span class="op">)</span>
<span class="va">x_alt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">acos</a></span><span class="op">(</span><span class="va">xy</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sign.html">sign</a></span><span class="op">(</span><span class="va">xy</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span></code></pre></div>
<p>To compare the simulated data with two mixture components to the model and
a smoothed density, we implement an R function that computes the density
for an angle argument using the function <code>dmovMF</code> that takes a unit circle
argument.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">dvM</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">theta</span>, <span class="va">alpha</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">xx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Trig.html">cos</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span>
  <span class="fu"><a href="https://rdrr.io/pkg/movMF/man/movMF_distribution.html">dmovMF</a></span><span class="op">(</span><span class="va">xx</span>, <span class="va">theta</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">alpha</span>, <span class="fl">1</span> <span class="op">-</span> <span class="va">alpha</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>Note that <code>dmovMF</code> uses normalized <a href="https://en.wikipedia.org/wiki/Spherical_measure">spherical measure</a>
on the unit circle as reference
measure, thus the need for the <span class="math inline">\(2\pi\)</span> division if we want the result to be
comparable to histograms and density estimates that use Lebesgue measure on
<span class="math inline">\((-\pi, \pi]\)</span> as the reference measure.</p>

<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">x</span>, breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="va">pi</span>, <span class="va">pi</span>, length.out <span class="op">=</span> <span class="fl">15</span><span class="op">)</span>, prob <span class="op">=</span> <span class="cn">TRUE</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.5</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/rug.html">rug</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">x</span>, bw <span class="op">=</span> <span class="st">"SJ"</span>, cut <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="fu">dvM</span><span class="op">(</span><span class="va">x</span>, <span class="va">theta</span>, <span class="va">alpha</span><span class="op">)</span>, col <span class="op">=</span> <span class="st">"blue"</span>, lwd <span class="op">=</span> <span class="fl">2</span>, add <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">x_alt</span>, breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="va">pi</span>, <span class="va">pi</span>, length.out <span class="op">=</span> <span class="fl">15</span><span class="op">)</span>, prob <span class="op">=</span> <span class="cn">TRUE</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.5</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/rug.html">rug</a></span><span class="op">(</span><span class="va">x_alt</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">x_alt</span>, bw <span class="op">=</span> <span class="st">"SJ"</span>, cut <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="fu">dvM</span><span class="op">(</span><span class="va">x</span>, <span class="va">theta</span>, <span class="va">alpha</span><span class="op">)</span>, col <span class="op">=</span> <span class="st">"blue"</span>, lwd <span class="op">=</span> <span class="fl">2</span>, add <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:mixvMdata"></span>
<img src="CSwR_files/figure-html/mixvMdata-1.png" alt="Histograms of 500 simulated data points from a mixture of two von Mises distributions using either the explicit construction of the mixture (left) or the functionality in rmovMF to simulate mixtures directly (right). A smoothed density estimate (red) and the true density (blue) are added to the plot." width="49%"><img src="CSwR_files/figure-html/mixvMdata-2.png" alt="Histograms of 500 simulated data points from a mixture of two von Mises distributions using either the explicit construction of the mixture (left) or the functionality in rmovMF to simulate mixtures directly (right). A smoothed density estimate (red) and the true density (blue) are added to the plot." width="49%"><p class="caption">
Figure 1.9: Histograms of 500 simulated data points from a mixture of two von Mises distributions using either the explicit construction of the mixture (left) or the functionality in <code>rmovMF</code> to simulate mixtures directly (right). A smoothed density estimate (red) and the true density (blue) are added to the plot.
</p>
</div>
<p>Simulation of data from a distribution finds many applications. The technique
is widely used whenever we want to investigate a statistical methodology in
terms of its frequentistic performance under various data sampling models, and
simulation is a tool of fundamental importance for
the practical application of Bayesian statistical methods. Another important
application is as a tool for computing approximations of integrals. This
is usually called Monte Carlo integration and is a form of numerical
integration. Computing probabilities or distribution functions, say,
are notable examples of integrals, and we consider here the computation of
the probability of the interval <span class="math inline">\((0, 1)\)</span> for the above mixture of two von Mises
distributions.</p>
<p>It is straightforward to compute this probability via Monte Carlo integration
as a simple average. Note that we will use a large number of samples,
50,000 in this case, of simulated angles for this computation. Increasing
the number even further will make the result more accurate. Chapter <a href="mci.html#mci">5</a>
deals with the assessment of the accuracy of Monte Carlo integrals, and how
this random error can be estimated, bounded and minimized.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">xy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/movMF/man/movMF_distribution.html">rmovMF</a></span><span class="op">(</span><span class="fl">50000</span>, <span class="va">theta</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">alpha</span>, <span class="fl">1</span> <span class="op">-</span> <span class="va">alpha</span><span class="op">)</span><span class="op">)</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">acos</a></span><span class="op">(</span><span class="va">xy</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sign.html">sign</a></span><span class="op">(</span><span class="va">xy</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span> <span class="op">&gt;</span> <span class="fl">0</span> <span class="op">&amp;</span> <span class="va">x</span> <span class="op">&lt;</span> <span class="fl">1</span><span class="op">)</span>  <span class="co"># Estimate of the probability of the interval (0, 1)</span></code></pre></div>
<pre><code>## [1] 0.08508</code></pre>
<p>The probability above could, of course, be expressed using the
distribution function of the mixture of von Mises distributions, which
in turn can be computed in terms of integrals of von Mises densities.
Specifically, the probability is
<span class="math display">\[p = \frac{\alpha}{\varphi(\theta_A)} \int_0^1 e^{\theta_{A, 1} \cos(x) + \theta_{A, 2} \sin(x)} \mathrm{d} x +  
\frac{1 - \alpha}{\varphi(\theta_B)} \int_0^1 e^{\theta_{B, 1} \cos(x) + \theta_{B, 2} \sin(x)} \mathrm{d} x,\]</span>
but these integrals do not have a simple analytic representation – just as the
distribution function of the von Mises distribution doesn’t have a simple analytic
expression. Thus the computation of the probability requires numerical
computation of the integrals.</p>
<p>The R function <code>integrate</code> can be used for numerical integration of univariate
functions using standard numerical integration techniques. We can thus
compute the probability by integrating the density of the mixture,
as implemented above as the R function <code>dvM</code>. Note the arguments passed
to <code>integrate</code> below. The first argument is the density function, then follows
the lower and the upper limits of the integration, and then follows additional
arguments to the density – in this case parameter values.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/integrate.html">integrate</a></span><span class="op">(</span><span class="va">dvM</span>, <span class="fl">0</span>, <span class="fl">1</span>, theta <span class="op">=</span> <span class="va">theta</span>, alpha <span class="op">=</span> <span class="va">alpha</span><span class="op">)</span></code></pre></div>
<pre><code>## 0.08635171 with absolute error &lt; 9.6e-16</code></pre>
<p>The <code>integrate</code> function in R is an interface to a couple of classical
<a href="https://en.wikipedia.org/wiki/QUADPACK">QUADPACK</a> Fortran routines for
numerical integration via <a href="https://en.wikipedia.org/wiki/Adaptive_quadrature">adaptive quadrature</a>.
Specifically, the computations rely on approximations of the form
<span class="math display">\[\int_a^b f(x) \mathrm{d} x \simeq \sum_i w_i f(x_i)\]</span>
for certain <em>grid points</em> <span class="math inline">\(x_i\)</span> and weights <span class="math inline">\(w_i\)</span>, which are computed using
<a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Kronrod_quadrature_formula">Gauss-Kronrod quadrature</a>.
This method provides an estimate of the approximation error in addition to
the numerical approximation of the integral itself.</p>
<p>It is noteworthy that <code>integrate</code> as a function implemented in R
takes another function, in this case the density <code>dvM</code>, as an argument. R
is a <a href="https://adv-r.hadley.nz/fp.html">functional programming language</a>
and functions are <a href="https://en.wikipedia.org/wiki/First-class_function">first-class citizens</a>.
This implies, for instance, that functions can be passed as arguments to other
functions using a variable name – just like any other variable can be passed
as an argument to a function. In the parlance of functional programming,
<code>integrate</code> is a <a href="https://adv-r.hadley.nz/functionals.html">functional</a>:
a higher-order function that takes a function as argument and returns a
numerical value. One of the themes of this book is how to
make good use of functional (and object oriented) programming features in R
to write clear, expressive and modular code without sacrificing computational
efficiency.</p>
<p>Returning to the specific problem of the computation of an integral, we may ask
what the purpose of Monte Carlo integration is? Apparently we can
just do numerical integration using e.g. <code>integrate</code>. There are at least two
reasons why Monte Carlo integration is sometimes preferable. First, it is straightforward
to implement and often works quite well for multivariate and even high-dimensional
integrals, whereas grid-based numerical integration schemes scale poorly with
the dimension. Second, it does not require that we have an analytic representation
of the density. It is common in statistical applications that we are interested
in the distribution of a statistic, which is a complicated transformation of
data, and whose density is difficult or impossible to find analytically. Yet if
we can just simulate data, we can simulate from the distribution of the statistic,
and we can then use Monte Carlo integration to compute whatever
probability or integral w.r.t. the distribution of the statistic that we are
interested in.</p>
</div>
<div id="large-scale-monte-carlo-methods" class="section level3" number="1.2.3">
<h3>
<span class="header-section-number">1.2.3</span> Large scale Monte Carlo methods<a class="anchor" aria-label="anchor" href="#large-scale-monte-carlo-methods"><i class="fas fa-link"></i></a>
</h3>
<p>Monte Carlo methods are used pervasively in statistics and in many other
sciences today. It is nowadays trivial to
simulate millions of data points from a simple univariate distribution like
the von Mises distribution, and Monte Carlo methods are generally attractive
because they allow us to solve computational problems approximately
in many cases where exact or analytic computations are impossible and alternative
deterministic numerical computations require more adaptation to the specific problem.</p>
<p>Monte Carlo methods are thus really good off-the-shelf methods, but scaling
the methods up can be a challenge and is a contemporary research topic.
For the simulation of multivariate <span class="math inline">\(p\)</span>-dimensional data it can, for
instance, make a big difference whether the algorithms scale linearly in <span class="math inline">\(p\)</span>
or like <span class="math inline">\(O(p^a)\)</span> for some <span class="math inline">\(a &gt; 1\)</span>. Likewise, some Monte Carlo methods (e.g. 
Bayesian computations) depend on a data set of size <span class="math inline">\(n\)</span>, and for large
scale data sets, algorithms that scale like <span class="math inline">\(O(n)\)</span> are really the only
algorithms that can be used in practice.</p>
</div>
</div>
<div id="intro-op" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> Optimization<a class="anchor" aria-label="anchor" href="#intro-op"><i class="fas fa-link"></i></a>
</h2>
<p>The third part of the book is on optimization. This is a huge research field
in itself and the focus of the book is on its relatively narrow application to
parameter estimation in statistics. That is, when a statistical model is
given by a parametrized family of probability distributions, optimization of some
criterion – often the likelihood function – is used to find a model that
fits the data.</p>
<p>Classical and generic optimization algorithms based on first and second order
derivatives can often
be used, but its important to understand how convergence can be measured and monitored,
and how the different algorithms scale with the size of the data set and
the dimension of the parameter space. The choice of the right data structure,
such as sparse matrices, can be pivotal for efficient implementations of
the criterion function that is to be optimized.</p>
<p>For a mixture of von Mises distributions it is possible to compute the likelihood
and optimize it using standard algorithms. However, it can also be
optimized using the EM-algorithm, which is a clever algorithm for
optimizing likelihood functions for models that can be formulated
in terms of latent variables.</p>
<p>This section demonstrates how to fit a mixture of von Mises distributions to the
angle data using the EM-algorithm as implemented in the R package movMF. This
will illustrate some of the many practical choices we have to make when using numerical
optimization such as: the choice of starting value; the stopping criterion;
the precise specification of the steps that the algorithm should use; and
even whether it should use randomized steps.</p>
<div id="the-em-algorithm" class="section level3" number="1.3.1">
<h3>
<span class="header-section-number">1.3.1</span> The EM-algorithm<a class="anchor" aria-label="anchor" href="#the-em-algorithm"><i class="fas fa-link"></i></a>
</h3>
<p>The <code><a href="https://rdrr.io/pkg/movMF/man/movMF.html">movMF()</a></code> function implements the EM-algorithm for mixtures of von Mises
distributions. The code below shows one example call of <code><a href="https://rdrr.io/pkg/movMF/man/movMF.html">movMF()</a></code> with one
of the algorithmic control arguments specified. It is common in R that
such arguments are all bundled together in a single list argument called <code>control</code>.
It can make the function call appear a bit more complicated than it needed to be,
but it allows for easier reuse of the – sometimes fairly long – list of control
arguments. Note that as the movMF package works with data as elements on the
unit circle, the angle data must be transformed using cos and sin.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">psi_circle</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Trig.html">cos</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span><span class="op">)</span><span class="op">)</span>
<span class="va">vM_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/movMF/man/movMF.html">movMF</a></span><span class="op">(</span>
  x <span class="op">=</span> <span class="va">psi_circle</span>, 
  k <span class="op">=</span> <span class="fl">2</span>,              <span class="co"># The number of mixture components</span>
  control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>
    start <span class="op">=</span> <span class="st">"S"</span>       <span class="co"># Determines how starting values are chosen</span>
  <span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>The function <code><a href="https://rdrr.io/pkg/movMF/man/movMF.html">movMF()</a></code> returns <em>an object of class <code>movMF</code></em> as the documentation
says (see <code><a href="https://rdrr.io/pkg/movMF/man/movMF.html">?movMF</a></code>). In this case it means that <code>vM_fit</code> is a list with a class
label <code>movMF</code>, which controls how generic functions work for this particular list.
When the object is printed, for instance, some of the the content of the list
is formatted and printed as follows.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">vM_fit</span></code></pre></div>
<pre><code>## theta:
##        [,1]      [,2]
## 1  3.472846 -1.935807
## 2 -4.508098  3.872847
## alpha:
## [1] 0.5486586 0.4513414
## L:
## [1] 193.3014</code></pre>
<p>What we see above is the estimated <span class="math inline">\(\theta\)</span>-parameters for each of the two mixture
components printed as a matrix, the mixture proportions (<code>alpha</code>) and the
value of the log-likelihood function (<code>L</code>) in the estimated parameters.</p>
<p>We can compare the fitted model to the data using the density function
as implemented above and the parameters estimated by the EM-algorithm.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span>, breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="va">pi</span>, <span class="va">pi</span>, length.out <span class="op">=</span> <span class="fl">15</span><span class="op">)</span>, prob <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/rug.html">rug</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">phipsi</span><span class="op">$</span><span class="va">psi</span>, bw <span class="op">=</span> <span class="st">"SJ"</span>, cut <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span>col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="fu">dvM</span><span class="op">(</span><span class="va">x</span>, <span class="va">vM_fit</span><span class="op">$</span><span class="va">theta</span>, <span class="va">vM_fit</span><span class="op">$</span><span class="va">alpha</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>, add <span class="op">=</span> <span class="cn">TRUE</span>, col <span class="op">=</span> <span class="st">"blue"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="CSwR_files/figure-html/vMfitHist-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>As we can see from the figure, this looks like a fairly good fit to the data, and
this is reassuring for two reasons. First, it shows that the two-component mixture
of von Mises distributions is a good model.
This is a statistical reassurance. Second, it shows that the optimization over
the parameter space found a good fit. This is a numerical reassurance. If
we optimize over a parameter space and subsequently find that the model
does not fit the data well, it is either because the numerical optimization
failed or because the model is wrong.</p>
<p>Numerical optimization can fail for a number of reasons. For once, the algorithm
can get stuck in a local optimum, but it can also stop prematurely either because
the maximal number of iterations is reached or because the stopping criterion
is fulfilled (even though the algorithm has not reached an optimum). Some information
related to the two last problems can be gathered from the algorithm
itself, and for <code><a href="https://rdrr.io/pkg/movMF/man/movMF.html">movMF()</a></code> such information is found in a list entry of <code>vM_fit</code>.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">vM_fit</span><span class="op">$</span><span class="va">details</span></code></pre></div>
<pre><code>## $reltol
## [1] 1.490116e-08
## 
## $iter
##    iter maxiter 
##      16     100 
## 
## $logLiks
## [1] 193.3014
## 
## $E
## [1] "softmax"
## 
## $kappa
## NULL
## 
## $minalpha
## [1] 0
## 
## $converge
## [1] TRUE</code></pre>
<p>We see that the number of iterations used was 16 (and less than the maximal
number of 100), and the stopping criterion (small relative improvement)
was active (<code>converge</code> is <code>TRUE</code>, if <code>FALSE</code> the algorithm is run for a
fixed number of iterations). The tolerance parameter used for the
stopping criterion was <span class="math inline">\(1.49 \times 10^{-8}\)</span>.</p>
<p>The small relative improvement criterion for stopping in iteration <span class="math inline">\(n\)</span> is
<span class="math display">\[ |L(\theta_{n-1}) - L(\theta_n)| &lt; \varepsilon (|L(\theta_{n-1})| + \varepsilon)\]</span>
where <span class="math inline">\(L\)</span> is the log-likelihood and <span class="math inline">\(\varepsilon\)</span> is the tolerance parameter above.
The default tolerance parameter is the square root of the <a href="https://en.wikipedia.org/wiki/Machine_epsilon">machine epsilon</a>, see also <code><a href="https://rdrr.io/r/base/zMachine.html">?.Machine</a></code>.
This is a commonly encountered default for a “small-but-not-too-small-number,”
but outside of numerical differentiation this default may not be supported by
much theory.</p>
<p>By choosing different control arguments we can change how the numerical
optimization proceeds. A different method for setting the
starting value is chosen below, which contains a random component. Here we consider
the results in four different runs of the entire algorithm with different random
starting values. We also decrease the number of maximal
iterations to 10 and make the algorithm print out information about its progress
along the way.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">vM_control</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>
    verbose <span class="op">=</span> <span class="cn">TRUE</span>,   <span class="co"># Print output showing algorithmic progress</span>
    maxiter <span class="op">=</span> <span class="fl">10</span>,     
    nruns <span class="op">=</span> <span class="fl">4</span>         <span class="co"># Effectively 4 runs with randomized starting values</span>
  <span class="op">)</span>
<span class="va">vM_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/movMF/man/movMF.html">movMF</a></span><span class="op">(</span><span class="va">psi_circle</span>, <span class="fl">2</span>, control <span class="op">=</span> <span class="va">vM_control</span><span class="op">)</span>
<span class="co">## Run: 1</span>
<span class="co">## Iteration: 0 *** L: 158.572</span>
<span class="co">## Iteration: 1 *** L: 190.999</span>
<span class="co">## Iteration: 2 *** L: 193.118</span>
<span class="co">## Iteration: 3 *** L: 193.238</span>
<span class="co">## Iteration: 4 *** L: 193.279</span>
<span class="co">## Iteration: 5 *** L: 193.293</span>
<span class="co">## Iteration: 6 *** L: 193.299</span>
<span class="co">## Iteration: 7 *** L: 193.3</span>
<span class="co">## Iteration: 8 *** L: 193.301</span>
<span class="co">## Iteration: 9 *** L: 193.301</span>
<span class="co">## Run: 2</span>
<span class="co">## Iteration: 0 *** L: 148.59</span>
<span class="co">## Iteration: 1 *** L: 188.737</span>
<span class="co">## Iteration: 2 *** L: 192.989</span>
<span class="co">## Iteration: 3 *** L: 193.197</span>
<span class="co">## Iteration: 4 *** L: 193.264</span>
<span class="co">## Iteration: 5 *** L: 193.288</span>
<span class="co">## Iteration: 6 *** L: 193.297</span>
<span class="co">## Iteration: 7 *** L: 193.3</span>
<span class="co">## Iteration: 8 *** L: 193.301</span>
<span class="co">## Iteration: 9 *** L: 193.301</span>
<span class="co">## Run: 3</span>
<span class="co">## Iteration: 0 *** L: 168.643</span>
<span class="co">## Iteration: 1 *** L: 189.946</span>
<span class="co">## Iteration: 2 *** L: 192.272</span>
<span class="co">## Iteration: 3 *** L: 192.914</span>
<span class="co">## Iteration: 4 *** L: 193.157</span>
<span class="co">## Iteration: 5 *** L: 193.249</span>
<span class="co">## Iteration: 6 *** L: 193.282</span>
<span class="co">## Iteration: 7 *** L: 193.295</span>
<span class="co">## Iteration: 8 *** L: 193.299</span>
<span class="co">## Iteration: 9 *** L: 193.301</span>
<span class="co">## Run: 4</span>
<span class="co">## Iteration: 0 *** L: 4.43876</span>
<span class="co">## Iteration: 1 *** L: 5.33851</span>
<span class="co">## Iteration: 2 *** L: 6.27046</span>
<span class="co">## Iteration: 3 *** L: 8.54826</span>
<span class="co">## Iteration: 4 *** L: 14.4429</span>
<span class="co">## Iteration: 5 *** L: 29.0476</span>
<span class="co">## Iteration: 6 *** L: 61.3225</span>
<span class="co">## Iteration: 7 *** L: 116.819</span>
<span class="co">## Iteration: 8 *** L: 172.812</span>
<span class="co">## Iteration: 9 *** L: 190.518</span></code></pre></div>
<p>In all four cases it appears that the algorithm is approaching the same value of
the log-likelihood as what we found above, though the last run starts out in a
much lower value and takes more iterations to reach a large log-likelihood value.
Note also that in all runs the log-likelihood is increasing. It is a feature of
the EM-algorithm that every step of the algorithm will increase the likelihood.</p>
<p>Variations of the EM-algorithm are possible, like in <code>movMF</code> where the
control argument <code>E</code> determines the so-called E-step of the algorithm.
Here the default (<code>softmax</code>)
gives the actual EM-algorithm whereas <code>hardmax</code> and <code>stochmax</code> give alternatives.
While the alternatives do not guarantee that the log-likelihood increases in every
iteration they can be numerically beneficial. We rerun the algorithm from the
same four starting values as above but using <code>hardmax</code> as the E-step. Note
how we can reuse the control list by just adding a single element to it.</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">vM_control</span><span class="op">$</span><span class="va">E</span> <span class="op">&lt;-</span> <span class="st">"hardmax"</span>
<span class="va">vM_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/movMF/man/movMF.html">movMF</a></span><span class="op">(</span><span class="va">psi_circle</span>, <span class="fl">2</span>, control <span class="op">=</span> <span class="va">vM_control</span><span class="op">)</span>
<span class="co">## Run: 1</span>
<span class="co">## Iteration: 0 *** L: 158.572</span>
<span class="co">## Iteration: 1 *** L: 193.052</span>
<span class="co">## Iteration: 2 *** L: 193.052</span>
<span class="co">## Run: 2</span>
<span class="co">## Iteration: 0 *** L: 148.59</span>
<span class="co">## Iteration: 1 *** L: 192.443</span>
<span class="co">## Iteration: 2 *** L: 192.812</span>
<span class="co">## Iteration: 3 *** L: 193.052</span>
<span class="co">## Iteration: 4 *** L: 193.052</span>
<span class="co">## Run: 3</span>
<span class="co">## Iteration: 0 *** L: 168.643</span>
<span class="co">## Iteration: 1 *** L: 191.953</span>
<span class="co">## Iteration: 2 *** L: 192.812</span>
<span class="co">## Iteration: 3 *** L: 193.052</span>
<span class="co">## Iteration: 4 *** L: 193.052</span>
<span class="co">## Run: 4</span>
<span class="co">## Iteration: 0 *** L: 4.43876</span>
<span class="co">## Iteration: 1 *** L: 115.34</span>
<span class="co">## Iteration: 2 *** L: 191.839</span>
<span class="co">## Iteration: 3 *** L: 192.912</span>
<span class="co">## Iteration: 4 *** L: 192.912</span></code></pre></div>
<p>It is striking that all runs now stopped before the maximal number of iterations
was reached, and run four is particularly noteworthy as it jumps from its low
starting value to above 190 in just two iterations. However, <code>hardmax</code> is a
heuristic algorithm, whose fixed points are not necessarily stationary points
of the log-likelihood. We can also see above that all four runs stopped at
values that are clearly smaller than the log-likelihood of about 193.30 that
was reached using the real EM-algorithm. Whether this is a problem from a
statistical viewpoint is a different matter; using <code>hardmax</code> could give
an estimator that is just as efficient as the maximum likelihood estimator.</p>
<p>Which optimization algorithm should we then use? This is in general a very
difficult question to answer, and it is non-trivial to correctly assess
which algorithm is “the best.” As the application of <code><a href="https://rdrr.io/pkg/movMF/man/movMF.html">movMF()</a></code> above illustrates,
optimization algorithms may have a number of different parameters
that can be tweaked, and when it comes to actually implementing an optimization
algorithm we need to: make it easy to tweak parameters; investigate and
quantify the effect of the parameters on the algorithm; and
choose sensible defaults. Without this work it is impossible to have a
meaningful discussion about the benefits and deficits of various algorithms.</p>
</div>
<div id="large-scale-optimization" class="section level3" number="1.3.2">
<h3>
<span class="header-section-number">1.3.2</span> Large scale optimization<a class="anchor" aria-label="anchor" href="#large-scale-optimization"><i class="fas fa-link"></i></a>
</h3>
<p>Numerical optimization is the tool that has made statistical models useful for
real data analysis – most importantly by making it possible to compute maximum
likelihood estimators in practice. This works very well when the model can be given
a parametrization with a concave log-likelihood, while optimization of more
complicated log-likelihood surfaces can be numerically challenging and lead to
statistically dubious results.</p>
<p>In contemporary statistics and machine learning, numerical optimization
has come to play an even more important role as structural model constraints
are now often also part of the optimization, for instance via penalty terms in the
criterion function. Instead of working with simple models selected for each
specific problem and with few parameters, we use complex models with
thousands or millions of parameters. They are flexible and adaptable but need
careful, regularized optimization to not overfit the data. With large amounts
of data the regularization can be turned down and we can discover aspects of data that
the simple models would never show. However, we need to pay a computational price.</p>
<p>When the dimension of the parameter
space, <span class="math inline">\(p\)</span>, and the number of data points, <span class="math inline">\(n\)</span>, are both large, evaluation of
the log-likelihood or its gradient can become prohibitively costly.
Optimization algorithms based on higher order derivatives are then completely out
of the question, and even if the (penalized) log-likelihood and its gradient
can be computed in <span class="math inline">\(O(pn)\)</span>-operations this may still be too much for an optimization
algorithm that does this in each iteration. Such large scale
optimization problems have spurred a substantial development of stochastic
gradient methods and related stochastic optimization algorithms that only use
parts of data in each iteration, and which are currently the only way to fit sufficiently
complicated models to data.</p>

</div>
</div>
</div>



<div class="chapter-nav">
<div class="prev"><a href="index.html">Preface</a></div>
<div class="next"><a href="density.html"><span class="header-section-number">2</span> Density estimation</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#intro"><span class="header-section-number">1</span> Introduction</a></li>
<li>
<a class="nav-link" href="#intro-smooth"><span class="header-section-number">1.1</span> Smoothing</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#intro-angles"><span class="header-section-number">1.1.1</span> Angle distributions in proteins</a></li>
<li><a class="nav-link" href="#using-ggplot2"><span class="header-section-number">1.1.2</span> Using ggplot2</a></li>
<li><a class="nav-link" href="#changing-the-defaults"><span class="header-section-number">1.1.3</span> Changing the defaults</a></li>
<li><a class="nav-link" href="#multivariate-smoothing"><span class="header-section-number">1.1.4</span> Multivariate methods</a></li>
<li><a class="nav-link" href="#large-scale-smoothing"><span class="header-section-number">1.1.5</span> Large scale smoothing</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#intro-mc"><span class="header-section-number">1.2</span> Monte Carlo methods</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#vM"><span class="header-section-number">1.2.1</span> Univariate von Mises distributions</a></li>
<li><a class="nav-link" href="#mixtures-of-von-mises-distributions"><span class="header-section-number">1.2.2</span> Mixtures of von Mises distributions</a></li>
<li><a class="nav-link" href="#large-scale-monte-carlo-methods"><span class="header-section-number">1.2.3</span> Large scale Monte Carlo methods</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#intro-op"><span class="header-section-number">1.3</span> Optimization</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-em-algorithm"><span class="header-section-number">1.3.1</span> The EM-algorithm</a></li>
<li><a class="nav-link" href="#large-scale-optimization"><span class="header-section-number">1.3.2</span> Large scale optimization</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/nielsrhansen/CSwR/blob/master/01-intro.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/nielsrhansen/CSwR/edit/master/01-intro.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>
</div>
  

  

</div>
 <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Computational Statistics with R</strong>" was written by Niels Richard Hansen. It was last built on 2021-10-12, Git version: 6b05821.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>

<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.2 Descent direction algorithms | Computational Statistics with R</title>
  <meta name="description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="7.2 Descent direction algorithms | Computational Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.2 Descent direction algorithms | Computational Statistics with R" />
  
  <meta name="twitter:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

<meta name="author" content="Niels Richard Hansen" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="7-1-algorithms-and-convergence.html"/>
<link rel="next" href="7-3-Newton.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-intro.html"><a href="1-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html"><i class="fa fa-check"></i><b>1.1</b> Smoothing</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#intro-angles"><i class="fa fa-check"></i><b>1.1.1</b> Angle distributions in proteins</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#using-ggplot2"><i class="fa fa-check"></i><b>1.1.2</b> Using ggplot2</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#changing-the-defaults"><i class="fa fa-check"></i><b>1.1.3</b> Changing the defaults</a></li>
<li class="chapter" data-level="1.1.4" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#multivariate-smoothing"><i class="fa fa-check"></i><b>1.1.4</b> Multivariate methods</a></li>
<li class="chapter" data-level="1.1.5" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#large-scale-smoothing"><i class="fa fa-check"></i><b>1.1.5</b> Large scale smoothing</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#vM"><i class="fa fa-check"></i><b>1.2.1</b> Univariate von Mises distributions</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#mixtures-of-von-mises-distributions"><i class="fa fa-check"></i><b>1.2.2</b> Mixtures of von Mises distributions</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#large-scale-monte-carlo-methods"><i class="fa fa-check"></i><b>1.2.3</b> Large scale Monte Carlo methods</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-optimization.html"><a href="1-3-optimization.html"><i class="fa fa-check"></i><b>1.3</b> Optimization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-optimization.html"><a href="1-3-optimization.html#the-em-algorithm"><i class="fa fa-check"></i><b>1.3.1</b> The EM-algorithm</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-3-optimization.html"><a href="1-3-optimization.html#large-scale-optimization"><i class="fa fa-check"></i><b>1.3.2</b> Large scale optimization</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Smoothing</b></span></li>
<li class="chapter" data-level="2" data-path="2-density.html"><a href="2-density.html"><i class="fa fa-check"></i><b>2</b> Density estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-unidens.html"><a href="2-1-unidens.html"><i class="fa fa-check"></i><b>2.1</b> Univariate density estimation</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html"><i class="fa fa-check"></i><b>2.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#benchmarking"><i class="fa fa-check"></i><b>2.2.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html"><i class="fa fa-check"></i><b>2.3</b> Bandwidth selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#rectangular"><i class="fa fa-check"></i><b>2.3.1</b> Revisiting the rectangular kernel</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#ise-mise-and-mse-for-kernel-estimators"><i class="fa fa-check"></i><b>2.3.2</b> ISE, MISE and MSE for kernel estimators</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#plug-in-estimation-of-the-oracle-bandwidth"><i class="fa fa-check"></i><b>2.3.3</b> Plug-in estimation of the oracle bandwidth</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#cv"><i class="fa fa-check"></i><b>2.3.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-likelihood.html"><a href="2-4-likelihood.html"><i class="fa fa-check"></i><b>2.4</b> Likelihood considerations</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-4-likelihood.html"><a href="2-4-likelihood.html#sieves"><i class="fa fa-check"></i><b>2.4.1</b> Method of sieves</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-4-likelihood.html"><a href="2-4-likelihood.html#basis-density"><i class="fa fa-check"></i><b>2.4.2</b> Basis expansions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-5-density-ex.html"><a href="2-5-density-ex.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="2-5-density-ex.html"><a href="2-5-density-ex.html#kernel-density-estimation"><i class="fa fa-check"></i>Kernel density estimation</a></li>
<li class="chapter" data-level="" data-path="2-5-density-ex.html"><a href="2-5-density-ex.html#benchmarking-1"><i class="fa fa-check"></i>Benchmarking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-bivariate.html"><a href="3-bivariate.html"><i class="fa fa-check"></i><b>3</b> Bivariate smoothing</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html"><i class="fa fa-check"></i><b>3.1</b> Nearest neighbor smoothers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#linear-smoothers"><i class="fa fa-check"></i><b>3.1.1</b> Linear smoothers</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#implementing-the-running-mean"><i class="fa fa-check"></i><b>3.1.2</b> Implementing the running mean</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#choose-k-by-cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Choose <span class="math inline">\(k\)</span> by cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html"><i class="fa fa-check"></i><b>3.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html#nadarayawatson-kernel-smoothing"><i class="fa fa-check"></i><b>3.2.1</b> Nadaraya–Watson kernel smoothing</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html#local-regression-smoothers"><i class="fa fa-check"></i><b>3.2.2</b> Local regression smoothers</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-sparse-linear-algebra.html"><a href="3-3-sparse-linear-algebra.html"><i class="fa fa-check"></i><b>3.3</b> Sparse linear algebra</a></li>
<li class="chapter" data-level="3.4" data-path="3-4-onb.html"><a href="3-4-onb.html"><i class="fa fa-check"></i><b>3.4</b> Orthogonal basis expansions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-4-onb.html"><a href="3-4-onb.html#polynomial-expansions"><i class="fa fa-check"></i><b>3.4.1</b> Polynomial expansions</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-4-onb.html"><a href="3-4-onb.html#fourier-expansions"><i class="fa fa-check"></i><b>3.4.2</b> Fourier expansions</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-5-splines.html"><a href="3-5-splines.html"><i class="fa fa-check"></i><b>3.5</b> Splines</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-5-splines.html"><a href="3-5-splines.html#smoothing-splines"><i class="fa fa-check"></i><b>3.5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-5-splines.html"><a href="3-5-splines.html#splines-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Splines in R</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-5-splines.html"><a href="3-5-splines.html#efficient-splines"><i class="fa fa-check"></i><b>3.5.3</b> Efficient computation with splines</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-6-gaussian-processes.html"><a href="3-6-gaussian-processes.html"><i class="fa fa-check"></i><b>3.6</b> Gaussian processes</a></li>
<li class="chapter" data-level="3.7" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html"><i class="fa fa-check"></i><b>3.7</b> The Kalman filter</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#ar1-example"><i class="fa fa-check"></i><b>3.7.1</b> AR(1)-example</a></li>
<li class="chapter" data-level="3.7.2" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#the-kalman-smoother"><i class="fa fa-check"></i><b>3.7.2</b> The Kalman smoother</a></li>
<li class="chapter" data-level="3.7.3" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#implementation"><i class="fa fa-check"></i><b>3.7.3</b> Implementation</a></li>
<li class="chapter" data-level="3.7.4" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#the-kalman-filter-1"><i class="fa fa-check"></i><b>3.7.4</b> The Kalman filter</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3-8-bivariate-ex.html"><a href="3-8-bivariate-ex.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="3-8-bivariate-ex.html"><a href="3-8-bivariate-ex.html#nearest-neighbors"><i class="fa fa-check"></i>Nearest neighbors</a></li>
<li class="chapter" data-level="" data-path="3-8-bivariate-ex.html"><a href="3-8-bivariate-ex.html#kernel-estimators"><i class="fa fa-check"></i>Kernel estimators</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Monte Carlo Methods</b></span></li>
<li class="chapter" data-level="4" data-path="4-univariate-random-variables.html"><a href="4-univariate-random-variables.html"><i class="fa fa-check"></i><b>4</b> Univariate random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html"><i class="fa fa-check"></i><b>4.1</b> Pseudorandom number generators</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html#implementing-a-pseudorandom-number-generator"><i class="fa fa-check"></i><b>4.1.1</b> Implementing a pseudorandom number generator</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html#pseudorandom-number-packages"><i class="fa fa-check"></i><b>4.1.2</b> Pseudorandom number packages</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-2-transformation-techniques.html"><a href="4-2-transformation-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Transformation techniques</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-2-transformation-techniques.html"><a href="4-2-transformation-techniques.html#sampling-from-a-t-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Sampling from a <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html#vMsim"><i class="fa fa-check"></i><b>4.3.1</b> von Mises distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html"><i class="fa fa-check"></i><b>4.4</b> Adaptive envelopes</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html#beta-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Beta distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html#von-mises-distribution"><i class="fa fa-check"></i><b>4.4.2</b> von Mises distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-5-univariate-ex.html"><a href="4-5-univariate-ex.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a><ul>
<li class="chapter" data-level="4.5.1" data-path="4-5-univariate-ex.html"><a href="4-5-univariate-ex.html#rejection-sampling-of-gaussian-random-variables"><i class="fa fa-check"></i><b>4.5.1</b> Rejection sampling of Gaussian random variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-mci.html"><a href="5-mci.html"><i class="fa fa-check"></i><b>5</b> Monte Carlo integration</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-assessment.html"><a href="5-1-assessment.html"><i class="fa fa-check"></i><b>5.1</b> Assessment</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-1-assessment.html"><a href="5-1-assessment.html#CLT-gamma"><i class="fa fa-check"></i><b>5.1.1</b> Using the central limit theorem</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-1-assessment.html"><a href="5-1-assessment.html#concentration-inequalities"><i class="fa fa-check"></i><b>5.1.2</b> Concentration inequalities</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-1-assessment.html"><a href="5-1-assessment.html#exponential-tail-bound-for-gamma-distributed-variables"><i class="fa fa-check"></i><b>5.1.3</b> Exponential tail bound for Gamma distributed variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html"><i class="fa fa-check"></i><b>5.2</b> Importance sampling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html#unknown-normalization-constants"><i class="fa fa-check"></i><b>5.2.1</b> Unknown normalization constants</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html#hd-int"><i class="fa fa-check"></i><b>5.2.2</b> Computing a high-dimensional integral</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-3-network.html"><a href="5-3-network.html"><i class="fa fa-check"></i><b>5.3</b> Network failure</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-3-network.html"><a href="5-3-network.html#object-oriented-implementations"><i class="fa fa-check"></i><b>5.3.1</b> Object oriented implementations</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part III: Optimization</b></span></li>
<li class="chapter" data-level="6" data-path="6-four-examples.html"><a href="6-four-examples.html"><i class="fa fa-check"></i><b>6</b> Four Examples</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html"><i class="fa fa-check"></i><b>6.1</b> Exponential families</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#full-exponential-families"><i class="fa fa-check"></i><b>6.1.1</b> Full exponential families</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#bayes-net"><i class="fa fa-check"></i><b>6.1.2</b> Exponential family Bayesian networks</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#exp-fam-deriv"><i class="fa fa-check"></i><b>6.1.3</b> Likelihood computations</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#curved-exponential-families"><i class="fa fa-check"></i><b>6.1.4</b> Curved exponential families</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-2-multinomial-models.html"><a href="6-2-multinomial-models.html"><i class="fa fa-check"></i><b>6.2</b> Multinomial models</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-2-multinomial-models.html"><a href="6-2-multinomial-models.html#pep-moth"><i class="fa fa-check"></i><b>6.2.1</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-3-regression.html"><a href="6-3-regression.html"><i class="fa fa-check"></i><b>6.3</b> Regression models</a></li>
<li class="chapter" data-level="6.4" data-path="6-4-finite-mixture-models.html"><a href="6-4-finite-mixture-models.html"><i class="fa fa-check"></i><b>6.4</b> Finite mixture models</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-4-finite-mixture-models.html"><a href="6-4-finite-mixture-models.html#Gaus-mix-ex"><i class="fa fa-check"></i><b>6.4.1</b> Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-5-mixed-models.html"><a href="6-5-mixed-models.html"><i class="fa fa-check"></i><b>6.5</b> Mixed models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-numopt.html"><a href="7-numopt.html"><i class="fa fa-check"></i><b>7</b> Numerical optimization</a><ul>
<li class="chapter" data-level="7.1" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html"><i class="fa fa-check"></i><b>7.1</b> Algorithms and convergence</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#descent-algorithms"><i class="fa fa-check"></i><b>7.1.1</b> Descent algorithms</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#maps-and-fixed-points"><i class="fa fa-check"></i><b>7.1.2</b> Maps and fixed points</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#convergence-rate"><i class="fa fa-check"></i><b>7.1.3</b> Convergence rate</a></li>
<li class="chapter" data-level="7.1.4" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#stopping-criteria"><i class="fa fa-check"></i><b>7.1.4</b> Stopping criteria</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html"><i class="fa fa-check"></i><b>7.2</b> Descent direction algorithms</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#line-search"><i class="fa fa-check"></i><b>7.2.1</b> Line search</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>7.2.2</b> Gradient descent</a></li>
<li class="chapter" data-level="7.2.3" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#conjugate-gradients"><i class="fa fa-check"></i><b>7.2.3</b> Conjugate gradients</a></li>
<li class="chapter" data-level="7.2.4" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#pep-moth-descent"><i class="fa fa-check"></i><b>7.2.4</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-3-Newton.html"><a href="7-3-Newton.html"><i class="fa fa-check"></i><b>7.3</b> Newton-type algorithms</a><ul>
<li class="chapter" data-level="7.3.1" data-path="7-3-Newton.html"><a href="7-3-Newton.html#poisson-regression"><i class="fa fa-check"></i><b>7.3.1</b> Poisson regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="7-3-Newton.html"><a href="7-3-Newton.html#quasi-newton-algorithms"><i class="fa fa-check"></i><b>7.3.2</b> Quasi-Newton algorithms</a></li>
<li class="chapter" data-level="7.3.3" data-path="7-3-Newton.html"><a href="7-3-Newton.html#sparsity"><i class="fa fa-check"></i><b>7.3.3</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7-4-misc-.html"><a href="7-4-misc-.html"><i class="fa fa-check"></i><b>7.4</b> Misc.</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-em.html"><a href="8-em.html"><i class="fa fa-check"></i><b>8</b> Expectation maximization algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html"><i class="fa fa-check"></i><b>8.1</b> Basic properties</a><ul>
<li class="chapter" data-level="8.1.1" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#incomplete-data-likelihood"><i class="fa fa-check"></i><b>8.1.1</b> Incomplete data likelihood</a></li>
<li class="chapter" data-level="8.1.2" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#monotonicity-of-the-em-algorithm"><i class="fa fa-check"></i><b>8.1.2</b> Monotonicity of the EM algorithm</a></li>
<li class="chapter" data-level="8.1.3" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#peppered-moths"><i class="fa fa-check"></i><b>8.1.3</b> Peppered moths</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8-2-EM-exp.html"><a href="8-2-EM-exp.html"><i class="fa fa-check"></i><b>8.2</b> Exponential families</a></li>
<li class="chapter" data-level="8.3" data-path="8-3-fisher-information.html"><a href="8-3-fisher-information.html"><i class="fa fa-check"></i><b>8.3</b> Fisher information</a></li>
<li class="chapter" data-level="8.4" data-path="8-4-revisiting-gaussian-mixtures.html"><a href="8-4-revisiting-gaussian-mixtures.html"><i class="fa fa-check"></i><b>8.4</b> Revisiting Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-StochOpt.html"><a href="9-StochOpt.html"><i class="fa fa-check"></i><b>9</b> Stochastic Optimization</a><ul>
<li class="chapter" data-level="9.1" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html"><i class="fa fa-check"></i><b>9.1</b> Stochastic gradient algorithms</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#section"><i class="fa fa-check"></i><b>9.1.1</b> </a></li>
<li class="chapter" data-level="9.1.2" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#online-stochastic-gradient-descent"><i class="fa fa-check"></i><b>9.1.2</b> Online stochastic gradient descent</a></li>
<li class="chapter" data-level="9.1.3" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>9.1.3</b> Stochastic gradient descent</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9-2-nonlinear-least-squares.html"><a href="9-2-nonlinear-least-squares.html"><i class="fa fa-check"></i><b>9.2</b> Nonlinear least squares</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-app-R.html"><a href="A-app-R.html"><i class="fa fa-check"></i><b>A</b> R programming</a><ul>
<li class="chapter" data-level="A.1" data-path="A-1-functions.html"><a href="A-1-functions.html"><i class="fa fa-check"></i><b>A.1</b> Functions</a><ul>
<li class="chapter" data-level="A.1.1" data-path="A-1-functions.html"><a href="A-1-functions.html#vectorization"><i class="fa fa-check"></i><b>A.1.1</b> Vectorization</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="A-2-objects-and-methods.html"><a href="A-2-objects-and-methods.html"><i class="fa fa-check"></i><b>A.2</b> Objects and methods</a></li>
<li class="chapter" data-level="A.3" data-path="A-3-environments.html"><a href="A-3-environments.html"><i class="fa fa-check"></i><b>A.3</b> Environments</a><ul>
<li class="chapter" data-level="A.3.1" data-path="A-3-environments.html"><a href="A-3-environments.html#function-factories"><i class="fa fa-check"></i><b>A.3.1</b> Function factories</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="A-4-performance.html"><a href="A-4-performance.html"><i class="fa fa-check"></i><b>A.4</b> Performance</a><ul>
<li class="chapter" data-level="A.4.1" data-path="A-4-performance.html"><a href="A-4-performance.html#parallel-computations"><i class="fa fa-check"></i><b>A.4.1</b> Parallel computations</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html"><i class="fa fa-check"></i><b>A.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#functions-1"><i class="fa fa-check"></i>Functions</a></li>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#histograms-with-non-equidistant-breaks"><i class="fa fa-check"></i>Histograms with non-equidistant breaks</a></li>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#functions-and-objects"><i class="fa fa-check"></i>Functions and objects</a></li>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#functions-and-environments"><i class="fa fa-check"></i>Functions and environments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="descent-direction-algorithms" class="section level2">
<h2><span class="header-section-number">7.2</span> Descent direction algorithms</h2>
<p>The negative gradient of <span class="math inline">\(H\)</span> in <span class="math inline">\(\theta\)</span> is the direction of steepest descent.
Starting from <span class="math inline">\(\theta_0\)</span> and with the goal of minimizing <span class="math inline">\(H\)</span>, it is natural
to move away from <span class="math inline">\(\theta_0\)</span> in the direction of <span class="math inline">\(-\nabla H(\theta_0)\)</span>.
Thus we could define<br />
<span class="math display">\[\theta_1 = \theta_0 - \gamma \nabla H(\theta_0)\]</span>
for a suitably chosen <span class="math inline">\(\gamma &gt; 0\)</span>. By Taylor’s theorem
<span class="math display">\[H(\theta_1) = H(\theta_0) - \gamma \|\nabla H(\theta_0)\|^2_2 + o(\gamma),\]</span>
which means that if <span class="math inline">\(\theta_0\)</span> is not a stationary point (<span class="math inline">\(\nabla H(\theta_0) \neq 0\)</span>)
then
<span class="math display">\[H(\theta_1) &lt; H(\theta_0)\]</span>
for <span class="math inline">\(\gamma\)</span> small enough.</p>
<p>More generally, we define a <em>descent direction</em> in <span class="math inline">\(\theta_0\)</span>
as a vector <span class="math inline">\(\rho_0 \in \mathbb{R}^p\)</span> such that
<span class="math display">\[\nabla H(\theta_0)^T \rho_0 &lt; 0.\]</span>
By the same kind of Taylor argument as above, <span class="math inline">\(H\)</span> will descent for a sufficiently
small step size in the direction of any descent direction. And if <span class="math inline">\(\theta_0\)</span>
is not a stationary point, <span class="math inline">\(-\nabla H(\theta_0)^T\)</span> is a descent direction.</p>
<p>One strategy for choosing <span class="math inline">\(\gamma\)</span> is to minimize the univariate
function
<span class="math display">\[\gamma \mapsto H(\theta_0 + \gamma \rho_0),\]</span>
which is an example of a <em>line search</em> method. Such a minimization
would give the maximal possible descent in the direction <span class="math inline">\(\rho_0\)</span>,
and as we have argued, if <span class="math inline">\(\rho_0\)</span> is a descent direction, a minimizer <span class="math inline">\(\gamma &gt; 0\)</span>
guarantees descent of <span class="math inline">\(H\)</span>. However, unless the minimization can be
done analytically it is often computationally too expensive.
Less will also do, and as shown in Example <a href="7-1-algorithms-and-convergence.html#exm:grad-descent">7.1</a>,
if the Hessian has uniformly bounded numerical radius it is possible to
fix one (sufficiently small) step length that will guarantee descent.</p>
<div id="line-search" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Line search</h3>
<p>We consider algorithms of the form
<span class="math display">\[\theta_{n+1} = \theta_n + \gamma_{n} \rho_n\]</span>
for descent directions <span class="math inline">\(\rho_n\)</span> and starting in <span class="math inline">\(\theta_0\)</span>.
The step lengths, <span class="math inline">\(\gamma_n\)</span>, are chosen so as to give
sufficient descent in each iteration.</p>
<p>We let <span class="math inline">\(h(\gamma) = H(\theta_{n} + \gamma \rho_{n})\)</span>
denote the univariate and differentiable function of <span class="math inline">\(\gamma\)</span>,
<span class="math display">\[h : [0,\infty) \to \mathbb{R},\]</span>
that gives the value of <span class="math inline">\(H\)</span> in the direction of the descent direction
<span class="math inline">\(\rho_n\)</span>. We can observe that
<span class="math display">\[h&#39;(\gamma) = \nabla H(\theta_{n} + \gamma \rho_{n})^T \rho_{n},\]</span>
and maximal descent in direction <span class="math inline">\(\rho_n\)</span> can be found by solving
<span class="math inline">\(h&#39;(\gamma) = 0\)</span> for <span class="math inline">\(\gamma\)</span>. As mentioned above, less will do. First note
that
<span class="math display">\[h&#39;(0) = \nabla H(\theta_{n})^T \rho_{n} &lt; 0,\]</span>
so <span class="math inline">\(h\)</span> has a negative slope in <span class="math inline">\(0\)</span>. It descents in a sufficiently
small interval <span class="math inline">\([0, \varepsilon)\)</span>, and it is even true that for any <span class="math inline">\(c \in (0, 1)\)</span>
there is an <span class="math inline">\(\varepsilon &gt; 0\)</span> such that
<span class="math display">\[h(\gamma) \leq h(0) + c \gamma h&#39;(0)\]</span>
for <span class="math inline">\(\gamma \in [0, \varepsilon)\)</span>. We note that this inequality can
be checked easily for any given <span class="math inline">\(\gamma &gt; 0\)</span>, and is known as the
<em>sufficient descent</em> condition. Sufficient descent is not enough
in itself as the step length could be arbitrarily small, and the algorithm
could effectively get stuck.</p>
<p>To prevent too small steps we can enforce another condition. Very close
to <span class="math inline">\(0\)</span>, <span class="math inline">\(h\)</span> will have almost the same slope, <span class="math inline">\(h&#39;(0)\)</span>, as it has in <span class="math inline">\(0\)</span>. If we
therefore require that the slope in <span class="math inline">\(\gamma\)</span> should be larger than <span class="math inline">\(\tilde{c} h&#39;(0)\)</span>
for some <span class="math inline">\(\tilde{c} \in (0, 1)\)</span>, <span class="math inline">\(\gamma\)</span> is forced away from <span class="math inline">\(0\)</span>. This is
known as the <em>curvature condition</em>.</p>
<p>The combined conditions on <span class="math inline">\(\gamma\)</span>,
<span class="math display">\[h(\gamma) \leq h(0) + c \gamma h&#39;(0)\]</span>
for a <span class="math inline">\(c \in (0, 1)\)</span> and
<span class="math display">\[h&#39;(\gamma) \geq \tilde{c} h&#39;(0)\]</span>
for a <span class="math inline">\(\tilde{c} \in (c, 1)\)</span> are known collectively as
the <em>Wolfe conditions</em>. It can be shown that if <span class="math inline">\(h\)</span> is bounded below there
exists a step length satisfying the Wolfe conditions (Lemma 3.1 in <span class="citation">Nocedal and Wright (<a href="#ref-Nocedal:2006" role="doc-biblioref">2006</a>)</span>).</p>
<p>Even when choosing <span class="math inline">\(\gamma_{n}\)</span> to fulfill
the Wolfe conditions there is no guarantee that <span class="math inline">\(\theta_n\)</span>
will converge let alone converge toward a global minimizer. The best we
can hope for in general is that
<span class="math display">\[\|\nabla H(\theta_n)\|_2 \rightarrow 0\]</span>
for <span class="math inline">\(n \to \infty\)</span>, and this will happen under some relatively weak
conditions on <span class="math inline">\(H\)</span> (Theorem 3.2 <span class="citation">Nocedal and Wright (<a href="#ref-Nocedal:2006" role="doc-biblioref">2006</a>)</span>) under the assumption
that
<span class="math display">\[\frac{\nabla H(\theta_n)^T \rho_n}{\|\nabla H(\theta_n)\|_2 \| \rho_n\|_2} \leq - \delta &lt; 0.\]</span>
That is, the angle between the descent direction and the gradient should be
uniformly bounded away from <span class="math inline">\(90^{\circ}\)</span>.</p>
<p>A practical way of searching for a step length is via <em>backtracking</em>.
Choosing a <span class="math inline">\(\gamma_0\)</span> and a constant <span class="math inline">\(d \in (0, 1)\)</span> we
can search through the sequence of step lengths
<span class="math display">\[\gamma_0, d \gamma_0, d^2 \gamma_0, d^3 \gamma_0, \ldots\]</span>
and stop the first time we find a step length satisfying the Wolfe
conditions.</p>
<p>Using backtracking, we can actually dispense of the curvature condition
and simply check the sufficient descent condition</p>
<p><span class="math display">\[H(\theta_{n} + d^k \gamma_0 \rho_{n}) \leq H(\theta_n) + cd^k \gamma_0 \nabla H(\theta_{n})^T \rho_{n}\]</span></p>
<p>for <span class="math inline">\(c \in (0, 1)\)</span>. The implementation of backtracking requires the choice
of the three parameters: <span class="math inline">\(\gamma_0 &gt; 0\)</span>, <span class="math inline">\(d \in (0, 1)\)</span> and <span class="math inline">\(c \in (0, 1)\)</span>.
A good choice depends quite a lot on the algorithm used for choosing
the descent direction, but choosing <span class="math inline">\(c\)</span> too close to 1 can make the algorithm
take too small steps, and taking <span class="math inline">\(d\)</span> too small can likewise
generate small step lengths. Thus <span class="math inline">\(d = 0.8\)</span> or <span class="math inline">\(d = 0.9\)</span>
and <span class="math inline">\(c = 0.1\)</span> or even smaller are sensible choices. For some algorithms,
like the Newton algorithm to be dealt with below, there is a natural
choice of <span class="math inline">\(\gamma_0 = 1\)</span>. But for other algorithms a good choice depends
crucially on the scale of the parameters, and there is then no general
advice on choosing <span class="math inline">\(\gamma_0\)</span>.</p>
</div>
<div id="gradient-descent" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Gradient descent</h3>
<p>We implement gradient descent with backtracking below as the function <code>GD()</code>.
For gradient descent, the sufficient descent condition amounts to
choosing the smallest <span class="math inline">\(k \geq 0\)</span> such that</p>
<p><span class="math display">\[H(\theta_{n} + d^k \gamma_0 \nabla H(\theta_{n})) \leq H(\theta_n) -  cd^k \gamma_0 \|\nabla H(\theta_{n})\|_2^2.\]</span></p>
<p>The <code>GD()</code> function takes the starting point, <span class="math inline">\(\theta_0\)</span>, the objective function,
<span class="math inline">\(H\)</span>, and its gradient, <span class="math inline">\(\nabla H\)</span>, as arguments. The four parameters <span class="math inline">\(d\)</span>, <span class="math inline">\(c\)</span>, <span class="math inline">\(\gamma_0\)</span> and
<span class="math inline">\(\varepsilon\)</span> that control the algorithm can also be specified as additional arguments,
but are given some reasonable default values. The implementation uses the <em>squared</em> norm of
the gradient as a stopping criterion, but it also has a maximal number of
iterations as a safeguard. Note that if the maximal number is reached, a
warning is printed.</p>
<p>Finally, we include a callback argument (the <code>cb</code> argument).
If a function is passed to this argument, it will be evaluated in each iteration
of the algorithm. This gives us the possibility of logging or
printing values of variables during evaluation, which can be highly useful
for understanding the inner workings of the algorithm. Monitoring or logging
intermediate values during the evaluation of code is referred to as
<em>tracing</em>.</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="7-2-descent-direction-algorithms.html#cb286-1"></a>GD &lt;-<span class="st"> </span><span class="cf">function</span>(</span>
<span id="cb286-2"><a href="7-2-descent-direction-algorithms.html#cb286-2"></a>  par, </span>
<span id="cb286-3"><a href="7-2-descent-direction-algorithms.html#cb286-3"></a>  H,</span>
<span id="cb286-4"><a href="7-2-descent-direction-algorithms.html#cb286-4"></a>  gr,</span>
<span id="cb286-5"><a href="7-2-descent-direction-algorithms.html#cb286-5"></a>  <span class="dt">d =</span> <span class="fl">0.8</span>, </span>
<span id="cb286-6"><a href="7-2-descent-direction-algorithms.html#cb286-6"></a>  <span class="dt">c =</span> <span class="fl">0.1</span>, </span>
<span id="cb286-7"><a href="7-2-descent-direction-algorithms.html#cb286-7"></a>  <span class="dt">gamma0 =</span> <span class="fl">0.01</span>, </span>
<span id="cb286-8"><a href="7-2-descent-direction-algorithms.html#cb286-8"></a>  <span class="dt">epsilon =</span> <span class="fl">1e-4</span>, </span>
<span id="cb286-9"><a href="7-2-descent-direction-algorithms.html#cb286-9"></a>  <span class="dt">maxit =</span> <span class="dv">1000</span>,</span>
<span id="cb286-10"><a href="7-2-descent-direction-algorithms.html#cb286-10"></a>  <span class="dt">cb =</span> <span class="ot">NULL</span></span>
<span id="cb286-11"><a href="7-2-descent-direction-algorithms.html#cb286-11"></a>) {</span>
<span id="cb286-12"><a href="7-2-descent-direction-algorithms.html#cb286-12"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>maxit) {</span>
<span id="cb286-13"><a href="7-2-descent-direction-algorithms.html#cb286-13"></a>    value &lt;-<span class="st"> </span><span class="kw">H</span>(par)</span>
<span id="cb286-14"><a href="7-2-descent-direction-algorithms.html#cb286-14"></a>    grad &lt;-<span class="st"> </span><span class="kw">gr</span>(par)</span>
<span id="cb286-15"><a href="7-2-descent-direction-algorithms.html#cb286-15"></a>    h_prime &lt;-<span class="st"> </span><span class="kw">sum</span>(grad<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb286-16"><a href="7-2-descent-direction-algorithms.html#cb286-16"></a>    <span class="cf">if</span>(<span class="op">!</span><span class="kw">is.null</span>(cb)) <span class="kw">cb</span>()</span>
<span id="cb286-17"><a href="7-2-descent-direction-algorithms.html#cb286-17"></a>    <span class="co"># Convergence criterion based on gradient norm</span></span>
<span id="cb286-18"><a href="7-2-descent-direction-algorithms.html#cb286-18"></a>    <span class="cf">if</span>(h_prime <span class="op">&lt;=</span><span class="st"> </span>epsilon) <span class="cf">break</span></span>
<span id="cb286-19"><a href="7-2-descent-direction-algorithms.html#cb286-19"></a>    gamma &lt;-<span class="st"> </span>gamma0</span>
<span id="cb286-20"><a href="7-2-descent-direction-algorithms.html#cb286-20"></a>    <span class="co"># Proposed descent step</span></span>
<span id="cb286-21"><a href="7-2-descent-direction-algorithms.html#cb286-21"></a>    par1 &lt;-<span class="st"> </span>par <span class="op">-</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>grad</span>
<span id="cb286-22"><a href="7-2-descent-direction-algorithms.html#cb286-22"></a>    <span class="co"># Backtracking while descent is insufficient</span></span>
<span id="cb286-23"><a href="7-2-descent-direction-algorithms.html#cb286-23"></a>    <span class="cf">while</span>(<span class="kw">H</span>(par1) <span class="op">&gt;</span><span class="st"> </span>value <span class="op">-</span><span class="st"> </span>c <span class="op">*</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>h_prime) {</span>
<span id="cb286-24"><a href="7-2-descent-direction-algorithms.html#cb286-24"></a>      gamma &lt;-<span class="st"> </span>d <span class="op">*</span><span class="st"> </span>gamma</span>
<span id="cb286-25"><a href="7-2-descent-direction-algorithms.html#cb286-25"></a>      par1 &lt;-<span class="st"> </span>par <span class="op">-</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>grad</span>
<span id="cb286-26"><a href="7-2-descent-direction-algorithms.html#cb286-26"></a>    }</span>
<span id="cb286-27"><a href="7-2-descent-direction-algorithms.html#cb286-27"></a>    par &lt;-<span class="st"> </span>par1</span>
<span id="cb286-28"><a href="7-2-descent-direction-algorithms.html#cb286-28"></a>  }</span>
<span id="cb286-29"><a href="7-2-descent-direction-algorithms.html#cb286-29"></a>  <span class="cf">if</span>(i <span class="op">==</span><span class="st"> </span>maxit)</span>
<span id="cb286-30"><a href="7-2-descent-direction-algorithms.html#cb286-30"></a>    <span class="kw">warning</span>(<span class="st">&quot;Maximal number, &quot;</span>, maxit, <span class="st">&quot;, of iterations reached&quot;</span>)</span>
<span id="cb286-31"><a href="7-2-descent-direction-algorithms.html#cb286-31"></a>  par</span>
<span id="cb286-32"><a href="7-2-descent-direction-algorithms.html#cb286-32"></a>}</span></code></pre></div>
<p>We will use the Poisson regression example to illustrate the use of
gradient descent and other optimization algorithms, and we need to
implement functions in R for computing the negative log-likelihood and
its gradient.</p>
<p>The implementation below uses a function factory to produce a
list containing a parameter vector, the negative log-likelihood function
and its gradient. We anticipate that for the Newton algorithm in Section
<a href="7-3-Newton.html#Newton">7.3</a> we also need an implementation of the Hessian, which is
thus included here as well.</p>
<p>We will exploit the <code>model.matrix()</code> function to construct the model matrix
from the data via a formula, and the sufficient statistic is also
computed. The implementations use linear algebra and vectorized computations
relying on access to the model matrix and the sufficient statistics in
their enclosing environment.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="7-2-descent-direction-algorithms.html#cb287-1"></a>poisson_model &lt;-<span class="st"> </span><span class="cf">function</span>(form, data, response) {</span>
<span id="cb287-2"><a href="7-2-descent-direction-algorithms.html#cb287-2"></a>  X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(form, data)</span>
<span id="cb287-3"><a href="7-2-descent-direction-algorithms.html#cb287-3"></a>  y &lt;-<span class="st"> </span>data[[response]]</span>
<span id="cb287-4"><a href="7-2-descent-direction-algorithms.html#cb287-4"></a>  <span class="co"># The function drop() drops the dim attribute and turns, for instance,</span></span>
<span id="cb287-5"><a href="7-2-descent-direction-algorithms.html#cb287-5"></a>  <span class="co"># a matrix with one column into a vector</span></span>
<span id="cb287-6"><a href="7-2-descent-direction-algorithms.html#cb287-6"></a>  t_map &lt;-<span class="st"> </span><span class="kw">drop</span>(<span class="kw">crossprod</span>(X, y))  <span class="co"># More efficient than drop(t(X) %*% y)</span></span>
<span id="cb287-7"><a href="7-2-descent-direction-algorithms.html#cb287-7"></a>  n &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)</span>
<span id="cb287-8"><a href="7-2-descent-direction-algorithms.html#cb287-8"></a>  p &lt;-<span class="st"> </span><span class="kw">ncol</span>(X)</span>
<span id="cb287-9"><a href="7-2-descent-direction-algorithms.html#cb287-9"></a>  </span>
<span id="cb287-10"><a href="7-2-descent-direction-algorithms.html#cb287-10"></a>  H &lt;-<span class="st"> </span><span class="cf">function</span>(beta) </span>
<span id="cb287-11"><a href="7-2-descent-direction-algorithms.html#cb287-11"></a>    <span class="kw">drop</span>(<span class="kw">sum</span>(<span class="kw">exp</span>(X <span class="op">%*%</span><span class="st"> </span>beta)) <span class="op">-</span><span class="st"> </span>beta <span class="op">%*%</span><span class="st"> </span>t_map) <span class="op">/</span>n</span>
<span id="cb287-12"><a href="7-2-descent-direction-algorithms.html#cb287-12"></a>  </span>
<span id="cb287-13"><a href="7-2-descent-direction-algorithms.html#cb287-13"></a>  grad_H &lt;-<span class="st"> </span><span class="cf">function</span>(beta) </span>
<span id="cb287-14"><a href="7-2-descent-direction-algorithms.html#cb287-14"></a>    (<span class="kw">drop</span>(<span class="kw">crossprod</span>(X, <span class="kw">exp</span>(X <span class="op">%*%</span><span class="st"> </span>beta))) <span class="op">-</span><span class="st"> </span>t_map) <span class="op">/</span><span class="st"> </span>n</span>
<span id="cb287-15"><a href="7-2-descent-direction-algorithms.html#cb287-15"></a>  </span>
<span id="cb287-16"><a href="7-2-descent-direction-algorithms.html#cb287-16"></a>  Hessian_H &lt;-<span class="st"> </span><span class="cf">function</span>(beta)</span>
<span id="cb287-17"><a href="7-2-descent-direction-algorithms.html#cb287-17"></a>    <span class="kw">crossprod</span>(X, <span class="kw">drop</span>(<span class="kw">exp</span>(X <span class="op">%*%</span><span class="st"> </span>beta)) <span class="op">*</span><span class="st"> </span>X) <span class="op">/</span><span class="st"> </span>n</span>
<span id="cb287-18"><a href="7-2-descent-direction-algorithms.html#cb287-18"></a>  </span>
<span id="cb287-19"><a href="7-2-descent-direction-algorithms.html#cb287-19"></a>  <span class="kw">list</span>(<span class="dt">par =</span> <span class="kw">rep</span>(<span class="dv">0</span>, p), <span class="dt">H =</span> H, <span class="dt">grad_H =</span> grad_H, <span class="dt">Hessian_H =</span> Hessian_H)</span>
<span id="cb287-20"><a href="7-2-descent-direction-algorithms.html#cb287-20"></a>}</span></code></pre></div>
<p>We choose to normalize the log-likelihood by the number of observations <span class="math inline">\(n\)</span> (the number
of rows in the model matrix). This does have a small computational
cost, but the resulting numerical values become less dependent
upon <span class="math inline">\(n\)</span>, which makes it easier to choose sensible default values
of various parameters for the numerical optimization algorithms.
Gradient descent is very slow for the large Poisson model with individual
store effects, so we consider only the simple model with two parameters.</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="7-2-descent-direction-algorithms.html#cb288-1"></a>veg_pois &lt;-<span class="st"> </span><span class="kw">poisson_model</span>(<span class="op">~</span><span class="st"> </span><span class="kw">log</span>(normalSale), vegetables, <span class="dt">response =</span> <span class="st">&quot;sale&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="7-2-descent-direction-algorithms.html#cb289-1"></a>pois_GD &lt;-<span class="st"> </span><span class="kw">GD</span>(veg_pois<span class="op">$</span>par, veg_pois<span class="op">$</span>H, veg_pois<span class="op">$</span>grad_H)</span></code></pre></div>
<p>The gradient descent implementation is tested by comparing the minimizer to
the estimated parameters as computed by <code>glm()</code>.</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="7-2-descent-direction-algorithms.html#cb290-1"></a><span class="kw">rbind</span>(<span class="dt">pois_glm =</span> <span class="kw">coefficients</span>(pois_model_null), pois_GD)</span></code></pre></div>
<pre><code>##          (Intercept) log(normalSale)
## pois_glm    1.461440       0.9215699
## pois_GD     1.460352       0.9219358</code></pre>
<p>We get the same result up to the first two decimals. The convergence
criterion on our gradient descent algorithm was quite loose (<span class="math inline">\(\varepsilon = 10^{-4}\)</span>,
which means that the norm of the gradient is smaller than <span class="math inline">\(10^{-2}\)</span> when
the algorithm stops). This choice of <span class="math inline">\(\varepsilon\)</span> in combination with <span class="math inline">\(\gamma_0 = 0.01\)</span>
implies that the algorithm stops when the gradient is so small that the changes
are at most of norm <span class="math inline">\(10^{-4}\)</span>.</p>
<p>Comparing the resulting values of the negative log-likelihood shows agreement
up to the first five decimals, but we notice that the negative log-likelihood
for the parameters fitted using <code>glm()</code> is just slightly smaller.</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="7-2-descent-direction-algorithms.html#cb292-1"></a>veg_pois<span class="op">$</span><span class="kw">H</span>(<span class="kw">coefficients</span>(pois_model_null))</span>
<span id="cb292-2"><a href="7-2-descent-direction-algorithms.html#cb292-2"></a>veg_pois<span class="op">$</span><span class="kw">H</span>(pois_GD)</span></code></pre></div>
<pre><code>## [1] -124.406827879897
## [1] -124.406825325047</code></pre>
<p>To investigate what actually went on inside the gradient descent
algorithm we will use the callback argument to trace the internals
of a call to <code>GD()</code>. The <code>tracer()</code> function from the <a href="https://github.com/nielsrhansen/CSwR/tree/master/CSwR_package">CSwR package</a>
can be used to construct a tracer object with a <code>tracer()</code> function that we can pass as the callback
argument. The tracer object and its <code>tracer()</code> function work by storing
information in the enclosing environment of <code>tracer()</code>. When used as
the callback argument to e.g. <code>GD()</code> the <code>tracer()</code> function will look up
variables in the evaluation environment of <code>GD()</code>, store them and
print them if requested, and store run time information as well.</p>
<p>When <code>GD()</code> has returned, the trace information can be accessed
via the <code>summary()</code> method for the tracer object. The tracer objects
and their <code>tracer()</code> function should not be confused with the <code>trace()</code>
function from the R base package, but the tracer object’s <code>tracer()</code> function
can be passed as the <code>tracer</code> argument to <code>trace()</code> to interactively
inject tracing code into any R function. Here, the tracer objects will
only be used together with a callback argument.</p>
<p>We use the tracer object with our gradient descent implementation
and print trace information every 50th iteration.</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="7-2-descent-direction-algorithms.html#cb294-1"></a>GD_tracer &lt;-<span class="st"> </span><span class="kw">tracer</span>(<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>, <span class="st">&quot;h_prime&quot;</span>, <span class="st">&quot;gamma&quot;</span>), <span class="dt">N =</span> <span class="dv">50</span>)</span>
<span id="cb294-2"><a href="7-2-descent-direction-algorithms.html#cb294-2"></a>pois_GD &lt;-<span class="st"> </span><span class="kw">GD</span>(veg_pois<span class="op">$</span>par, veg_pois<span class="op">$</span>H, veg_pois<span class="op">$</span>grad_H, <span class="dt">cb =</span> GD_tracer<span class="op">$</span>tracer)</span></code></pre></div>
<pre><code>## n = 1: value = 1; h_prime = 14268.59; gamma = NA; 
## n = 50: value = -123.9395; h_prime = 15.45722; gamma = 0.004096; 
## n = 100: value = -124.3243; h_prime = 3.133931; gamma = 0.00512; 
## n = 150: value = -124.3935; h_prime = 0.601431; gamma = 0.00512; 
## n = 200: value = -124.4048; h_prime = 0.09805907; gamma = 0.00512; 
## n = 250: value = -124.4065; h_prime = 0.0109742; gamma = 0.00512; 
## n = 300: value = -124.4068; h_prime = 0.002375827; gamma = 0.00512; 
## n = 350: value = -124.4068; h_prime = 0.000216502; gamma = 0.004096;</code></pre>
<p>We see that the gradient descent algorithm runs for a little more than 350
iterations, and we can observe how the value of the negative log-likelihood is
descending. We can also see that the step length <span class="math inline">\(\gamma\)</span> bounces between
<span class="math inline">\(0.004096 = 0.8^4 \times 0.01\)</span> and
<span class="math inline">\(0.00512 = 0.8^3 \times 0.01\)</span>, thus the backtracking
takes 3 to 4 iterations to find a step length with sufficient descent.</p>
<p>The printed trace does not reveal the run time information. The
run time is measured for each iteration of the algorithm and the
cumulative run time is of greater interest. This information
can be computed and inspected after the algorithm has converged,
and it is returned by the summary method for tracer objects.</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="7-2-descent-direction-algorithms.html#cb296-1"></a><span class="kw">tail</span>(<span class="kw">summary</span>(GD_tracer))</span></code></pre></div>
<pre><code>##         value      h_prime    gamma      .time
## 372 -124.4068 1.125779e-04 0.005120 0.06352470
## 373 -124.4068 1.218925e-04 0.005120 0.06364027
## 374 -124.4068 1.323878e-04 0.005120 0.06375466
## 375 -124.4068 1.441965e-04 0.005120 0.06387087
## 376 -124.4068 1.574572e-04 0.005120 0.06398542
## 377 -124.4068 7.600796e-05 0.004096 0.06411624</code></pre>
<p>The trace information is stored in a list. The summary method transforms
the trace information into a data frame with one row per iteration. We can also access
individual entries of the list of trace information via subsetting.</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="7-2-descent-direction-algorithms.html#cb298-1"></a>GD_tracer[<span class="dv">377</span>] </span></code></pre></div>
<pre><code>## $value
## [1] -124.4068
## 
## $h_prime
## [1] 7.600796e-05
## 
## $gamma
## [1] 0.004096
## 
## $.time
## [1] 0.000130819</code></pre>
<div class="figure" style="text-align: center"><span id="fig:GD-trace-plot"></span>
<img src="CSwR_files/figure-html/GD-trace-plot-1.png" alt="Gradient norm (left) and value of the negative log-likelihood (right) above the limit value $H(\theta_{\infty})$. The straight line is fitted to the data points except the first ten using least squares, and the rate is computed from this estimate and reported per ms." width="100%" />
<p class="caption">
Figure 7.1: Gradient norm (left) and value of the negative log-likelihood (right) above the limit value <span class="math inline">\(H(\theta_{\infty})\)</span>. The straight line is fitted to the data points except the first ten using least squares, and the rate is computed from this estimate and reported per ms.
</p>
</div>
</div>
<div id="conjugate-gradients" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Conjugate gradients</h3>
<p>The gradient direction is not the best descent direction. It is too
local, and convergence can be quite slow. One of the better algorithms that
is still a <em>first order algorithm</em> (using only gradient information) is the <a href="https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method">nonlinear conjugate
gradient</a> algorithm.
In the Fletcher–Reeves version of the algorithm
the descent direction is initialized as the negative gradient
<span class="math inline">\(\rho_0 = - \nabla H(\theta_{0})\)</span> and then updated as
<span class="math display">\[\rho_{n} = - \nabla H(\theta_{n}) + \frac{\|\nabla H(\theta_n)\|_2^2}{\|\nabla H(\theta_{n-1})\|_2^2} \rho_{n-1}.\]</span>
That is, the descent direction, <span class="math inline">\(\rho_{n}\)</span>, is the negative gradient but modified according to
the previous descent direction. There is plenty of opportunity to vary the prefactor
of <span class="math inline">\(\rho_{n-1}\)</span>, and the one presented here is what makes it the Fletcher–Reeves
version. Other versions go by the names of their inventors such as Polak–Ribière
or Hestenes–Stiefel.</p>
<p>In fact, <span class="math inline">\(\rho_{n}\)</span> need not be a descent direction unless we put some
restrictions on the step lengths. One possibility is to require that
the step length <span class="math inline">\(\gamma_{n}\)</span> satisfies the <em>strong</em> curvature condition
<span class="math display">\[|h&#39;(\gamma)| = |\nabla H(\theta_n + \gamma \rho_n)^T \rho_n | \leq \tilde{c} |\nabla H(\theta_n)^T \rho_n| = \tilde{c} |h&#39;(0)|\]</span>
for a <span class="math inline">\(\tilde{c} &lt; \frac{1}{2}\)</span>. Then <span class="math inline">\(\rho_{n + 1}\)</span> can be shown to be a descent
direction if <span class="math inline">\(\rho_{n}\)</span> is.</p>
<p>We implement the conjugate gradient method in a slightly different way. Instead
of introducing the more advanced curvature condition, we simply reset the
algorithm to use the gradient direction in any case where a non-descent direction
has been chosen. Resets of descent direction every <span class="math inline">\(p\)</span>-th iteration is recommended
anyway for the nonlinear conjugate gradient algorithm.</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="7-2-descent-direction-algorithms.html#cb300-1"></a>CG &lt;-<span class="st"> </span><span class="cf">function</span>(</span>
<span id="cb300-2"><a href="7-2-descent-direction-algorithms.html#cb300-2"></a>  par, </span>
<span id="cb300-3"><a href="7-2-descent-direction-algorithms.html#cb300-3"></a>  H,</span>
<span id="cb300-4"><a href="7-2-descent-direction-algorithms.html#cb300-4"></a>  gr,</span>
<span id="cb300-5"><a href="7-2-descent-direction-algorithms.html#cb300-5"></a>  <span class="dt">d =</span> <span class="fl">0.8</span>, </span>
<span id="cb300-6"><a href="7-2-descent-direction-algorithms.html#cb300-6"></a>  <span class="dt">c =</span> <span class="fl">0.1</span>, </span>
<span id="cb300-7"><a href="7-2-descent-direction-algorithms.html#cb300-7"></a>  <span class="dt">gamma0 =</span> <span class="dv">1</span>, </span>
<span id="cb300-8"><a href="7-2-descent-direction-algorithms.html#cb300-8"></a>  <span class="dt">epsilon =</span> <span class="fl">1e-6</span>,</span>
<span id="cb300-9"><a href="7-2-descent-direction-algorithms.html#cb300-9"></a>  <span class="dt">maxit =</span> <span class="dv">1000</span>,</span>
<span id="cb300-10"><a href="7-2-descent-direction-algorithms.html#cb300-10"></a>  <span class="dt">cb =</span> <span class="ot">NULL</span></span>
<span id="cb300-11"><a href="7-2-descent-direction-algorithms.html#cb300-11"></a>) {</span>
<span id="cb300-12"><a href="7-2-descent-direction-algorithms.html#cb300-12"></a>  p &lt;-<span class="st"> </span><span class="kw">length</span>(par)</span>
<span id="cb300-13"><a href="7-2-descent-direction-algorithms.html#cb300-13"></a>  m &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb300-14"><a href="7-2-descent-direction-algorithms.html#cb300-14"></a>  rho0 &lt;-<span class="st"> </span><span class="kw">numeric</span>(p)</span>
<span id="cb300-15"><a href="7-2-descent-direction-algorithms.html#cb300-15"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>maxit) {</span>
<span id="cb300-16"><a href="7-2-descent-direction-algorithms.html#cb300-16"></a>    value &lt;-<span class="st"> </span><span class="kw">H</span>(par)</span>
<span id="cb300-17"><a href="7-2-descent-direction-algorithms.html#cb300-17"></a>    grad &lt;-<span class="st"> </span><span class="kw">gr</span>(par)</span>
<span id="cb300-18"><a href="7-2-descent-direction-algorithms.html#cb300-18"></a>    grad_norm_sq &lt;-<span class="st"> </span><span class="kw">sum</span>(grad<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb300-19"><a href="7-2-descent-direction-algorithms.html#cb300-19"></a>    <span class="cf">if</span>(<span class="op">!</span><span class="kw">is.null</span>(cb)) <span class="kw">cb</span>()</span>
<span id="cb300-20"><a href="7-2-descent-direction-algorithms.html#cb300-20"></a>    <span class="cf">if</span>(grad_norm_sq <span class="op">&lt;=</span><span class="st"> </span>epsilon) <span class="cf">break</span></span>
<span id="cb300-21"><a href="7-2-descent-direction-algorithms.html#cb300-21"></a>    gamma &lt;-<span class="st"> </span>gamma0</span>
<span id="cb300-22"><a href="7-2-descent-direction-algorithms.html#cb300-22"></a>    <span class="co"># Descent direction</span></span>
<span id="cb300-23"><a href="7-2-descent-direction-algorithms.html#cb300-23"></a>    rho &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>grad <span class="op">+</span><span class="st"> </span>grad_norm_sq <span class="op">*</span><span class="st"> </span>rho0</span>
<span id="cb300-24"><a href="7-2-descent-direction-algorithms.html#cb300-24"></a>    h_prime &lt;-<span class="st"> </span><span class="kw">drop</span>(<span class="kw">t</span>(grad) <span class="op">%*%</span><span class="st"> </span>rho)</span>
<span id="cb300-25"><a href="7-2-descent-direction-algorithms.html#cb300-25"></a>    <span class="co"># Reset to gradient descent if m &gt; p or rho is not a descent direction</span></span>
<span id="cb300-26"><a href="7-2-descent-direction-algorithms.html#cb300-26"></a>    <span class="cf">if</span>(m <span class="op">&gt;</span><span class="st"> </span>p <span class="op">||</span><span class="st"> </span>h_prime <span class="op">&gt;=</span><span class="st"> </span><span class="dv">0</span>) {</span>
<span id="cb300-27"><a href="7-2-descent-direction-algorithms.html#cb300-27"></a>      rho &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>grad</span>
<span id="cb300-28"><a href="7-2-descent-direction-algorithms.html#cb300-28"></a>      h_prime &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>grad_norm_sq </span>
<span id="cb300-29"><a href="7-2-descent-direction-algorithms.html#cb300-29"></a>      m &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb300-30"><a href="7-2-descent-direction-algorithms.html#cb300-30"></a>    }</span>
<span id="cb300-31"><a href="7-2-descent-direction-algorithms.html#cb300-31"></a>    par1 &lt;-<span class="st"> </span>par <span class="op">+</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>rho</span>
<span id="cb300-32"><a href="7-2-descent-direction-algorithms.html#cb300-32"></a>    <span class="co"># Backtracking</span></span>
<span id="cb300-33"><a href="7-2-descent-direction-algorithms.html#cb300-33"></a>    <span class="cf">while</span>(<span class="kw">H</span>(par1) <span class="op">&gt;</span><span class="st"> </span>value <span class="op">+</span><span class="st"> </span>c <span class="op">*</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>h_prime) {</span>
<span id="cb300-34"><a href="7-2-descent-direction-algorithms.html#cb300-34"></a>      gamma &lt;-<span class="st"> </span>d <span class="op">*</span><span class="st"> </span>gamma</span>
<span id="cb300-35"><a href="7-2-descent-direction-algorithms.html#cb300-35"></a>      par1 &lt;-<span class="st"> </span>par <span class="op">+</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>rho</span>
<span id="cb300-36"><a href="7-2-descent-direction-algorithms.html#cb300-36"></a>    }</span>
<span id="cb300-37"><a href="7-2-descent-direction-algorithms.html#cb300-37"></a>    rho0 &lt;-<span class="st"> </span>rho <span class="op">/</span><span class="st"> </span>grad_norm_sq</span>
<span id="cb300-38"><a href="7-2-descent-direction-algorithms.html#cb300-38"></a>    par &lt;-<span class="st"> </span>par1</span>
<span id="cb300-39"><a href="7-2-descent-direction-algorithms.html#cb300-39"></a>    m &lt;-<span class="st"> </span>m <span class="op">+</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb300-40"><a href="7-2-descent-direction-algorithms.html#cb300-40"></a>  }</span>
<span id="cb300-41"><a href="7-2-descent-direction-algorithms.html#cb300-41"></a>  <span class="cf">if</span>(i <span class="op">==</span><span class="st"> </span>maxit)</span>
<span id="cb300-42"><a href="7-2-descent-direction-algorithms.html#cb300-42"></a>    <span class="kw">warning</span>(<span class="st">&quot;Maximal number, &quot;</span>, maxit, <span class="st">&quot;, of iterations reached&quot;</span>)</span>
<span id="cb300-43"><a href="7-2-descent-direction-algorithms.html#cb300-43"></a>  par</span>
<span id="cb300-44"><a href="7-2-descent-direction-algorithms.html#cb300-44"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="7-2-descent-direction-algorithms.html#cb301-1"></a>CG_tracer &lt;-<span class="st"> </span><span class="kw">tracer</span>(<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;grad_norm_sq&quot;</span>), <span class="dt">N =</span> <span class="dv">10</span>)</span>
<span id="cb301-2"><a href="7-2-descent-direction-algorithms.html#cb301-2"></a>pois_CG &lt;-<span class="st"> </span><span class="kw">CG</span>(veg_pois<span class="op">$</span>par, veg_pois<span class="op">$</span>H, veg_pois<span class="op">$</span>grad_H, <span class="dt">cb =</span> CG_tracer<span class="op">$</span>tracer)</span></code></pre></div>
<pre><code>## n = 1: value = 1; gamma = NA; grad_norm_sq = 14269; 
## n = 10: value = -123.15; gamma = 0.018014; grad_norm_sq = 129.78; 
## n = 20: value = -123.92; gamma = 0.022518; grad_norm_sq = 77.339; 
## n = 30: value = -124.23; gamma = 0.018014; grad_norm_sq = 22.227; 
## n = 40: value = -124.41; gamma = 0.0092234; grad_norm_sq = 0.179; 
## n = 50: value = -124.41; gamma = 0.10737; grad_norm_sq = 0.028232; 
## n = 60: value = -124.41; gamma = 0.0092234; grad_norm_sq = 0.00021747; 
## n = 70: value = -124.41; gamma = 0.0092234; grad_norm_sq = 1.7488e-06;</code></pre>
<div class="figure" style="text-align: center"><span id="fig:GD-CG-trace-plot"></span>
<img src="CSwR_files/figure-html/GD-CG-trace-plot-1.png" alt="Gradient norms (top) and negative log-likelihoods (bottom) for gradient descent (left) and conjugate gradient (right)." width="100%" />
<p class="caption">
Figure 7.2: Gradient norms (top) and negative log-likelihoods (bottom) for gradient descent (left) and conjugate gradient (right).
</p>
</div>
<p>This algorithm is fast enough to fit the large Poisson regression model.</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="7-2-descent-direction-algorithms.html#cb303-1"></a>veg_pois &lt;-<span class="st"> </span><span class="kw">poisson_model</span>(<span class="op">~</span><span class="st"> </span>store <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(normalSale) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, vegetables, <span class="dt">response =</span> <span class="st">&quot;sale&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="7-2-descent-direction-algorithms.html#cb304-1"></a>CG_tracer &lt;-<span class="st"> </span><span class="kw">tracer</span>(<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;grad_norm_sq&quot;</span>), <span class="dt">N =</span> <span class="dv">100</span>)</span>
<span id="cb304-2"><a href="7-2-descent-direction-algorithms.html#cb304-2"></a>pois_CG &lt;-<span class="st"> </span><span class="kw">CG</span>(veg_pois<span class="op">$</span>par, veg_pois<span class="op">$</span>H, veg_pois<span class="op">$</span>grad_H, <span class="dt">cb =</span> CG_tracer<span class="op">$</span>tracer)</span></code></pre></div>
<pre><code>## n = 1: value = 1; gamma = NA; grad_norm_sq = 12737; 
## n = 100: value = -127.9; gamma = 0.018014; grad_norm_sq = 1.676; 
## n = 200: value = -128.28; gamma = 0.011529; grad_norm_sq = 2.5128; 
## n = 300: value = -128.55; gamma = 0.0037779; grad_norm_sq = 0.068176; 
## n = 400: value = -128.59; gamma = 0.022518; grad_norm_sq = 0.0028747; 
## n = 500: value = -128.59; gamma = 0.0092234; grad_norm_sq = 0.0652; 
## n = 600: value = -128.59; gamma = 0.018014; grad_norm_sq = 0.00020555; 
## n = 700: value = -128.59; gamma = 0.0019343; grad_norm_sq = 5.3952e-06; 
## n = 800: value = -128.59; gamma = 0.005903; grad_norm_sq = 3.7118e-06; 
## n = 900: value = -128.59; gamma = 0.0037779; grad_norm_sq = 1.0621e-06;</code></pre>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="7-2-descent-direction-algorithms.html#cb306-1"></a><span class="kw">tail</span>(<span class="kw">summary</span>(CG_tracer))</span></code></pre></div>
<pre><code>##         value       gamma grad_norm_sq    .time
## 899 -128.5894 0.005902958 5.214667e-06 11.65685
## 900 -128.5894 0.003777893 1.062092e-06 11.67087
## 901 -128.5894 0.068719477 2.119915e-05 11.67842
## 902 -128.5894 0.107374182 2.047029e-04 11.68518
## 903 -128.5894 0.004722366 7.056181e-06 11.70461
## 904 -128.5894 0.003777893 8.881615e-07 11.71881</code></pre>
<p>Using <code>optim()</code> with the conjugate gradient method.</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="7-2-descent-direction-algorithms.html#cb308-1"></a><span class="kw">system.time</span>(</span>
<span id="cb308-2"><a href="7-2-descent-direction-algorithms.html#cb308-2"></a>  pois_optim_CG &lt;-<span class="st"> </span><span class="kw">optim</span>(</span>
<span id="cb308-3"><a href="7-2-descent-direction-algorithms.html#cb308-3"></a>    veg_pois<span class="op">$</span>par, </span>
<span id="cb308-4"><a href="7-2-descent-direction-algorithms.html#cb308-4"></a>    veg_pois<span class="op">$</span>H, </span>
<span id="cb308-5"><a href="7-2-descent-direction-algorithms.html#cb308-5"></a>    veg_pois<span class="op">$</span>grad_H, </span>
<span id="cb308-6"><a href="7-2-descent-direction-algorithms.html#cb308-6"></a>    <span class="dt">method =</span> <span class="st">&quot;CG&quot;</span>, </span>
<span id="cb308-7"><a href="7-2-descent-direction-algorithms.html#cb308-7"></a>    <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">maxit =</span> <span class="dv">10000</span>)</span>
<span id="cb308-8"><a href="7-2-descent-direction-algorithms.html#cb308-8"></a>  )</span>
<span id="cb308-9"><a href="7-2-descent-direction-algorithms.html#cb308-9"></a>)</span></code></pre></div>
<pre><code>##    user  system elapsed 
##  10.593   0.032  10.650</code></pre>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="7-2-descent-direction-algorithms.html#cb310-1"></a>pois_optim_CG[<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>, <span class="st">&quot;counts&quot;</span>)]</span></code></pre></div>
<pre><code>## $value
## [1] -128.5895
## 
## $counts
## function gradient 
##    10674     4549</code></pre>
</div>
<div id="pep-moth-descent" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Peppered Moths</h3>
<p>Returning to the peppered moth from Section <a href="6-2-multinomial-models.html#pep-moth">6.2.1</a> we implemented
in that section the log-likelihood for general multinomial cell collapsing
and applied the implementation to compute the maximum-likelihood estimate.
In this section we implement the gradient as well. From the
expression for the log-likelihood in <a href="6-2-multinomial-models.html#eq:mult-col-loglik">(6.2)</a> it follows
that the gradient equals</p>
<p><span class="math display">\[\nabla \ell(\theta) = \sum_{j = 1}^{K_0}  \frac{ x_j }{ M(p(\theta))_j}\nabla M(p(\theta))_j = \sum_{j = 1}^{K_0} \sum_{k \in A_j}  \frac{ x_j}{ M(p(\theta))_j} \nabla p_k(\theta).\]</span></p>
<p>Letting <span class="math inline">\(j(k)\)</span> be defined by <span class="math inline">\(k \in A_{j(k)}\)</span> we see that the gradient
can also be written as
<span class="math display">\[\nabla \ell(\theta) = \sum_{k=1}^K    \frac{x_{j(k)}}{ M(p(\theta))_{j(k)}} \nabla p_k(\theta) = \mathbf{\tilde{x}}(\theta) \mathrm{D}p(\theta),\]</span>
where <span class="math inline">\(\mathrm{D}p(\theta)\)</span> is the Jacobian of the parametrization <span class="math inline">\(\theta \mapsto p(\theta)\)</span>,
and <span class="math inline">\(\mathbf{\tilde{x}}(\theta)\)</span> is the vector with
<span class="math display">\[\mathbf{\tilde{x}}(\theta)_k = \frac{ x_{j(k)}}{M(p(\theta))_{j(k)}}.\]</span></p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="7-2-descent-direction-algorithms.html#cb312-1"></a>grad_loglik &lt;-<span class="st"> </span><span class="cf">function</span>(par, x, prob, Dprob, group) {</span>
<span id="cb312-2"><a href="7-2-descent-direction-algorithms.html#cb312-2"></a>  p &lt;-<span class="st"> </span><span class="kw">prob</span>(par)</span>
<span id="cb312-3"><a href="7-2-descent-direction-algorithms.html#cb312-3"></a>  <span class="cf">if</span>(<span class="kw">is.null</span>(p)) <span class="kw">return</span>(<span class="kw">rep</span>(<span class="ot">NA</span>, <span class="kw">length</span>(par)))</span>
<span id="cb312-4"><a href="7-2-descent-direction-algorithms.html#cb312-4"></a>  <span class="op">-</span><span class="st"> </span>(x[group] <span class="op">/</span><span class="st"> </span><span class="kw">M</span>(p, group)[group]) <span class="op">%*%</span><span class="st"> </span><span class="kw">Dprob</span>(par)</span>
<span id="cb312-5"><a href="7-2-descent-direction-algorithms.html#cb312-5"></a>}</span></code></pre></div>
<p>The Jacobian needs to be implemented for the specific example
of peppered moths.</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="7-2-descent-direction-algorithms.html#cb313-1"></a>Dprob &lt;-<span class="st"> </span><span class="cf">function</span>(p) {</span>
<span id="cb313-2"><a href="7-2-descent-direction-algorithms.html#cb313-2"></a>  p[<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>p[<span class="dv">2</span>]</span>
<span id="cb313-3"><a href="7-2-descent-direction-algorithms.html#cb313-3"></a>  <span class="kw">matrix</span>(</span>
<span id="cb313-4"><a href="7-2-descent-direction-algorithms.html#cb313-4"></a>    <span class="kw">c</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>],             <span class="dv">0</span>, </span>
<span id="cb313-5"><a href="7-2-descent-direction-algorithms.html#cb313-5"></a>      <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>],             <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>], </span>
<span id="cb313-6"><a href="7-2-descent-direction-algorithms.html#cb313-6"></a>      <span class="dv">2</span><span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>] <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>],  <span class="dv">-2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>],</span>
<span id="cb313-7"><a href="7-2-descent-direction-algorithms.html#cb313-7"></a>      <span class="dv">0</span>,                    <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>],         </span>
<span id="cb313-8"><a href="7-2-descent-direction-algorithms.html#cb313-8"></a>      <span class="dv">-2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>],            <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>] <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>], </span>
<span id="cb313-9"><a href="7-2-descent-direction-algorithms.html#cb313-9"></a>      <span class="dv">-2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>],           <span class="dv">-2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>]),</span>
<span id="cb313-10"><a href="7-2-descent-direction-algorithms.html#cb313-10"></a>    <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> <span class="dv">6</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</span>
<span id="cb313-11"><a href="7-2-descent-direction-algorithms.html#cb313-11"></a>}</span></code></pre></div>
<p>We can then use the conjugate gradient algorithm to compute the
maximum-likelihood estimate.</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="7-2-descent-direction-algorithms.html#cb314-1"></a><span class="kw">optim</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), loglik, grad_loglik, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), </span>
<span id="cb314-2"><a href="7-2-descent-direction-algorithms.html#cb314-2"></a>      <span class="dt">prob =</span> prob, <span class="dt">Dprob =</span> Dprob, <span class="dt">group =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>), </span>
<span id="cb314-3"><a href="7-2-descent-direction-algorithms.html#cb314-3"></a>      <span class="dt">method =</span> <span class="st">&quot;CG&quot;</span>)</span></code></pre></div>
<pre><code>## $par
## [1] 0.07083691 0.18873652
## 
## $value
## [1] 600.481
## 
## $counts
## function gradient 
##       92       19 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>The peppered Moth example is very simple.
The log-likelihood can easily be computed, and we used this
problem to illustrate ways of implementing a
likelihood in R and how to use <code>optim</code> to maximize it.</p>
<p>One of the likelihood implementations was very problem specific
while the other more abstract and general, and we used the same general and abstract
approach to implement the gradient above. The gradient could then
be used for other optimization algorithms, still using <code>optim</code>, such as
conjugate gradient. In fact, you can use conjugate gradient without
computing and implementing the gradient.</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="7-2-descent-direction-algorithms.html#cb316-1"></a><span class="kw">optim</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), loglik, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), </span>
<span id="cb316-2"><a href="7-2-descent-direction-algorithms.html#cb316-2"></a>      <span class="dt">prob =</span> prob, <span class="dt">group =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>), </span>
<span id="cb316-3"><a href="7-2-descent-direction-algorithms.html#cb316-3"></a>      <span class="dt">method =</span> <span class="st">&quot;CG&quot;</span>)</span></code></pre></div>
<pre><code>## $par
## [1] 0.07084109 0.18873718
## 
## $value
## [1] 600.481
## 
## $counts
## function gradient 
##      107       15 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>If we do not implement a gradient, a numerical gradient is
used by <code>optim()</code>. This can result in a slower algorithm than if the
gradient is implemented, but more seriously, in can result in convergence
problems. This is because there is a subtle tradeoff between numerical
accuracy and accuracy of the finite difference approximation used to
approximate the gradient. We did not
experience convergence problems in the example above, but one way to remedy such problems
is to set the <code>parscale</code> or <code>fnscale</code> entries in the <code>control</code>
list argument to <code>optim()</code>.</p>
<p>In the following chapter the peppered moth example is used to illustrate
the EM algorithm. It is important to understand that the EM algorithm does not
rely on the ability to compute the likelihood or the gradient of the
likelihood for that matter. In many real applications
of the EM algorithm the computation of the likelihood is challenging or even
impossible, thus most standard optimization algorithms will not be directly
applicable.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Nocedal:2006">
<p>Nocedal, Jorge, and Stephen J. Wright. 2006. <em>Numerical Optimization</em>. Second. Springer Series in Operations Research and Financial Engineering. New York: Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="7-1-algorithms-and-convergence.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="7-3-Newton.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

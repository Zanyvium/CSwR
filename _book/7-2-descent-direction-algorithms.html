<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.2 Descent direction algorithms | Computational Statistics with R</title>
  <meta name="description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="7.2 Descent direction algorithms | Computational Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.2 Descent direction algorithms | Computational Statistics with R" />
  
  <meta name="twitter:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

<meta name="author" content="Niels Richard Hansen" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="7-1-algorithms-and-convergence.html"/>
<link rel="next" href="7-3-newton-type-algorithms.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-intro.html"><a href="1-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html"><i class="fa fa-check"></i><b>1.1</b> Smoothing</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#intro-angles"><i class="fa fa-check"></i><b>1.1.1</b> Angle distributions in proteins</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#using-ggplot2"><i class="fa fa-check"></i><b>1.1.2</b> Using ggplot2</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#changing-the-defaults"><i class="fa fa-check"></i><b>1.1.3</b> Changing the defaults</a></li>
<li class="chapter" data-level="1.1.4" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#multivariate-smoothing"><i class="fa fa-check"></i><b>1.1.4</b> Multivariate methods</a></li>
<li class="chapter" data-level="1.1.5" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#large-scale-smoothing"><i class="fa fa-check"></i><b>1.1.5</b> Large scale smoothing</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#vM"><i class="fa fa-check"></i><b>1.2.1</b> Univariate von Mises distributions</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#mixtures-of-von-mises-distributions"><i class="fa fa-check"></i><b>1.2.2</b> Mixtures of von Mises distributions</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#large-scale-monte-carlo-methods"><i class="fa fa-check"></i><b>1.2.3</b> Large scale Monte Carlo methods</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-optimization.html"><a href="1-3-optimization.html"><i class="fa fa-check"></i><b>1.3</b> Optimization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-optimization.html"><a href="1-3-optimization.html#the-em-algorithm"><i class="fa fa-check"></i><b>1.3.1</b> The EM-algorithm</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-3-optimization.html"><a href="1-3-optimization.html#large-scale-optimization"><i class="fa fa-check"></i><b>1.3.2</b> Large scale optimization</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Smoothing</b></span></li>
<li class="chapter" data-level="2" data-path="2-density.html"><a href="2-density.html"><i class="fa fa-check"></i><b>2</b> Density estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-unidens.html"><a href="2-1-unidens.html"><i class="fa fa-check"></i><b>2.1</b> Univariate density estimation</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html"><i class="fa fa-check"></i><b>2.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#benchmarking"><i class="fa fa-check"></i><b>2.2.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html"><i class="fa fa-check"></i><b>2.3</b> Bandwidth selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#rectangular"><i class="fa fa-check"></i><b>2.3.1</b> Revisiting the rectangular kernel</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#ise-mise-and-mse-for-kernel-estimators"><i class="fa fa-check"></i><b>2.3.2</b> ISE, MISE and MSE for kernel estimators</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#plug-in-estimation-of-the-oracle-bandwidth"><i class="fa fa-check"></i><b>2.3.3</b> Plug-in estimation of the oracle bandwidth</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#cross-validation"><i class="fa fa-check"></i><b>2.3.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-likelihood.html"><a href="2-4-likelihood.html"><i class="fa fa-check"></i><b>2.4</b> Likelihood considerations</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-4-likelihood.html"><a href="2-4-likelihood.html#sieves"><i class="fa fa-check"></i><b>2.4.1</b> Method of sieves</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-4-likelihood.html"><a href="2-4-likelihood.html#basis-density"><i class="fa fa-check"></i><b>2.4.2</b> Basis expansions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-5-exercises.html"><a href="2-5-exercises.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="2-5-exercises.html"><a href="2-5-exercises.html#kernel-density-estimation"><i class="fa fa-check"></i>Kernel density estimation</a></li>
<li class="chapter" data-level="" data-path="2-5-exercises.html"><a href="2-5-exercises.html#benchmarking-1"><i class="fa fa-check"></i>Benchmarking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-bivariate.html"><a href="3-bivariate.html"><i class="fa fa-check"></i><b>3</b> Bivariate smoothing</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html"><i class="fa fa-check"></i><b>3.1</b> Nearest neighbor smoothers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#linear-smoothers"><i class="fa fa-check"></i><b>3.1.1</b> Linear smoothers</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#implementing-the-running-mean"><i class="fa fa-check"></i><b>3.1.2</b> Implementing the running mean</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#choose-k-by-cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Choose <span class="math inline">\(k\)</span> by cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html"><i class="fa fa-check"></i><b>3.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html#nadarayawatson-kernel-smoothing"><i class="fa fa-check"></i><b>3.2.1</b> Nadaraya–Watson kernel smoothing</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html#local-regression-smoothers"><i class="fa fa-check"></i><b>3.2.2</b> Local regression smoothers</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-sparse-linear-algebra.html"><a href="3-3-sparse-linear-algebra.html"><i class="fa fa-check"></i><b>3.3</b> Sparse linear algebra</a></li>
<li class="chapter" data-level="3.4" data-path="3-4-onb.html"><a href="3-4-onb.html"><i class="fa fa-check"></i><b>3.4</b> Orthogonal basis expansions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-4-onb.html"><a href="3-4-onb.html#polynomial-expansions"><i class="fa fa-check"></i><b>3.4.1</b> Polynomial expansions</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-4-onb.html"><a href="3-4-onb.html#fourier-expansions"><i class="fa fa-check"></i><b>3.4.2</b> Fourier expansions</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-5-splines.html"><a href="3-5-splines.html"><i class="fa fa-check"></i><b>3.5</b> Splines</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-5-splines.html"><a href="3-5-splines.html#smoothing-splines"><i class="fa fa-check"></i><b>3.5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-5-splines.html"><a href="3-5-splines.html#splines-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Splines in R</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-5-splines.html"><a href="3-5-splines.html#efficient-splines"><i class="fa fa-check"></i><b>3.5.3</b> Efficient computation with splines</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-6-gaussian-processes.html"><a href="3-6-gaussian-processes.html"><i class="fa fa-check"></i><b>3.6</b> Gaussian processes</a></li>
<li class="chapter" data-level="3.7" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html"><i class="fa fa-check"></i><b>3.7</b> The Kalman filter</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#ar1-example"><i class="fa fa-check"></i><b>3.7.1</b> AR(1)-example</a></li>
<li class="chapter" data-level="3.7.2" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#the-kalman-smoother"><i class="fa fa-check"></i><b>3.7.2</b> The Kalman smoother</a></li>
<li class="chapter" data-level="3.7.3" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#implementation"><i class="fa fa-check"></i><b>3.7.3</b> Implementation</a></li>
<li class="chapter" data-level="3.7.4" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#the-kalman-filter-1"><i class="fa fa-check"></i><b>3.7.4</b> The Kalman filter</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="2-5-exercises.html"><a href="2-5-exercises.html#exercises"><i class="fa fa-check"></i><b>3.8</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="3-8-exercises.html"><a href="3-8-exercises.html"><i class="fa fa-check"></i>Nearest neighbors</a></li>
<li class="chapter" data-level="" data-path="3-8-exercises.html"><a href="3-8-exercises.html#kernel-estimators"><i class="fa fa-check"></i>Kernel estimators</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Monte Carlo Methods</b></span></li>
<li class="chapter" data-level="4" data-path="4-univariate-random-variables.html"><a href="4-univariate-random-variables.html"><i class="fa fa-check"></i><b>4</b> Univariate random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html"><i class="fa fa-check"></i><b>4.1</b> Pseudorandom number generators</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html#implementing-a-pseudorandom-number-generator"><i class="fa fa-check"></i><b>4.1.1</b> Implementing a pseudorandom number generator</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html#pseudorandom-number-packages"><i class="fa fa-check"></i><b>4.1.2</b> Pseudorandom number packages</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-2-transformation-techniques.html"><a href="4-2-transformation-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Transformation techniques</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-2-transformation-techniques.html"><a href="4-2-transformation-techniques.html#sampling-from-a-t-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Sampling from a <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html#vMsim"><i class="fa fa-check"></i><b>4.3.1</b> von Mises distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html"><i class="fa fa-check"></i><b>4.4</b> Adaptive envelopes</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html#beta-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Beta distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html#von-mises-distribution"><i class="fa fa-check"></i><b>4.4.2</b> von Mises distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="2-5-exercises.html"><a href="2-5-exercises.html#exercises"><i class="fa fa-check"></i><b>4.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="4-5-exercises.html"><a href="4-5-exercises.html"><i class="fa fa-check"></i>Rejection sampling of Gaussian random variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-mci.html"><a href="5-mci.html"><i class="fa fa-check"></i><b>5</b> Monte Carlo integration</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-assessment.html"><a href="5-1-assessment.html"><i class="fa fa-check"></i><b>5.1</b> Assessment</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-1-assessment.html"><a href="5-1-assessment.html#using-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.1.1</b> Using the central limit theorem</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-1-assessment.html"><a href="5-1-assessment.html#concentration-inequalities"><i class="fa fa-check"></i><b>5.1.2</b> Concentration inequalities</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-1-assessment.html"><a href="5-1-assessment.html#exponential-tail-bound-for-gamma-distributed-variables"><i class="fa fa-check"></i><b>5.1.3</b> Exponential tail bound for Gamma distributed variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html"><i class="fa fa-check"></i><b>5.2</b> Importance sampling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html#unknown-normalization-constants"><i class="fa fa-check"></i><b>5.2.1</b> Unknown normalization constants</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html#hd-int"><i class="fa fa-check"></i><b>5.2.2</b> Computing a high-dimensional integral</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-3-network-failure.html"><a href="5-3-network-failure.html"><i class="fa fa-check"></i><b>5.3</b> Network failure</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-3-network-failure.html"><a href="5-3-network-failure.html#object-oriented-implementation"><i class="fa fa-check"></i><b>5.3.1</b> Object oriented implementation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part III: Optimization</b></span></li>
<li class="chapter" data-level="6" data-path="6-four-examples.html"><a href="6-four-examples.html"><i class="fa fa-check"></i><b>6</b> Four Examples</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html"><i class="fa fa-check"></i><b>6.1</b> Exponential families</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#full-exponential-families"><i class="fa fa-check"></i><b>6.1.1</b> Full exponential families</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#bayes-net"><i class="fa fa-check"></i><b>6.1.2</b> Exponential family Bayesian networks</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#exp-fam-deriv"><i class="fa fa-check"></i><b>6.1.3</b> Likelihood computations</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#curved-exponential-families"><i class="fa fa-check"></i><b>6.1.4</b> Curved exponential families</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-2-multinomial-models.html"><a href="6-2-multinomial-models.html"><i class="fa fa-check"></i><b>6.2</b> Multinomial models</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-2-multinomial-models.html"><a href="6-2-multinomial-models.html#pep-moth"><i class="fa fa-check"></i><b>6.2.1</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-3-regression.html"><a href="6-3-regression.html"><i class="fa fa-check"></i><b>6.3</b> Regression models</a></li>
<li class="chapter" data-level="6.4" data-path="6-4-finite-mixture-models.html"><a href="6-4-finite-mixture-models.html"><i class="fa fa-check"></i><b>6.4</b> Finite mixture models</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-4-finite-mixture-models.html"><a href="6-4-finite-mixture-models.html#Gaus-mix-ex"><i class="fa fa-check"></i><b>6.4.1</b> Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-5-mixed-models.html"><a href="6-5-mixed-models.html"><i class="fa fa-check"></i><b>6.5</b> Mixed models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-numopt.html"><a href="7-numopt.html"><i class="fa fa-check"></i><b>7</b> Numerical optimization</a><ul>
<li class="chapter" data-level="7.1" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html"><i class="fa fa-check"></i><b>7.1</b> Algorithms and convergence</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#descent-algorithms"><i class="fa fa-check"></i><b>7.1.1</b> Descent algorithms</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#maps-and-fixed-points"><i class="fa fa-check"></i><b>7.1.2</b> Maps and fixed points</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#convergence-rate"><i class="fa fa-check"></i><b>7.1.3</b> Convergence rate</a></li>
<li class="chapter" data-level="7.1.4" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#stopping-criteria"><i class="fa fa-check"></i><b>7.1.4</b> Stopping criteria</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html"><i class="fa fa-check"></i><b>7.2</b> Descent direction algorithms</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#line-search"><i class="fa fa-check"></i><b>7.2.1</b> Line search</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>7.2.2</b> Gradient descent</a></li>
<li class="chapter" data-level="7.2.3" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#conjugate-gradients"><i class="fa fa-check"></i><b>7.2.3</b> Conjugate gradients</a></li>
<li class="chapter" data-level="7.2.4" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#pep-moth-descent"><i class="fa fa-check"></i><b>7.2.4</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-3-newton-type-algorithms.html"><a href="7-3-newton-type-algorithms.html"><i class="fa fa-check"></i><b>7.3</b> Newton-type algorithms</a><ul>
<li class="chapter" data-level="7.3.1" data-path="7-3-newton-type-algorithms.html"><a href="7-3-newton-type-algorithms.html#poisson-regression"><i class="fa fa-check"></i><b>7.3.1</b> Poisson regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="7-3-newton-type-algorithms.html"><a href="7-3-newton-type-algorithms.html#quasi-newton-algorithms"><i class="fa fa-check"></i><b>7.3.2</b> Quasi-Newton algorithms</a></li>
<li class="chapter" data-level="7.3.3" data-path="7-3-newton-type-algorithms.html"><a href="7-3-newton-type-algorithms.html#sparsity"><i class="fa fa-check"></i><b>7.3.3</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7-4-misc-.html"><a href="7-4-misc-.html"><i class="fa fa-check"></i><b>7.4</b> Misc.</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-em.html"><a href="8-em.html"><i class="fa fa-check"></i><b>8</b> Expectation maximization algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html"><i class="fa fa-check"></i><b>8.1</b> Basic properties</a><ul>
<li class="chapter" data-level="8.1.1" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#incomplete-data-likelihood"><i class="fa fa-check"></i><b>8.1.1</b> Incomplete data likelihood</a></li>
<li class="chapter" data-level="8.1.2" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#monotonicity-of-the-em-algorithm"><i class="fa fa-check"></i><b>8.1.2</b> Monotonicity of the EM algorithm</a></li>
<li class="chapter" data-level="8.1.3" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#peppered-moths"><i class="fa fa-check"></i><b>8.1.3</b> Peppered moths</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8-2-EM-exp.html"><a href="8-2-EM-exp.html"><i class="fa fa-check"></i><b>8.2</b> Exponential families</a></li>
<li class="chapter" data-level="8.3" data-path="8-3-fisher-information.html"><a href="8-3-fisher-information.html"><i class="fa fa-check"></i><b>8.3</b> Fisher information</a></li>
<li class="chapter" data-level="8.4" data-path="8-4-revisiting-gaussian-mixtures.html"><a href="8-4-revisiting-gaussian-mixtures.html"><i class="fa fa-check"></i><b>8.4</b> Revisiting Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-StochOpt.html"><a href="9-StochOpt.html"><i class="fa fa-check"></i><b>9</b> Stochastic Optimization</a><ul>
<li class="chapter" data-level="9.1" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html"><i class="fa fa-check"></i><b>9.1</b> Stochastic gradient algorithms</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#section"><i class="fa fa-check"></i><b>9.1.1</b> </a></li>
<li class="chapter" data-level="9.1.2" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#online-stochastic-gradient-descent"><i class="fa fa-check"></i><b>9.1.2</b> Online stochastic gradient descent</a></li>
<li class="chapter" data-level="9.1.3" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>9.1.3</b> Stochastic gradient descent</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9-2-nonlinear-least-squares.html"><a href="9-2-nonlinear-least-squares.html"><i class="fa fa-check"></i><b>9.2</b> Nonlinear least squares</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-app-R.html"><a href="A-app-R.html"><i class="fa fa-check"></i><b>A</b> R programming</a><ul>
<li class="chapter" data-level="A.1" data-path="A-1-functions.html"><a href="A-1-functions.html"><i class="fa fa-check"></i><b>A.1</b> Functions</a><ul>
<li class="chapter" data-level="A.1.1" data-path="A-1-functions.html"><a href="A-1-functions.html#vectorization"><i class="fa fa-check"></i><b>A.1.1</b> Vectorization</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="A-2-objects-and-methods.html"><a href="A-2-objects-and-methods.html"><i class="fa fa-check"></i><b>A.2</b> Objects and methods</a></li>
<li class="chapter" data-level="A.3" data-path="A-3-environments.html"><a href="A-3-environments.html"><i class="fa fa-check"></i><b>A.3</b> Environments</a><ul>
<li class="chapter" data-level="A.3.1" data-path="A-3-environments.html"><a href="A-3-environments.html#function-factories"><i class="fa fa-check"></i><b>A.3.1</b> Function factories</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="A-4-performance.html"><a href="A-4-performance.html"><i class="fa fa-check"></i><b>A.4</b> Performance</a><ul>
<li class="chapter" data-level="A.4.1" data-path="A-4-performance.html"><a href="A-4-performance.html#parallel-computations"><i class="fa fa-check"></i><b>A.4.1</b> Parallel computations</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html"><i class="fa fa-check"></i><b>A.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#functions-1"><i class="fa fa-check"></i>Functions</a></li>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#histograms-with-non-equidistant-breaks"><i class="fa fa-check"></i>Histograms with non-equidistant breaks</a></li>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#functions-and-objects"><i class="fa fa-check"></i>Functions and objects</a></li>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#functions-and-environments"><i class="fa fa-check"></i>Functions and environments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="descent-direction-algorithms" class="section level2">
<h2><span class="header-section-number">7.2</span> Descent direction algorithms</h2>
<p>The negative gradient of <span class="math inline">\(H\)</span> in <span class="math inline">\(\theta\)</span> is the direction of steepest descent.
Starting from <span class="math inline">\(\theta_0\)</span> and with the goal of minimizing <span class="math inline">\(H\)</span>, it is natural
to move away from <span class="math inline">\(\theta_0\)</span> in the direction of <span class="math inline">\(-\nabla H(\theta_0)\)</span>.
Thus we could define<br />
<span class="math display">\[\theta_1 = \theta_0 - \gamma \nabla H(\theta_0)\]</span>
for a suitably chosen <span class="math inline">\(\gamma &gt; 0\)</span>. By Taylor’s theorem
<span class="math display">\[H(\theta_1) = H(\theta_0) - \gamma \|\nabla H(\theta_0)\|^2_2 + o(\gamma),\]</span>
which means that if <span class="math inline">\(\theta_0\)</span> is not a stationary point (<span class="math inline">\(\nabla H(\theta_0) \neq 0\)</span>)
then
<span class="math display">\[H(\theta_1) &lt; H(\theta_0)\]</span>
for <span class="math inline">\(\gamma\)</span> small enough.</p>
<p>More generally, we define a <em>descent direction</em> in <span class="math inline">\(\theta_0\)</span>
as a vector <span class="math inline">\(\rho_0 \in \mathbb{R}^p\)</span> such that
<span class="math display">\[\nabla H(\theta_0)^T \rho_0 &lt; 0.\]</span>
By the same kind of Taylor argument as above, <span class="math inline">\(H\)</span> will descent for a sufficiently
small step size in the direction of any descent direction. And if <span class="math inline">\(\theta_0\)</span>
is not a stationary point, <span class="math inline">\(-\nabla H(\theta_0)^T\)</span> is a descent direction.</p>
<p>One strategy for choosing <span class="math inline">\(\gamma\)</span> is to minimize the univariate
function
<span class="math display">\[\gamma \mapsto H(\theta_0 + \gamma \rho_0),\]</span>
which is an example of a <em>line search</em> method. Such a minimization
would give the maximal possible descent in the direction <span class="math inline">\(\rho_0\)</span>,
and as we have argued, if <span class="math inline">\(\rho_0\)</span> is a descent direction, a minimizer <span class="math inline">\(\gamma &gt; 0\)</span>
guarantees descent of <span class="math inline">\(H\)</span>. However, unless the minimization can be
done analytically it is often computationally too expensive.
Less will also do, and as shown in Example <a href="7-1-algorithms-and-convergence.html#exm:grad-descent">7.1</a>,
if the Hessian has uniformly bounded numerical radius it is possible to
fix one (sufficiently small) step length that will guarantee descent.</p>
<div id="line-search" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Line search</h3>
<p>We consider algorithms of the form
<span class="math display">\[\theta_{n+1} = \theta_n + \gamma_{n} \rho_n\]</span>
for descent directions <span class="math inline">\(\rho_n\)</span> and starting in <span class="math inline">\(\theta_0\)</span>.
The step lengths, <span class="math inline">\(\gamma_n\)</span>, are chosen so as to give
sufficient descent in each iteration.</p>
<p>We let <span class="math inline">\(h(\gamma) = H(\theta_{n} + \gamma \rho_{n})\)</span>
denote the univariate and differentiable function of <span class="math inline">\(\gamma\)</span>,
<span class="math display">\[h : [0,\infty) \to \mathbb{R},\]</span>
that gives the value of <span class="math inline">\(H\)</span> in the direction of the descent direction
<span class="math inline">\(\rho_n\)</span>. We can observe that
<span class="math display">\[h&#39;(\gamma) = \nabla H(\theta_{n} + \gamma \rho_{n})^T \rho_{n},\]</span>
and maximal descent in direction <span class="math inline">\(\rho_n\)</span> can be found by solving
<span class="math inline">\(h&#39;(\gamma) = 0\)</span> for <span class="math inline">\(\gamma\)</span>. As mentioned above, less will do. First note
that
<span class="math display">\[h&#39;(0) = \nabla H(\theta_{n})^T \rho_{n} &lt; 0,\]</span>
so <span class="math inline">\(h\)</span> has a negative slope in <span class="math inline">\(0\)</span>. It descents in a sufficiently
small interval <span class="math inline">\([0, \varepsilon)\)</span>, and it is even true that for any <span class="math inline">\(c \in (0, 1)\)</span>
there is an <span class="math inline">\(\varepsilon &gt; 0\)</span> such that
<span class="math display">\[h(\gamma) \leq h(0) + c \gamma h&#39;(0)\]</span>
for <span class="math inline">\(\gamma \in [0, \varepsilon)\)</span>. We note that this inequality can
be checked easily for any given <span class="math inline">\(\gamma &gt; 0\)</span>, and is known as the
<em>sufficient descent</em> condition. Sufficient descent is not enough
in itself as the step length could be arbitrarily small, and the algorithm
could effectively get stuck.</p>
<p>To prevent too small steps we can enforce another condition. Very close
to <span class="math inline">\(0\)</span>, <span class="math inline">\(h\)</span> will have almost the same slope, <span class="math inline">\(h&#39;(0)\)</span>, as it has in <span class="math inline">\(0\)</span>. If we
therefore require that the slope in <span class="math inline">\(\gamma\)</span> should be larger than <span class="math inline">\(\tilde{c} h&#39;(0)\)</span>
for some <span class="math inline">\(\tilde{c} \in (0, 1)\)</span>, <span class="math inline">\(\gamma\)</span> is forced away from <span class="math inline">\(0\)</span>. This is
known as the <em>curvature condition</em>.</p>
<p>The combined conditions on <span class="math inline">\(\gamma\)</span>,
<span class="math display">\[h(\gamma) \leq h(0) + c \gamma h&#39;(0)\]</span>
for a <span class="math inline">\(c \in (0, 1)\)</span> and
<span class="math display">\[h&#39;(\gamma) \geq \tilde{c} h&#39;(0)\]</span>
for a <span class="math inline">\(\tilde{c} \in (c, 1)\)</span> are known collectively as
the <em>Wolfe conditions</em>. It can be shown that if <span class="math inline">\(h\)</span> is bounded below there
exists a step length satisfying the Wolfe conditions (Lemma 3.1 in <span class="citation">Nocedal and Wright (<a href="#ref-Nocedal:2006" role="doc-biblioref">2006</a>)</span>).</p>
<p>Even when choosing <span class="math inline">\(\gamma_{n}\)</span> to fulfill
the Wolfe conditions there is no guarantee that <span class="math inline">\(\theta_n\)</span>
will converge let alone converge toward a global minimizer. The best we
can hope for in general is that
<span class="math display">\[\|\nabla H(\theta_n)\|_2 \rightarrow 0\]</span>
for <span class="math inline">\(n \to \infty\)</span>, and this will happen under some relatively weak
conditions on <span class="math inline">\(H\)</span> (Theorem 3.2 <span class="citation">Nocedal and Wright (<a href="#ref-Nocedal:2006" role="doc-biblioref">2006</a>)</span>) under the assumption
that
<span class="math display">\[\frac{\nabla H(\theta_n)^T \rho_n}{\|\nabla H(\theta_n)\|_2 \| \rho_n\|_2} \leq - \delta &lt; 0.\]</span>
That is, the angle between the descent direction and the gradient should be
uniformly bounded away from <span class="math inline">\(90^{\circ}\)</span>.</p>
<p>A practical way of searching for a step length is via <em>backtracking</em>.
Choosing a <span class="math inline">\(\gamma_0\)</span> and a constant <span class="math inline">\(d \in (0, 1)\)</span> we
can search through the sequence of step lengths
<span class="math display">\[\gamma_0, d \gamma_0, d^2 \gamma_0, d^3 \gamma_0, \ldots\]</span>
and stop the first time we find a step length satisfying the Wolfe
conditions.</p>
<p>Using backtracking, we can actually dispense of the curvature condition
and simply check the sufficient descent condition</p>
<p><span class="math display">\[H(\theta_{n} + d^k \gamma_0 \rho_{n}) \leq H(\theta_n) + cd^k \gamma_0 \nabla H(\theta_{n})^T \rho_{n}\]</span></p>
<p>for <span class="math inline">\(c \in (0, 1)\)</span>. The implementation of backtracking requires the choice
of the three parameters: <span class="math inline">\(\gamma_0 &gt; 0\)</span>, <span class="math inline">\(d \in (0, 1)\)</span> and <span class="math inline">\(c \in (0, 1)\)</span>.
A good choice depends quite a lot on the algorithm used for choosing
the descent direction, but choosing <span class="math inline">\(c\)</span> too close to 1 can make the algorithm
take too small steps, and taking <span class="math inline">\(d\)</span> too small can likewise
generate small step lengths. Thus <span class="math inline">\(d = 0.8\)</span> or <span class="math inline">\(d = 0.9\)</span>
and <span class="math inline">\(c = 0.1\)</span> or even smaller are sensible choices. For some algorithms,
like the Newton algorithm to be dealt with below, there is a natural
choice of <span class="math inline">\(\gamma_0 = 1\)</span>. But for other algorithms a good choice depends
crucially on the scale of the parameters, and there is then no general
advice on choosing <span class="math inline">\(\gamma_0\)</span> that can be justified theoretically.</p>
</div>
<div id="gradient-descent" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Gradient descent</h3>
<p>We return to the Poisson regression example and implement functions
in R for computing the negative log-likelihood and its gradient.
We exploit the <code>model.matrix</code> function to construct the model matrix
from the data via a formula. The sufficient statistic is
computed upfront, and the implementations use this vector
and relies on linear algebra and vectorized computations. We
choose to normalize by the number of observations <span class="math inline">\(n\)</span> (the number
of rows in the model matrix). This does have a small computational
cost, but the resulting numerical values become less dependent
upon <span class="math inline">\(n\)</span>, which makes it easier to choose sensible default values
of various parameters for the numerical optimization algorithms.</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="7-2-descent-direction-algorithms.html#cb274-1"></a>X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(sale <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(normalSale), <span class="dt">data =</span> vegetables)</span>
<span id="cb274-2"><a href="7-2-descent-direction-algorithms.html#cb274-2"></a>y &lt;-<span class="st"> </span>vegetables<span class="op">$</span>sale</span>
<span id="cb274-3"><a href="7-2-descent-direction-algorithms.html#cb274-3"></a><span class="co">## The function `drop` drops the dimensions attribute</span></span>
<span id="cb274-4"><a href="7-2-descent-direction-algorithms.html#cb274-4"></a>t_map &lt;-<span class="st"> </span><span class="kw">drop</span>(<span class="kw">crossprod</span>(X, y))  <span class="co">## More efficient than drop(t(X) %*% y)</span></span>
<span id="cb274-5"><a href="7-2-descent-direction-algorithms.html#cb274-5"></a></span>
<span id="cb274-6"><a href="7-2-descent-direction-algorithms.html#cb274-6"></a>H &lt;-<span class="st"> </span><span class="cf">function</span>(beta) </span>
<span id="cb274-7"><a href="7-2-descent-direction-algorithms.html#cb274-7"></a>  (<span class="kw">drop</span>(<span class="kw">sum</span>(<span class="kw">exp</span>(X <span class="op">%*%</span><span class="st"> </span>beta)) <span class="op">-</span><span class="st"> </span>beta <span class="op">%*%</span><span class="st"> </span>t_map)) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(X)</span>
<span id="cb274-8"><a href="7-2-descent-direction-algorithms.html#cb274-8"></a></span>
<span id="cb274-9"><a href="7-2-descent-direction-algorithms.html#cb274-9"></a>grad_H &lt;-<span class="st"> </span><span class="cf">function</span>(beta) </span>
<span id="cb274-10"><a href="7-2-descent-direction-algorithms.html#cb274-10"></a>  (<span class="kw">colSums</span>(<span class="kw">drop</span>(<span class="kw">exp</span>(X <span class="op">%*%</span><span class="st"> </span>beta)) <span class="op">*</span><span class="st"> </span>X) <span class="op">-</span><span class="st"> </span>t_map) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(X)</span></code></pre></div>
<p>We implement a gradient descent algorithm with backtracking
that uses the <em>squared</em> norm of the gradient as a stopping criterion.
For gradient descent, the sufficient descent condition amounts to
choosing the smallest <span class="math inline">\(k \geq 0\)</span> such that</p>
<p><span class="math display">\[H(\theta_{n} + d^k \gamma_0 \nabla H(\theta_{n})) \leq H(\theta_n) -  cd^k \gamma_0 \|\nabla H(\theta_{n})\|_2^2.\]</span></p>
<p>We include a callback argument (the <code>cb</code> argument) in the implementation.
If a function is passed to this argument, it will be evaluated in each iteration
of the algorithm. This gives us the possibility of logging or
printing values of variables during evaluation, which can be highly useful
for understanding the inner workings of the algorithm. Monitoring or logging
intermediate values during the evaluation of code is often refered to as
<em>tracing</em>. The tracer function implemented below can be used to construct
a tracer object with a trace function that can be passed to the callback
argument. It can be adapted as we like to provide the information we want.</p>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="7-2-descent-direction-algorithms.html#cb275-1"></a>GD &lt;-<span class="st"> </span><span class="cf">function</span>(par, </span>
<span id="cb275-2"><a href="7-2-descent-direction-algorithms.html#cb275-2"></a>               <span class="dt">d =</span> <span class="fl">0.8</span>, </span>
<span id="cb275-3"><a href="7-2-descent-direction-algorithms.html#cb275-3"></a>               <span class="dt">c =</span> <span class="fl">0.1</span>, </span>
<span id="cb275-4"><a href="7-2-descent-direction-algorithms.html#cb275-4"></a>               <span class="dt">gamma0 =</span> <span class="fl">0.01</span>, </span>
<span id="cb275-5"><a href="7-2-descent-direction-algorithms.html#cb275-5"></a>               <span class="dt">epsilon =</span> <span class="fl">1e-4</span>, </span>
<span id="cb275-6"><a href="7-2-descent-direction-algorithms.html#cb275-6"></a>               <span class="dt">cb =</span> <span class="ot">NULL</span>) {</span>
<span id="cb275-7"><a href="7-2-descent-direction-algorithms.html#cb275-7"></a>  <span class="cf">repeat</span> {</span>
<span id="cb275-8"><a href="7-2-descent-direction-algorithms.html#cb275-8"></a>    value &lt;-<span class="st"> </span><span class="kw">H</span>(par)</span>
<span id="cb275-9"><a href="7-2-descent-direction-algorithms.html#cb275-9"></a>    grad &lt;-<span class="st"> </span><span class="kw">grad_H</span>(par)</span>
<span id="cb275-10"><a href="7-2-descent-direction-algorithms.html#cb275-10"></a>    h_prime &lt;-<span class="st"> </span><span class="kw">sum</span>(grad<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb275-11"><a href="7-2-descent-direction-algorithms.html#cb275-11"></a>    <span class="cf">if</span>(<span class="op">!</span><span class="kw">is.null</span>(cb)) <span class="kw">cb</span>()</span>
<span id="cb275-12"><a href="7-2-descent-direction-algorithms.html#cb275-12"></a>    <span class="co">## Convergence criterion based on gradient norm</span></span>
<span id="cb275-13"><a href="7-2-descent-direction-algorithms.html#cb275-13"></a>    <span class="cf">if</span>(h_prime <span class="op">&lt;=</span><span class="st"> </span>epsilon) <span class="cf">break</span></span>
<span id="cb275-14"><a href="7-2-descent-direction-algorithms.html#cb275-14"></a>    gamma &lt;-<span class="st"> </span>gamma0</span>
<span id="cb275-15"><a href="7-2-descent-direction-algorithms.html#cb275-15"></a>    <span class="co">## First proposed descent step</span></span>
<span id="cb275-16"><a href="7-2-descent-direction-algorithms.html#cb275-16"></a>    par1 &lt;-<span class="st"> </span>par <span class="op">-</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>grad</span>
<span id="cb275-17"><a href="7-2-descent-direction-algorithms.html#cb275-17"></a>    <span class="co">## Backtracking while descent is insufficient</span></span>
<span id="cb275-18"><a href="7-2-descent-direction-algorithms.html#cb275-18"></a>    <span class="cf">while</span>(<span class="kw">H</span>(par1) <span class="op">&gt;</span><span class="st"> </span>value <span class="op">-</span><span class="st"> </span>c <span class="op">*</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>h_prime) {</span>
<span id="cb275-19"><a href="7-2-descent-direction-algorithms.html#cb275-19"></a>      gamma &lt;-<span class="st"> </span>d <span class="op">*</span><span class="st"> </span>gamma</span>
<span id="cb275-20"><a href="7-2-descent-direction-algorithms.html#cb275-20"></a>      par1 &lt;-<span class="st"> </span>par <span class="op">-</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>grad</span>
<span id="cb275-21"><a href="7-2-descent-direction-algorithms.html#cb275-21"></a>    }</span>
<span id="cb275-22"><a href="7-2-descent-direction-algorithms.html#cb275-22"></a>    par &lt;-<span class="st"> </span>par1</span>
<span id="cb275-23"><a href="7-2-descent-direction-algorithms.html#cb275-23"></a>  }</span>
<span id="cb275-24"><a href="7-2-descent-direction-algorithms.html#cb275-24"></a>  par</span>
<span id="cb275-25"><a href="7-2-descent-direction-algorithms.html#cb275-25"></a>}</span></code></pre></div>
<p>Gradient descent is very slow for the large Poisson model with individual
store effects, so we consider only the simple model with two parameters.</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="7-2-descent-direction-algorithms.html#cb276-1"></a>pois_GD &lt;-<span class="st"> </span><span class="kw">GD</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">ncol</span>(X)))</span></code></pre></div>
<p>The gradient descent implementation is tested by comparing the minimizer to
the estimated parameters as computed by <code>glm</code>.</p>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="7-2-descent-direction-algorithms.html#cb277-1"></a><span class="kw">as.numeric</span>(<span class="kw">coefficients</span>(pois_model_null))  <span class="co">## as.numeric just to strip names</span></span>
<span id="cb277-2"><a href="7-2-descent-direction-algorithms.html#cb277-2"></a><span class="kw">as.numeric</span>(pois_GD)</span></code></pre></div>
<pre><code>## [1] 1.4614403396 0.9215698864
## [1] 1.4603520741 0.9219357553</code></pre>
<p>We get the same result up to the first two decimals. The convergence
criterion on our gradient descent algorithm was quite loose (<span class="math inline">\(\varepsilon = 10^{-4}\)</span>,
which means that the norm of the gradient is smaller than <span class="math inline">\(10^{-2}\)</span> when
the algorithm stops). This choice of <span class="math inline">\(\varepsilon\)</span> in combination with <span class="math inline">\(\gamma_0 = 0.01\)</span>
implies that the algorithm stops when the gradient is so small that the changes
are at most of norm <span class="math inline">\(10^{-4}\)</span>.</p>
<p>Comparing the resulting values of the negative log-likelihood shows agreement
up to the first five decimals, but we notice that the value for the
parameters fitted using <code>glm</code> is just slightly smaller.</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="7-2-descent-direction-algorithms.html#cb279-1"></a><span class="kw">H</span>(<span class="kw">coefficients</span>(pois_model_null))</span>
<span id="cb279-2"><a href="7-2-descent-direction-algorithms.html#cb279-2"></a><span class="kw">H</span>(pois_GD)</span></code></pre></div>
<pre><code>## [1] -124.406827879897
## [1] -124.406825325047</code></pre>
<p>To investigate what actually went on inside the gradient descent
algorithm we implement a trace function. In fact, we implement a
function for constructing a tracer object, which has a way of saving
and printing trace information during the evaluation of the
gradient descent algorithm – or any other algorithm that implements
a similar trace functionality. The tracer object does this by storing
information in the enclosing environment of the trace function
that is passed to the <code>GD</code> function. This trace function looks up
variables in the evaluation environment of <code>GD</code>, stores them and
prints them if requested, and store run time information as well.
After the algorithm has converged the trace information can be accessed
via the <code>summary</code> method for the tracer object. This implementation
of tracer objects should not be confused with the <code>trace</code> function from
the R base package. It has a related functionality that can be used
with any function and is used for debugging. The tracer object as implemented
here can be used with functions such as <code>GD</code> above that explicitly support calling
a trace function, and it monitors the internal state during evaluation
of the function without interrupting evaluation.</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="7-2-descent-direction-algorithms.html#cb281-1"></a><span class="co">## Function &#39;tracer&#39; constructs a tracer object containing a &#39;trace&#39; and </span></span>
<span id="cb281-2"><a href="7-2-descent-direction-algorithms.html#cb281-2"></a><span class="co">## a &#39;get&#39; function. The &#39;trace&#39; function can print object values from its</span></span>
<span id="cb281-3"><a href="7-2-descent-direction-algorithms.html#cb281-3"></a><span class="co">## calling environment (the parent frame) when evaluated, and it can also </span></span>
<span id="cb281-4"><a href="7-2-descent-direction-algorithms.html#cb281-4"></a><span class="co">## store those object values in a local list. The time between &#39;trace&#39; calls</span></span>
<span id="cb281-5"><a href="7-2-descent-direction-algorithms.html#cb281-5"></a><span class="co">## can be recorded as well. It is measured by the &#39;hires_time&#39; function from the </span></span>
<span id="cb281-6"><a href="7-2-descent-direction-algorithms.html#cb281-6"></a><span class="co">## bench package. The &#39;get&#39; function can subsequently access the traced values</span></span>
<span id="cb281-7"><a href="7-2-descent-direction-algorithms.html#cb281-7"></a><span class="co">## in the list, though the user will typically do so via the S3 methods &#39;print&#39; </span></span>
<span id="cb281-8"><a href="7-2-descent-direction-algorithms.html#cb281-8"></a><span class="co">## or &#39;summary&#39; below.</span></span>
<span id="cb281-9"><a href="7-2-descent-direction-algorithms.html#cb281-9"></a><span class="co">## </span></span>
<span id="cb281-10"><a href="7-2-descent-direction-algorithms.html#cb281-10"></a><span class="co">## The call of the &#39;trace&#39; function can be manually inserted into the body of </span></span>
<span id="cb281-11"><a href="7-2-descent-direction-algorithms.html#cb281-11"></a><span class="co">## any function, it can be inserted using &#39;base::trace&#39;, or it can be passed</span></span>
<span id="cb281-12"><a href="7-2-descent-direction-algorithms.html#cb281-12"></a><span class="co">## as an argument to any function with a callback argument. </span></span>
<span id="cb281-13"><a href="7-2-descent-direction-algorithms.html#cb281-13"></a><span class="co">##</span></span>
<span id="cb281-14"><a href="7-2-descent-direction-algorithms.html#cb281-14"></a><span class="co">## Arguments of &#39;tracer&#39; are:</span></span>
<span id="cb281-15"><a href="7-2-descent-direction-algorithms.html#cb281-15"></a><span class="co">##</span></span>
<span id="cb281-16"><a href="7-2-descent-direction-algorithms.html#cb281-16"></a><span class="co">## object: a character vector of names of the objects in the calling </span></span>
<span id="cb281-17"><a href="7-2-descent-direction-algorithms.html#cb281-17"></a><span class="co">##         environment of the &#39;trace&#39; function that are to be traced. Objects </span></span>
<span id="cb281-18"><a href="7-2-descent-direction-algorithms.html#cb281-18"></a><span class="co">##         created by the &#39;expr&#39; argument can be traced. </span></span>
<span id="cb281-19"><a href="7-2-descent-direction-algorithms.html#cb281-19"></a><span class="co">## N:      an integer specifying if and how often trace information is printed. </span></span>
<span id="cb281-20"><a href="7-2-descent-direction-algorithms.html#cb281-20"></a><span class="co">##         N = 0 means never, and otherwise trace information is printed every </span></span>
<span id="cb281-21"><a href="7-2-descent-direction-algorithms.html#cb281-21"></a><span class="co">##         N-th iteration. N = 1 is the default.</span></span>
<span id="cb281-22"><a href="7-2-descent-direction-algorithms.html#cb281-22"></a><span class="co">## save:   a logical value. Sets if the trace information is saved.</span></span>
<span id="cb281-23"><a href="7-2-descent-direction-algorithms.html#cb281-23"></a><span class="co">## time:   a logical value. Sets if run time information is saved.</span></span>
<span id="cb281-24"><a href="7-2-descent-direction-algorithms.html#cb281-24"></a><span class="co">## expr:   an expression that will be evaluated in the calling environment </span></span>
<span id="cb281-25"><a href="7-2-descent-direction-algorithms.html#cb281-25"></a><span class="co">##         of the &#39;trace&#39; function.</span></span>
<span id="cb281-26"><a href="7-2-descent-direction-algorithms.html#cb281-26"></a><span class="co">## ...:    other arguments passed to `format` for printing.</span></span>
<span id="cb281-27"><a href="7-2-descent-direction-algorithms.html#cb281-27"></a></span>
<span id="cb281-28"><a href="7-2-descent-direction-algorithms.html#cb281-28"></a>tracer &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">objects =</span> <span class="ot">NULL</span>, <span class="dt">N =</span> <span class="dv">1</span>, <span class="dt">save =</span> <span class="ot">TRUE</span>, <span class="dt">time =</span> <span class="ot">TRUE</span>, <span class="dt">expr =</span> <span class="ot">NULL</span>, ...) {</span>
<span id="cb281-29"><a href="7-2-descent-direction-algorithms.html#cb281-29"></a>  n &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb281-30"><a href="7-2-descent-direction-algorithms.html#cb281-30"></a>  values_save &lt;-<span class="st"> </span><span class="kw">list</span>()</span>
<span id="cb281-31"><a href="7-2-descent-direction-algorithms.html#cb281-31"></a>  last_time &lt;-<span class="st"> </span>bench<span class="op">::</span><span class="kw">hires_time</span>()</span>
<span id="cb281-32"><a href="7-2-descent-direction-algorithms.html#cb281-32"></a>  trace &lt;-<span class="st"> </span><span class="cf">function</span>() {</span>
<span id="cb281-33"><a href="7-2-descent-direction-algorithms.html#cb281-33"></a>    time_diff &lt;-<span class="st"> </span>bench<span class="op">::</span><span class="kw">hires_time</span>() <span class="op">-</span><span class="st"> </span>last_time</span>
<span id="cb281-34"><a href="7-2-descent-direction-algorithms.html#cb281-34"></a>    <span class="cf">if</span>(<span class="kw">is.expression</span>(expr))</span>
<span id="cb281-35"><a href="7-2-descent-direction-algorithms.html#cb281-35"></a>      <span class="kw">eval</span>(expr, <span class="dt">envir =</span> <span class="kw">parent.frame</span>())</span>
<span id="cb281-36"><a href="7-2-descent-direction-algorithms.html#cb281-36"></a>    <span class="cf">if</span>(<span class="kw">is.null</span>(objects))</span>
<span id="cb281-37"><a href="7-2-descent-direction-algorithms.html#cb281-37"></a>      objects &lt;-<span class="st"> </span><span class="kw">ls</span>(<span class="kw">parent.frame</span>())</span>
<span id="cb281-38"><a href="7-2-descent-direction-algorithms.html#cb281-38"></a>    </span>
<span id="cb281-39"><a href="7-2-descent-direction-algorithms.html#cb281-39"></a>    values &lt;-<span class="st"> </span><span class="kw">mget</span>(objects, <span class="dt">envir =</span> <span class="kw">parent.frame</span>(), <span class="dt">ifnotfound =</span> <span class="kw">list</span>(<span class="ot">NA</span>))</span>
<span id="cb281-40"><a href="7-2-descent-direction-algorithms.html#cb281-40"></a>    <span class="cf">if</span>(N <span class="op">&amp;&amp;</span><span class="st"> </span>(n <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>n <span class="op">%%</span><span class="st"> </span>N <span class="op">==</span><span class="st"> </span><span class="dv">0</span>))</span>
<span id="cb281-41"><a href="7-2-descent-direction-algorithms.html#cb281-41"></a>      <span class="kw">cat</span>(<span class="st">&quot;n = &quot;</span>, n, <span class="st">&quot;: &quot;</span>,  <span class="kw">paste</span>(<span class="kw">names</span>(values), <span class="st">&quot; = &quot;</span>, <span class="kw">format</span>(values, ...), </span>
<span id="cb281-42"><a href="7-2-descent-direction-algorithms.html#cb281-42"></a>                                  <span class="st">&quot;; &quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb281-43"><a href="7-2-descent-direction-algorithms.html#cb281-43"></a>    <span class="cf">if</span>(save) {</span>
<span id="cb281-44"><a href="7-2-descent-direction-algorithms.html#cb281-44"></a>      <span class="cf">if</span>(time)</span>
<span id="cb281-45"><a href="7-2-descent-direction-algorithms.html#cb281-45"></a>        values[[<span class="st">&quot;.time&quot;</span>]] &lt;-<span class="st"> </span>time_diff</span>
<span id="cb281-46"><a href="7-2-descent-direction-algorithms.html#cb281-46"></a>      values_save[[n]] &lt;&lt;-<span class="st"> </span>values</span>
<span id="cb281-47"><a href="7-2-descent-direction-algorithms.html#cb281-47"></a>    }</span>
<span id="cb281-48"><a href="7-2-descent-direction-algorithms.html#cb281-48"></a>    n &lt;&lt;-<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb281-49"><a href="7-2-descent-direction-algorithms.html#cb281-49"></a>    last_time &lt;&lt;-<span class="st"> </span>bench<span class="op">::</span><span class="kw">hires_time</span>()</span>
<span id="cb281-50"><a href="7-2-descent-direction-algorithms.html#cb281-50"></a>    <span class="kw">invisible</span>(<span class="ot">NULL</span>)</span>
<span id="cb281-51"><a href="7-2-descent-direction-algorithms.html#cb281-51"></a>  }</span>
<span id="cb281-52"><a href="7-2-descent-direction-algorithms.html#cb281-52"></a>  get &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">simplify =</span> <span class="ot">FALSE</span>) {</span>
<span id="cb281-53"><a href="7-2-descent-direction-algorithms.html#cb281-53"></a>    <span class="cf">if</span>(simplify) {</span>
<span id="cb281-54"><a href="7-2-descent-direction-algorithms.html#cb281-54"></a>      col_names &lt;-<span class="st"> </span><span class="kw">unique</span>(<span class="kw">unlist</span>(<span class="kw">lapply</span>(values_save, names)))</span>
<span id="cb281-55"><a href="7-2-descent-direction-algorithms.html#cb281-55"></a>      values_save &lt;-<span class="st"> </span><span class="kw">lapply</span>(</span>
<span id="cb281-56"><a href="7-2-descent-direction-algorithms.html#cb281-56"></a>        col_names, </span>
<span id="cb281-57"><a href="7-2-descent-direction-algorithms.html#cb281-57"></a>        <span class="cf">function</span>(x) <span class="kw">do.call</span>(rbind, <span class="kw">unlist</span>(<span class="kw">lapply</span>(values_save, <span class="cf">function</span>(y) y[x]), </span>
<span id="cb281-58"><a href="7-2-descent-direction-algorithms.html#cb281-58"></a>                                          <span class="dt">recursive =</span> <span class="ot">FALSE</span>))</span>
<span id="cb281-59"><a href="7-2-descent-direction-algorithms.html#cb281-59"></a>      )</span>
<span id="cb281-60"><a href="7-2-descent-direction-algorithms.html#cb281-60"></a>      <span class="kw">names</span>(values_save) &lt;-<span class="st"> </span>col_names</span>
<span id="cb281-61"><a href="7-2-descent-direction-algorithms.html#cb281-61"></a>      values_save &lt;-<span class="st"> </span><span class="kw">lapply</span>(col_names, <span class="cf">function</span>(x) {</span>
<span id="cb281-62"><a href="7-2-descent-direction-algorithms.html#cb281-62"></a>        x_val &lt;-<span class="st"> </span>values_save[[x]] </span>
<span id="cb281-63"><a href="7-2-descent-direction-algorithms.html#cb281-63"></a>        <span class="cf">if</span>(<span class="op">!</span><span class="kw">is.null</span>(<span class="kw">ncol</span>(x_val)) <span class="op">&amp;&amp;</span><span class="st"> </span><span class="kw">ncol</span>(x_val) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {</span>
<span id="cb281-64"><a href="7-2-descent-direction-algorithms.html#cb281-64"></a>          <span class="kw">colnames</span>(x_val) &lt;-<span class="st"> </span>x</span>
<span id="cb281-65"><a href="7-2-descent-direction-algorithms.html#cb281-65"></a>        } <span class="cf">else</span> {</span>
<span id="cb281-66"><a href="7-2-descent-direction-algorithms.html#cb281-66"></a>          <span class="cf">if</span>(<span class="kw">is.null</span>(<span class="kw">colnames</span>(x_val)))</span>
<span id="cb281-67"><a href="7-2-descent-direction-algorithms.html#cb281-67"></a>            <span class="kw">colnames</span>(x_val) &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(x_val)</span>
<span id="cb281-68"><a href="7-2-descent-direction-algorithms.html#cb281-68"></a>          <span class="kw">colnames</span>(x_val) &lt;-<span class="st"> </span><span class="kw">paste</span>(x, <span class="st">&quot;.&quot;</span>, <span class="kw">colnames</span>(x_val), <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb281-69"><a href="7-2-descent-direction-algorithms.html#cb281-69"></a>        }</span>
<span id="cb281-70"><a href="7-2-descent-direction-algorithms.html#cb281-70"></a>        x_val</span>
<span id="cb281-71"><a href="7-2-descent-direction-algorithms.html#cb281-71"></a>      })</span>
<span id="cb281-72"><a href="7-2-descent-direction-algorithms.html#cb281-72"></a>      values_save &lt;-<span class="st"> </span><span class="kw">do.call</span>(cbind, values_save)</span>
<span id="cb281-73"><a href="7-2-descent-direction-algorithms.html#cb281-73"></a>      <span class="kw">row.names</span>(values_save) &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(values_save)</span>
<span id="cb281-74"><a href="7-2-descent-direction-algorithms.html#cb281-74"></a>    }</span>
<span id="cb281-75"><a href="7-2-descent-direction-algorithms.html#cb281-75"></a>    values_save</span>
<span id="cb281-76"><a href="7-2-descent-direction-algorithms.html#cb281-76"></a>  }</span>
<span id="cb281-77"><a href="7-2-descent-direction-algorithms.html#cb281-77"></a>  <span class="kw">structure</span>(<span class="kw">list</span>(<span class="dt">trace =</span> trace, <span class="dt">get =</span> get), <span class="dt">class =</span> <span class="st">&quot;tracer&quot;</span>)</span>
<span id="cb281-78"><a href="7-2-descent-direction-algorithms.html#cb281-78"></a>}</span>
<span id="cb281-79"><a href="7-2-descent-direction-algorithms.html#cb281-79"></a></span>
<span id="cb281-80"><a href="7-2-descent-direction-algorithms.html#cb281-80"></a></span>
<span id="cb281-81"><a href="7-2-descent-direction-algorithms.html#cb281-81"></a><span class="co">## Methods for subsetting, printing and summarizing tracer objects</span></span>
<span id="cb281-82"><a href="7-2-descent-direction-algorithms.html#cb281-82"></a><span class="st">&#39;[.tracer&#39;</span> &lt;-<span class="st"> </span><span class="cf">function</span>(x, i, j, ..., <span class="dt">drop =</span> <span class="ot">TRUE</span>) {</span>
<span id="cb281-83"><a href="7-2-descent-direction-algorithms.html#cb281-83"></a>  values &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">get</span>(...)[i] </span>
<span id="cb281-84"><a href="7-2-descent-direction-algorithms.html#cb281-84"></a>  <span class="cf">if</span> (drop <span class="op">&amp;&amp;</span><span class="st"> </span><span class="kw">length</span>(i) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>)</span>
<span id="cb281-85"><a href="7-2-descent-direction-algorithms.html#cb281-85"></a>    values &lt;-<span class="st"> </span>values[[<span class="dv">1</span>]]</span>
<span id="cb281-86"><a href="7-2-descent-direction-algorithms.html#cb281-86"></a>  values</span>
<span id="cb281-87"><a href="7-2-descent-direction-algorithms.html#cb281-87"></a>}</span>
<span id="cb281-88"><a href="7-2-descent-direction-algorithms.html#cb281-88"></a>print.tracer &lt;-<span class="st"> </span><span class="cf">function</span>(x, ...) <span class="kw">print</span>(x<span class="op">$</span><span class="kw">get</span>(...))</span>
<span id="cb281-89"><a href="7-2-descent-direction-algorithms.html#cb281-89"></a>summary.tracer &lt;-<span class="st"> </span><span class="cf">function</span>(x, ...) {</span>
<span id="cb281-90"><a href="7-2-descent-direction-algorithms.html#cb281-90"></a>  x &lt;-<span class="st"> </span><span class="kw">suppressWarnings</span>(x<span class="op">$</span><span class="kw">get</span>(<span class="dt">simplify =</span> <span class="ot">TRUE</span>))</span>
<span id="cb281-91"><a href="7-2-descent-direction-algorithms.html#cb281-91"></a>  x[, <span class="st">&quot;.time&quot;</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">cumsum</span>(x[<span class="op">-</span><span class="dv">1</span>, <span class="st">&quot;.time&quot;</span>]))</span>
<span id="cb281-92"><a href="7-2-descent-direction-algorithms.html#cb281-92"></a>  <span class="kw">as.data.frame</span>(x)</span>
<span id="cb281-93"><a href="7-2-descent-direction-algorithms.html#cb281-93"></a>}</span></code></pre></div>
<p>We use the tracer object with our gradient descent implementation
and print trace information every 50th iteration.</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="7-2-descent-direction-algorithms.html#cb282-1"></a>GD_tracer &lt;-<span class="st"> </span><span class="kw">tracer</span>(<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>, <span class="st">&quot;h_prime&quot;</span>, <span class="st">&quot;gamma&quot;</span>), <span class="dt">N =</span> <span class="dv">50</span>)</span>
<span id="cb282-2"><a href="7-2-descent-direction-algorithms.html#cb282-2"></a><span class="kw">system.time</span>(pois_GD &lt;-<span class="st"> </span><span class="kw">GD</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">ncol</span>(X)), <span class="dt">cb =</span> GD_tracer<span class="op">$</span>trace))</span></code></pre></div>
<pre><code>## n = 1: value = 1; h_prime = 14268.59; gamma = NA; 
## n = 50: value = -123.9395; h_prime = 15.45722; gamma = 0.004096; 
## n = 100: value = -124.3243; h_prime = 3.133931; gamma = 0.00512; 
## n = 150: value = -124.3935; h_prime = 0.601431; gamma = 0.00512; 
## n = 200: value = -124.4048; h_prime = 0.09805907; gamma = 0.00512; 
## n = 250: value = -124.4065; h_prime = 0.0109742; gamma = 0.00512; 
## n = 300: value = -124.4068; h_prime = 0.002375827; gamma = 0.00512; 
## n = 350: value = -124.4068; h_prime = 0.000216502; gamma = 0.004096;</code></pre>
<pre><code>##    user  system elapsed 
##   0.065   0.001   0.066</code></pre>
<p>We see that the gradient descent algorithm runs for a little more than 350
iterations, and we can observe how the value of the negative log-likelihood is
descending. We can also see that the step length <span class="math inline">\(\gamma\)</span> bounces between
<span class="math inline">\(0.004096 = 0.8^4 \times 0.01\)</span> and
<span class="math inline">\(0.00512 = 0.8^3 \times 0.01\)</span>, thus the backtracking
takes 3 to 4 iterations to find a step length with sufficient descent.</p>
<p>The printed trace does not reveal the run time information. The run time
information is computed and stored as differences between process timings at each iteration
of the algorithm, and the precision is at best of the order of one millisecond
(see <code>?proc.time</code>). Hence the run time associated to one single iteration may be
fairly inaccurate for fast iterations, but the cumulative run time can still
give a reasonable indication of time usage over many iterations. This information
is, however, best inspected and computed after the algorithm has converged,
and it is computed and returned by the summary method for tracer objects.</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="7-2-descent-direction-algorithms.html#cb285-1"></a><span class="kw">tail</span>(<span class="kw">summary</span>(GD_tracer))</span></code></pre></div>
<pre><code>##         value      h_prime    gamma      .time
## 372 -124.4068 1.125779e-04 0.005120 0.05789811
## 373 -124.4068 1.218925e-04 0.005120 0.05802726
## 374 -124.4068 1.323878e-04 0.005120 0.05815254
## 375 -124.4068 1.441965e-04 0.005120 0.05827804
## 376 -124.4068 1.574572e-04 0.005120 0.05840378
## 377 -124.4068 7.600796e-05 0.004096 0.05854574</code></pre>
<p>The trace information is stored in a list. The summary method transforms
the trace information into a data frame with one row per iteration. We can also access
individual entries of the list of trace information via subsetting.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="7-2-descent-direction-algorithms.html#cb287-1"></a>GD_tracer[<span class="dv">377</span>] </span></code></pre></div>
<pre><code>## $value
## [1] -124.4068
## 
## $h_prime
## [1] 7.600796e-05
## 
## $gamma
## [1] 0.004096
## 
## $.time
## [1] 0.000141957</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;
## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<div class="figure" style="text-align: center"><span id="fig:GD-trace-plot"></span>
<img src="CSwR_files/figure-html/GD-trace-plot-1.png" alt="Gradient norm (left) and value of the negative log-likelihood (right) above the limit value $H(\theta_{\infty})$. The straight line is fitted to the data points except the first ten using least squares, and the rate is computed from this estimate and reported per ms." width="100%" />
<p class="caption">
Figure 7.1: Gradient norm (left) and value of the negative log-likelihood (right) above the limit value <span class="math inline">\(H(\theta_{\infty})\)</span>. The straight line is fitted to the data points except the first ten using least squares, and the rate is computed from this estimate and reported per ms.
</p>
</div>
</div>
<div id="conjugate-gradients" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Conjugate gradients</h3>
<p>The gradient direction is typically not the best descent direction. It is too
local, and convergence can be quite slow. One of the better algorithms that
is still a “first order algorithm” (using only gradient information) is the <a href="https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method"><em>nonlinear conjugate
gradient</em></a> algorithm.
In the Fletcher–Reeves version of the algorithm
the descent direction is initialized as the negative gradient
<span class="math inline">\(\rho_0 = - \nabla H(\theta_{0})\)</span> and then updated as
<span class="math display">\[\rho_{n} = - \nabla H(\theta_{n}) + \frac{\|\nabla H(\theta_n)\|_2^2}{\|\nabla H(\theta_{n-1})\|_2^2} \rho_{n-1}.\]</span>
That is, the descent direction, <span class="math inline">\(\rho_{n}\)</span>, is the negative gradient but modified according to
the previous descent direction. There is plenty of opportunity to vary the the prefactor
of <span class="math inline">\(\rho_{n-1}\)</span>, and the one presented here is what makes it the Fletcher–Reeves
version. Other versions go by the names of their inventors such as Polak–Ribière
or Hestenes–Stiefel.</p>
<p>In fact, <span class="math inline">\(\rho_{n}\)</span> need not be a descent direction unless we put some
restrictions on the step lengths. One possibility is to require that
the step length <span class="math inline">\(\gamma_{n}\)</span> satisfies the <em>strong</em> curvature condition
<span class="math display">\[|h&#39;(\gamma)| = |\nabla H(\theta_n + \gamma \rho_n)^T \rho_n | \leq \tilde{c} |\nabla H(\theta_n)^T \rho_n| = \tilde{c} |h&#39;(0)|\]</span>
for a <span class="math inline">\(\tilde{c} &lt; \frac{1}{2}\)</span>. Then <span class="math inline">\(\rho_{n + 1}\)</span> can be shown to be a descent
direction if <span class="math inline">\(\rho_{n}\)</span> is.</p>
<p>We implement the conjugate gradient method in a slightly different way. Instead
of introducing the more advanced curvature condition, we simply reset the
algorithm to use the gradient direction in any case where a non-descent direction
has been chosen. Resets of descent direction every <span class="math inline">\(p\)</span>th iteration is recommended
anyway for the nonlinear conjugate gradient algorithm.</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="7-2-descent-direction-algorithms.html#cb290-1"></a>CG &lt;-<span class="st"> </span><span class="cf">function</span>(par, </span>
<span id="cb290-2"><a href="7-2-descent-direction-algorithms.html#cb290-2"></a>               <span class="dt">d =</span> <span class="fl">0.8</span>, </span>
<span id="cb290-3"><a href="7-2-descent-direction-algorithms.html#cb290-3"></a>               <span class="dt">c =</span> <span class="fl">0.1</span>, </span>
<span id="cb290-4"><a href="7-2-descent-direction-algorithms.html#cb290-4"></a>               <span class="dt">gamma0 =</span> <span class="dv">1</span>, </span>
<span id="cb290-5"><a href="7-2-descent-direction-algorithms.html#cb290-5"></a>               <span class="dt">epsilon =</span> <span class="fl">1e-6</span>, </span>
<span id="cb290-6"><a href="7-2-descent-direction-algorithms.html#cb290-6"></a>               <span class="dt">cb =</span> <span class="ot">NULL</span>) {</span>
<span id="cb290-7"><a href="7-2-descent-direction-algorithms.html#cb290-7"></a>  p &lt;-<span class="st"> </span><span class="kw">length</span>(par)</span>
<span id="cb290-8"><a href="7-2-descent-direction-algorithms.html#cb290-8"></a>  m &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb290-9"><a href="7-2-descent-direction-algorithms.html#cb290-9"></a>  rho0 &lt;-<span class="st"> </span><span class="kw">numeric</span>(p)</span>
<span id="cb290-10"><a href="7-2-descent-direction-algorithms.html#cb290-10"></a>  <span class="cf">repeat</span> {</span>
<span id="cb290-11"><a href="7-2-descent-direction-algorithms.html#cb290-11"></a>    value &lt;-<span class="st"> </span><span class="kw">H</span>(par)</span>
<span id="cb290-12"><a href="7-2-descent-direction-algorithms.html#cb290-12"></a>    grad &lt;-<span class="st"> </span><span class="kw">grad_H</span>(par)</span>
<span id="cb290-13"><a href="7-2-descent-direction-algorithms.html#cb290-13"></a>    grad_norm_sq &lt;-<span class="st"> </span><span class="kw">sum</span>(grad<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb290-14"><a href="7-2-descent-direction-algorithms.html#cb290-14"></a>    <span class="cf">if</span>(<span class="op">!</span><span class="kw">is.null</span>(cb)) <span class="kw">cb</span>()</span>
<span id="cb290-15"><a href="7-2-descent-direction-algorithms.html#cb290-15"></a>    <span class="cf">if</span>(grad_norm_sq <span class="op">&lt;=</span><span class="st"> </span>epsilon) <span class="cf">break</span></span>
<span id="cb290-16"><a href="7-2-descent-direction-algorithms.html#cb290-16"></a>    gamma &lt;-<span class="st"> </span>gamma0</span>
<span id="cb290-17"><a href="7-2-descent-direction-algorithms.html#cb290-17"></a>    <span class="co">## Descent direction</span></span>
<span id="cb290-18"><a href="7-2-descent-direction-algorithms.html#cb290-18"></a>    rho &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>grad <span class="op">+</span><span class="st"> </span>grad_norm_sq <span class="op">*</span><span class="st"> </span>rho0</span>
<span id="cb290-19"><a href="7-2-descent-direction-algorithms.html#cb290-19"></a>    h_prime &lt;-<span class="st"> </span><span class="kw">drop</span>(<span class="kw">t</span>(grad) <span class="op">%*%</span><span class="st"> </span>rho)</span>
<span id="cb290-20"><a href="7-2-descent-direction-algorithms.html#cb290-20"></a>    <span class="co">## Reset to gradient descent if m &gt; p or rho is not a descent direction</span></span>
<span id="cb290-21"><a href="7-2-descent-direction-algorithms.html#cb290-21"></a>    <span class="cf">if</span>(m <span class="op">&gt;</span><span class="st"> </span>p <span class="op">||</span><span class="st"> </span>h_prime <span class="op">&gt;=</span><span class="st"> </span><span class="dv">0</span>) {</span>
<span id="cb290-22"><a href="7-2-descent-direction-algorithms.html#cb290-22"></a>      rho &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>grad</span>
<span id="cb290-23"><a href="7-2-descent-direction-algorithms.html#cb290-23"></a>      h_prime &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>grad_norm_sq </span>
<span id="cb290-24"><a href="7-2-descent-direction-algorithms.html#cb290-24"></a>      m &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb290-25"><a href="7-2-descent-direction-algorithms.html#cb290-25"></a>    }</span>
<span id="cb290-26"><a href="7-2-descent-direction-algorithms.html#cb290-26"></a>    par1 &lt;-<span class="st"> </span>par <span class="op">+</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>rho</span>
<span id="cb290-27"><a href="7-2-descent-direction-algorithms.html#cb290-27"></a>    <span class="co">## Backtracking</span></span>
<span id="cb290-28"><a href="7-2-descent-direction-algorithms.html#cb290-28"></a>    <span class="cf">while</span>(<span class="kw">H</span>(par1) <span class="op">&gt;</span><span class="st"> </span>value <span class="op">+</span><span class="st"> </span>c <span class="op">*</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>h_prime) {</span>
<span id="cb290-29"><a href="7-2-descent-direction-algorithms.html#cb290-29"></a>      gamma &lt;-<span class="st"> </span>d <span class="op">*</span><span class="st"> </span>gamma</span>
<span id="cb290-30"><a href="7-2-descent-direction-algorithms.html#cb290-30"></a>      par1 &lt;-<span class="st"> </span>par <span class="op">+</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>rho</span>
<span id="cb290-31"><a href="7-2-descent-direction-algorithms.html#cb290-31"></a>    }</span>
<span id="cb290-32"><a href="7-2-descent-direction-algorithms.html#cb290-32"></a>    rho0 &lt;-<span class="st"> </span>rho <span class="op">/</span><span class="st"> </span>grad_norm_sq</span>
<span id="cb290-33"><a href="7-2-descent-direction-algorithms.html#cb290-33"></a>    par &lt;-<span class="st"> </span>par1</span>
<span id="cb290-34"><a href="7-2-descent-direction-algorithms.html#cb290-34"></a>    m &lt;-<span class="st"> </span>m <span class="op">+</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb290-35"><a href="7-2-descent-direction-algorithms.html#cb290-35"></a>  }</span>
<span id="cb290-36"><a href="7-2-descent-direction-algorithms.html#cb290-36"></a>  par</span>
<span id="cb290-37"><a href="7-2-descent-direction-algorithms.html#cb290-37"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="7-2-descent-direction-algorithms.html#cb291-1"></a>CG_tracer &lt;-<span class="st"> </span><span class="kw">tracer</span>(<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;grad_norm_sq&quot;</span>), <span class="dt">N =</span> <span class="dv">10</span>)</span>
<span id="cb291-2"><a href="7-2-descent-direction-algorithms.html#cb291-2"></a>pois_CG &lt;-<span class="st"> </span><span class="kw">CG</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">ncol</span>(X)), <span class="dt">cb =</span> CG_tracer<span class="op">$</span>trace)</span></code></pre></div>
<pre><code>## n = 1: value = 1; gamma = NA; grad_norm_sq = 14269; 
## n = 10: value = -123.15; gamma = 0.018014; grad_norm_sq = 129.78; 
## n = 20: value = -123.92; gamma = 0.022518; grad_norm_sq = 77.339; 
## n = 30: value = -124.23; gamma = 0.018014; grad_norm_sq = 22.227; 
## n = 40: value = -124.41; gamma = 0.0092234; grad_norm_sq = 0.179; 
## n = 50: value = -124.41; gamma = 0.10737; grad_norm_sq = 0.028232; 
## n = 60: value = -124.41; gamma = 0.0092234; grad_norm_sq = 0.00021747; 
## n = 70: value = -124.41; gamma = 0.0092234; grad_norm_sq = 1.7488e-06;</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;
## `geom_smooth()` using formula &#39;y ~ x&#39;
## `geom_smooth()` using formula &#39;y ~ x&#39;
## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<div class="figure" style="text-align: center"><span id="fig:GD-CG-trace-plot"></span>
<img src="CSwR_files/figure-html/GD-CG-trace-plot-1.png" alt="Gradient norms (top) and negative log-likelihoods (bottom) for gradient descent (left) and conjugate gradient (right)." width="100%" />
<p class="caption">
Figure 7.2: Gradient norms (top) and negative log-likelihoods (bottom) for gradient descent (left) and conjugate gradient (right).
</p>
</div>
<p>This algorithm is fast enough to fit the large Poisson regression model.</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="7-2-descent-direction-algorithms.html#cb294-1"></a>X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(sale <span class="op">~</span><span class="st"> </span>store <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(normalSale) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, </span>
<span id="cb294-2"><a href="7-2-descent-direction-algorithms.html#cb294-2"></a>                  <span class="dt">data =</span> vegetables)</span>
<span id="cb294-3"><a href="7-2-descent-direction-algorithms.html#cb294-3"></a>t_map &lt;-<span class="st"> </span><span class="kw">drop</span>(<span class="kw">crossprod</span>(X, y)) </span></code></pre></div>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="7-2-descent-direction-algorithms.html#cb295-1"></a>CG_tracer &lt;-<span class="st"> </span><span class="kw">tracer</span>(<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;grad_norm_sq&quot;</span>), <span class="dt">N =</span> <span class="dv">100</span>)</span>
<span id="cb295-2"><a href="7-2-descent-direction-algorithms.html#cb295-2"></a>pois_CG &lt;-<span class="st"> </span><span class="kw">CG</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">ncol</span>(X)), <span class="dt">cb =</span> CG_tracer<span class="op">$</span>trace)</span></code></pre></div>
<pre><code>## n = 1: value = 1; gamma = NA; grad_norm_sq = 12737; 
## n = 100: value = -127.7; gamma = 0.014412; grad_norm_sq = 4.6263; 
## n = 200: value = -128.02; gamma = 0.014412; grad_norm_sq = 0.41723; 
## n = 300: value = -128.35; gamma = 0.0024179; grad_norm_sq = 0.063094; 
## n = 400: value = -128.56; gamma = 0.04398; grad_norm_sq = 0.64184; 
## n = 500: value = -128.58; gamma = 0.028147; grad_norm_sq = 0.012616; 
## n = 600: value = -128.59; gamma = 0.0030223; grad_norm_sq = 0.00028513; 
## n = 700: value = -128.59; gamma = 0.0030223; grad_norm_sq = 1.0811e-05; 
## n = 800: value = -128.59; gamma = 0.0019343; grad_norm_sq = 4.2685e-06;</code></pre>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="7-2-descent-direction-algorithms.html#cb297-1"></a><span class="kw">tail</span>(<span class="kw">summary</span>(CG_tracer))</span></code></pre></div>
<pre><code>##         value       gamma grad_norm_sq    .time
## 890 -128.5894 0.134217728 2.968376e-04 13.20100
## 891 -128.5894 0.003777893 2.511863e-06 13.21707
## 892 -128.5894 0.004722366 1.510951e-06 13.23362
## 893 -128.5894 1.000000000 1.835397e-04 13.23780
## 894 -128.5894 0.005902958 3.238532e-05 13.25282
## 895 -128.5894 0.003777893 6.586532e-07 13.26814</code></pre>
<p>Using <code>optim</code> with the conjugate gradient method.</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="7-2-descent-direction-algorithms.html#cb299-1"></a><span class="kw">system.time</span>(pois_optim_CG &lt;-<span class="st"> </span><span class="kw">optim</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dt">length =</span> <span class="kw">ncol</span>(X)), H, grad_H, </span>
<span id="cb299-2"><a href="7-2-descent-direction-algorithms.html#cb299-2"></a>                                   <span class="dt">method =</span> <span class="st">&quot;CG&quot;</span>, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">maxit =</span> <span class="dv">10000</span>)))</span></code></pre></div>
<pre><code>##    user  system elapsed 
##  17.087   5.196  22.179</code></pre>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="7-2-descent-direction-algorithms.html#cb301-1"></a>pois_optim_CG[<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>, <span class="st">&quot;counts&quot;</span>)]</span></code></pre></div>
<pre><code>## $value
## [1] -128.5895
## 
## $counts
## function gradient 
##    12008     5118</code></pre>
</div>
<div id="pep-moth-descent" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Peppered Moths</h3>
<p>Returning to the peppered moth from Section <a href="6-2-multinomial-models.html#pep-moth">6.2.1</a> we implemented
in that section the log-likelihood for general multinomial cell collapsing
and applied the implementation to compute the maximum-likelihood estimate.
In this section we implement the gradient as well. From the
expression for the log-likelihood in <a href="6-2-multinomial-models.html#eq:mult-col-loglik">(6.2)</a> it follows
that the gradient equals</p>
<p><span class="math display">\[\nabla \ell(\theta) = \sum_{j = 1}^{K_0}  \frac{ x_j }{ M(p(\theta))_j}\nabla M(p(\theta))_j = \sum_{j = 1}^{K_0} \sum_{k \in A_j}  \frac{ x_j}{ M(p(\theta))_j} \nabla p_k(\theta).\]</span></p>
<p>Letting <span class="math inline">\(j(k)\)</span> be defined by <span class="math inline">\(k \in A_{j(k)}\)</span> we see that the gradient
can also be written as
<span class="math display">\[\nabla \ell(\theta) = \sum_{k=1}^K    \frac{x_{j(k)}}{ M(p(\theta))_{j(k)}} \nabla p_k(\theta) = \mathbf{\tilde{x}}(\theta) \mathrm{D}p(\theta),\]</span>
where <span class="math inline">\(\mathrm{D}p(\theta)\)</span> is the Jacobian of the parametrization <span class="math inline">\(\theta \mapsto p(\theta)\)</span>,
and <span class="math inline">\(\mathbf{\tilde{x}}(\theta)\)</span> is the vector with
<span class="math display">\[\mathbf{\tilde{x}}(\theta)_k = \frac{ x_{j(k)}}{M(p(\theta))_{j(k)}}.\]</span></p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="7-2-descent-direction-algorithms.html#cb303-1"></a>grad_loglik &lt;-<span class="st"> </span><span class="cf">function</span>(par, x, prob, Dprob, group) {</span>
<span id="cb303-2"><a href="7-2-descent-direction-algorithms.html#cb303-2"></a>  p &lt;-<span class="st"> </span><span class="kw">prob</span>(par)</span>
<span id="cb303-3"><a href="7-2-descent-direction-algorithms.html#cb303-3"></a>  <span class="cf">if</span>(<span class="kw">is.null</span>(p)) <span class="kw">return</span>(<span class="kw">rep</span>(<span class="ot">NA</span>, <span class="kw">length</span>(par)))</span>
<span id="cb303-4"><a href="7-2-descent-direction-algorithms.html#cb303-4"></a>  <span class="op">-</span><span class="st"> </span>(x[group] <span class="op">/</span><span class="st"> </span><span class="kw">M</span>(p, group)[group]) <span class="op">%*%</span><span class="st"> </span><span class="kw">Dprob</span>(par)</span>
<span id="cb303-5"><a href="7-2-descent-direction-algorithms.html#cb303-5"></a>}</span></code></pre></div>
<p>The Jacobian needs to be implemented for the specific example
of peppered moths.</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="7-2-descent-direction-algorithms.html#cb304-1"></a>Dprob &lt;-<span class="st"> </span><span class="cf">function</span>(p) {</span>
<span id="cb304-2"><a href="7-2-descent-direction-algorithms.html#cb304-2"></a>  p[<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>p[<span class="dv">2</span>]</span>
<span id="cb304-3"><a href="7-2-descent-direction-algorithms.html#cb304-3"></a>  <span class="kw">matrix</span>(</span>
<span id="cb304-4"><a href="7-2-descent-direction-algorithms.html#cb304-4"></a>    <span class="kw">c</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>],             <span class="dv">0</span>, </span>
<span id="cb304-5"><a href="7-2-descent-direction-algorithms.html#cb304-5"></a>      <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>],             <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>], </span>
<span id="cb304-6"><a href="7-2-descent-direction-algorithms.html#cb304-6"></a>      <span class="dv">2</span><span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>] <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>],  <span class="dv">-2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>],</span>
<span id="cb304-7"><a href="7-2-descent-direction-algorithms.html#cb304-7"></a>      <span class="dv">0</span>,                    <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>],         </span>
<span id="cb304-8"><a href="7-2-descent-direction-algorithms.html#cb304-8"></a>      <span class="dv">-2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>],            <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>] <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>], </span>
<span id="cb304-9"><a href="7-2-descent-direction-algorithms.html#cb304-9"></a>      <span class="dv">-2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>],           <span class="dv">-2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>]),</span>
<span id="cb304-10"><a href="7-2-descent-direction-algorithms.html#cb304-10"></a>    <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> <span class="dv">6</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</span>
<span id="cb304-11"><a href="7-2-descent-direction-algorithms.html#cb304-11"></a>}</span></code></pre></div>
<p>We can then use the conjugate gradient algorithm to compute the
maximum-likelihood estimate.</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="7-2-descent-direction-algorithms.html#cb305-1"></a><span class="kw">optim</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), loglik, grad_loglik, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), </span>
<span id="cb305-2"><a href="7-2-descent-direction-algorithms.html#cb305-2"></a>      <span class="dt">prob =</span> prob, <span class="dt">Dprob =</span> Dprob, <span class="dt">group =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>), </span>
<span id="cb305-3"><a href="7-2-descent-direction-algorithms.html#cb305-3"></a>      <span class="dt">method =</span> <span class="st">&quot;CG&quot;</span>)</span></code></pre></div>
<pre><code>## $par
## [1] 0.07083691 0.18873652
## 
## $value
## [1] 600.481
## 
## $counts
## function gradient 
##       92       19 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>The peppered Moth example is very simple.
The log-likelihood can easily be computed, and we used this
problem to illustrate ways of implementing a
likelihood in R and how to use <code>optim</code> to maximize it.</p>
<p>One of the likelihood implementations was very problem specific
while the other more abstract and general, and we used the same general and abstract
approach to implement the gradient above. The gradient could then
be used for other optimization algorithms, still using <code>optim</code>, such as
conjugate gradient. In fact, you can use conjugate gradient without
computing and implementing the gradient.</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="7-2-descent-direction-algorithms.html#cb307-1"></a><span class="kw">optim</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), loglik, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), </span>
<span id="cb307-2"><a href="7-2-descent-direction-algorithms.html#cb307-2"></a>      <span class="dt">prob =</span> prob, <span class="dt">group =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>), </span>
<span id="cb307-3"><a href="7-2-descent-direction-algorithms.html#cb307-3"></a>      <span class="dt">method =</span> <span class="st">&quot;CG&quot;</span>)</span></code></pre></div>
<pre><code>## $par
## [1] 0.07084109 0.18873718
## 
## $value
## [1] 600.481
## 
## $counts
## function gradient 
##      107       15 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>If we don’t implement a gradient, a numerical gradient is
used by <code>optim</code>. This can very well result in a slower algorithm than if the
gradient is implemented, but more seriously, in can result in convergence
problems. This is because there is a subtle tradeoff between numerical
accuracy and accuracy of the finite difference approximation used to
approximate the gradient. We didn’t
experience convergence problems in the example above, but one way to remedy such problems
is to set the <code>parscale</code> or <code>fnscale</code> entries in the <code>control</code>
list argument to <code>optim</code>.</p>
<p>In the following chapter the peppered moth example is used to illustrate
the EM algorithm. It is important to understand that the EM algorithm doesn’t
rely on the ability to compute the likelihood or the gradient of the
likelihood for that matter. In many real applications
of the EM algorithm the computation of the likelihood is challenging or even
impossible, thus most standard optimization algorithms will not be directly
applicable.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Nocedal:2006">
<p>Nocedal, Jorge, and Stephen J. Wright. 2006. <em>Numerical Optimization</em>. Second. Springer Series in Operations Research and Financial Engineering. New York: Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="7-1-algorithms-and-convergence.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="7-3-newton-type-algorithms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

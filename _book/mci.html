<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 5 Monte Carlo integration | Computational Statistics with R</title>
<meta name="author" content="Niels Richard Hansen">
<meta name="description" content="A typical usage of simulation of random variables is Monte Carlo integration. With \(X_1, \ldots, X_n\) i.i.d. with density \(f\) \[\hat{\mu}_{\textrm{MC}} := \frac{1}{n} \sum_{i=1}^n h(X_i)...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 5 Monte Carlo integration | Computational Statistics with R">
<meta property="og:type" content="book">
<meta property="og:description" content="A typical usage of simulation of random variables is Monte Carlo integration. With \(X_1, \ldots, X_n\) i.i.d. with density \(f\) \[\hat{\mu}_{\textrm{MC}} := \frac{1}{n} \sum_{i=1}^n h(X_i)...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 5 Monte Carlo integration | Computational Statistics with R">
<meta name="twitter:description" content="A typical usage of simulation of random variables is Monte Carlo integration. With \(X_1, \ldots, X_n\) i.i.d. with density \(f\) \[\hat{\mu}_{\textrm{MC}} := \frac{1}{n} \sum_{i=1}^n h(X_i)...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/_Roboto%20Slab-0.4.0/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.3.0/transition.js"></script><script src="libs/bs3compat-0.3.0/tabs.js"></script><script src="libs/bs3compat-0.3.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Computational Statistics with R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li class="book-part">Part I: Smoothing</li>
<li><a class="" href="density.html"><span class="header-section-number">2</span> Density estimation</a></li>
<li><a class="" href="bivariate.html"><span class="header-section-number">3</span> Bivariate smoothing</a></li>
<li class="book-part">Part II: Monte Carlo Methods</li>
<li><a class="" href="univariate-random-variables.html"><span class="header-section-number">4</span> Univariate random variables</a></li>
<li><a class="active" href="mci.html"><span class="header-section-number">5</span> Monte Carlo integration</a></li>
<li class="book-part">Part III: Optimization</li>
<li><a class="" href="four-examples.html"><span class="header-section-number">6</span> Four Examples</a></li>
<li><a class="" href="numopt.html"><span class="header-section-number">7</span> Numerical optimization</a></li>
<li><a class="" href="em.html"><span class="header-section-number">8</span> Expectation maximization algorithms</a></li>
<li><a class="" href="StochOpt.html"><span class="header-section-number">9</span> Stochastic Optimization</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="app-R.html"><span class="header-section-number">A</span> R programming</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/nielsrhansen/CSwR">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="mci" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Monte Carlo integration<a class="anchor" aria-label="anchor" href="#mci"><i class="fas fa-link"></i></a>
</h1>
<p>A typical usage of simulation of random variables is Monte Carlo integration.
With <span class="math inline">\(X_1, \ldots, X_n\)</span> i.i.d. with density <span class="math inline">\(f\)</span>
<span class="math display">\[\hat{\mu}_{\textrm{MC}} := \frac{1}{n} \sum_{i=1}^n h(X_i) 
\rightarrow \mu := E(h(X_1)) = \int h(x) f(x) \ \mathrm{d}x\]</span>
for <span class="math inline">\(n \to \infty\)</span> by the law of large numbers (LLN).</p>
<p>Monte Carlo integration is a clever idea, where we use the computer
to simulate i.i.d. random variables and compute an average as
an approximation of an integral. The idea may be applied in a statistical
context, but it way also have applications outside of statistics and be
a direct competitor to numerical integration. By increasing <span class="math inline">\(n\)</span> the LLN
tells us that the average will eventually become a good approximation
of the integral. However, the LLN does not quantify how large <span class="math inline">\(n\)</span> should
be, and a fundamental question of Monte Carlo integration is therefore
to quantify the precision of the average.</p>
<p>This chapter first deals with the quantification of the precision — mostly
via the asymptotic variance in the central limit theorem. This will
on the one hand provide us with a quantification of precision for any
specific Monte Carlo approximation, and it will on the other hand provide
us with a way to compare different Monte Carlo integration techniques. The
direct use of the average above requires that we can simulate from
the distribution with density <span class="math inline">\(f\)</span>, but that might have low precision or it
might just be plain difficult. In the second half of the chapter we will treat importance
sampling, which is a technique for simulating from a different distribution
and use a <em>weighted</em> average to obtain the approximation of the integral.</p>
<div id="assessment" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> Assessment<a class="anchor" aria-label="anchor" href="#assessment"><i class="fas fa-link"></i></a>
</h2>
<p>The error analysis of Monte Carlo integration differs from that of
ordinary (deterministic) numerical integration methods. For the latter,
error analysis provides bounds on the error of the computable
approximation in terms of properties of the function to be integrated. Such
bounds provide a guarantee on what the error at most can be. It
is generally impossible to provide such a guarantee when using Monte
Carlo integration because the computed approximation is by construction
(pseudo)random. Thus the error analysis and assessment of the precision of
<span class="math inline">\(\hat{\mu}_{\textrm{MC}}\)</span> as an approximation of <span class="math inline">\(\mu\)</span> will be probabilistic.</p>
<p>There are two main approaches. We can use approximations of the distribution
of <span class="math inline">\(\hat{\mu}_{\textrm{MC}}\)</span> to assess the precision by computing a confidence
interval, say. Or we can provide finite sample upper bounds, known as
concentration inequalities, on the probability
that the error of <span class="math inline">\(\hat{\mu}_{\textrm{MC}}\)</span> is larger than a given <span class="math inline">\(\varepsilon\)</span>.
A concentration inequality can be turned into a confidence interval, if needed, or it can be used directly
to answer a question such as: if I want the approximation to have an error
smaller than <span class="math inline">\(\varepsilon = 10^{-3}\)</span>, how large does <span class="math inline">\(n\)</span> need to be
to guarantee this error bound with probability at least <span class="math inline">\(99.99\)</span>%?</p>
<p>Confidence intervals are typically computed using the central limit theorem and
an estimated value of the asymptotic variance. The most notable practical
problem is the estimation of that asymptotic variance, but otherwise the
method is straightforward to use. A major deficit of this method is
that the central limit theorem does not provide bounds – only approximations
of unknown precision for a finite <span class="math inline">\(n\)</span>. Thus without further analysis, we
cannot really be certain that the results from the central limit theorem
reflect the accuracy of <span class="math inline">\(\hat{\mu}_{\textrm{MC}}\)</span>. Concentration inequalities
provide actual guarantees, albeit probabilistic. They are, however, typically
problem specific and harder to derive, they involve constants that
are difficult to compute or estimate, and they tend to be pessimistic
in real applications. The focus in this chapter is therefore on using the
central limit theorem, but we do emphasize the example in Section <a href="mci.html#hd-int">5.2.2</a>
that shows how potentially misleading confidence intervals can be
when the convergence is slow.</p>
<div id="CLT-gamma" class="section level3" number="5.1.1">
<h3>
<span class="header-section-number">5.1.1</span> Using the central limit theorem<a class="anchor" aria-label="anchor" href="#CLT-gamma"><i class="fas fa-link"></i></a>
</h3>
<p>The CLT gives that
<span class="math display">\[\hat{\mu}_{\textrm{MC}} = \frac{1}{n} \sum_{i=1}^n h(X_i) \overset{\textrm{approx}} \sim 
\mathcal{N}(\mu, \sigma^2_{\textrm{MC}} / n)\]</span>
where
<span class="math display">\[\sigma^2_{\textrm{MC}} = V(h(X_1)) = \int (h(x) - \mu)^2 f(x) \ \mathrm{d}x.\]</span></p>
<p>We can estimate <span class="math inline">\(\sigma^2_{\textrm{MC}}\)</span> using the empirical variance
<span class="math display">\[\hat{\sigma}^2_{\textrm{MC}} = \frac{1}{n - 1} \sum_{i=1}^n (h(X_i) - 
\hat{\mu}_{\textrm{MC}})^2,\]</span></p>
<p>then the variance of <span class="math inline">\(\hat{\mu}_{\textrm{MC}}\)</span> is estimated as
<span class="math inline">\(\hat{\sigma}^2_{\textrm{MC}} / n\)</span> and a standard 95%
confidence interval for <span class="math inline">\(\mu\)</span> is
<span class="math display">\[\hat{\mu}_{\textrm{MC}} \pm 1.96 \frac{\hat{\sigma}_{\textrm{MC}}}{\sqrt{n}}.\]</span></p>
<div class="sourceCode" id="cb198"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/GammaDist.html">rgamma</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">8</span><span class="op">)</span> <span class="co"># h(x) = x</span>
<span class="va">mu_hat</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="co"># Cumulative average</span>
<span class="va">sigma_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="va">mu_hat</span><span class="op">[</span><span class="va">n</span><span class="op">]</span>  <span class="co"># Theoretical value 8</span></code></pre></div>
<pre><code>## [1] 8.09556</code></pre>
<div class="sourceCode" id="cb200"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sigma_hat</span>  <span class="co"># Theoretical value sqrt(8) = 2.8284</span></code></pre></div>
<pre><code>## [1] 2.861755</code></pre>
<div class="sourceCode" id="cb202"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/qplot.html">qplot</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span>, <span class="va">mu_hat</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_ribbon.html">geom_ribbon</a></span><span class="op">(</span>
  mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>
    ymin <span class="op">=</span> <span class="va">mu_hat</span> <span class="op">-</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="va">sigma_hat</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span>, 
    ymax <span class="op">=</span> <span class="va">mu_hat</span> <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="va">sigma_hat</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span>
  <span class="op">)</span>, fill <span class="op">=</span> <span class="st">"gray"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/coord_cartesian.html">coord_cartesian</a></span><span class="op">(</span>ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">6</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> </code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:gammaMCCLT"></span>
<img src="CSwR_files/figure-html/gammaMCCLT-1.png" alt="Sample path with confidence band for Monte Carlo integration of the mean of a gamma distributed random variable." width="70%"><p class="caption">
Figure 5.1: Sample path with confidence band for Monte Carlo integration of the mean of a gamma distributed random variable.
</p>
</div>
</div>
<div id="concentration-inequalities" class="section level3" number="5.1.2">
<h3>
<span class="header-section-number">5.1.2</span> Concentration inequalities<a class="anchor" aria-label="anchor" href="#concentration-inequalities"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(X\)</span> is a real valued random variable with finite second moment, <span class="math inline">\(\mu = E(X)\)</span>
and <span class="math inline">\(\sigma^2 = V(X)\)</span>, Chebychev’s
inequality holds
<span class="math display">\[P(|X - \mu| &gt; \varepsilon) \leq \frac {\sigma^2}{\varepsilon^2}\]</span>
for all <span class="math inline">\(\varepsilon &gt; 0\)</span>.
This inequality implies, for instance, that for the simple Monte Carlo average
we have the inequality
<span class="math display">\[P(|\hat{\mu}_{\textrm{MC}} - \mu| &gt; \varepsilon) \leq \frac{\sigma^2_{\textrm{MC}}}{n\varepsilon^2}.\]</span>
A common usage of this inequality is for the qualitative statement known as the
<em>law of large numbers</em>: for any <span class="math inline">\(\varepsilon &gt; 0\)</span>
<span class="math display">\[P(|\hat{\mu}_{\textrm{MC}} - \mu| &gt; \varepsilon) \rightarrow 0\]</span>
for <span class="math inline">\(n \to \infty\)</span>. Or <span class="math inline">\(\hat{\mu}_{\textrm{MC}}\)</span> <em>converges in probability</em> towards
<span class="math inline">\(\mu\)</span> as <span class="math inline">\(n\)</span> tends to infinity. However, the inequality actually also provides
a quantitative statement about how accurate <span class="math inline">\(\hat{\mu}_{\textrm{MC}}\)</span> is as an
approximation of <span class="math inline">\(\mu\)</span>.</p>
<p>Chebyshev’s inequality is useful due to its minimal assumption of a finite
second moment. However, it typically doesn’t give a very tight bound on the
probability <span class="math inline">\(P(|X - \mu| &gt; \varepsilon)\)</span>. Much better inequalities can be obtained
under stronger assumptions, in particular finite exponential moments.</p>
<p>Assuming that the moment generating function of <span class="math inline">\(X\)</span> is finite,
<span class="math inline">\(M(t) = E(e^{tX}) &lt; \infty\)</span>, for some suitable <span class="math inline">\(t \in \mathbb{R}\)</span>, it follows from
<a href="https://en.wikipedia.org/wiki/Markov%27s_inequality">Markov’s inequality</a>
that
<span class="math display">\[P(X - \mu &gt; \varepsilon) = P(e^{tX} &gt; e^{t(\varepsilon + \mu)}) \leq e^{-t(\varepsilon + \mu)}M(t),\]</span>
which can provide a very tight upper bound by minimizing the bound over <span class="math inline">\(t\)</span>. This
requires some knowledge of the moment generating function. We illustrate the
usage of this inequality below by considering the gamma distribution where the
moment generating function is well known.</p>
</div>
<div id="exponential-tail-bound-for-gamma-distributed-variables" class="section level3" number="5.1.3">
<h3>
<span class="header-section-number">5.1.3</span> Exponential tail bound for Gamma distributed variables<a class="anchor" aria-label="anchor" href="#exponential-tail-bound-for-gamma-distributed-variables"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(X\)</span> follows a Gamma distribution with shape parameter
<span class="math inline">\(\lambda &gt; 0\)</span> and <span class="math inline">\(t &lt;1\)</span>, then
<span class="math display">\[M(t) = \frac{1}{\Gamma(\lambda)} \int_0^{\infty} x^{\lambda - 1} e^{-(1-t) x} \,
\mathrm{d} x = \frac{1}{(1-t)^{\lambda}}.\]</span>
Whence
<span class="math display">\[P(X-\lambda &gt; \varepsilon) \leq e^{-t(\varepsilon + \lambda)}
\frac{1}{(1-t)^{\lambda}}.\]</span>
Minimization over <span class="math inline">\(t\)</span> of the right hand side gives the minimizer
<span class="math inline">\(t = \varepsilon/(\varepsilon + \lambda)\)</span> and the upper bound
<span class="math display">\[P(X-\lambda &gt; \varepsilon) \leq e^{-\varepsilon} \left(\frac{\varepsilon + \lambda}{\lambda
    }\right)^{\lambda}.\]</span>
Compare this to the bound
<span class="math display">\[P(|X-\lambda| &gt; \varepsilon) \leq \frac{\lambda}{\varepsilon^2}\]</span>
from Chebychev’s inequality.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tailbounds"></span>
<img src="CSwR_files/figure-html/tailbounds-1.png" alt="Actual tail probabilities (left) for the gamma distribution, computed via the pgamma function, compared it to the tight bound (red) and the weaker bound from Chebychev’s inequality (blue). The differences in the tail are more clearly seen for the log-probabilities (right)" width="49%"><img src="CSwR_files/figure-html/tailbounds-2.png" alt="Actual tail probabilities (left) for the gamma distribution, computed via the pgamma function, compared it to the tight bound (red) and the weaker bound from Chebychev’s inequality (blue). The differences in the tail are more clearly seen for the log-probabilities (right)" width="49%"><p class="caption">
Figure 5.2: Actual tail probabilities (left) for the gamma distribution, computed via the <code>pgamma</code> function, compared it to the tight bound (red) and the weaker bound from Chebychev’s inequality (blue). The differences in the tail are more clearly seen for the log-probabilities (right)
</p>
</div>
</div>
</div>
<div id="importance-sampling" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Importance sampling<a class="anchor" aria-label="anchor" href="#importance-sampling"><i class="fas fa-link"></i></a>
</h2>
<p>When we are only interested in Monte Carlo integration, we do not
need to sample from the target distribution.</p>
<p>Observe that
<span class="math display">\[\begin{align}
\mu = \int h(x) f(x) \ \mathrm{d}x &amp; = \int h(x) \frac{f(x)}{g(x)} g(x) \ \mathrm{d}x \\
&amp; = \int h(x) w^*(x) g(x) \ \mathrm{d}x
\end{align}\]</span></p>
<p>whenever <span class="math inline">\(g\)</span> is a density fulfilling that
<span class="math display">\[g(x) = 0 \Rightarrow f(x) = 0.\]</span></p>
<p>With <span class="math inline">\(X_1, \ldots, X_n\)</span> i.i.d. with density <span class="math inline">\(g\)</span> define the <em>weights</em>
<span class="math display">\[w^*(X_i) = f(X_i) / g(X_i).\]</span>
The <em>importance sampling</em> estimator is
<span class="math display">\[\hat{\mu}_{\textrm{IS}}^* := \frac{1}{n} \sum_{i=1}^n h(X_i)w^*(X_i).\]</span>
It has mean <span class="math inline">\(\mu\)</span>. Again by the LLN
<span class="math display">\[\hat{\mu}_{\textrm{IS}}^* \rightarrow E(h(X_1) w^*(X_1)) = \mu.\]</span></p>
<p>We will illustrate the use of importance sampling by computing the mean in
the gamma distribution via simulations from a Gaussian distribution, cf. also
Section <a href="mci.html#CLT-gamma">5.1.1</a>.</p>
<div class="sourceCode" id="cb203"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">10</span>, <span class="fl">3</span><span class="op">)</span>
<span class="va">w_star</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/GammaDist.html">dgamma</a></span><span class="op">(</span><span class="va">x</span>, <span class="fl">8</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span>, <span class="fl">10</span>, <span class="fl">3</span><span class="op">)</span> 
<span class="va">mu_hat_IS</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">x</span> <span class="op">*</span> <span class="va">w_star</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span><span class="op">)</span>
<span class="va">mu_hat_IS</span><span class="op">[</span><span class="va">n</span><span class="op">]</span>  <span class="co"># Theoretical value 8</span></code></pre></div>
<pre><code>## [1] 7.995228</code></pre>
<p>To assess the precision of the importance sampling estimate via the CLT we
need the variance of the average just as for plain Monte Carlo integration.
By the CLT
<span class="math display">\[\hat{\mu}_{\textrm{IS}}^* \overset{\textrm{approx}} \sim 
\mathcal{N}(\mu, \sigma^{*2}_{\textrm{IS}} / n)\]</span>
where
<span class="math display">\[\sigma^{*2}_{\textrm{IS}} = V (h(X_1)w^*(X_1)) = \int (h(x) w^*(x) - \mu)^2 g(x) \ \mathrm{d}x.\]</span></p>
<p>The importance sampling variance can be estimated similarly as the Monte Carlo variance
<span class="math display">\[\hat{\sigma}^{*2}_{\textrm{IS}} = \frac{1}{n - 1} \sum_{i=1}^n (h(X_i)w^*(X_i) - \hat{\mu}_{\textrm{IS}}^*)^2,\]</span>
and a 95% standard confidence interval is computed as
<span class="math display">\[\hat{\mu}^*_{\textrm{IS}} \pm 1.96 \frac{\hat{\sigma}^*_{\textrm{IS}}}{\sqrt{n}}.\]</span></p>
<div class="sourceCode" id="cb205"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sigma_hat_IS</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">x</span> <span class="op">*</span> <span class="va">w_star</span><span class="op">)</span>
<span class="va">sigma_hat_IS</span>  <span class="co"># Theoretical value ??</span></code></pre></div>
<pre><code>## [1] 3.499995</code></pre>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:gamma-IS-fig2"></span>
<img src="CSwR_files/figure-html/gamma-IS-fig2-1.png" alt="Sample path with confidence band for importance sampling Monte Carlo integration of the mean of a gamma distributed random variable via simulations from a Gaussian distribution." width="500"><p class="caption">
Figure 5.3: Sample path with confidence band for importance sampling Monte Carlo integration of the mean of a gamma distributed random variable via simulations from a Gaussian distribution.
</p>
</div>
<p>It may happen that <span class="math inline">\(\sigma^{*2}_{\textrm{IS}} &gt; \sigma^2_{\textrm{MC}}\)</span> or
<span class="math inline">\(\sigma^{*2}_{\textrm{IS}} &lt; \sigma^2_{\textrm{MC}}\)</span> depending on <span class="math inline">\(h\)</span> and <span class="math inline">\(g\)</span>, but
by choosing <span class="math inline">\(g\)</span> cleverly so that <span class="math inline">\(h(x) w^*(x)\)</span> becomes as constant as possible,
importance sampling can often reduce the variance compared to plain
Monte Carlo integration.</p>
<p>For the mean of the gamma distribution above, <span class="math inline">\(\sigma^{*2}_{\textrm{IS}}\)</span>
is about 50% larger than <span class="math inline">\(\sigma^2_{\textrm{MC}}\)</span>, so we loose precision
by using importance sampling this way when compared to plain Monte Carlo integration.
In Section <a href="mci.html#network">5.3</a> we consider a different example where we achieve
a considerable variance reduction by using importance sampling.</p>
<div id="unknown-normalization-constants" class="section level3" number="5.2.1">
<h3>
<span class="header-section-number">5.2.1</span> Unknown normalization constants<a class="anchor" aria-label="anchor" href="#unknown-normalization-constants"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(f = c^{-1} q\)</span> with <span class="math inline">\(c\)</span> unknown then
<span class="math display">\[c = \int q(x) \ \mathrm{d}x = \int \frac{q(x)}{g(x)} g(x) \ d x,\]</span>
and
<span class="math display">\[\mu = \frac{\int h(x) w^*(x) g(x) \ d x}{\int w^*(x) g(x) \ d x},\]</span>
where <span class="math inline">\(w^*(x) = q(x) / g(x).\)</span></p>
<p>If <span class="math inline">\(X_1, \ldots, X_n\)</span> are then i.i.d. from the distribution with density <span class="math inline">\(g\)</span>,
an importance sampling estimate of <span class="math inline">\(\mu\)</span> can be computed as
<span class="math display">\[\hat{\mu}_{\textrm{IS}} = \frac{\sum_{i=1}^n h(X_i) w^*(X_i)}{\sum_{i=1}^n w^*(X_i)} = \sum_{i=1}^n h(X_i) w(X_i),\]</span>
where <span class="math inline">\(w^*(X_i) = q(X_i) / g(X_i)\)</span> and
<span class="math display">\[w(X_i) = \frac{w^*(X_i)}{\sum_{i=1}^n w^*(X_i)}\]</span>
are the <em>standardized weights</em>. This works irrespectively of the value of the
normalizing constant <span class="math inline">\(c\)</span>, and it actually works if also <span class="math inline">\(g\)</span> is unnormalized.</p>
<p>Revisiting the mean of the gamma distribution, we can implement importance
sampling via samples from a Gaussian distribution but using weights computed
without the normalization constants.</p>
<div class="sourceCode" id="cb207"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">w_star</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>
<span class="va">x_pos</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="va">x</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">]</span>
<span class="va">w_star</span><span class="op">[</span><span class="va">x</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">(</span><span class="va">x_pos</span> <span class="op">-</span> <span class="fl">10</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="fl">18</span> <span class="op">-</span> <span class="va">x_pos</span> <span class="op">+</span> <span class="fl">7</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">x_pos</span><span class="op">)</span><span class="op">)</span>
<span class="va">mu_hat_IS</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">x</span> <span class="op">*</span> <span class="va">w_star</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">w_star</span><span class="op">)</span>
<span class="va">mu_hat_IS</span><span class="op">[</span><span class="va">n</span><span class="op">]</span>  <span class="co"># Theoretical value 8</span></code></pre></div>
<pre><code>## [1] 8.102177</code></pre>
<p>The variance of the IS estimator with standardized weights is a little more
complicated, because the estimator is a ratio of random variables. From the
multivariate CLT
<span class="math display">\[\frac{1}{n} \sum_{i=1}^n \left(\begin{array}{c}
 h(X_i) w^*(X_i) \\
 w^*(X_i) 
\end{array}\right) \overset{\textrm{approx}}{\sim} 
\mathcal{N}\left( c \left(\begin{array}{c} \mu  \\   {1} \end{array}\right),
\frac{1}{n} \left(\begin{array}{cc} \sigma^{*2}_{\textrm{IS}} &amp; \gamma \\ \gamma &amp; \sigma^2_{w^*}
\end{array} \right)\right),\]</span>
where
<span class="math display">\[\begin{align}
\sigma^{*2}_{\textrm{IS}} &amp; = V(h(X_1)w^*(X_1)) \\
\gamma &amp; = \mathrm{cov}(h(X_1)w^*(X_1), w^*(X_1)) \\
\sigma_{w^*}^2 &amp; = V (w^*(X_1)).
\end{align}\]</span></p>
<p>We can then apply the <span class="math inline">\(\Delta\)</span>-method with <span class="math inline">\(t(x, y) = x / y\)</span>. Note that
<span class="math inline">\(Dt(x, y) = (1 / y, - x / y^2)\)</span>, whence
<span class="math display">\[Dt(c\mu, c)   \left(\begin{array}{cc} \hat{\sigma}^{*2}_{\textrm{IS}} &amp; \gamma \\ \gamma &amp; \sigma^2_{w^*}
\end{array} \right) Dt(c\mu, c)^T = c^{-2} (\sigma^{*2}_{\textrm{IS}} + \mu^2 \sigma_{w^*}^2 - 2 \mu \gamma).\]</span></p>
<p>By the <span class="math inline">\(\Delta\)</span>-method
<span class="math display">\[\hat{\mu}_{\textrm{IS}} \overset{\textrm{approx}}{\sim} 
\mathcal{N}(\mu, c^{-2} (\sigma^{*2}_{\textrm{IS}} + \mu^2 \sigma_{w^*}^2 - 2 \mu \gamma) / n).\]</span>
The unknown quantities in the asymptotic variance must be estimated using e.g. 
their empirical equivalents, and if <span class="math inline">\(c \neq 1\)</span> (we have used unnormalized densities)
it is necessary to estimate <span class="math inline">\(c\)</span> as
<span class="math inline">\(\hat{c} = \frac{1}{n} \sum_{i=1}^n w^*(X_i)\)</span>
to compute an estimate of the variance.</p>
<p>For the example with the mean of the gamma distribution, we find the following
estimate of the variance.</p>
<div class="sourceCode" id="cb209"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">c_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">w_star</span><span class="op">)</span>
<span class="va">sigma_hat_IS</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">x</span> <span class="op">*</span> <span class="va">w_star</span><span class="op">)</span>
<span class="va">sigma_hat_w_star</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">w_star</span><span class="op">)</span>
<span class="va">gamma_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">x</span> <span class="op">*</span> <span class="va">w_star</span>, <span class="va">w_star</span><span class="op">)</span>
<span class="va">sigma_hat_IS_w</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">sigma_hat_IS</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">mu_hat_IS</span><span class="op">[</span><span class="va">n</span><span class="op">]</span><span class="op">^</span><span class="fl">2</span> <span class="op">*</span> <span class="va">sigma_hat_w_star</span><span class="op">^</span><span class="fl">2</span> <span class="op">-</span> 
                         <span class="fl">2</span> <span class="op">*</span> <span class="va">mu_hat_IS</span><span class="op">[</span><span class="va">n</span><span class="op">]</span> <span class="op">*</span> <span class="va">gamma_hat</span><span class="op">)</span> <span class="op">/</span> <span class="va">c_hat</span>
<span class="va">sigma_hat_IS_w</span></code></pre></div>
<pre><code>## [1] 3.198314</code></pre>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:gamma-IS-fig3"></span>
<img src="CSwR_files/figure-html/gamma-IS-fig3-1.png" alt="Sample path with confidence band for importance sampling Monte Carlo integration of the mean of a gamma distributed random variable via simulations from a Gaussian distribution and using standardized weights." width="500"><p class="caption">
Figure 5.4: Sample path with confidence band for importance sampling Monte Carlo integration of the mean of a gamma distributed random variable via simulations from a Gaussian distribution and using standardized weights.
</p>
</div>
<p>In this example, the variance when using standardized weights is a little lower
than using unstandardized weights, but still larger than the variance for
the plain Monte Carlo average.</p>
</div>
<div id="hd-int" class="section level3" number="5.2.2">
<h3>
<span class="header-section-number">5.2.2</span> Computing a high-dimensional integral<a class="anchor" aria-label="anchor" href="#hd-int"><i class="fas fa-link"></i></a>
</h3>
<p>To further illustrate the usage but also the limitations of Monte Carlo
integration, consider the following <span class="math inline">\(p\)</span>-dimensional integral
<span class="math display">\[ \int e^{-\frac{1}{2}\left(x_1^2 + \sum_{i=2}^p (x_i - \alpha x_{i-1})^2\right)}  \mathrm{d} x.\]</span>
Now this integral is not even expressed as an expectation w.r.t. any distribution
in the first place – it is an integral w.r.t. Lebesgue measure in <span class="math inline">\(\mathbb{R}^p\)</span>.
We use the same idea as in importance sampling to rewrite the integral
as an expectation w.r.t. a probability distribution. There might be many ways
to do this, and the following is just one.</p>
<p>Rewrite the exponent as
<span class="math display">\[||x||_2^2 + \sum_{i = 2}^p \alpha^2 x_{i-1}^2 - 2\alpha x_i x_{i-1}\]</span>
so that</p>
<p><span class="math display">\[\begin{align*}
\int e^{-\frac{1}{2}\left(x_1^2 + \sum_{i=2}^p (x_i - \alpha x_{i-1})^2\right)}  \mathrm{d} x &amp; =  \int e^{- \frac{1}{2} \sum_{i = 2}^p \alpha^2 
x_{i-1}^2 - 2\alpha x_i x_{i-1}}  e^{-\frac{||x||_2^2}{2}} \mathrm{d} x \\
&amp; =  (2 \pi)^{p/2} \int e^{- \frac{1}{2} \sum_{i = 2}^p \alpha^2 
x_{i-1}^2 - 2\alpha x_i x_{i-1}} f(x) \mathrm{d} x
\end{align*}\]</span></p>
<p>where <span class="math inline">\(f\)</span> is the density for the <span class="math inline">\(\mathcal{N}(0, I_p)\)</span> distribution. Thus if
<span class="math inline">\(X \sim \mathcal{N}(0, I_p)\)</span>,
<span class="math display">\[ \int e^{-\frac{1}{2}\left(x_1^2 + \sum_{i=2}^p (x_i - \alpha x_{i-1})^2\right)}  \mathrm{d} x = (2 \pi)^{p/2}  E\left( e^{- \frac{1}{2} \sum_{i = 2}^p \alpha^2 
X_{i-1}^2 - 2\alpha X_i X_{i-1}} \right).\]</span></p>
<p>The Monte Carlo integration below computes
<span class="math display">\[\mu = E\left( e^{- \frac{1}{2} \sum_{i = 2}^p \alpha^2 
X_{i-1}^2 - 2\alpha X_i X_{i-1}} \right)\]</span>
by generating <span class="math inline">\(p\)</span>-dimensional random
variables from <span class="math inline">\(\mathcal{N}(0, I_p)\)</span>. It can actually be shown that <span class="math inline">\(\mu = 1\)</span>,
but we skip the proof of that.</p>
<p>First, we implement the function we want to integrate.</p>
<div class="sourceCode" id="cb211"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">h</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">alpha</span> <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span><span class="op">{</span> 
  <span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
  <span class="va">tmp</span> <span class="op">&lt;-</span> <span class="va">alpha</span> <span class="op">*</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">p</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">]</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">tmp</span> <span class="op">/</span> <span class="fl">2</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="va">p</span><span class="op">]</span><span class="op">)</span> <span class="op">*</span> <span class="va">tmp</span><span class="op">)</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>Then we specify various parameters.</p>
<div class="sourceCode" id="cb212"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">10000</span> <span class="co"># The number of random variables to generate</span>
<span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">100</span>   <span class="co"># The dimension of each random variable</span></code></pre></div>
<p>The actual computation is implemented using the <code>apply</code> function. We
first look at the case with <span class="math inline">\(\alpha = 0.1\)</span>.</p>
<div class="sourceCode" id="cb213"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span> <span class="op">*</span> <span class="va">p</span><span class="op">)</span>, <span class="va">n</span>, <span class="va">p</span><span class="op">)</span>
<span class="va">evaluations</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">x</span>, <span class="fl">1</span>, <span class="va">h</span><span class="op">)</span></code></pre></div>
<p>We can then plot the cumulative average and compare it to the
actual value of the integral that we know is 1.</p>
<div class="sourceCode" id="cb214"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">mu_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">evaluations</span><span class="op">)</span> <span class="op">/</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">mu_hat</span>, pch <span class="op">=</span> <span class="fl">20</span>, xlab <span class="op">=</span> <span class="st">"n"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h <span class="op">=</span> <span class="fl">1</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="CSwR_files/figure-html/MC_path-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>If we want to control the error with probability 0.95 we can use Chebychev’s inequality
and solve for <span class="math inline">\(\varepsilon\)</span> using the estimated variance.</p>
<div class="sourceCode" id="cb215"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">mu_hat</span>, pch <span class="op">=</span> <span class="fl">20</span>, xlab <span class="op">=</span> <span class="st">"n"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h <span class="op">=</span> <span class="fl">1</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span>
<span class="va">sigma_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">evaluations</span><span class="op">)</span>
<span class="va">epsilon</span> <span class="op">&lt;-</span> <span class="va">sigma_hat</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span> <span class="op">*</span> <span class="fl">0.05</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span>, <span class="va">mu_hat</span> <span class="op">+</span> <span class="va">epsilon</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span>, <span class="va">mu_hat</span> <span class="op">-</span> <span class="va">epsilon</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="CSwR_files/figure-html/MC_cheb-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>The confidence bands provided by the
central limit theorem are typically more accurate estimates of the actual uncertainty
than the upper bounds provided by Chebychev’s inequality.</p>
<div class="sourceCode" id="cb216"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">mu_hat</span>, pch <span class="op">=</span> <span class="fl">20</span>, xlab <span class="op">=</span> <span class="st">"n"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h <span class="op">=</span> <span class="fl">1</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span>, <span class="va">mu_hat</span> <span class="op">+</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">sigma_hat</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span>, <span class="va">mu_hat</span> <span class="op">-</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">sigma_hat</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="CSwR_files/figure-html/MC_CLT-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>To illustrate the limitations of Monte Carlo integration we increase <span class="math inline">\(\alpha\)</span> to
<span class="math inline">\(\alpha = 0.4\)</span>.</p>
<div class="sourceCode" id="cb217"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">evaluations</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">x</span>, <span class="fl">1</span>, <span class="va">h</span>, alpha <span class="op">=</span> <span class="fl">0.4</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="CSwR_files/figure-html/MC_CLT2-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>The sample path above is not carefully selected to be pathological. Due to
occasional large values, the typical sample path will show occasional large jumps,
and the variance may easily be grossly underestimated.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:MC-CLT3"></span>
<img src="CSwR_files/figure-html/MC-CLT3-1.png" alt="Four sample paths of the cumulative average for $\alpha = 0.4$." width="100%"><p class="caption">
Figure 5.5: Four sample paths of the cumulative average for <span class="math inline">\(\alpha = 0.4\)</span>.
</p>
</div>
<p>To be fair, it is the choice of a standard multivariate normal distribution as
the reference distribution for large <span class="math inline">\(\alpha\)</span> that is problematic rather than
Monte Carlo integration and importance sampling as such. However, in high
dimensions it can be difficult to choose a suitable distribution to sample
from.</p>
<p>The difficulty for the specific integral is due to the exponent of the integrand,
which can become large and positive if <span class="math inline">\(x_i \simeq x_{i-1}\)</span> for enough coordinates.
This happens rarely for independent random variables, but large values of rare
events can, nevertheless, contribute notably to the integral. The
larger <span class="math inline">\(\alpha \in (0, 1)\)</span> is, the more pronounced is the problem with occasional
large values of the integrand. It is possible to use importance sampling and
instead sample from a distribution where the large values are more likely. For this
particular example we would need to simulate from a distribution where the
variables are dependent, and we will not pursue that.</p>
</div>
</div>
<div id="network" class="section level2" number="5.3">
<h2>
<span class="header-section-number">5.3</span> Network failure<a class="anchor" aria-label="anchor" href="#network"><i class="fas fa-link"></i></a>
</h2>
<p>In this section we consider a more serious application of importance
sampling. Though still a toy example, where we can find an exact solution,
the example illustrates well the type of application where we want to
approximate a small probability using a Monte Carlo average. Importance sampling can
then increase the probability of the rare event and as a result make the
variance of the Monte Carlo average smaller.</p>
<p>We will consider the following network consisting of ten nodes and with some of
the nodes connected.</p>
<div class="inline-figure"><img src="figures/networkfig.png" width="85%" style="display: block; margin: auto;"></div>
<p>The network could be a computer network with ten computers. The different
connections (edges) may “fail” independently with probability <span class="math inline">\(p\)</span>, and
we ask the question: what is the probability that node 1 and node 10 are disconnected?</p>
<p>We can answer this question by computing an integral of an indicator
function, that is, by computing the sum
<span class="math display">\[\mu = \sum_{x \in \{0,1\}^{18}} 1_B(x) f_p(x)\]</span>
where <span class="math inline">\(f_p(x)\)</span> is the point probability of <span class="math inline">\(x\)</span>, with <span class="math inline">\(x\)</span> representing which of the 18 edges
in the graph that fail, and <span class="math inline">\(B\)</span> represents the set of edges where node 1 and node 10 are
disconnected. By simulating edge failures we can approximate the sum as a
Monte Carlo average.</p>
<p>The network of nodes can be represented as a graph adjacency matrix <span class="math inline">\(A\)</span> such
that <span class="math inline">\(A_{ij} = 1\)</span> if and only if there is an edge between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> (and <span class="math inline">\(A_{ij} = 0\)</span>
otherwise).</p>
<div class="sourceCode" id="cb218"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">A</span>  <span class="co"># Graph adjacency matrix</span></code></pre></div>
<pre><code>##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
##  [1,]    0    1    0    1    1    0    0    0    0     0
##  [2,]    1    0    1    0    0    1    0    0    0     0
##  [3,]    0    1    0    0    0    1    1    1    0     1
##  [4,]    1    0    0    0    1    0    0    1    0     0
##  [5,]    1    0    0    1    0    0    0    1    1     0
##  [6,]    0    1    1    0    0    0    1    0    0     1
##  [7,]    0    0    1    0    0    1    0    0    0     1
##  [8,]    0    0    1    1    1    0    0    0    1     0
##  [9,]    0    0    0    0    1    0    0    1    0     1
## [10,]    0    0    1    0    0    1    1    0    1     0</code></pre>
<p>To compute the probability that 1 and 10 are disconnected by Monte Carlo
integration, we need to sample (sub)graphs by randomly removing some of the
edges. This is implemented using the upper triangular part of the (symmetric)
adjacency matrix.</p>
<div class="sourceCode" id="cb220"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sim_net</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">Aup</span>, <span class="va">p</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">ones</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">Aup</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span>
  <span class="va">Aup</span><span class="op">[</span><span class="va">ones</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span>
    <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>, 
    <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">ones</span><span class="op">)</span>, 
    replace <span class="op">=</span> <span class="cn">TRUE</span>,
    prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">p</span>, <span class="fl">1</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span>
  <span class="op">)</span>
  <span class="va">Aup</span> 
<span class="op">}</span></code></pre></div>
<p>The core of the implementation above uses the <code><a href="https://rdrr.io/r/base/sample.html">sample()</a></code> function, which
can sample with replacement from the set <span class="math inline">\(\{0, 1\}\)</span>. The vector <code>ones</code>
contains indices of the (upper triangular part of the) adjacency matrix
containing a <code>1</code>, and these positions are replaced by the sampled values before
the matrix is returned.</p>
<p>It is fairly fast to sample even a large number of random graphs this way.</p>
<div class="sourceCode" id="cb221"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">Aup</span> <span class="op">&lt;-</span> <span class="va">A</span>
<span class="va">Aup</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/lower.tri.html">lower.tri</a></span><span class="op">(</span><span class="va">Aup</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">0</span>
<span class="fu"><a href="https://rdrr.io/r/base/system.time.html">system.time</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="fl">1e5</span>, <span class="op">{</span><span class="fu">sim_net</span><span class="op">(</span><span class="va">Aup</span>, <span class="fl">0.5</span><span class="op">)</span>; <span class="cn">NULL</span><span class="op">}</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##    user  system elapsed 
##   1.854   0.975   2.297</code></pre>
<p>The second function we implement checks network connectivity based on
the upper triangular part of the adjacency matrix.
It relies on the fact that there is a path from node 1 to node 10 consisting
of <span class="math inline">\(k\)</span> edges if and only if <span class="math inline">\((A^k)_{1,10} &gt; 0\)</span>. We see directly that such a
path needs to consist of at least <span class="math inline">\(k = 3\)</span> edges. Also, we don’t need to
check paths with more than <span class="math inline">\(k = 9\)</span> edges as they will contain the one node
multiple times and can thus be shortened.</p>
<div class="sourceCode" id="cb223"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">discon</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">Aup</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">A</span> <span class="op">&lt;-</span> <span class="va">Aup</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">Aup</span><span class="op">)</span>
  <span class="va">i</span> <span class="op">&lt;-</span> <span class="fl">3</span>
  <span class="va">Apow</span> <span class="op">&lt;-</span> <span class="va">A</span> <span class="op">%*%</span> <span class="va">A</span> <span class="op">%*%</span> <span class="va">A</span> <span class="co"># A%^%3</span>
  <span class="kw">while</span><span class="op">(</span><span class="va">Apow</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">10</span><span class="op">]</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">&amp;</span> <span class="va">i</span> <span class="op">&lt;</span> <span class="fl">9</span><span class="op">)</span> <span class="op">{</span>
    <span class="va">Apow</span> <span class="op">&lt;-</span> <span class="va">Apow</span> <span class="op">%*%</span> <span class="va">A</span>
    <span class="va">i</span> <span class="op">&lt;-</span> <span class="va">i</span> <span class="op">+</span> <span class="fl">1</span>    
  <span class="op">}</span>
  <span class="va">Apow</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">10</span><span class="op">]</span> <span class="op">==</span> <span class="fl">0</span>  <span class="co"># TRUE if nodes 1 and 10 not connected</span>
<span class="op">}</span></code></pre></div>
<p>We then obtain the following estimate of the probability of nodes 1 and 10 being
disconnected using Monte Carlo integration.</p>
<div class="sourceCode" id="cb224"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">seed</span> <span class="op">&lt;-</span> <span class="fl">27092016</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="va">seed</span><span class="op">)</span>
<span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1e5</span>
<span class="va">tmp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="va">n</span>, <span class="fu">discon</span><span class="op">(</span><span class="fu">sim_net</span><span class="op">(</span><span class="va">Aup</span>, <span class="fl">0.05</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="va">mu_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">tmp</span><span class="op">)</span></code></pre></div>
<p>As this is a random approximation, we should report not only the Monte Carlo
estimate but also the confidence interval. Since the estimate is an average of
0-1-variables, we can estimate the variance, <span class="math inline">\(\sigma^2\)</span>, of the individual terms
using that <span class="math inline">\(\sigma^2 = \mu (1 - \mu)\)</span>. We could just as well have used the
empirical variance, which would give almost the same numerical value
as <span class="math inline">\(\hat{\mu} (1 - \hat{\mu})\)</span>, but we use the latter estimator to illustrate
that any (good) estimator of <span class="math inline">\(\sigma^2\)</span> can be used when estimating the asymptotic
variance.</p>
<div class="sourceCode" id="cb225"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">mu_hat</span> <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">mu_hat</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">mu_hat</span><span class="op">)</span> <span class="op">/</span> <span class="va">n</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 0.000226 0.000340 0.000454</code></pre>
<p>The estimated probability is low and only in about 1 of 3000 simulated graphs
will node 1 and 10 be disconnected. This suggests that importance sampling
can be useful if we sample from a probability distribution with a larger
probability of edge failure.</p>
<p>To implement importance sampling we note that the point probabilities
(the density w.r.t. counting measure) for sampling the 18 independent 0-1-variables
<span class="math inline">\(x = (x_1, \ldots, x_{18})\)</span> with <span class="math inline">\(P(X_i = 0) = p\)</span> is</p>
<p><span class="math display">\[f_p(x) = p^{18 - s} (1- p)^{s}\]</span></p>
<p>where <span class="math inline">\(s = \sum_{i=1}^{18} x_i\)</span>. In the implementation,
weights are computed that correspond to using probability <span class="math inline">\(p_0\)</span> (with density <span class="math inline">\(g = f_{p_0}\)</span>)
instead of <span class="math inline">\(p\)</span>, and the weights are only computed if node 1
and 10 are disconnected.</p>
<div class="sourceCode" id="cb227"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">weights</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">Aup</span>, <span class="va">Aup0</span>, <span class="va">p0</span>, <span class="va">p</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">w</span> <span class="op">&lt;-</span> <span class="fu">discon</span><span class="op">(</span><span class="va">Aup0</span><span class="op">)</span>
  <span class="kw">if</span> <span class="op">(</span><span class="va">w</span><span class="op">)</span> <span class="op">{</span>
    <span class="va">s</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">Aup0</span><span class="op">)</span>
    <span class="va">w</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">p</span> <span class="op">/</span> <span class="va">p0</span><span class="op">)</span><span class="op">^</span><span class="fl">18</span> <span class="op">*</span> <span class="op">(</span><span class="va">p0</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">p</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p0</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="va">s</span>
  <span class="op">}</span>
  <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">w</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>The implementation uses the formula</p>
<p><span class="math display">\[
w(x) = \frac{f_p(x)}{f_{p_0}(x)} = \frac{p^{18 - s} (1- p)^{s}}{p_0^{18 - s} (1- p_0)^{s}} = 
\left(\frac{p}{p_0}\right)^{18} \left(\frac{p_0 (1- p)}{p (1- p_0)}\right)^s.
\]</span></p>
<p>The importance sampling estimate of <span class="math inline">\(\mu\)</span> is then computed.</p>
<div class="sourceCode" id="cb228"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="va">seed</span><span class="op">)</span>
<span class="va">tmp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="va">n</span>, <span class="fu"><a href="https://rdrr.io/r/stats/weights.html">weights</a></span><span class="op">(</span><span class="va">Aup</span>, <span class="fu">sim_net</span><span class="op">(</span><span class="va">Aup</span>, <span class="fl">0.2</span><span class="op">)</span>, <span class="fl">0.2</span>, <span class="fl">0.05</span><span class="op">)</span><span class="op">)</span>
<span class="va">mu_hat_IS</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">tmp</span><span class="op">)</span></code></pre></div>
<p>And we obtain the following confidence interval using the
empirical variance estimate <span class="math inline">\(\hat{\sigma}^2\)</span>.</p>
<div class="sourceCode" id="cb229"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">mu_hat_IS</span> <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">tmp</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 0.000262 0.000296 0.000330</code></pre>
<p>The ratio of estimated variances for the plain Monte Carlo estimate and the
importance sampling estimate is</p>
<div class="sourceCode" id="cb231"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">mu_hat</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">mu_hat</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">tmp</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 11.22476</code></pre>
<p>Thus we need around 11 times more samples
if using plain Monte Carlo integration when compared to importance sampling to obtain the same precision.
A benchmark will show that the extra computing time for importance sampling is small compared
to the reduction of variance.
It is therefore worth the coding effort if used repeatedly, but not
if it is a one-off computation.</p>
<p>The graph is, in fact, small enough for complete enumeration and thus the computation
of an exact solution. There are <span class="math inline">\(2^{18} = 262,144\)</span> different networks with
any number of the edges failing. To
systematically walk through all possible combinations of edges failing,
we use the function <code>intToBits</code> that converts an integer to its binary
representation for integers from 0 to 262,143. This is a quick and convenient
way of representing all the different fail and non-fail combinations
for the edges.</p>
<div class="sourceCode" id="cb233"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ones</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">Aup</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span>
<span class="va">Atmp</span> <span class="op">&lt;-</span> <span class="va">Aup</span>
<span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.05</span>
<span class="va">prob</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="fl">2</span><span class="op">^</span><span class="fl">18</span><span class="op">)</span>
<span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="fl">2</span><span class="op">^</span><span class="fl">18</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">on</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rawConversion.html">intToBits</a></span><span class="op">(</span><span class="va">i</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">18</span><span class="op">]</span><span class="op">)</span>
  <span class="va">Atmp</span><span class="op">[</span><span class="va">ones</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">on</span>
  <span class="kw">if</span> <span class="op">(</span><span class="fu">discon</span><span class="op">(</span><span class="va">Atmp</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span>
    <span class="va">s</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">on</span><span class="op">)</span>
    <span class="va">prob</span><span class="op">[</span><span class="va">i</span> <span class="op">+</span> <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">p</span><span class="op">^</span><span class="op">(</span><span class="fl">18</span> <span class="op">-</span> <span class="va">s</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span><span class="op">^</span><span class="va">s</span>
  <span class="op">}</span>
<span class="op">}</span></code></pre></div>
<p>The probability that nodes 1 and 10 are disconnected can then be computed as
the sum of all the probabilities in <code>prob</code>.</p>
<div class="sourceCode" id="cb234"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">prob</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 0.000288295</code></pre>
<p>This number should be compared to the estimates computed above. For a more complete
comparison, we have used importance sampling with edge fail probability
ranging from 0.1 to 0.4, see Figure <a href="mci.html#fig:networkImp">5.6</a>. The results
show that a failure probability of 0.2 is close to optimal in terms of
giving an importance sampling estimate with minimal variance. For smaller
values, the event that 1 and 10 become disconnected is too rare, and for
larger values the importance weights become too variable. A choice of 0.2
strikes a good balance.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:networkImp"></span>
<img src="CSwR_files/figure-html/networkImp-1.png" alt="Confidence intervals for importance sampling estimates of network nodes 1 and 10 being disconnected under independent edge failures with probability 0.05. The red line is the true probability computed by complete enumeration." width="70%"><p class="caption">
Figure 5.6: Confidence intervals for importance sampling estimates of network nodes 1 and 10 being disconnected under independent edge failures with probability 0.05. The red line is the true probability computed by complete enumeration.
</p>
</div>
<div id="object-oriented-implementations" class="section level3" number="5.3.1">
<h3>
<span class="header-section-number">5.3.1</span> Object oriented implementations<a class="anchor" aria-label="anchor" href="#object-oriented-implementations"><i class="fas fa-link"></i></a>
</h3>
<p>Implementations of algorithms that handle graphs and simulate graphs, as in the Monte Carlo
computations above, can benefit from using an object oriented approach. To
this end we first implement a so-called <em>constructor</em>, which is a function
that takes the adjacency matrix and the edge failure probability as arguments and
returns a list with class label <code>network</code>.</p>
<div class="sourceCode" id="cb236"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">network</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">A</span>, <span class="va">p</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">Aup</span> <span class="op">&lt;-</span> <span class="va">A</span>
  <span class="va">Aup</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/lower.tri.html">lower.tri</a></span><span class="op">(</span><span class="va">Aup</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">0</span> 
  <span class="va">ones</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="op">(</span><span class="va">Aup</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span>
  <span class="fu"><a href="https://rdrr.io/r/base/structure.html">structure</a></span><span class="op">(</span>
    <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>
      A <span class="op">=</span> <span class="va">A</span>, 
      Aup <span class="op">=</span> <span class="va">Aup</span>, 
      ones <span class="op">=</span> <span class="va">ones</span>, 
      p <span class="op">=</span> <span class="va">p</span>
    <span class="op">)</span>, 
    class <span class="op">=</span> <span class="st">"network"</span>
  <span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>We use the constructor function <code>network()</code> to construct and object of class
<code>network</code> for our specific adjacency matrix.</p>
<div class="sourceCode" id="cb237"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">my_net</span> <span class="op">&lt;-</span> <span class="fu">network</span><span class="op">(</span><span class="va">A</span>, p <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">my_net</span><span class="op">)</span> </code></pre></div>
<pre><code>## List of 4
##  $ A   : num [1:10, 1:10] 0 1 0 1 1 0 0 0 0 0 ...
##  $ Aup : num [1:10, 1:10] 0 0 0 0 0 0 0 0 0 0 ...
##  $ ones: int [1:18] 11 22 31 41 44 52 53 63 66 73 ...
##  $ p   : num 0.05
##  - attr(*, "class")= chr "network"</code></pre>
<div class="sourceCode" id="cb239"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span><span class="op">(</span><span class="va">my_net</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] "network"</code></pre>
<p>The network object contains, in addition to <code>A</code> and <code>p</code>, two precomputed components:
the upper triangular part of the adjacency matrix; and the indices in that matrix
containing a <code>1</code>.</p>
<p>The intention is then to write two methods for the network class. A method <code>sim()</code> that will
simulate a graph where some edges have failed, and a method <code>failure()</code> that will
estimate the probability of node 1 and 10 being disconnected by Monte Carlo
integration. To do so we need to define the two corresponding generic functions.</p>
<div class="sourceCode" id="cb241"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sim</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">...</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/UseMethod.html">UseMethod</a></span><span class="op">(</span><span class="st">"sim"</span><span class="op">)</span>
<span class="va">failure</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">...</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/UseMethod.html">UseMethod</a></span><span class="op">(</span><span class="st">"failure"</span><span class="op">)</span></code></pre></div>
<p>The method for simulation is then implemented as a function with name
<code>sim.network</code>.</p>
<div class="sourceCode" id="cb242"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sim.network</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">Aup</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">$</span><span class="va">Aup</span>
  <span class="va">Aup</span><span class="op">[</span><span class="va">x</span><span class="op">$</span><span class="va">ones</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span>
    <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>, 
    <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">ones</span><span class="op">)</span>, 
    replace <span class="op">=</span> <span class="cn">TRUE</span>, 
    prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">p</span>, <span class="fl">1</span> <span class="op">-</span> <span class="va">x</span><span class="op">$</span><span class="va">p</span><span class="op">)</span>
  <span class="op">)</span>
  <span class="va">Aup</span>
<span class="op">}</span></code></pre></div>
<p>It is implemented using essentially the same implementation as <code>sim_net()</code>
except that <code>Aup</code> and <code>p</code> are extracted as components from the object <code>x</code>
instead of being arguments, and <code>ones</code> is extracted from <code>x</code> as well instead
of being computed. One could argue that the <code>sim()</code> method should return an object
of class network — that would be natural. However, then we need to call the<br>
constructor with the full adjacency matrix, this will take some time and we do
not want to do that as a default. Thus we simply return the upper triangular
part of the adjacency matrix from <code>sim()</code>.</p>
<p>The <code>failure()</code> method implements plain Monte Carlo integration as well
as importance sampling and returns a vector containing the estimate as
well as the 95% confidence interval. This implementation relies on the
already implemented functions <code>discon()</code> and <code><a href="https://rdrr.io/r/stats/weights.html">weights()</a></code>.</p>
<div class="sourceCode" id="cb243"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">failure.network</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">n</span>, <span class="va">p0</span> <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span>
  <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html">is.null</a></span><span class="op">(</span><span class="va">p0</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span> 
    <span class="co"># Plain Monte Carlo simulation</span>
    <span class="va">tmp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="va">n</span>, <span class="fu">discon</span><span class="op">(</span><span class="fu">sim</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
    <span class="va">mu_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">tmp</span><span class="op">)</span>
    <span class="va">se</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">mu_hat</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">mu_hat</span><span class="op">)</span> <span class="op">/</span> <span class="va">n</span><span class="op">)</span>
  <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span> 
    <span class="co"># Importance sampling</span>
    <span class="va">p</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">$</span><span class="va">p</span>
    <span class="va">x</span><span class="op">$</span><span class="va">p</span> <span class="op">&lt;-</span> <span class="va">p0</span>
    <span class="va">tmp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="va">n</span>, <span class="fu"><a href="https://rdrr.io/r/stats/weights.html">weights</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">Aup</span>, <span class="fu">sim</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="va">p0</span>, <span class="va">p</span><span class="op">)</span><span class="op">)</span>
    <span class="va">se</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">tmp</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>
    <span class="va">mu_hat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">tmp</span><span class="op">)</span>
  <span class="op">}</span> 
  <span class="va">value</span> <span class="op">&lt;-</span> <span class="va">mu_hat</span> <span class="op">+</span> <span class="fl">1.96</span> <span class="op">*</span> <span class="va">se</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>  
  <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">value</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"low"</span>, <span class="st">"estimate"</span>, <span class="st">"high"</span><span class="op">)</span>
  <span class="va">value</span>
<span class="op">}</span></code></pre></div>
<p>We test the implementation against the previously computed results.</p>
<div class="sourceCode" id="cb244"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="va">seed</span><span class="op">)</span>  <span class="co"># Resetting seed</span>
<span class="fu">failure</span><span class="op">(</span><span class="va">my_net</span>, <span class="va">n</span><span class="op">)</span></code></pre></div>
<pre><code>##      low estimate     high 
## 0.000226 0.000340 0.000454</code></pre>
<div class="sourceCode" id="cb246"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="va">seed</span><span class="op">)</span>  <span class="co"># Resetting seed</span>
<span class="fu">failure</span><span class="op">(</span><span class="va">my_net</span>, <span class="va">n</span>, p0 <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span></code></pre></div>
<pre><code>##      low estimate     high 
## 0.000262 0.000296 0.000330</code></pre>
<p>We find that these are the same numbers as computed above, thus the object
oriented implementation concurs with the non-object oriented on this example.</p>
<p>We benchmark the object oriented implementation to measure if there is
any run time loss or improvement due to using objects.
One should expect a small computational overhead due to method dispatching,
that is, the procedure that R uses to look up the appropriate <code>sim()</code> method
for an object of class <code>network</code>. On the other hand, <code>sim()</code> does not recompute
<code>ones</code> every time.</p>
<div class="sourceCode" id="cb248"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html">microbenchmark</a></span><span class="op">(</span>
  <span class="fu">sim_net</span><span class="op">(</span><span class="va">Aup</span>, <span class="fl">0.05</span><span class="op">)</span>,
  <span class="fu">sim</span><span class="op">(</span><span class="va">my_net</span><span class="op">)</span>,
  times <span class="op">=</span> <span class="fl">1e4</span>
<span class="op">)</span></code></pre></div>
<pre><code>## Unit: microseconds
##                expr  min   lq mean median   uq   max neval
##  sim_net(Aup, 0.05) 8.21 12.6 19.1   14.1 15.7 11529 10000
##         sim(my_net) 9.22 14.0 19.3   15.2 16.8  5915 10000</code></pre>
<p>From the benchmark, the object oriented solution using <code>sim()</code> appears to be
a bit slower than <code>sim_net()</code> despite the latter recomputing <code>ones</code>, and
this can be explained by method dispatching taking of the order of 1 microsecond
during these benchmark computations.</p>
<p>Once we have taken an object oriented approach, we can also implement methods
for some standard generic functions, e.g. the <code>print</code> function. As this generic
function already exists, we simply need to implement a method for class <code>network</code>.</p>
<div class="sourceCode" id="cb250"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">print.network</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span>
  <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"#vertices: "</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">A</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span>
  <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"#edges:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">Aup</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span>
  <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"p = "</span>, <span class="va">x</span><span class="op">$</span><span class="va">p</span>, <span class="st">"\n"</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<div class="sourceCode" id="cb251"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">my_net</span>  <span class="co"># Implicitly calls 'print'</span></code></pre></div>
<pre><code>## #vertices:  10 
## #edges: 18 
## p =  0.05</code></pre>
<p>Our print method now prints out some summary information about the graph instead
of just the raw list.</p>
<p>If you want to work more seriously with graphs, it is likely that you want to use
an existing R package instead of reimplementing many graph algorithms. One of
these packages is igraph, which also illustrates well an object oriented implementation
of graph classes in R.</p>
<p>We start by constructing a new graph object from the adjacency matrix.</p>
<div class="sourceCode" id="cb253"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">net</span> <span class="op">&lt;-</span> <span class="fu">graph_from_adjacency_matrix</span><span class="op">(</span><span class="va">A</span>, mode <span class="op">=</span> <span class="st">"undirected"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span><span class="op">(</span><span class="va">net</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] "igraph"</code></pre>
<div class="sourceCode" id="cb255"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">net</span> <span class="co"># Illustrates the print method for objects of class 'igraph'</span></code></pre></div>
<pre><code>## IGRAPH ea76a4a U--- 10 18 -- 
## + edges from ea76a4a:
##  [1] 1-- 2 1-- 4 1-- 5 2-- 3 2-- 6 3-- 6 3-- 7 3-- 8 3--10 4-- 5 4-- 8 5-- 8 5-- 9 6-- 7 6--10
## [16] 7--10 8-- 9 9--10</code></pre>
<p>The igraph package supports a vast number of graph computation, manipulation and
visualization tools. We will here illustrate how igraph can be used to plot the
graph and how we can implement a simulation method for objects of class igraph.</p>
<p>You can use <code><a href="https://rdrr.io/r/graphics/plot.default.html">plot(net)</a></code>, which will call the plot method for objects of class igraph.
But before doing so, we will specify a layout of the graph.</p>
<div class="sourceCode" id="cb257"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># You can generate a layout ...</span>
<span class="va">net_layout</span> <span class="op">&lt;-</span> <span class="fu">layout_</span><span class="op">(</span><span class="va">net</span>, <span class="fu">nicely</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>
<span class="co"># ... or you can specify one yourself </span>
<span class="va">net_layout</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span>
  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">20</span>,  <span class="fl">1</span>,
    <span class="op">-</span><span class="fl">4</span>,  <span class="fl">3</span>,
    <span class="op">-</span><span class="fl">4</span>,  <span class="fl">1</span>,
    <span class="op">-</span><span class="fl">4</span>, <span class="op">-</span><span class="fl">1</span>,
    <span class="op">-</span><span class="fl">4</span>, <span class="op">-</span><span class="fl">3</span>,
     <span class="fl">4</span>,  <span class="fl">3</span>,
     <span class="fl">4</span>,  <span class="fl">1</span>,
     <span class="fl">4</span>, <span class="op">-</span><span class="fl">1</span>,
     <span class="fl">4</span>, <span class="op">-</span><span class="fl">3</span>,
     <span class="fl">20</span>,  <span class="op">-</span><span class="fl">1</span><span class="op">)</span>,
  ncol <span class="op">=</span> <span class="fl">2</span>, nrow <span class="op">=</span> <span class="fl">10</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<p>The layout we have specified makes it easy to recognize the graph.</p>
<div class="sourceCode" id="cb258"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">net</span>, layout <span class="op">=</span> <span class="va">net_layout</span>, asp <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="CSwR_files/figure-html/network_plot-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>We then use two functions from the igraph package to implement simulation of
a new graph. We need <code>gsize()</code>, which gives the number of edges in the graph,
and we need <code>delete_edges()</code>, which removes edges from the graph. Otherwise
the simulation is still based on <code><a href="https://rdrr.io/r/base/sample.html">sample()</a></code>.</p>
<div class="sourceCode" id="cb259"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sim.igraph</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">p</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">deledges</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span>
    <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>, 
    <span class="fu">gsize</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>,        <span class="co"># 'gsize()' returns the number of edges </span>
    replace <span class="op">=</span> <span class="cn">TRUE</span>, 
    prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">p</span>, <span class="fl">1</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span>
  <span class="op">)</span>
  <span class="fu">delete_edges</span><span class="op">(</span><span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">deledges</span><span class="op">)</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>Note that this method is also called <code>sim()</code>, yet there is no conflict here with
the method for objects of class network because the generic <code>sim()</code> function
will delegate the call to the correct method based on the objects class label.</p>
<p>If we combine our new <code>sim()</code> method for igraphs with the plot method, we can
plot a simulated graph.</p>
<div class="sourceCode" id="cb260"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu">sim</span><span class="op">(</span><span class="va">net</span>, <span class="fl">0.25</span><span class="op">)</span>, layout <span class="op">=</span> <span class="va">net_layout</span>, asp <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="CSwR_files/figure-html/network_simulated-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>The implementation using igraph turns out to be a little slower than using
the matrix representation alone as in <code>sim_net()</code>.</p>
<div class="sourceCode" id="cb261"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/system.time.html">system.time</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="fl">1e5</span>, <span class="op">{</span><span class="fu">sim</span><span class="op">(</span><span class="va">net</span>, <span class="fl">0.05</span><span class="op">)</span>; <span class="cn">NULL</span><span class="op">}</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##    user  system elapsed 
##   6.092   7.149  13.955</code></pre>
<p>One could also implement the function for testing if nodes 1 and 10 are
disconnected using the <code>shortest_paths()</code> function, but this is not faster
than the simple matrix multiplications used in <code>discon()</code> either. However, one
should be careful not to draw any general conclusions from this. Our graph is
admittedly a small toy example, and we implemented our solutions largely
to handle this particular graph.</p>

</div>
</div>
</div>



<div class="chapter-nav">
<div class="prev"><a href="univariate-random-variables.html"><span class="header-section-number">4</span> Univariate random variables</a></div>
<div class="next"><a href="four-examples.html"><span class="header-section-number">6</span> Four Examples</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#mci"><span class="header-section-number">5</span> Monte Carlo integration</a></li>
<li>
<a class="nav-link" href="#assessment"><span class="header-section-number">5.1</span> Assessment</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#CLT-gamma"><span class="header-section-number">5.1.1</span> Using the central limit theorem</a></li>
<li><a class="nav-link" href="#concentration-inequalities"><span class="header-section-number">5.1.2</span> Concentration inequalities</a></li>
<li><a class="nav-link" href="#exponential-tail-bound-for-gamma-distributed-variables"><span class="header-section-number">5.1.3</span> Exponential tail bound for Gamma distributed variables</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#importance-sampling"><span class="header-section-number">5.2</span> Importance sampling</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#unknown-normalization-constants"><span class="header-section-number">5.2.1</span> Unknown normalization constants</a></li>
<li><a class="nav-link" href="#hd-int"><span class="header-section-number">5.2.2</span> Computing a high-dimensional integral</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#network"><span class="header-section-number">5.3</span> Network failure</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#object-oriented-implementations"><span class="header-section-number">5.3.1</span> Object oriented implementations</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/nielsrhansen/CSwR/blob/master/14-MC.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/nielsrhansen/CSwR/edit/master/14-MC.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>
</div>
  

  

</div>
 <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Computational Statistics with R</strong>" was written by Niels Richard Hansen. It was last built on 2021-10-20, Git version: 86c4ccf.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>

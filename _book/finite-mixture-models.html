<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.4 Finite mixture models | Computational Statistics with R</title>
  <meta name="description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="6.4 Finite mixture models | Computational Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.4 Finite mixture models | Computational Statistics with R" />
  
  <meta name="twitter:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

<meta name="author" content="Niels Richard Hansen" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression.html"/>
<link rel="next" href="mixed-models.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro-smooth.html"><a href="intro-smooth.html"><i class="fa fa-check"></i><b>1.1</b> Smoothing</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro-smooth.html"><a href="intro-smooth.html#intro-angles"><i class="fa fa-check"></i><b>1.1.1</b> Angle distributions in proteins</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-smooth.html"><a href="intro-smooth.html#using-ggplot2"><i class="fa fa-check"></i><b>1.1.2</b> Using ggplot2</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-smooth.html"><a href="intro-smooth.html#changing-the-defaults"><i class="fa fa-check"></i><b>1.1.3</b> Changing the defaults</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-smooth.html"><a href="intro-smooth.html#multivariate-smoothing"><i class="fa fa-check"></i><b>1.1.4</b> Multivariate methods</a></li>
<li class="chapter" data-level="1.1.5" data-path="intro-smooth.html"><a href="intro-smooth.html#large-scale-smoothing"><i class="fa fa-check"></i><b>1.1.5</b> Large scale smoothing</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo Methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#vM"><i class="fa fa-check"></i><b>1.2.1</b> Univariate von Mises distributions</a></li>
<li class="chapter" data-level="1.2.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#mixtures-of-von-mises-distributions"><i class="fa fa-check"></i><b>1.2.2</b> Mixtures of von Mises distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="optimization.html"><a href="optimization.html"><i class="fa fa-check"></i><b>1.3</b> Optimization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="optimization.html"><a href="optimization.html#the-em-algorithm"><i class="fa fa-check"></i><b>1.3.1</b> The EM-algorithm</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.4</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#r-training-exercises"><i class="fa fa-check"></i>R training exercises</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#histograms-with-non-equidistant-breaks"><i class="fa fa-check"></i>Histograms with non-equidistant breaks</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#functions-and-functional-programming"><i class="fa fa-check"></i>Functions and functional programming</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Smoothing</b></span></li>
<li class="chapter" data-level="2" data-path="density.html"><a href="density.html"><i class="fa fa-check"></i><b>2</b> Density estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="unidens.html"><a href="unidens.html"><i class="fa fa-check"></i><b>2.1</b> Univariate density estimation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="unidens.html"><a href="unidens.html#likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Likelihood considerations</a></li>
<li class="chapter" data-level="2.1.2" data-path="unidens.html"><a href="unidens.html#sieves"><i class="fa fa-check"></i><b>2.1.2</b> Method of sieves</a></li>
<li class="chapter" data-level="2.1.3" data-path="unidens.html"><a href="unidens.html#basis-density"><i class="fa fa-check"></i><b>2.1.3</b> Basis expansions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="kernel-density.html"><a href="kernel-density.html"><i class="fa fa-check"></i><b>2.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="kernel-density.html"><a href="kernel-density.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="kernel-density.html"><a href="kernel-density.html#benchmarking"><i class="fa fa-check"></i><b>2.2.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bandwidth.html"><a href="bandwidth.html"><i class="fa fa-check"></i><b>2.3</b> Bandwidth selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bandwidth.html"><a href="bandwidth.html#rectangular"><i class="fa fa-check"></i><b>2.3.1</b> Revisiting the rectangular kernel</a></li>
<li class="chapter" data-level="2.3.2" data-path="bandwidth.html"><a href="bandwidth.html#ise-mise-and-mse-for-kernel-estimators"><i class="fa fa-check"></i><b>2.3.2</b> ISE, MISE and MSE for kernel estimators</a></li>
<li class="chapter" data-level="2.3.3" data-path="bandwidth.html"><a href="bandwidth.html#plug-in-estimation-of-the-oracle-bandwidth"><i class="fa fa-check"></i><b>2.3.3</b> Plug-in estimation of the oracle bandwidth</a></li>
<li class="chapter" data-level="2.3.4" data-path="bandwidth.html"><a href="bandwidth.html#cross-validation"><i class="fa fa-check"></i><b>2.3.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#kernel-density-estimation"><i class="fa fa-check"></i>Kernel density estimation</a></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#benchmarking-1"><i class="fa fa-check"></i>Benchmarking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i><b>3</b> Bivariate smoothing</a><ul>
<li class="chapter" data-level="3.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html"><i class="fa fa-check"></i><b>3.1</b> Nearest neighbor smoothers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#linear-smoothers"><i class="fa fa-check"></i><b>3.1.1</b> Linear smoothers</a></li>
<li class="chapter" data-level="3.1.2" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#implementing-the-running-mean"><i class="fa fa-check"></i><b>3.1.2</b> Implementing the running mean</a></li>
<li class="chapter" data-level="3.1.3" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#choose-k-by-cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Choose <span class="math inline">\(k\)</span> by cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="kernel-methods.html"><a href="kernel-methods.html"><i class="fa fa-check"></i><b>3.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="kernel-methods.html"><a href="kernel-methods.html#nadarayawatson-kernel-smoothing"><i class="fa fa-check"></i><b>3.2.1</b> Nadaraya–Watson kernel smoothing</a></li>
<li class="chapter" data-level="3.2.2" data-path="kernel-methods.html"><a href="kernel-methods.html#local-regression-smoothers"><i class="fa fa-check"></i><b>3.2.2</b> Local regression smoothers</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sparse-linear-algebra.html"><a href="sparse-linear-algebra.html"><i class="fa fa-check"></i><b>3.3</b> Sparse linear algebra</a></li>
<li class="chapter" data-level="3.4" data-path="onb.html"><a href="onb.html"><i class="fa fa-check"></i><b>3.4</b> Orthogonal basis expansions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="onb.html"><a href="onb.html#polynomial-expansions"><i class="fa fa-check"></i><b>3.4.1</b> Polynomial expansions</a></li>
<li class="chapter" data-level="3.4.2" data-path="onb.html"><a href="onb.html#fourier-expansions"><i class="fa fa-check"></i><b>3.4.2</b> Fourier expansions</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>3.5</b> Splines</a><ul>
<li class="chapter" data-level="3.5.1" data-path="splines.html"><a href="splines.html#smoothing-splines"><i class="fa fa-check"></i><b>3.5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="3.5.2" data-path="splines.html"><a href="splines.html#splines-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Splines in R</a></li>
<li class="chapter" data-level="3.5.3" data-path="splines.html"><a href="splines.html#efficient-splines"><i class="fa fa-check"></i><b>3.5.3</b> Efficient computation with splines</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="gaussian-processes.html"><a href="gaussian-processes.html"><i class="fa fa-check"></i><b>3.6</b> Gaussian processes</a></li>
<li class="chapter" data-level="3.7" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html"><i class="fa fa-check"></i><b>3.7</b> The Kalman filter</a><ul>
<li class="chapter" data-level="3.7.1" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#ar1-example"><i class="fa fa-check"></i><b>3.7.1</b> AR(1)-example</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-smoother"><i class="fa fa-check"></i><b>3.7.2</b> The Kalman smoother</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#implementation-1"><i class="fa fa-check"></i><b>3.7.3</b> Implementation</a></li>
<li class="chapter" data-level="3.7.4" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-filter-1"><i class="fa fa-check"></i><b>3.7.4</b> The Kalman filter</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#nearest-neighbors"><i class="fa fa-check"></i>Nearest neighbors</a></li>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#kernel-estimators"><i class="fa fa-check"></i>Kernel estimators</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Monte Carlo Methods</b></span></li>
<li class="chapter" data-level="4" data-path="univariate-random-variables.html"><a href="univariate-random-variables.html"><i class="fa fa-check"></i><b>4</b> Univariate random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="pseudorandom-number-generators.html"><a href="pseudorandom-number-generators.html"><i class="fa fa-check"></i><b>4.1</b> Pseudorandom number generators</a><ul>
<li class="chapter" data-level="4.1.1" data-path="pseudorandom-number-generators.html"><a href="pseudorandom-number-generators.html#implementing-a-pseudorandom-number-generator"><i class="fa fa-check"></i><b>4.1.1</b> Implementing a pseudorandom number generator</a></li>
<li class="chapter" data-level="4.1.2" data-path="pseudorandom-number-generators.html"><a href="pseudorandom-number-generators.html#pseudorandom-number-packages"><i class="fa fa-check"></i><b>4.1.2</b> Pseudorandom number packages</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="transformation-techniques.html"><a href="transformation-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Transformation techniques</a><ul>
<li class="chapter" data-level="4.2.1" data-path="transformation-techniques.html"><a href="transformation-techniques.html#sampling-from-a-t-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Sampling from a <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="reject-samp.html"><a href="reject-samp.html"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a><ul>
<li class="chapter" data-level="4.3.1" data-path="reject-samp.html"><a href="reject-samp.html#vMsim"><i class="fa fa-check"></i><b>4.3.1</b> von Mises distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="reject-samp.html"><a href="reject-samp.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="adaptive.html"><a href="adaptive.html"><i class="fa fa-check"></i><b>4.4</b> Adaptive envelopes</a><ul>
<li class="chapter" data-level="4.4.1" data-path="adaptive.html"><a href="adaptive.html#beta-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Beta distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="adaptive.html"><a href="adaptive.html#von-mises-distribution"><i class="fa fa-check"></i><b>4.4.2</b> von Mises distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-3.html"><a href="exercises-3.html#rejection-sampling-of-gaussian-random-variables"><i class="fa fa-check"></i>Rejection sampling of Gaussian random variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mci.html"><a href="mci.html"><i class="fa fa-check"></i><b>5</b> Monte Carlo integration</a><ul>
<li class="chapter" data-level="5.1" data-path="assessment.html"><a href="assessment.html"><i class="fa fa-check"></i><b>5.1</b> Assessment</a><ul>
<li class="chapter" data-level="5.1.1" data-path="assessment.html"><a href="assessment.html#using-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.1.1</b> Using the central limit theorem</a></li>
<li class="chapter" data-level="5.1.2" data-path="assessment.html"><a href="assessment.html#concentration-inequalities"><i class="fa fa-check"></i><b>5.1.2</b> Concentration inequalities</a></li>
<li class="chapter" data-level="5.1.3" data-path="assessment.html"><a href="assessment.html#exponential-tail-bound-for-gamma-distributed-variables"><i class="fa fa-check"></i><b>5.1.3</b> Exponential tail bound for Gamma distributed variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>5.2</b> Importance sampling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importance-sampling.html"><a href="importance-sampling.html#unknown-normalization-constants"><i class="fa fa-check"></i><b>5.2.1</b> Unknown normalization constants</a></li>
<li class="chapter" data-level="5.2.2" data-path="importance-sampling.html"><a href="importance-sampling.html#hd-int"><i class="fa fa-check"></i><b>5.2.2</b> Computing a high-dimensional integral</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="network-failure.html"><a href="network-failure.html"><i class="fa fa-check"></i><b>5.3</b> Network failure</a><ul>
<li class="chapter" data-level="5.3.1" data-path="network-failure.html"><a href="network-failure.html#object-oriented-implementation"><i class="fa fa-check"></i><b>5.3.1</b> Object oriented implementation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part III: Optimization</b></span></li>
<li class="chapter" data-level="6" data-path="four-examples.html"><a href="four-examples.html"><i class="fa fa-check"></i><b>6</b> Four Examples</a><ul>
<li class="chapter" data-level="6.1" data-path="exp-fam.html"><a href="exp-fam.html"><i class="fa fa-check"></i><b>6.1</b> Exponential families</a><ul>
<li class="chapter" data-level="6.1.1" data-path="exp-fam.html"><a href="exp-fam.html#full-exponential-families"><i class="fa fa-check"></i><b>6.1.1</b> Full exponential families</a></li>
<li class="chapter" data-level="6.1.2" data-path="exp-fam.html"><a href="exp-fam.html#bayes-net"><i class="fa fa-check"></i><b>6.1.2</b> Exponential family Bayesian networks</a></li>
<li class="chapter" data-level="6.1.3" data-path="exp-fam.html"><a href="exp-fam.html#exp-fam-deriv"><i class="fa fa-check"></i><b>6.1.3</b> Likelihood computations</a></li>
<li class="chapter" data-level="6.1.4" data-path="exp-fam.html"><a href="exp-fam.html#curved-exponential-families"><i class="fa fa-check"></i><b>6.1.4</b> Curved exponential families</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multinomial-models.html"><a href="multinomial-models.html"><i class="fa fa-check"></i><b>6.2</b> Multinomial models</a><ul>
<li class="chapter" data-level="6.2.1" data-path="multinomial-models.html"><a href="multinomial-models.html#pep-moth"><i class="fa fa-check"></i><b>6.2.1</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>6.3</b> Regression models</a></li>
<li class="chapter" data-level="6.4" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html"><i class="fa fa-check"></i><b>6.4</b> Finite mixture models</a><ul>
<li class="chapter" data-level="6.4.1" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#Gaus-mix-ex"><i class="fa fa-check"></i><b>6.4.1</b> Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="mixed-models.html"><a href="mixed-models.html"><i class="fa fa-check"></i><b>6.5</b> Mixed models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="numopt.html"><a href="numopt.html"><i class="fa fa-check"></i><b>7</b> Numerical optimization</a><ul>
<li class="chapter" data-level="7.1" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html"><i class="fa fa-check"></i><b>7.1</b> Algorithms and convergence</a><ul>
<li class="chapter" data-level="7.1.1" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#descent-algorithms"><i class="fa fa-check"></i><b>7.1.1</b> Descent algorithms</a></li>
<li class="chapter" data-level="7.1.2" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#maps-and-fixed-points"><i class="fa fa-check"></i><b>7.1.2</b> Maps and fixed points</a></li>
<li class="chapter" data-level="7.1.3" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#convergence-rate"><i class="fa fa-check"></i><b>7.1.3</b> Convergence rate</a></li>
<li class="chapter" data-level="7.1.4" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#stopping-criteria"><i class="fa fa-check"></i><b>7.1.4</b> Stopping criteria</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html"><i class="fa fa-check"></i><b>7.2</b> Descent direction algorithms</a><ul>
<li class="chapter" data-level="7.2.1" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#line-search"><i class="fa fa-check"></i><b>7.2.1</b> Line search</a></li>
<li class="chapter" data-level="7.2.2" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>7.2.2</b> Gradient descent</a></li>
<li class="chapter" data-level="7.2.3" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#conjugate-gradients"><i class="fa fa-check"></i><b>7.2.3</b> Conjugate gradients</a></li>
<li class="chapter" data-level="7.2.4" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#pep-moth-descent"><i class="fa fa-check"></i><b>7.2.4</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html"><i class="fa fa-check"></i><b>7.3</b> Newton-type algorithms</a><ul>
<li class="chapter" data-level="7.3.1" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#poisson-regression"><i class="fa fa-check"></i><b>7.3.1</b> Poisson regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#quasi-newton-algorithms"><i class="fa fa-check"></i><b>7.3.2</b> Quasi-Newton algorithms</a></li>
<li class="chapter" data-level="7.3.3" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#sparsity"><i class="fa fa-check"></i><b>7.3.3</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="misc-.html"><a href="misc-.html"><i class="fa fa-check"></i><b>7.4</b> Misc.</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="em.html"><a href="em.html"><i class="fa fa-check"></i><b>8</b> Expectation maximization algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="basic-properties.html"><a href="basic-properties.html"><i class="fa fa-check"></i><b>8.1</b> Basic properties</a><ul>
<li class="chapter" data-level="8.1.1" data-path="basic-properties.html"><a href="basic-properties.html#incomplete-data-likelihood"><i class="fa fa-check"></i><b>8.1.1</b> Incomplete data likelihood</a></li>
<li class="chapter" data-level="8.1.2" data-path="basic-properties.html"><a href="basic-properties.html#monotonicity-of-the-em-algorithm"><i class="fa fa-check"></i><b>8.1.2</b> Monotonicity of the EM algorithm</a></li>
<li class="chapter" data-level="8.1.3" data-path="basic-properties.html"><a href="basic-properties.html#peppered-moths"><i class="fa fa-check"></i><b>8.1.3</b> Peppered moths</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="EM-exp.html"><a href="EM-exp.html"><i class="fa fa-check"></i><b>8.2</b> Exponential families</a></li>
<li class="chapter" data-level="8.3" data-path="fisher-information.html"><a href="fisher-information.html"><i class="fa fa-check"></i><b>8.3</b> Fisher information</a></li>
<li class="chapter" data-level="8.4" data-path="revisiting-gaussian-mixtures.html"><a href="revisiting-gaussian-mixtures.html"><i class="fa fa-check"></i><b>8.4</b> Revisiting Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="finite-mixture-models" class="section level2">
<h2><span class="header-section-number">6.4</span> Finite mixture models</h2>
<p>A finite mixture model is a model of a pair of random variables <span class="math inline">\((Y, Z)\)</span>
with <span class="math inline">\(Z \in \{1, \ldots, K\}\)</span>, <span class="math inline">\(P(Z = k) = p_k\)</span>, and the conditional distribution
of <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z = k\)</span> having density <span class="math inline">\(f_k( \cdot \mid \theta_k)\)</span>. The joint
density is then
<span class="math display">\[(y, k) \mapsto f_k(y \mid \theta_k) p_k,\]</span>
and the marginal density for the distribution of <span class="math inline">\(Y\)</span> is
<span class="math display">\[f(y \mid \theta) =  \sum_{k=1}^K f_k(y \mid \theta_k) p_k\]</span>
where <span class="math inline">\(\theta\)</span> is the vector of all parameters. We say that the
model has <span class="math inline">\(K\)</span> mixture components and call <span class="math inline">\(f_k( \cdot \mid \theta_k)\)</span>
the mixture distributions and <span class="math inline">\(p_k\)</span> the mixture weights.</p>
<p>The main usage of mixture models is to situations where <span class="math inline">\(Z\)</span> is not observed.
In practice, only <span class="math inline">\(Y\)</span> is observed, and parameter estimation has to be
based on the marginal distribution of <span class="math inline">\(Y\)</span> with density <span class="math inline">\(f(\cdot \mid \theta)\)</span>,
which is a weighted sum of the mixture distributions.</p>
<p>The set of all probability measures on <span class="math inline">\(\{1, \ldots, K\}\)</span> is an exponential
family with sufficient statistic
<span class="math display">\[\tilde{t}_0(k) = (\delta_{1k}, \delta_{2k}, \ldots, \delta_{(K-1)k}) \in \mathbb{R}^{K-1},\]</span>
canonical parameter <span class="math inline">\(\alpha = (\alpha_1, \ldots, \alpha_{K-1}) \in \mathbb{R}^{K-1}\)</span>
and
<span class="math display">\[p_k = \frac{e^{\alpha_k}}{1 + \sum_{l=1}^{K-1} e^{\alpha_l}}.\]</span></p>
<p>When <span class="math inline">\(f_k\)</span> is an exponential family as well with sufficient statistic
<span class="math inline">\(\tilde{t}_k : \mathcal{Y} \to \mathbb{R}^{p_k}\)</span> and <span class="math inline">\(\theta_k\)</span> the
canonical parameter, we bundle <span class="math inline">\(\alpha, \theta_1, \ldots, \theta_K\)</span>
into <span class="math inline">\(\theta\)</span> and define
<span class="math display">\[t_1(y) = \left(\begin{array}{c}
\tilde{t}_0 \\
0 \\
0 \\
\vdots \\
0
\end{array}
\right)\]</span>
together with
<span class="math display">\[t_2(y \mid k) = \left(\begin{array}{c}
0 \\
\delta_{1k} \tilde{t}_1(y) \\
\delta_{2k} \tilde{t}_2(y) \\
\vdots \\
\delta_{Kk} \tilde{t}_K(y)
\end{array}
\right)\]</span>
we see that we have an exponential family of the joint distribution of <span class="math inline">\((Y, Z)\)</span>
with the <span class="math inline">\(p = K-1 + p_1 + \ldots + p_K\)</span>-dimensional canonical parameter <span class="math inline">\(\theta\)</span>,
with the sufficient statistic <span class="math inline">\(t_1\)</span> determining the marginal distribution of <span class="math inline">\(Z\)</span>
and with the sufficient statistic <span class="math inline">\(t_2\)</span> determining the <em>conditional</em> distribution
of <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z\)</span>. We have here made the conditioning variable explicit.</p>
<p>The marginal density of <span class="math inline">\(Y\)</span> in the exponential family parametrization then
becomes
<span class="math display">\[f(y \mid \theta) = \sum_{k=1}^K  e^{\theta^T (t_1(k) + t_2(y \mid k)) - \log \varphi_1(\theta) - \log \varphi_2(\theta \mid k)}.\]</span>
For small <span class="math inline">\(K\)</span> it is usually unproblematic to implement the computation of
the marginal density using the formula above, and the computation of derivatives
can likewise be implemented based on the formulas derived in Section <a href="exp-fam.html#exp-fam-deriv">6.1.3</a>.</p>
<div id="Gaus-mix-ex" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Gaussian mixtures</h3>
<p>A Gaussian mixture model is a mixture model where all the mixture distributions are<br />
Gaussian distributions but potentially with different means and variances. In this
section we focus on the simplest Gaussian mixture model with <span class="math inline">\(K = 2\)</span> mixture
components.</p>
<p>When <span class="math inline">\(K = 2\)</span>, the Gaussian mixture model is parametrized by the five parameters:
the mixture weight <span class="math inline">\(p = P(Z = 1) \in (0, 1)\)</span>, the two means <span class="math inline">\(\mu_1, \mu_2 \in \mathbb{R}\)</span>,
and the two variances <span class="math inline">\(\sigma_1, \sigma_2 &gt; 0\)</span>. This is <em>not</em> the parametrization
using canonical exponential family parameters, which we return to below. First
we will simply simulate random variables from this model.</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb268-1" data-line-number="1">sigma1 &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb268-2" data-line-number="2">sigma2 &lt;-<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb268-3" data-line-number="3">mu1 &lt;-<span class="st"> </span><span class="fl">-0.5</span></a>
<a class="sourceLine" id="cb268-4" data-line-number="4">mu2 &lt;-<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb268-5" data-line-number="5">p &lt;-<span class="st"> </span><span class="fl">0.5</span></a>
<a class="sourceLine" id="cb268-6" data-line-number="6">n &lt;-<span class="st"> </span><span class="dv">5000</span></a>
<a class="sourceLine" id="cb268-7" data-line-number="7">z &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="ot">TRUE</span>, <span class="ot">FALSE</span>), n, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="kw">c</span>(p, <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p))</a>
<a class="sourceLine" id="cb268-8" data-line-number="8"><span class="co">## Conditional simulation from the mixture components</span></a>
<a class="sourceLine" id="cb268-9" data-line-number="9">y &lt;-<span class="st"> </span><span class="kw">numeric</span>(n)</a>
<a class="sourceLine" id="cb268-10" data-line-number="10">n1 &lt;-<span class="st"> </span><span class="kw">sum</span>(z)</a>
<a class="sourceLine" id="cb268-11" data-line-number="11">y[z] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n1, mu1, sigma1)</a>
<a class="sourceLine" id="cb268-12" data-line-number="12">y[<span class="op">!</span>z] &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n <span class="op">-</span><span class="st"> </span>n1, mu2, sigma2)</a></code></pre></div>
<p>The simulation above generated 5000 samples from a two-component
Gaussian mixture model with mixture distributions <span class="math inline">\(\mathcal{N}(-0.5, 1)\)</span>
and <span class="math inline">\(\mathcal{N}(4, 4)\)</span>, and with each component having weight <span class="math inline">\(0.5\)</span>.
This gives a bimodal distribution as illustrated by the histogram on Figure
<a href="finite-mixture-models.html#fig:mixture-gaus-histogram">6.1</a>.</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb269-1" data-line-number="1">yy &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">11</span>, <span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb269-2" data-line-number="2">dens &lt;-<span class="st"> </span>p <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(yy, mu1, sigma1) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p) <span class="op">*</span><span class="st"> </span><span class="kw">dnorm</span>(yy, mu2, sigma2)</a>
<a class="sourceLine" id="cb269-3" data-line-number="3"><span class="kw">ggplot</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(y, ..density..)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb269-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">20</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb269-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(yy, dens), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb269-6" data-line-number="6"><span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">bw =</span> <span class="st">&quot;SJ&quot;</span>, <span class="dt">geom=</span><span class="st">&quot;line&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">size =</span> <span class="dv">1</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:mixture-gaus-histogram"></span>
<img src="CSwR_files/figure-html/mixture-gaus-histogram-1.png" alt="Histogram and density estimate (red) of data simulated from a two-component Gaussian mixture distribution. The true mixture distribution has " width="70%" />
<p class="caption">
Figure 6.1: Histogram and density estimate (red) of data simulated from a two-component Gaussian mixture distribution. The true mixture distribution has
</p>
</div>
<p>It is possible to give a mathematically different representation of the
marginal distribution of <span class="math inline">\(Y\)</span> that is sometimes useful. Though it gives
the same marginal distribution from the same components, it does provide a
different interpretation of what a mixture model is a model of, and it does
provide a different way of simulating from a mixture distribution.</p>
<p>If we let <span class="math inline">\(Y_1 \sim \mathcal{N}(\mu_1, \sigma_1^2)\)</span> and <span class="math inline">\(Y_2 \sim \mathcal{N}(\mu_2, \sigma_2^2)\)</span>
be independent, and independent of <span class="math inline">\(Z\)</span>, we can define</p>
<p><span class="math display" id="eq:mixturerep">\[\begin{equation}
Y = 1(Z = 1) Y_1 + 1(Z = 2) Y_2 = Y_{Z}.
\tag{6.3}
\end{equation}\]</span></p>
<p>Clearly, the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z = k\)</span> is <span class="math inline">\(f_k\)</span>.
From <a href="finite-mixture-models.html#eq:mixturerep">(6.3)</a> it follows directly that
<span class="math display">\[E(Y^n) = P(Z = 1) E(Y_1^n) + P(Z = 2) E(Y_2^n) = p m_1(n) + (1 - p) m_2(n)\]</span>
where <span class="math inline">\(m_k(n)\)</span> denotes the <span class="math inline">\(n\)</span>th non-central moment of the <span class="math inline">\(k\)</span>th mixture
component. In particular,</p>
<p><span class="math display">\[E(Y) = p \mu_1 + (1 - p) \mu_2.\]</span></p>
<p>The variance can be found from the second moment as</p>
<p><span class="math display">\[\begin{align}
V(Y) &amp; = p(\mu_1^2 + \sigma_1^2) + (1-p)(\mu_2^2 + \sigma_2^2) -  (p \mu_1 + (1 - p) \mu_2)^2 \\
&amp; = p\sigma_1^2 + (1 - p) \sigma_2^2 + p(1-p)(\mu_1^2 + \mu_2^2 - 2 \mu_1 \mu_2).
\end{align}\]</span></p>
<p>While it is certainly possible to derive this formula by other means, using
<a href="finite-mixture-models.html#eq:mixturerep">(6.3)</a> gives a simple argument based on
elementary properties of expectation and independence
of <span class="math inline">\((Y_1, Y_2)\)</span> and <span class="math inline">\(Z\)</span>.</p>
<p>The construction via <a href="finite-mixture-models.html#eq:mixturerep">(6.3)</a> has an interpretation that differs
from how a mixture model was defined in the first place. Though <span class="math inline">\((Y, Z)\)</span> has
the correct joint distribution, <span class="math inline">\(Y\)</span> is by <a href="finite-mixture-models.html#eq:mixturerep">(6.3)</a> the result of <span class="math inline">\(Z\)</span> <em>selecting</em> one
out of two possible observations. The difference can best be illustrated by
an example. Suppose that we have a large population consisting of married couples entirely.
We can draw a sample of individuals (ignoring the marriage relations
completely) from this population and let <span class="math inline">\(Z\)</span> denote the sex and <span class="math inline">\(Y\)</span> the
height of the individual. Then <span class="math inline">\(Y\)</span> follows a mixture distribution with two
components corresponding to males and females according to the definition.
At the risk of being heteronormative, suppose that all couples consist
of one male and one female. We could then also draw a sample
of married couples, and for each couple flip a coin to decide whether to report
the male’s or the female’s height. This corresponds to the construction
of <span class="math inline">\(Y\)</span> by <a href="finite-mixture-models.html#eq:mixturerep">(6.3)</a>. We get the same marginal mixture
distribution of heights though.</p>
<p>Arguably the heights of individuals in a marriage are not
independent, but this is actually immaterial. Any dependence structure between
<span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> is lost in the transformation <a href="finite-mixture-models.html#eq:mixturerep">(6.3)</a>, and we
can just as well assume them independent for mathematical convenience.
We won’t be able to tell the difference from observing only <span class="math inline">\(Y\)</span> (and <span class="math inline">\(Z\)</span>) anyway.</p>
<p>We illustrate below how <a href="finite-mixture-models.html#eq:mixturerep">(6.3)</a> can be used for alternative
implementations of ways to simulate from the mixture model. We compare empirical
means and variances to the theoretical values to test if all the implementations
actually simulate from the Gaussian mixture model.</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb270-1" data-line-number="1"><span class="co">## Mean</span></a>
<a class="sourceLine" id="cb270-2" data-line-number="2">mu &lt;-<span class="st"> </span>p <span class="op">*</span><span class="st"> </span>mu1 <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p) <span class="op">*</span><span class="st"> </span>mu2</a>
<a class="sourceLine" id="cb270-3" data-line-number="3"></a>
<a class="sourceLine" id="cb270-4" data-line-number="4"><span class="co">## Variance </span></a>
<a class="sourceLine" id="cb270-5" data-line-number="5">sigmasq &lt;-<span class="st"> </span>p <span class="op">*</span><span class="st"> </span>sigma1<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p) <span class="op">*</span><span class="st"> </span>sigma2<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb270-6" data-line-number="6"><span class="st">  </span>p <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>p)<span class="op">*</span>(mu1<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>mu2<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>mu1 <span class="op">*</span><span class="st"> </span>mu2)</a>
<a class="sourceLine" id="cb270-7" data-line-number="7"></a>
<a class="sourceLine" id="cb270-8" data-line-number="8"><span class="co">## Simulation using the selection formulation via &#39;ifelse&#39;</span></a>
<a class="sourceLine" id="cb270-9" data-line-number="9">y2 &lt;-<span class="st"> </span><span class="kw">ifelse</span>(z, <span class="kw">rnorm</span>(n, mu1, sigma1), <span class="kw">rnorm</span>(n, mu2, sigma2))</a>
<a class="sourceLine" id="cb270-10" data-line-number="10"></a>
<a class="sourceLine" id="cb270-11" data-line-number="11"><span class="co">## Yet another way of simulating from a mixture model</span></a>
<a class="sourceLine" id="cb270-12" data-line-number="12"><span class="co">## using arithmetic instead of &#39;ifelse&#39; for the selection </span></a>
<a class="sourceLine" id="cb270-13" data-line-number="13"><span class="co">## and with Y_1 and Y_2 actually being dependent</span></a>
<a class="sourceLine" id="cb270-14" data-line-number="14">y3 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n)</a>
<a class="sourceLine" id="cb270-15" data-line-number="15">y3 &lt;-<span class="st"> </span>z <span class="op">*</span><span class="st"> </span>(sigma1 <span class="op">*</span><span class="st"> </span>y3 <span class="op">+</span><span class="st"> </span>mu1) <span class="op">+</span><span class="st"> </span>(<span class="op">!</span>z) <span class="op">*</span><span class="st"> </span>(sigma2 <span class="op">*</span><span class="st"> </span>y3 <span class="op">+</span><span class="st"> </span>mu2)</a>
<a class="sourceLine" id="cb270-16" data-line-number="16"></a>
<a class="sourceLine" id="cb270-17" data-line-number="17"><span class="co">## Returning to the definition again, this last method simulates conditionally </span></a>
<a class="sourceLine" id="cb270-18" data-line-number="18"><span class="co">## from the mixture components via transformation of an underlying Gaussian</span></a>
<a class="sourceLine" id="cb270-19" data-line-number="19"><span class="co">## variable with mean 0 and variance 1</span></a>
<a class="sourceLine" id="cb270-20" data-line-number="20">y4 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n)</a>
<a class="sourceLine" id="cb270-21" data-line-number="21">y4[z] &lt;-<span class="st"> </span>sigma1 <span class="op">*</span><span class="st"> </span>y4[z] <span class="op">+</span><span class="st"> </span>mu1</a>
<a class="sourceLine" id="cb270-22" data-line-number="22">y4[<span class="op">!</span>z] &lt;-<span class="st"> </span>sigma2 <span class="op">*</span><span class="st"> </span>y4[<span class="op">!</span>z] <span class="op">+</span><span class="st"> </span>mu2</a>
<a class="sourceLine" id="cb270-23" data-line-number="23"></a>
<a class="sourceLine" id="cb270-24" data-line-number="24"><span class="co">## Tests</span></a>
<a class="sourceLine" id="cb270-25" data-line-number="25"><span class="kw">data.frame</span>(<span class="dt">mean =</span> <span class="kw">c</span>(mu, <span class="kw">mean</span>(y), <span class="kw">mean</span>(y2), <span class="kw">mean</span>(y3), <span class="kw">mean</span>(y4)),</a>
<a class="sourceLine" id="cb270-26" data-line-number="26">      <span class="dt">variance =</span> <span class="kw">c</span>(sigmasq, <span class="kw">var</span>(y), <span class="kw">var</span>(y2), <span class="kw">var</span>(y3), <span class="kw">var</span>(y4)), </a>
<a class="sourceLine" id="cb270-27" data-line-number="27">      <span class="dt">row.names =</span> <span class="kw">c</span>(<span class="st">&quot;true&quot;</span>, <span class="st">&quot;conditional&quot;</span>, <span class="st">&quot;ifelse&quot;</span>, <span class="st">&quot;arithmetic&quot;</span>, </a>
<a class="sourceLine" id="cb270-28" data-line-number="28">                    <span class="st">&quot;conditional2&quot;</span> ))</a></code></pre></div>
<pre><code>##                  mean variance
## true         1.750000 7.562500
## conditional  1.728195 7.381843
## ifelse       1.713098 7.513963
## arithmetic   1.708420 7.566312
## conditional2 1.700052 7.463941</code></pre>
<p>In terms of run time there is not a big difference between three of
the ways of simulating from a mixture model. A benchmark study (not shown) will
reveal that the first and third methods are comparable in terms of run time
and slightly faster than the fourth, while the second using <code>ifelse</code> takes more
than twice as much time as the others. This is
unsurprising as the <code>ifelse</code> method takes <a href="finite-mixture-models.html#eq:mixturerep">(6.3)</a> very literally
and generates twice the number of Gaussian variables actually needed.</p>
<p>The marginal density of <span class="math inline">\(Y\)</span> is
<span class="math display">\[f(y) = p \frac{1}{\sqrt{2 \pi \sigma_1^2}} e^{-\frac{(y - \mu_1)^2}{2 \sigma_1^2}} + 
(1 - p)\frac{1}{\sqrt{2 \pi \sigma_2^2}}e^{-\frac{(y - \mu_2)^2}{2 \sigma_2^2}}\]</span>
as given in terms of the parameters <span class="math inline">\(p\)</span>, <span class="math inline">\(\mu_1, \mu_2\)</span>, <span class="math inline">\(\sigma_1^2\)</span> and <span class="math inline">\(\sigma_2^2\)</span>.</p>
<p>Returning to the canonical parameters we see that they are given as
follows:
<span class="math display">\[\theta_1  = \log \frac{p}{1 - p}, \quad 
\theta_2  = \frac{\mu_1}{2\sigma_1^2}, \quad
\theta_3  = \frac{1}{2\sigma_1^2}, \quad
\theta_4  = \frac{\mu_2}{\sigma_2^2}, \quad
\theta_5  = \frac{1}{2\sigma_2^2}.\]</span></p>
<p>The joint density in this parametrization then becomes
<span class="math display">\[(y,k) \mapsto \left\{ \begin{array}{ll} 
\exp\left(\theta_1 + \theta_2 y - \theta_3 y^2 - \log (1 + e^{\theta_1}) - \frac{ \theta_2^2}{4\theta_3}+ \frac{1}{2} \log(\theta_3) \right) &amp; \quad \text{if } k = 1 \\
\exp\left(\theta_4 y - \theta_5 y^2 - \log (1 + e^{\theta_1}) - \frac{\theta_4^2}{4\theta_5} + \frac{1}{2}\log(\theta_5) \right) &amp;  \quad \text{if } k = 2 
\end{array} \right. \]</span>
and the marginal density for <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[\begin{align}
f(y \mid \theta) &amp; = \exp\left(\theta_1 + \theta_2 y - \theta_3 y^2 - \log (1 + e^{\theta_1}) - \frac{ \theta_2^2}{4\theta_3}+ \frac{1}{2} \log(\theta_3) \right) \\ 
&amp; + \exp\left(\theta_4 y - \theta_5 y^2 - \log (1 + e^{\theta_1}) - \frac{\theta_4^2}{4\theta_5} + \frac{1}{2}\log(\theta_5) \right).
\end{align}\]</span></p>
<p>There is no apparent benefit to the canonical parametrization when considering
the marginal density. It is, however, of value when we need the logarithm of
the joint density as we will for the EM-algorithm.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mixed-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

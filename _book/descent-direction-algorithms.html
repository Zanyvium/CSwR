<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Statistics with R</title>
  <meta name="description" content="Lecture notes providing an introduction to computational statistics using the R programming language.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Statistics with R" />
  
  <meta name="twitter:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

<meta name="author" content="Niels Richard Hansen">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="algorithms-and-convergence.html">
<link rel="next" href="newton-type-algorithms.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro-smooth.html"><a href="intro-smooth.html"><i class="fa fa-check"></i><b>1.1</b> Smoothing</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro-smooth.html"><a href="intro-smooth.html#intro-angles"><i class="fa fa-check"></i><b>1.1.1</b> Angle distributions in proteins</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-smooth.html"><a href="intro-smooth.html#using-ggplot2"><i class="fa fa-check"></i><b>1.1.2</b> Using ggplot2</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-smooth.html"><a href="intro-smooth.html#changing-the-defaults"><i class="fa fa-check"></i><b>1.1.3</b> Changing the defaults</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-smooth.html"><a href="intro-smooth.html#large-scale-smoothing"><i class="fa fa-check"></i><b>1.1.4</b> Large scale smoothing</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo Methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#vM"><i class="fa fa-check"></i><b>1.2.1</b> Univariate von Mises distributions</a></li>
<li class="chapter" data-level="1.2.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#mixtures-of-von-mises-distributions"><i class="fa fa-check"></i><b>1.2.2</b> Mixtures of von Mises distributions</a></li>
<li class="chapter" data-level="1.2.3" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#large-scale-simulation"><i class="fa fa-check"></i><b>1.2.3</b> Large scale simulation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="optimization.html"><a href="optimization.html"><i class="fa fa-check"></i><b>1.3</b> Optimization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="optimization.html"><a href="optimization.html#the-em-algorithm"><i class="fa fa-check"></i><b>1.3.1</b> The EM-algorithm</a></li>
<li class="chapter" data-level="1.3.2" data-path="optimization.html"><a href="optimization.html#large-scale-optimization"><i class="fa fa-check"></i><b>1.3.2</b> Large scale optimization</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.4</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#r-training-exercises"><i class="fa fa-check"></i>R training exercises</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#histograms-with-non-equidistant-breaks"><i class="fa fa-check"></i>Histograms with non-equidistant breaks</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#functions-and-functional-programming"><i class="fa fa-check"></i>Functions and functional programming</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Smoothing</b></span></li>
<li class="chapter" data-level="2" data-path="density.html"><a href="density.html"><i class="fa fa-check"></i><b>2</b> Density estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="unidens.html"><a href="unidens.html"><i class="fa fa-check"></i><b>2.1</b> Univariate density estimation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="unidens.html"><a href="unidens.html#likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Likelihood considerations</a></li>
<li class="chapter" data-level="2.1.2" data-path="unidens.html"><a href="unidens.html#sieves"><i class="fa fa-check"></i><b>2.1.2</b> Method of sieves</a></li>
<li class="chapter" data-level="2.1.3" data-path="unidens.html"><a href="unidens.html#basis-density"><i class="fa fa-check"></i><b>2.1.3</b> Basis expansions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="kernel-density.html"><a href="kernel-density.html"><i class="fa fa-check"></i><b>2.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="kernel-density.html"><a href="kernel-density.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="kernel-density.html"><a href="kernel-density.html#benchmarking"><i class="fa fa-check"></i><b>2.2.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bandwidth.html"><a href="bandwidth.html"><i class="fa fa-check"></i><b>2.3</b> Bandwidth selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bandwidth.html"><a href="bandwidth.html#rectangular"><i class="fa fa-check"></i><b>2.3.1</b> Revisiting the rectangular kernel</a></li>
<li class="chapter" data-level="2.3.2" data-path="bandwidth.html"><a href="bandwidth.html#ise-mise-and-mse-for-kernel-estimators"><i class="fa fa-check"></i><b>2.3.2</b> ISE, MISE and MSE for kernel estimators</a></li>
<li class="chapter" data-level="2.3.3" data-path="bandwidth.html"><a href="bandwidth.html#plug-in-estimation-of-the-oracle-bandwidth"><i class="fa fa-check"></i><b>2.3.3</b> Plug-in estimation of the oracle bandwidth</a></li>
<li class="chapter" data-level="2.3.4" data-path="bandwidth.html"><a href="bandwidth.html#cross-validation"><i class="fa fa-check"></i><b>2.3.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multivariate-smoothing.html"><a href="multivariate-smoothing.html"><i class="fa fa-check"></i><b>2.4</b> Multivariate methods</a></li>
<li class="chapter" data-level="2.5" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#kernel-density-estimation"><i class="fa fa-check"></i>Kernel density estimation</a></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#benchmarking-1"><i class="fa fa-check"></i>Benchmarking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i><b>3</b> Bivariate smoothing</a><ul>
<li class="chapter" data-level="3.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html"><i class="fa fa-check"></i><b>3.1</b> Nearest neighbor smoothers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#linear-smoothers"><i class="fa fa-check"></i><b>3.1.1</b> Linear smoothers</a></li>
<li class="chapter" data-level="3.1.2" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#implementing-the-running-mean"><i class="fa fa-check"></i><b>3.1.2</b> Implementing the running mean</a></li>
<li class="chapter" data-level="3.1.3" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#choose-k-by-cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Choose <span class="math inline">\(k\)</span> by cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="kernel-methods.html"><a href="kernel-methods.html"><i class="fa fa-check"></i><b>3.2</b> Kernel methods</a></li>
<li class="chapter" data-level="3.3" data-path="sparse-linear-algebra.html"><a href="sparse-linear-algebra.html"><i class="fa fa-check"></i><b>3.3</b> Sparse linear algebra</a></li>
<li class="chapter" data-level="3.4" data-path="onb.html"><a href="onb.html"><i class="fa fa-check"></i><b>3.4</b> Orthogonal basis expansions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="onb.html"><a href="onb.html#polynomial-expansions"><i class="fa fa-check"></i><b>3.4.1</b> Polynomial expansions</a></li>
<li class="chapter" data-level="3.4.2" data-path="onb.html"><a href="onb.html#fourier-expansions"><i class="fa fa-check"></i><b>3.4.2</b> Fourier expansions</a></li>
<li class="chapter" data-level="3.4.3" data-path="onb.html"><a href="onb.html#wavelets"><i class="fa fa-check"></i><b>3.4.3</b> Wavelets</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>3.5</b> Splines</a><ul>
<li class="chapter" data-level="3.5.1" data-path="splines.html"><a href="splines.html#smoothing-splines"><i class="fa fa-check"></i><b>3.5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="3.5.2" data-path="splines.html"><a href="splines.html#splines-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Splines in R</a></li>
<li class="chapter" data-level="3.5.3" data-path="splines.html"><a href="splines.html#efficient-computation-with-splines"><i class="fa fa-check"></i><b>3.5.3</b> Efficient computation with splines</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="gaussian-processes.html"><a href="gaussian-processes.html"><i class="fa fa-check"></i><b>3.6</b> Gaussian processes</a></li>
<li class="chapter" data-level="3.7" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html"><i class="fa fa-check"></i><b>3.7</b> The Kalman filter</a><ul>
<li class="chapter" data-level="3.7.1" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#ar1-example"><i class="fa fa-check"></i><b>3.7.1</b> AR(1)-example</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-smoother"><i class="fa fa-check"></i><b>3.7.2</b> The Kalman smoother</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#implementation-1"><i class="fa fa-check"></i><b>3.7.3</b> Implementation</a></li>
<li class="chapter" data-level="3.7.4" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-filter-1"><i class="fa fa-check"></i><b>3.7.4</b> The Kalman filter</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Monte Carlo Methods</b></span></li>
<li class="chapter" data-level="4" data-path="univariate-random-variables.html"><a href="univariate-random-variables.html"><i class="fa fa-check"></i><b>4</b> Univariate random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="pseudo-random-numbers.html"><a href="pseudo-random-numbers.html"><i class="fa fa-check"></i><b>4.1</b> Pseudo random numbers</a></li>
<li class="chapter" data-level="4.2" data-path="transformation-techniques.html"><a href="transformation-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Transformation techniques</a><ul>
<li class="chapter" data-level="4.2.1" data-path="transformation-techniques.html"><a href="transformation-techniques.html#sampling-from-a-t-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Sampling from a <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="reject-samp.html"><a href="reject-samp.html"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a><ul>
<li class="chapter" data-level="4.3.1" data-path="reject-samp.html"><a href="reject-samp.html#vMsim"><i class="fa fa-check"></i><b>4.3.1</b> von Mises distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="reject-samp.html"><a href="reject-samp.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="adaptive.html"><a href="adaptive.html"><i class="fa fa-check"></i><b>4.4</b> Adaptive envelopes</a><ul>
<li class="chapter" data-level="4.4.1" data-path="adaptive.html"><a href="adaptive.html#beta-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Beta distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="adaptive.html"><a href="adaptive.html#von-mises-distribution"><i class="fa fa-check"></i><b>4.4.2</b> von Mises distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#rejection-sampling-of-gaussian-random-variables"><i class="fa fa-check"></i>Rejection sampling of Gaussian random variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="MCI.html"><a href="MCI.html"><i class="fa fa-check"></i><b>5</b> Monte Carlo integration</a><ul>
<li class="chapter" data-level="5.1" data-path="assessment.html"><a href="assessment.html"><i class="fa fa-check"></i><b>5.1</b> Assessment</a><ul>
<li class="chapter" data-level="5.1.1" data-path="assessment.html"><a href="assessment.html#using-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.1.1</b> Using the central limit theorem</a></li>
<li class="chapter" data-level="5.1.2" data-path="assessment.html"><a href="assessment.html#concentration-inequalities"><i class="fa fa-check"></i><b>5.1.2</b> Concentration inequalities</a></li>
<li class="chapter" data-level="5.1.3" data-path="assessment.html"><a href="assessment.html#exponential-tail-bound-for-gamma-distributed-variables"><i class="fa fa-check"></i><b>5.1.3</b> Exponential tail bound for Gamma distributed variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>5.2</b> Importance sampling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importance-sampling.html"><a href="importance-sampling.html#unknown-normalization-constants"><i class="fa fa-check"></i><b>5.2.1</b> Unknown normalization constants</a></li>
<li class="chapter" data-level="5.2.2" data-path="importance-sampling.html"><a href="importance-sampling.html#computing-a-high-dimensional-integral"><i class="fa fa-check"></i><b>5.2.2</b> Computing a high-dimensional integral</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="network-failure.html"><a href="network-failure.html"><i class="fa fa-check"></i><b>5.3</b> Network failure</a><ul>
<li class="chapter" data-level="5.3.1" data-path="network-failure.html"><a href="network-failure.html#object-oriented-implementation"><i class="fa fa-check"></i><b>5.3.1</b> Object oriented implementation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="design-of-experiments.html"><a href="design-of-experiments.html"><i class="fa fa-check"></i><b>5.4</b> Design of experiments</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-random-variables.html"><a href="multivariate-random-variables.html"><i class="fa fa-check"></i><b>6</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="6.1" data-path="sequential-simulation.html"><a href="sequential-simulation.html"><i class="fa fa-check"></i><b>6.1</b> Sequential simulation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="sequential-simulation.html"><a href="sequential-simulation.html#sequential-mc-for-the-ar1-process"><i class="fa fa-check"></i><b>6.1.1</b> Sequential MC for the AR(1)-process</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="gaussian-random-variables.html"><a href="gaussian-random-variables.html"><i class="fa fa-check"></i><b>6.2</b> Gaussian random variables</a></li>
</ul></li>
<li class="part"><span><b>Part III: Optimization</b></span></li>
<li class="chapter" data-level="7" data-path="five-examples.html"><a href="five-examples.html"><i class="fa fa-check"></i><b>7</b> Five Examples</a><ul>
<li class="chapter" data-level="7.1" data-path="exp-fam.html"><a href="exp-fam.html"><i class="fa fa-check"></i><b>7.1</b> Exponential families</a><ul>
<li class="chapter" data-level="7.1.1" data-path="exp-fam.html"><a href="exp-fam.html#exp-fam"><i class="fa fa-check"></i><b>7.1.1</b> Full exponential families</a></li>
<li class="chapter" data-level="7.1.2" data-path="exp-fam.html"><a href="exp-fam.html#exponential-family-bayesian-networks"><i class="fa fa-check"></i><b>7.1.2</b> Exponential family Bayesian networks</a></li>
<li class="chapter" data-level="7.1.3" data-path="exp-fam.html"><a href="exp-fam.html#exp-fam-deriv"><i class="fa fa-check"></i><b>7.1.3</b> Likelihood computations</a></li>
<li class="chapter" data-level="7.1.4" data-path="exp-fam.html"><a href="exp-fam.html#curved-exponential-families"><i class="fa fa-check"></i><b>7.1.4</b> Curved exponential families</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="multinomial-models.html"><a href="multinomial-models.html"><i class="fa fa-check"></i><b>7.2</b> Multinomial models</a><ul>
<li class="chapter" data-level="7.2.1" data-path="multinomial-models.html"><a href="multinomial-models.html#peppered-moths"><i class="fa fa-check"></i><b>7.2.1</b> Peppered Moths</a></li>
<li class="chapter" data-level="7.2.2" data-path="multinomial-models.html"><a href="multinomial-models.html#multinomial-cell-collapsing"><i class="fa fa-check"></i><b>7.2.2</b> Multinomial cell collapsing</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>7.3</b> Regression models</a></li>
<li class="chapter" data-level="7.4" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html"><i class="fa fa-check"></i><b>7.4</b> Finite mixture models</a><ul>
<li class="chapter" data-level="7.4.1" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#gaussian-mixtures"><i class="fa fa-check"></i><b>7.4.1</b> Gaussian mixtures</a></li>
<li class="chapter" data-level="7.4.2" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#von-mises-mixtures"><i class="fa fa-check"></i><b>7.4.2</b> von Mises mixtures</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="mixed-models.html"><a href="mixed-models.html"><i class="fa fa-check"></i><b>7.5</b> Mixed models</a></li>
<li class="chapter" data-level="7.6" data-path="state-space-models.html"><a href="state-space-models.html"><i class="fa fa-check"></i><b>7.6</b> State space models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="NumOpt.html"><a href="NumOpt.html"><i class="fa fa-check"></i><b>8</b> Numerical optimization</a><ul>
<li class="chapter" data-level="8.1" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html"><i class="fa fa-check"></i><b>8.1</b> Algorithms and convergence</a><ul>
<li class="chapter" data-level="8.1.1" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#descent-algorithms"><i class="fa fa-check"></i><b>8.1.1</b> Descent algorithms</a></li>
<li class="chapter" data-level="8.1.2" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#maps-and-fixed-points"><i class="fa fa-check"></i><b>8.1.2</b> Maps and fixed points</a></li>
<li class="chapter" data-level="8.1.3" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#convergence-rate"><i class="fa fa-check"></i><b>8.1.3</b> Convergence rate</a></li>
<li class="chapter" data-level="8.1.4" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#stopping-criteria"><i class="fa fa-check"></i><b>8.1.4</b> Stopping criteria</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html"><i class="fa fa-check"></i><b>8.2</b> Descent direction algorithms</a><ul>
<li class="chapter" data-level="8.2.1" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#line-search"><i class="fa fa-check"></i><b>8.2.1</b> Line search</a></li>
<li class="chapter" data-level="8.2.2" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>8.2.2</b> Gradient descent</a></li>
<li class="chapter" data-level="8.2.3" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#conjugate-gradients"><i class="fa fa-check"></i><b>8.2.3</b> Conjugate gradients</a></li>
<li class="chapter" data-level="8.2.4" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#peppered-moths-1"><i class="fa fa-check"></i><b>8.2.4</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html"><i class="fa fa-check"></i><b>8.3</b> Newton-type algorithms</a><ul>
<li class="chapter" data-level="8.3.1" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#poisson-regression"><i class="fa fa-check"></i><b>8.3.1</b> Poisson regression</a></li>
<li class="chapter" data-level="8.3.2" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#quasi-newton-algorithms"><i class="fa fa-check"></i><b>8.3.2</b> Quasi-Newton algorithms</a></li>
<li class="chapter" data-level="8.3.3" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#sparsity"><i class="fa fa-check"></i><b>8.3.3</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="misc-.html"><a href="misc-.html"><i class="fa fa-check"></i><b>8.4</b> Misc.</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="EM.html"><a href="EM.html"><i class="fa fa-check"></i><b>9</b> Expectation Maximization algorithms</a><ul>
<li class="chapter" data-level="9.1" data-path="basic-properties.html"><a href="basic-properties.html"><i class="fa fa-check"></i><b>9.1</b> Basic properties</a><ul>
<li class="chapter" data-level="9.1.1" data-path="basic-properties.html"><a href="basic-properties.html#incomplete-data-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Incomplete data likelihood</a></li>
<li class="chapter" data-level="9.1.2" data-path="basic-properties.html"><a href="basic-properties.html#the-em-algorithm-is-ascending"><i class="fa fa-check"></i><b>9.1.2</b> The EM-algorithm is ascending</a></li>
<li class="chapter" data-level="9.1.3" data-path="basic-properties.html"><a href="basic-properties.html#multinomial-cell-collapsing-1"><i class="fa fa-check"></i><b>9.1.3</b> Multinomial cell collapsing</a></li>
<li class="chapter" data-level="9.1.4" data-path="basic-properties.html"><a href="basic-properties.html#peppered-moths-e--and-m-steps"><i class="fa fa-check"></i><b>9.1.4</b> Peppered Moths E- and M-steps</a></li>
<li class="chapter" data-level="9.1.5" data-path="basic-properties.html"><a href="basic-properties.html#inside-the-em"><i class="fa fa-check"></i><b>9.1.5</b> Inside the EM</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="EM-exp.html"><a href="EM-exp.html"><i class="fa fa-check"></i><b>9.2</b> Exponential families</a></li>
<li class="chapter" data-level="9.3" data-path="fisher-information.html"><a href="fisher-information.html"><i class="fa fa-check"></i><b>9.3</b> Fisher information</a></li>
<li class="chapter" data-level="9.4" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html"><i class="fa fa-check"></i><b>9.4</b> Two examples revisited</a><ul>
<li class="chapter" data-level="9.4.1" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html#gaussian-mixtures-1"><i class="fa fa-check"></i><b>9.4.1</b> Gaussian mixtures</a></li>
<li class="chapter" data-level="9.4.2" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html#gaussian-state-space"><i class="fa fa-check"></i><b>9.4.2</b> Gaussian state space</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="StochOpt.html"><a href="StochOpt.html"><i class="fa fa-check"></i><b>10</b> Stochastic Optimization</a><ul>
<li class="chapter" data-level="10.1" data-path="stochastic-gradient.html"><a href="stochastic-gradient.html"><i class="fa fa-check"></i><b>10.1</b> Stochastic gradient</a></li>
<li class="chapter" data-level="10.2" data-path="stochastic-em.html"><a href="stochastic-em.html"><i class="fa fa-check"></i><b>10.2</b> Stochastic EM</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="descent-direction-algorithms" class="section level2">
<h2><span class="header-section-number">8.2</span> Descent direction algorithms</h2>
<p>The negative gradient of <span class="math inline">\(H\)</span> in <span class="math inline">\(\theta\)</span> is the direction of steepest descent. Starting from <span class="math inline">\(\theta_0\)</span> and with the goal of minimizing <span class="math inline">\(H\)</span>, it is natural to move away from <span class="math inline">\(\theta_0\)</span> in the direction of <span class="math inline">\(-\nabla H(\theta_0)\)</span>. Thus we could define<br />
<span class="math display">\[\theta_1 = \theta_0 - \gamma \nabla H(\theta_0)\]</span> for a suitably chosen <span class="math inline">\(\gamma &gt; 0\)</span>. By Taylor’s theorem <span class="math display">\[H(\theta_1) = H(\theta_0) - \gamma \|\nabla H(\theta_0)\|^2_2 + o(\gamma),\]</span> which means that if <span class="math inline">\(\theta_0\)</span> is not a stationary point (<span class="math inline">\(\nabla H(\theta_0) \neq 0\)</span>) then <span class="math display">\[H(\theta_1) &lt; H(\theta_0)\]</span> for <span class="math inline">\(\gamma\)</span> small enough.</p>
<p>More generally, we define a <em>descent direction</em> in <span class="math inline">\(\theta_0\)</span> as a vector <span class="math inline">\(\rho_0 \in \mathbb{R}^p\)</span> such that <span class="math display">\[\nabla H(\theta_0)^T \rho_0 &lt; 0.\]</span> By the same kind of Taylor argument above, <span class="math inline">\(H\)</span> will descent for a sufficiently small step size in the direction of any descent direction. And if <span class="math inline">\(\theta_0\)</span> is not a stationary point, <span class="math inline">\(-\nabla H(\theta_0)^T\)</span> is a descent direction.</p>
<p>One strategy for choosing <span class="math inline">\(\gamma\)</span> is to minimize the univariate function <span class="math display">\[\gamma \mapsto H(\theta_0 + \gamma \rho_0),\]</span> which is an example of a <em>line search</em> method. Such a minimization would give the maximal possible descent in the direction <span class="math inline">\(\rho_0\)</span>, and as we have argued, if <span class="math inline">\(\rho_0\)</span> is a descent direction, a minimizer <span class="math inline">\(\gamma &gt; 0\)</span> guarantees descent of <span class="math inline">\(H\)</span>. However, unless the minimization can be done analytically it is often computationally too expensive. Less will also do, and as shown in Example <a href="algorithms-and-convergence.html#exm:grad-descent">8.1</a>, if the Hessian has uniformly bounded numerical radius it is possible to fix one (sufficiently small) step length that will guarantee descent.</p>
<div id="line-search" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Line search</h3>
<p>We consider algorithms of the form <span class="math display">\[\theta_{n+1} = \theta_n + \gamma_{n+1} \rho_n\]</span> for descent directions <span class="math inline">\(\rho_n\)</span> and starting in <span class="math inline">\(\theta_0\)</span>. The step lengths, <span class="math inline">\(\gamma_n\)</span>, are chosen so as to give sufficient descent in each iteration.</p>
<p>We let <span class="math inline">\(h(\gamma) = H(\theta_{n} + \gamma \rho_{n})\)</span> denote the univariate and differentiable function of <span class="math inline">\(\gamma\)</span>, <span class="math display">\[h : [0,\infty) \to \mathbb{R},\]</span> that gives the value of <span class="math inline">\(H\)</span> in the direction of the descent direction <span class="math inline">\(\rho_n\)</span>. We can observe that <span class="math display">\[h&#39;(\gamma) = \nabla H(\theta_{n} + \gamma \rho_{n})^T \rho_{n},\]</span> and maximal descent in direction <span class="math inline">\(\rho_n\)</span> can be found by solving <span class="math inline">\(h&#39;(\gamma) = 0\)</span> for <span class="math inline">\(\gamma\)</span>. As mentioned above, less will do. First note that <span class="math display">\[h&#39;(0) = \nabla H(\theta_{n})^T \rho_{n} &lt; 0,\]</span> so <span class="math inline">\(h\)</span> has a negative slope in 0. It descents in a sufficiently small interval <span class="math inline">\([0, \varepsilon)\)</span>, and it is even true that for any <span class="math inline">\(c \in (0, 1)\)</span> there is an <span class="math inline">\(\varepsilon &gt; 0\)</span> such that <span class="math display">\[h(\gamma) \leq h(0) + c \gamma h&#39;(0)\]</span> for <span class="math inline">\(\gamma \in [0, \varepsilon)\)</span>. We note that this inequality can be checked easily for any given <span class="math inline">\(\gamma &gt; 0\)</span>, and is known as the <em>sufficient descent</em> condition. Sufficient descent is not enough in itself as the step length could be arbitrarily small, and the algorithm could effectively get stuck.</p>
<p>To prevent too small steps we can enforce another condition. Very close to 0, <span class="math inline">\(h\)</span> will have almost the same slope, <span class="math inline">\(h&#39;(0)\)</span>, as it has in 0. If we therefore require that the slope in <span class="math inline">\(\gamma\)</span> should be larger than <span class="math inline">\(\tilde{c} h&#39;(0)\)</span> for some <span class="math inline">\(\tilde{c} \in (0, 1)\)</span>, it forces <span class="math inline">\(\gamma\)</span> away from 0. This is known as the <em>curvature condition</em>.</p>
<p>The combined conditions on <span class="math inline">\(\gamma\)</span>, <span class="math display">\[h(\gamma) \leq h(0) + c \gamma h&#39;(0)\]</span> for a <span class="math inline">\(c \in (0, 1)\)</span> and <span class="math display">\[h&#39;(\gamma) \geq \tilde{c} h&#39;(0)\]</span> for a <span class="math inline">\(\tilde{c} \in (c, 1)\)</span> are known collectively as the <em>Wolfe conditions</em>. It can be shown that if <span class="math inline">\(h\)</span> is bounded below there exists a step length satisfying the Wolfe conditions (Lemma 3.1 in <span class="citation">Nocedal and Wright (<a href="#ref-Nocedal:2006">2006</a>)</span>).</p>
<p>Even when choosing <span class="math inline">\(\gamma_{n}\)</span> to fulfill the Wolfe conditions there is no guarantee that <span class="math inline">\(\theta_n\)</span> will converge let alone converge toward a global minimizer. The best we can hope for in general is that <span class="math display">\[\|\nabla H(\theta_n)\|_2 \rightarrow 0\]</span> for <span class="math inline">\(n \to \infty\)</span>, and this will happen under some relatively weak conditions on <span class="math inline">\(H\)</span> (Theorem 3.2 <span class="citation">Nocedal and Wright (<a href="#ref-Nocedal:2006">2006</a>)</span>) under the assumption that <span class="math display">\[\frac{\nabla H(\theta_n)^T \rho_n}{\|\nabla H(\theta_n)\|_2 \| \rho_n\|_2} \leq - \delta &lt; 0.\]</span> That is, the angle between the descent direction and the gradient should be uniformly bounded away from <span class="math inline">\(90^{\circ}\)</span>.</p>
<p>A practical way of searching for a step length is via <em>backtracking</em>. Choosing a <span class="math inline">\(\gamma_0\)</span> and a constant <span class="math inline">\(d \in (0, 1)\)</span> we can search through the sequence of step lengths <span class="math display">\[\gamma_0, d \gamma_0, d^2 \gamma_0, d^3 \gamma_0, \ldots\]</span> and stop the first time we find a step length satisfying the Wolfe conditions.</p>
<p>Using backtracking, we can actually dispense of the curvature condition and simply check the sufficient descent condition</p>
<p><span class="math display">\[H(\theta_{n} + d^k \gamma_0 \rho_{n}) \leq H(\theta_n) + cd^k \gamma_0 \nabla H(\theta_{n})^T \rho_{n}\]</span></p>
<p>for <span class="math inline">\(c \in (0, 1)\)</span>. The implementation of backtracking requires the choice of the three parameters <span class="math inline">\(\gamma_0\)</span>, <span class="math inline">\(d \in (0, 1)\)</span> and <span class="math inline">\(c \in (0, 1)\)</span>. A good choice depends quite a lot on the algorithm used for choosing the descent direction, but choosing <span class="math inline">\(c\)</span> too close to 1 can make the algorithm take too small steps, and taking <span class="math inline">\(d\)</span> too small can likewise generate small step lengths. Thus <span class="math inline">\(d = 0.8\)</span> or <span class="math inline">\(d = 0.9\)</span> and <span class="math inline">\(c = 0.1\)</span> or even smaller are sensible choices. For some algorithms, like the Newton algorithm to be dealt with below, there is a natural choice of <span class="math inline">\(\gamma_0 = 1\)</span>. But for other algorithms a good choice depends crucially on the scale of the parameters, and there is then no general advice on choosing <span class="math inline">\(\gamma_0\)</span> that can be justified theoretically.</p>
</div>
<div id="gradient-descent" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Gradient descent</h3>
<p>We return to the Poisson regression example and implement functions in R for computing the negative log-likelihood and its gradient. We exploit the <code>model.matrix</code> function to construct the model matrix from the data via a formula. The sufficient statistic is computed upfront, and the implementations use this vector and relies on linear algebra and vectorized computations. We choose to normalize by the number of observations <span class="math inline">\(n\)</span> (the number of rows in the model matrix). This does have a small computational cost, but the resulting numerical values become less dependent upon <span class="math inline">\(n\)</span>, which makes it easier to choose sensible default values of various parameters for the numerical optimization algorithms.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(sale <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(normalSale), <span class="dt">data =</span> vegetables)
z &lt;-<span class="st"> </span>vegetables<span class="op">$</span>sale
## The function `drop` drops the dimensions attribute
t_map &lt;-<span class="st"> </span><span class="kw">drop</span>(<span class="kw">crossprod</span>(X, z))  ## More efficient than drop(t(X) %*% z)

H &lt;-<span class="st"> </span><span class="cf">function</span>(beta) 
  (<span class="kw">drop</span>(<span class="kw">sum</span>(<span class="kw">exp</span>(X <span class="op">%*%</span><span class="st"> </span>beta)) <span class="op">-</span><span class="st"> </span>beta <span class="op">%*%</span><span class="st"> </span>t_map)) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(X)

grad_H &lt;-<span class="st"> </span><span class="cf">function</span>(beta) 
  (<span class="kw">colSums</span>(<span class="kw">drop</span>(<span class="kw">exp</span>(X <span class="op">%*%</span><span class="st"> </span>beta)) <span class="op">*</span><span class="st"> </span>X) <span class="op">-</span><span class="st"> </span>t_map) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(X)</code></pre></div>
<p>We implement a gradient descent algorithm with backtracking that uses the <em>squared</em> norm of the gradient as a stopping criterion. For gradient descent, the sufficient descent condition amounts to choosing the smallest <span class="math inline">\(k \geq 0\)</span> such that</p>
<p><span class="math display">\[H(\theta_{n} + d^k \gamma_0 \nabla H(\theta_{n})) \leq H(\theta_n) -  cd^k \gamma_0 \|\nabla H(\theta_{n})\|_2^2.\]</span></p>
<p>We insert a call to a trace function in each iteration given that it has been supplied as an argument. This gives us the possibility of extracting or printing values of variables during evaluation, which can be highly useful for understanding the inner workings of the algorithm. The actual implementation of the a trace function is given below, and can be adapted as we like to provide the information we want.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">GD &lt;-<span class="st"> </span><span class="cf">function</span>(par, <span class="dt">d =</span> <span class="fl">0.8</span>, <span class="dt">c =</span> <span class="fl">0.1</span>, <span class="dt">gamma0 =</span> <span class="fl">0.01</span>, <span class="dt">epsilon =</span> <span class="fl">1e-4</span>, <span class="dt">trace =</span> <span class="ot">NULL</span>) {
  <span class="cf">repeat</span> {
    value &lt;-<span class="st"> </span><span class="kw">H</span>(par)
    grad &lt;-<span class="st"> </span><span class="kw">grad_H</span>(par)
    h_prime &lt;-<span class="st"> </span><span class="kw">sum</span>(grad<span class="op">^</span><span class="dv">2</span>)
    <span class="cf">if</span>(<span class="op">!</span><span class="kw">is.null</span>(trace)) <span class="kw">trace</span>()
    ## Convergence criterion based on gradient norm
    <span class="cf">if</span>(h_prime <span class="op">&lt;=</span><span class="st"> </span>epsilon) <span class="cf">break</span>
    gamma &lt;-<span class="st"> </span>gamma0
    ## First proposed descent step
    par1 &lt;-<span class="st"> </span>par <span class="op">-</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>grad
    ## Backtracking while descent is insufficient
    <span class="cf">while</span>(<span class="kw">H</span>(par1) <span class="op">&gt;</span><span class="st"> </span>value <span class="op">-</span><span class="st"> </span>c <span class="op">*</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>h_prime) {
      gamma &lt;-<span class="st"> </span>d <span class="op">*</span><span class="st"> </span>gamma
      par1 &lt;-<span class="st"> </span>par <span class="op">-</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>grad
    }
    par &lt;-<span class="st"> </span>par1
  }
  par
}</code></pre></div>
<p>Gradient descent is very slow for the large Poisson model with individual store effects, so we consider only the simple model with two parameters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pois_GD &lt;-<span class="st"> </span><span class="kw">GD</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">ncol</span>(X)))</code></pre></div>
<p>The gradient descent implementation is tested by comparing the minimizer to the estimated parameters as computed by <code>glm</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">as.numeric</span>(<span class="kw">coefficients</span>(pois_model_null))  ## as.numeric just to strip names
<span class="kw">as.numeric</span>(pois_GD)</code></pre></div>
<pre><code>## [1] 1.4614403396 0.9215698864
## [1] 1.4603520741 0.9219357553</code></pre>
<p>We get the same result up to the first two decimals. The convergence criterion on our gradient descent algorithm was quite loose (<span class="math inline">\(\varepsilon = 10^{-4}\)</span>, which means that the norm of the gradient is smaller than <span class="math inline">\(10^{-2}\)</span> when the algorithm stops). This choice of <span class="math inline">\(\varepsilon\)</span> in combination with <span class="math inline">\(\gamma_0 = 0.01\)</span> implies that the algorithm stops when the gradient is so small that the changes are at most of norm <span class="math inline">\(10^{-4}\)</span>.</p>
<p>Comparing the resulting values of the negative log-likelihood shows agreement up to the first five decimals, but we notice that the value for the parameters fitted using <code>glm</code> is just slightly smaller.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">H</span>(<span class="kw">coefficients</span>(pois_model_null))
<span class="kw">H</span>(pois_GD)</code></pre></div>
<pre><code>## [1] -124.406827879897
## [1] -124.406825325047</code></pre>
<p>To investigate what actually went on inside the gradient descent algorithm we implement a trace function. In fact, we implement a function for constructing a tracer object, which has a way of saving and printing trace information during the evaluation of the gradient descent algorithm – or any other algorithm that implements a similar trace functionality. The tracer object does this by storing information in the enclosing environment of the trace function that is passed to the <code>GD</code> function. This trace function looks up variables in the evaluation environment of <code>GD</code>, stores them and prints them if requested, and store run time information as well. After the algorithm has converged the trace information can be accessed via the <code>summary</code> method for the tracer object. This implementation of tracer objects should not be confused with the <code>trace</code> function from the R base package. It has a related functionality that can be used with any function and is used for debugging. The tracer object as implemented here can be used with functions such as <code>GD</code> above that explicitly support calling a trace function, and it monitors the internal state during evaluation of the function without interrupting evaluation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Function `tracer` constructs a tracer object containing a `trace` function
## that can be passed as an argument to other functions that support a trace
## function. Arguments are
##
## object: a character vector of names of the objects in the evaluation environment 
##         that are traced.
## N:      a numeric specifying if and how often trace information is printed. 
##         N = 1 (default) means every iteration. N = 0 means never.
## save:   a logical value. Should the trace information be saved or just printed.
## time:   a logical value. Should run time information be save.
## ...:    other arguments passed to `format` for printing.

tracer &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">objects =</span> <span class="ot">NULL</span>, <span class="dt">N =</span> <span class="dv">1</span>, <span class="dt">save =</span> <span class="ot">TRUE</span>, <span class="dt">time =</span> <span class="ot">TRUE</span>, ...) {
  n &lt;-<span class="st"> </span><span class="dv">1</span>
  values_save &lt;-<span class="st"> </span><span class="kw">list</span>()
  last_time &lt;-<span class="st"> </span><span class="kw">proc.time</span>()
  trace &lt;-<span class="st"> </span><span class="cf">function</span>() {
  <span class="cf">if</span>(<span class="kw">is.null</span>(objects))
    objects &lt;-<span class="st"> </span><span class="kw">ls</span>(<span class="kw">parent.frame</span>())
  values &lt;-<span class="st"> </span><span class="kw">mget</span>(objects, <span class="dt">envir =</span> <span class="kw">parent.frame</span>(), <span class="dt">ifnotfound =</span> <span class="kw">list</span>(<span class="ot">NA</span>))
   <span class="cf">if</span>(N <span class="op">&amp;&amp;</span><span class="st"> </span>(n <span class="op">==</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>n <span class="op">%%</span><span class="st"> </span>N <span class="op">==</span><span class="st"> </span><span class="dv">0</span>))
    <span class="kw">cat</span>(<span class="st">&quot;n = &quot;</span>, n, <span class="st">&quot;: &quot;</span>,  <span class="kw">paste</span>(<span class="kw">names</span>(values), <span class="st">&quot; = &quot;</span>, <span class="kw">format</span>(values, ...), 
                                <span class="st">&quot;; &quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)
  <span class="cf">if</span>(save)
    <span class="cf">if</span>(time) {
      time_diff &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">proc.time</span>() <span class="op">-</span><span class="st"> </span>last_time)[<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)]
      values[[<span class="st">&quot;.time&quot;</span>]] &lt;-<span class="st"> </span>time_diff[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>time_diff[<span class="dv">2</span>]
      last_time &lt;&lt;-<span class="st"> </span><span class="kw">proc.time</span>()
    }
  values_save[[n]] &lt;&lt;-<span class="st"> </span>values
  n &lt;&lt;-<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
  }
  get &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">simplify =</span> <span class="ot">FALSE</span>) {
    <span class="cf">if</span>(simplify) {
      col_names &lt;-<span class="st"> </span><span class="kw">unique</span>(<span class="kw">unlist</span>(<span class="kw">lapply</span>(values_save, names)))
      values_save &lt;-<span class="st"> </span><span class="kw">lapply</span>(
        col_names, 
        <span class="cf">function</span>(x) <span class="kw">do.call</span>(rbind, <span class="kw">unlist</span>(<span class="kw">lapply</span>(values_save, <span class="cf">function</span>(y) y[x]), 
                                          <span class="dt">recursive =</span> <span class="ot">FALSE</span>))
        )
      <span class="kw">names</span>(values_save) &lt;-<span class="st"> </span>col_names
      values_save &lt;-<span class="st"> </span><span class="kw">lapply</span>(col_names, <span class="cf">function</span>(x) {
        x_val &lt;-<span class="st"> </span>values_save[[x]] 
        <span class="cf">if</span>(<span class="op">!</span><span class="kw">is.null</span>(<span class="kw">ncol</span>(x_val)) <span class="op">&amp;&amp;</span><span class="st"> </span><span class="kw">ncol</span>(x_val) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {
          <span class="kw">colnames</span>(x_val) &lt;-<span class="st"> </span>x
        } <span class="cf">else</span> {
          <span class="kw">colnames</span>(x_val) &lt;-<span class="st"> </span><span class="kw">paste</span>(x, <span class="st">&quot;.&quot;</span>, <span class="kw">colnames</span>(x_val), <span class="dt">sep =</span> <span class="st">&quot;&quot;</span>)
        }
        x_val
      })
      values_save &lt;-<span class="st"> </span><span class="kw">do.call</span>(cbind, values_save)
      <span class="kw">row.names</span>(values_save) &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(values_save)
    }
    values_save
  }
  <span class="kw">structure</span>(<span class="kw">list</span>(<span class="dt">trace =</span> trace, <span class="dt">get =</span> get), <span class="dt">class =</span> <span class="st">&quot;tracer&quot;</span>)
}

## Methods for subsetting, printing and summarizing tracer objects
<span class="st">&#39;[.tracer&#39;</span> &lt;-<span class="st"> </span><span class="cf">function</span>(x, i, j, ..., <span class="dt">drop =</span> <span class="ot">TRUE</span>) {
  values &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">get</span>(...)[i] 
  <span class="cf">if</span> (drop <span class="op">&amp;&amp;</span><span class="st"> </span><span class="kw">length</span>(i) <span class="op">==</span><span class="st"> </span><span class="dv">1</span>)
    values &lt;-<span class="st"> </span>values[[<span class="dv">1</span>]]
  values
}
print.tracer &lt;-<span class="st"> </span><span class="cf">function</span>(x, ...) <span class="kw">print</span>(x<span class="op">$</span><span class="kw">get</span>(...))
summary.tracer &lt;-<span class="st"> </span><span class="cf">function</span>(x, ...) {
  x &lt;-<span class="st"> </span><span class="kw">suppressWarnings</span>(x<span class="op">$</span><span class="kw">get</span>(<span class="dt">simplify =</span> <span class="ot">TRUE</span>))
  x[, <span class="st">&quot;.time&quot;</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">cumsum</span>(x[<span class="op">-</span><span class="dv">1</span>, <span class="st">&quot;.time&quot;</span>]))
  <span class="kw">as.data.frame</span>(x)
}</code></pre></div>
<p>We use the tracer object with the our gradient descent implementation and print trace information every 50th iteration.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">GD_tracer &lt;-<span class="st"> </span><span class="kw">tracer</span>(<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>, <span class="st">&quot;h_prime&quot;</span>, <span class="st">&quot;gamma&quot;</span>), <span class="dt">N =</span> <span class="dv">50</span>)
<span class="kw">system.time</span>(pois_GD &lt;-<span class="st"> </span><span class="kw">GD</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">ncol</span>(X)), <span class="dt">trace =</span> GD_tracer<span class="op">$</span>trace))</code></pre></div>
<pre><code>## n = 1: value = 1; h_prime = 14268.59; gamma = NA; 
## n = 50: value = -123.9395; h_prime = 15.45722; gamma = 0.004096; 
## n = 100: value = -124.3243; h_prime = 3.133931; gamma = 0.00512; 
## n = 150: value = -124.3935; h_prime = 0.601431; gamma = 0.00512; 
## n = 200: value = -124.4048; h_prime = 0.09805907; gamma = 0.00512; 
## n = 250: value = -124.4065; h_prime = 0.0109742; gamma = 0.00512; 
## n = 300: value = -124.4068; h_prime = 0.002375827; gamma = 0.00512; 
## n = 350: value = -124.4068; h_prime = 0.000216502; gamma = 0.004096;</code></pre>
<pre><code>##    user  system elapsed 
##   0.096   0.010   0.105</code></pre>
<p>We see that the gradient descent algorithm runs for a little more than 350 iterations, and we can observe how the value of the negative log-likelihood is descending. We can also see that the step length <span class="math inline">\(\gamma\)</span> bounces between <span class="math inline">\(0.004096 = 0.8^4 \times 0.01\)</span> and <span class="math inline">\(0.00512 = 0.8^3 \times 0.01\)</span>, thus the backtracking takes 3 to 4 iterations to find a step length with sufficient descent.</p>
<p>The printed trace does not reveal the run time information. The run time information is computed and stored as differences between process timings at each iteration of the algorithm, and the precision is at best of the order of one millisecond (see <code>?proc.time</code>). Hence the run time associated to one single iteration may be fairly inaccurate for fast iterations, but the cumulative run time can still give a reasonable indication of time usage over many iterations. This information is, however, best inspected and computed after the algorithm has converged, and it is computed and returned by the summary method for tracer objects.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tail</span>(<span class="kw">summary</span>(GD_tracer))</code></pre></div>
<pre><code>##         value      h_prime    gamma .time
## 372 -124.4068 1.125779e-04 0.005120 0.082
## 373 -124.4068 1.218925e-04 0.005120 0.082
## 374 -124.4068 1.323878e-04 0.005120 0.082
## 375 -124.4068 1.441965e-04 0.005120 0.082
## 376 -124.4068 1.574572e-04 0.005120 0.082
## 377 -124.4068 7.600796e-05 0.004096 0.083</code></pre>
<p>The trace information is stored in a list. The summary method transforms the trace information into a data frame with one row per iteration. We can also access individual entries of the list of trace information via subscripting.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">GD_tracer[<span class="dv">377</span>] </code></pre></div>
<pre><code>## $value
## [1] -124.4068
## 
## $h_prime
## [1] 7.600796e-05
## 
## $gamma
## [1] 0.004096
## 
## $.time
## [1] 0.001</code></pre>
<div class="figure" style="text-align: center"><span id="fig:GD-trace-plot"></span>
<img src="CSwR_files/figure-html/GD-trace-plot-1.png" alt="Gradient norm (top) and value of the negative log-likelihood (bottom) above the limit value $H(\theta_{\infty})$. The straight line is fitted to the data points except the first ten using least squares, and the rate is computed from this estimate and reported per ms." width="100%" />
<p class="caption">
Figure 8.1: Gradient norm (top) and value of the negative log-likelihood (bottom) above the limit value <span class="math inline">\(H(\theta_{\infty})\)</span>. The straight line is fitted to the data points except the first ten using least squares, and the rate is computed from this estimate and reported per ms.
</p>
</div>
</div>
<div id="conjugate-gradients" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Conjugate gradients</h3>
<p>The gradient direction is typically not the best descent direction. It is too local, and convergence can be quite slow. One of the better algorithms that is still a “first order algorithm” (using only gradient information) is the <a href="https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method"><em>conjugate gradient</em></a> algorithm. In the Fletcher and Reeves version of the algorithm the descent direction is initialized as the negative gradient <span class="math inline">\(\rho_0 = - \nabla H(\theta_{0})\)</span> and then updated as <span class="math display">\[\rho_{n} = - \nabla H(\theta_{n}) + \frac{\|\nabla H(\theta_n)\|_2^2}{\|\nabla H(\theta_{n-1})\|_2^2} \rho_{n-1}.\]</span> That is, the descent direction, <span class="math inline">\(\rho_{n}\)</span>, is the negative gradient but modified according to the previous descent direction.</p>
<p>In fact, <span class="math inline">\(\rho_{n}\)</span> need not be a descent direction unless we put some restrictions on the step lengths. If one requires the step length <span class="math inline">\(\gamma_{n+1}\)</span> to satisfy the <em>strong</em> curvature condition <span class="math display">\[|h&#39;(\gamma)| = |\nabla H(\theta_n + \gamma \rho_n)^T \rho_n | \leq \tilde{c} |\nabla H(\theta_n)^T \rho_in = \tilde{c} |h&#39;(0)|\]</span> for <span class="math inline">\(\tilde{c} &lt; \frac{1}{2}\)</span>, then <span class="math inline">\(\rho_{n + 1}\)</span> can be shown to be a descent direction if <span class="math inline">\(\rho_n\)</span> is.</p>
<p>We implement the conjugate gradient method in a slightly different way. Instead of introducing the more advanced curvature condition, we simply reset the algorithm to use the gradient direction in any case where a non-descent direction has been chosen.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">CG &lt;-<span class="st"> </span><span class="cf">function</span>(par, <span class="dt">d =</span> <span class="fl">0.8</span>, <span class="dt">c =</span> <span class="fl">0.1</span>, <span class="dt">gamma0 =</span> <span class="dv">1</span>, <span class="dt">epsilon =</span> <span class="fl">1e-6</span>, <span class="dt">trace =</span> <span class="ot">NULL</span>) {
  rho0 &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="kw">length</span>(par))
  <span class="cf">repeat</span> {
    value &lt;-<span class="st"> </span><span class="kw">H</span>(par)
    grad &lt;-<span class="st"> </span><span class="kw">grad_H</span>(par)
    grad_norm_sq &lt;-<span class="st"> </span><span class="kw">sum</span>(grad<span class="op">^</span><span class="dv">2</span>)
    <span class="cf">if</span>(<span class="op">!</span><span class="kw">is.null</span>(trace)) <span class="kw">trace</span>()
    <span class="cf">if</span>(grad_norm_sq <span class="op">&lt;=</span><span class="st"> </span>epsilon) <span class="cf">break</span>
    gamma &lt;-<span class="st"> </span>gamma0
    ## Descent direction
    rho &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>grad <span class="op">+</span><span class="st"> </span>grad_norm_sq <span class="op">*</span><span class="st"> </span>rho0
    h_prime &lt;-<span class="st"> </span><span class="kw">drop</span>(<span class="kw">t</span>(grad) <span class="op">%*%</span><span class="st"> </span>rho)
    ## Reset to gradient descent if rho is not a descent direction
    <span class="cf">if</span>(h_prime <span class="op">&gt;=</span><span class="st"> </span><span class="dv">0</span>) {
      rho &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>grad
      h_prime &lt;-<span class="st"> </span><span class="op">-</span><span class="st"> </span>grad_norm_sq 
    }
    par1 &lt;-<span class="st"> </span>par <span class="op">+</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>rho
    ## Backtracking
    <span class="cf">while</span>(<span class="kw">H</span>(par1) <span class="op">&gt;</span><span class="st"> </span>value <span class="op">+</span><span class="st"> </span>c <span class="op">*</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>h_prime) {
      gamma &lt;-<span class="st"> </span>d <span class="op">*</span><span class="st"> </span>gamma
      par1 &lt;-<span class="st"> </span>par <span class="op">+</span><span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>rho
    }
    rho0 &lt;-<span class="st"> </span>rho <span class="op">/</span><span class="st"> </span>grad_norm_sq
    par &lt;-<span class="st"> </span>par1
  }
  par
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">CG_tracer &lt;-<span class="st"> </span><span class="kw">tracer</span>(<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;grad_norm_sq&quot;</span>), <span class="dt">N =</span> <span class="dv">10</span>)
pois_CG &lt;-<span class="st"> </span><span class="kw">CG</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">ncol</span>(X)), <span class="dt">trace =</span> CG_tracer<span class="op">$</span>trace)</code></pre></div>
<pre><code>## n = 1: value = 1; gamma = NA; grad_norm_sq = 14269; 
## n = 10: value = -123.43; gamma = 0.005903; grad_norm_sq = 106.1; 
## n = 20: value = -124.38; gamma = 0.011529; grad_norm_sq = 4.4081; 
## n = 30: value = -124.41; gamma = 0.005903; grad_norm_sq = 0.018209; 
## n = 40: value = -124.41; gamma = 0.005903; grad_norm_sq = 5.6554e-05; 
## n = 50: value = -124.41; gamma = 0.0073787; grad_norm_sq = 1.7171e-06;</code></pre>
<div class="figure" style="text-align: center"><span id="fig:GD-CG-trace-plot"></span>
<img src="CSwR_files/figure-html/GD-CG-trace-plot-1.png" alt="Gradient norms (top) and negative log-likelihoods (bottom) for gradient descent (left) and conjugate gradient (right)." width="100%" />
<p class="caption">
Figure 8.2: Gradient norms (top) and negative log-likelihoods (bottom) for gradient descent (left) and conjugate gradient (right).
</p>
</div>
<p>This algorithm is fast enough to fit the large Poisson regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(sale <span class="op">~</span><span class="st"> </span>store <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(normalSale) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, 
                  <span class="dt">data =</span> vegetables)
t_map &lt;-<span class="st"> </span><span class="kw">drop</span>(<span class="kw">crossprod</span>(X, z)) </code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">CG_tracer &lt;-<span class="st"> </span><span class="kw">tracer</span>(<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;grad_norm_sq&quot;</span>), <span class="dt">N =</span> <span class="dv">100</span>)
pois_CG &lt;-<span class="st"> </span><span class="kw">CG</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">ncol</span>(X)), <span class="dt">trace =</span> CG_tracer<span class="op">$</span>trace)</code></pre></div>
<pre><code>## n = 1: value = 1; gamma = NA; grad_norm_sq = 12737; 
## n = 100: value = -127.7; gamma = 0.014412; grad_norm_sq = 4.6263; 
## n = 200: value = -128.02; gamma = 0.014412; grad_norm_sq = 0.41723; 
## n = 300: value = -128.35; gamma = 0.0024179; grad_norm_sq = 0.063094; 
## n = 400: value = -128.57; gamma = 0.10737; grad_norm_sq = 0.0083182; 
## n = 500: value = -128.59; gamma = 0.0037779; grad_norm_sq = 0.00037643; 
## n = 600: value = -128.59; gamma = 0.035184; grad_norm_sq = 8.357e-05; 
## n = 700: value = -128.59; gamma = 0.011529; grad_norm_sq = 1.1652e-05; 
## n = 800: value = -128.59; gamma = 0.0092234; grad_norm_sq = 2.9202e-06;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tail</span>(<span class="kw">summary</span>(CG_tracer))</code></pre></div>
<pre><code>##         value       gamma grad_norm_sq .time
## 816 -128.5894 0.107374182 7.966112e-05 9.088
## 817 -128.5894 0.003777893 3.155961e-06 9.099
## 818 -128.5894 0.004722366 1.065707e-06 9.109
## 819 -128.5894 0.068719477 2.355075e-05 9.115
## 820 -128.5894 0.054975581 1.730998e-05 9.122
## 821 -128.5894 0.003022315 7.042119e-07 9.133</code></pre>
<p>Using <code>optim</code> with the conjugate gradient method.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system.time</span>(pois_optim_CG &lt;-<span class="st"> </span><span class="kw">optim</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dt">length =</span> <span class="kw">ncol</span>(X)), H, grad_H, 
                                   <span class="dt">method =</span> <span class="st">&quot;CG&quot;</span>, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">maxit =</span> <span class="dv">10000</span>)))</code></pre></div>
<pre><code>##    user  system elapsed 
##  13.499   1.438  15.002</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pois_optim_CG[<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>, <span class="st">&quot;counts&quot;</span>)]</code></pre></div>
<pre><code>## $value
## [1] -128.5895
## 
## $counts
## function gradient 
##    12008     5118</code></pre>
</div>
<div id="peppered-moths-1" class="section level3">
<h3><span class="header-section-number">8.2.4</span> Peppered Moths</h3>
<p>We can code a problem specific version of the negative log-likelihood and use <code>optim</code> to minimize it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## par = c(pC, pI), pT = 1 - pC - pI
## x is the data vector
loglik &lt;-<span class="st"> </span><span class="cf">function</span>(par, x) {
  pT &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>par[<span class="dv">2</span>]
  <span class="cf">if</span> (par[<span class="dv">1</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>pT <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>)
    <span class="kw">return</span>(<span class="ot">Inf</span>)
  PC &lt;-<span class="st"> </span>par[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>pT
  PI &lt;-<span class="st"> </span>par[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>pT
  PT &lt;-<span class="st"> </span>pT<span class="op">^</span><span class="dv">2</span>
  <span class="op">-</span><span class="st"> </span>(x[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(PC) <span class="op">+</span><span class="st"> </span>x[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(PI) <span class="op">+</span><span class="st"> </span>x[<span class="dv">3</span>]<span class="op">*</span><span class="st"> </span><span class="kw">log</span>(PT))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">optim</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), loglik, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), <span class="dt">method =</span> <span class="st">&quot;CG&quot;</span>)</code></pre></div>
<pre><code>## $par
## [1] 0.07084109 0.18873718
## 
## $value
## [1] 600.481
## 
## $counts
## function gradient 
##      110       15 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>The computations can beneficially be implemented in greater generality.</p>
<p>The function <code>M</code> sums the cells that are collapsed, which has to be specified by the <code>group</code> argument.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">M &lt;-<span class="st"> </span><span class="cf">function</span>(x, group)
  <span class="kw">as.vector</span>(<span class="kw">tapply</span>(x, group, sum))</code></pre></div>
<p>The function <code>prob</code> maps the parameters to the multinomial probability vector. This function will have to be problem specific.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prob &lt;-<span class="st"> </span><span class="cf">function</span>(p) {
  p[<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>p[<span class="dv">2</span>]
  <span class="kw">c</span>(p[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>], <span class="dv">2</span><span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>], 
    p[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>], p[<span class="dv">3</span>]<span class="op">^</span><span class="dv">2</span>)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">loglik &lt;-<span class="st"> </span><span class="cf">function</span>(par, x) {
  pT &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>par[<span class="dv">2</span>]
  <span class="cf">if</span> (par[<span class="dv">1</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>pT <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>)
    <span class="kw">return</span>(<span class="ot">Inf</span>)
  <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(x <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">M</span>(<span class="kw">prob</span>(par), <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>))))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">optim</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), loglik, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), <span class="dt">method =</span> <span class="st">&quot;CG&quot;</span>)</code></pre></div>
<pre><code>## $par
## [1] 0.07084109 0.18873718
## 
## $value
## [1] 600.481
## 
## $counts
## function gradient 
##      107       15 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>The Peppered Moth example is very simple. The marginal log-likelihood can easily be computed, and we used this problem to illustrate different ways of implementing a likelihood computation in R. One was very problem specific and one was more abstract and general. We demonstrated how to use standard optimization algorithms like conjugate gradient with or without implementing the gradient. If we don’t implement a gradient, a numerical gradient is used by <code>optim</code> This can very well result in a slower algorithm than if the gradient is implemented, but more seriously, in can result in convergence problems. This is because there is a subtle tradeoff between numerical accuracy and accuracy of the finite difference approximation. We did not experience this in the example above, but one way to remedy such problems is to set the <code>parscale</code> or <code>fnscale</code> entries in the <code>control</code> list argument to <code>optim</code>.</p>
<p>Below we will illustrate how to use Newton-type methods for optimizing the marginal likelihood, and in the following chapter this same example is used to illustrate the EM-algorithm. It is important to understand that the EM-algorithm does not rely on computations of the marginal likelihood. In many real applications computation of the marginal likelihood is computationally challenging or even impossible, thus most standard optimization algorithms will not be directly applicable, the EM-algorithm will.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Nocedal:2006">
<p>Nocedal, Jorge, and Stephen J. Wright. 2006. <em>Numerical Optimization</em>. Second. Springer Series in Operations Research and Financial Engineering. New York: Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="algorithms-and-convergence.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="newton-type-algorithms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

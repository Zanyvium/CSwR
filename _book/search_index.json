[
["index.html", "Computational Statistics with R Preface", " Computational Statistics with R Niels Richard Hansen 2018-08-30, Git version: 1aa2c50 Preface This is draft material for a book on computational statistics, which is being developed specifically for the master’s education in statistics at University of Copenhagen. A solid mathematical background is assumed throughout, while the computer science prerequisites are more modest. To be specific, the reader is expected to have a reasonable command of mathematical analysis, linear algebra and mathematical statistics, as exemplified by maximum likelihood estimation of multivariate parameters and asymptotic properties of such a multivariate estimator. The reader is expected to have an understanding of what an algorithm is, how numerical computations differ from symbolic computations, and be able write small computer programs. The intention of the material is to serve as a pedagogical introduction to computational statistics. No claim is made that the material is comprehensive or even representative, nor does it purport computational statistics as a single coherent field with a unifying theoretical foundation. This introduction is driven by statistical examples with the unifying theme being an experimental approach to solving computational problems in statistics. Contemporary challenges in computational statistics revolve around large scale computations, either because the amount of data is massive or because we want to apply ever more complicated and sophisticated models and methods for the analysis and visualization of data. The examples treated are all of a rather modest complexity compared to these challenges. This is deliberate! A solid understanding of how to solve simpler problems is a prerequisite for solving complex problems. It is the hope that this material provides the reader with a foundation in computational statistics that will subseqently make him or her able to attack and solve novel problems in computational statistics. "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction Computational statistics is about turning theory and methods into algorithms and actual numerical computations with data. It is about solving real computational problems that arise when we visualize, analyze and model data. Computational statistics is not a single coherent topic but rather a large number of vaguely related computational techniques that we use in statistics. This short book is in no way attempting to be comprehensive. Instead, a few selected statistical topics are treated in some detail with the intention that good computational practice can be learned from these topics and transferred to other parts of statistics as needed. Though the topics are arguably fundamental, they reflect the knowledge and interests of the author, and different topics could clearly have been chosen. The demarcation line between statistical methodology and computational statistics is also blurred. Most methodology involves mathematical formulas and even algorithms for computing estimates and other statistics of interest from data, or for evaluating probabilities or integrals numerically or via simulations. The viewpoint taken in this book is that the transition from methodology to computational statistics happens when the methodology is to be implemented, that is, when formulas, algorithms and pseudo code are transformed into actual code and statistical software. It is during this transition that a number of practical challenges reveal themselves, such as actual computation time and memory usage, the limitations of finite precision arithmetic, and the practical value of suboptimal or approximate but sufficiently accurate solutions. Statistical software development also requires some basic software engineering skills and knowledge of the most common programming paradigms. Implementing a single algorithm for a specific problem is one thing, but developing a piece of statistical software for others to use is something quite different. This book is not an introduction to statistical software development as such, but the process of developing good software plays a prominent role. Thus solutions are not presented as code that magically manifests itself, but code is developed and analyzed in cycles that resemble how real software development takes place. There is a notable practical and experimental component to software development. However important theoretical considerations are regarding correctness and complexity of algorithms, say, the actual code has to strike a balance between generality, readability, efficiency, accuracy, ease of usage and ease of development among other things. Finding a good balance requires that one is able to reason about benefits and deficiencies of different implementations. It is a major point of this book that such reasoning should rely on experiments and empirical facts and not speculations. R and RStudio is used throughout, and the reader is expected to have some basic knowledge of R programming. While RStudio is not a requirement for most of the book, it is a recommendable IDE (integrated development environment) for R, which offers a convenient framework for developing, benchmarking, profiling, testing, documenting and experimenting with statistical software. The excellent book Advanced R by Hadley Wickham is recommended as a companion book covering R programming in detail. In fact, direct references to that book are given whenever further explanations on e.g. functional programming or object oriented programming in R are required. This book is organized into three parts on smoothing, Monte Carlo methods and optimization. Each part is introduced in the following three sections to give the reader an overview of the topics covered, how they are related to each other and how they are related to some main trends and challenges in contemporary computational statistics. In this introduction, several R functions from various packages are used to illustrate how smoothing, simulation of random variables and optimization play a role in statistics. If you have no intention of moving beyond the role of a data analyst that rely on already implemented solutions, you can stop reading after the introduction. The remaining part of the book is about the development of such solutions at a deeper level and not how to use high-level interfaces to the plethora of already existing implementations. "],
["intro-smooth.html", "1.1 Smoothing", " 1.1 Smoothing Smoothing is a descriptive statistical tool for summarizing data, a practical visualization technique, as well as a nonparametric estimation methodology. The basic idea is that data is representative of an underlying distribution with some smoothness properties, and we would like to approximate or estimate this underlying distribution from data. There are two related but slightly different approaches. Either we attempt to estimate a smooth density of the observed variables, or we attempt to estimate a smooth conditional density of one variable given others. The latter can in principle be done by computing the conditional density from a smooth estimate of the joint distribution. Thus it appears that we really just need a way of computing smooth density estimates. In practice it may, however, be better to solve the conditional smoothing problem directly instead of solving a strictly more complicated problem. This is particularly so, if the conditioning variables are fixed e.g. by a design, or if our main interest is in the conditional mean or median, say, and not the entire conditional distribution. Conditional smoothing is dealt with in Chapter 3. In this introduction we focus on the univariate case, where there really only is one problem: smooth density estimation. Moreover, this is a very basic problem, and one viewpoint is that we simply need to “smooth out the jumps of the histogram”. Indeed, it does not need to be made more sophisticated than that! Humans are able to do this quite well using just a pen and a printed histogram, but it is a bit more complicated to automatize such a smoothing procedure. Moreover, an automatized procedure is likely to need calibration to yield a good tradeoff between smoothness and data fit. This is again something that humans can do quite well by eyeballing visualizations, but that approach does not scale, neither in terms of the number of density estimates we want to consider, nor in terms of going from univariate to multivariate densities. If we want to really discuss how a smoothing procedure works not just as a heuristic but also as an estimator of an underlying density, it is necessary to formalize how to quantify the performance of the procedure. This increases the level of mathematical sophistication, but it allows us to discuss optimality, and it lets us develop fully automatized procedures that do not rely on human calibration. While human inspection of visualizations is always a good idea, computational statistics is also about offloading humans from all computational tasks that can be automatized. This is true for smoothing as well, hence the need for automatic, robust and well calibrated smoothing procedures that call for a minimum of human effort. 1.1.1 Angle distributions in proteins We will illustrate smoothing using a small data set on angles formed between two subsequent peptide planes in 3D protein structures. This data set is selected because the angle distributions are multimodal and slightly non-standard, and these properties are well suited for illustrating fundamental considerations regarding smooth density estimation in practice. Figure 1.1: The 3D structure of proteins is largely given by the \\(\\phi\\)- and \\(\\psi\\)-angles of the peptide planes. (By Dcrjsr, CC BY 3.0 via Wikimedia Commons.) A protein is a large molecule consisting of a backbone with carbon and nitrogen atoms arranged sequentially: A hydrogen atom binds to each nitrogen (N) and an oxygen atom binds to each carbon without the \\(\\alpha\\) subscript (C), see Figure 1.1, and such four atoms form together what is known as a peptide bond between two alpha-carbon atoms (C\\(_{\\alpha}\\)). Each C\\(_{\\alpha}\\) atom binds a hydrogen atom and an amino acid side chain. There are 20 naturally occurring amino acids in genetically encoded proteins, each having a three letter code (such as Gly for Glycine, Pro for Proline, etc.). The protein will typically form a complicated 3D structure determined by the amino acids, which in turn determine the \\(\\phi\\)- and the \\(\\psi\\)-angles between the peptide planes as shown on Figure 1.1. We will consider a small data set, phipsi, of experimentally determined angles from a single protein, the human protein 1HMP, which is composed of two chains (denoted A and B). Figure 1.2 shows the 3D structure of the protein. head(phipsi) ## chain AA pos phi psi ## 1 A Pro 5 -1.6218794 0.2258685 ## 2 A Gly 6 1.1483709 -2.8314426 ## 3 A Val 7 -1.4160220 2.1190570 ## 4 A Val 8 -1.4926720 2.3941331 ## 5 A Ile 9 -2.1814653 1.4877618 ## 6 A Ser 10 -0.7525375 2.5676186 Figure 1.2: The 3D structure of the atoms constituting the protein 1HMP. The colors indicate the two different chains. We can use base R functions such as hist and density to visualize the marginal distributions of the two angles. hist(phipsi$phi, prob = TRUE) rug(phipsi$phi) density(phipsi$phi) %&gt;% lines(col = &quot;red&quot;, lwd = 2) hist(phipsi$psi, prob = TRUE) rug(phipsi$psi) density(phipsi$psi) %&gt;% lines(col = &quot;red&quot;, lwd = 2) Figure 1.3: Histograms equipped with a rug plot and smoothed density estimate (red line) of the distribution of \\(\\phi\\)-angles (left) and \\(\\psi\\)-angles (right). The smooth red density curve shown in Figure 1.3 can be thought of as a smooth version of a histogram. It is surprisingly difficult to find automatic smoothing procedures that perform uniformly well – it is even quite difficult to automatically select the number and positions of the breaks used for histograms. This is one of the important points that is taken up in this book: how to implement good default choices of various tuning parameters that are required by any smoothing procedure. 1.1.2 Using ggplot2 It is also possible to use ggplot2 to achieve similar results. library(ggplot2) ggplot(phipsi, aes(x = phi)) + geom_histogram(aes(y = ..density..), bins = 13) + geom_density(col = &quot;red&quot;, size = 1) + geom_rug() ggplot(phipsi, aes(x = psi)) + geom_histogram(aes(y = ..density..), bins = 13) + geom_density(col = &quot;red&quot;, size = 1) + geom_rug() Figure 1.4: Histograms and density estimates of \\(\\phi\\)-angles (left) and \\(\\psi\\)-angles (right) made with ggplot2. Histograms produced by ggplot2 have a non adaptive default number of bins equal to 30 (number of breaks equal to 31), which is different from hist that uses Sturges’ formula \\[\\text{number of breaks} = \\lceil \\log_2(n) + 1 \\rceil\\] with \\(n\\) the number of samples in the data set. In addition, this number is further modified by the function pretty that generates “nice” breaks, which results in 14 breaks for the angle data. For easier comparison, the number of bins used by geom_histogram above is set to 13, though it should be noticed that the breaks are not chosen in exactly the same way by geom_histogram and hist. Automatic and data adaptive bin selection is difficult, and geom_histogram implements a simple and fixed, but likely suboptimal, default while notifying the user that this default choice can be improved by setting binwidth. For the density, geom_density actually relies on the density function and its default choices of how and how much to smooth. Thus the figure may have a slightly different appearance, but the estimated density obtained by geom_density is identical to the one obtained by density. 1.1.3 Changing the defaults Note that the range of the angle data is known to be \\((-\\pi, \\pi]\\), which neither the histogram nor the density smoother take advantage of. The pretty function, for instance, chooses breaks in \\(-3\\) and \\(3\\), which results in the two extreme bars in the histogram to be misleading. Note also that for the \\(\\psi\\)-angle it appears that the defaults result in oversmoothing of the density estimate. That is, the density is more smoothed out than the data (and the histogram) appears to support. To obtain different – and perhaps better – results, we can try to change some of the defaults of the histogram and density functions. The two most important defaults to consider are the bandwidth and the kernel. Postponing the mathematics to Chapter 2, the kernel controls how neighboring data points are weighted relatively to each other, and the bandwidth controls the size of neighborhoods. A bandwidth can be specified manually as a specific numerical value, but for a fully automatic procedure, it is selected by a bandwidth selection algorithm. The density default is a rather simplistic algorithm known as Silverman’s rule-of-thumb. hist(phipsi$psi, breaks = seq(-pi, pi, length.out = 15), prob = TRUE) rug(phipsi$psi) density(phipsi$psi, adjust = 1, cut = 0) %&gt;% lines(col = &quot;red&quot;, lwd = 2) density(phipsi$psi, adjust = 0.5, cut = 0) %&gt;% lines(col = &quot;blue&quot;, lwd = 2) density(phipsi$psi, adjust = 2, cut = 0) %&gt;% lines(col = &quot;purple&quot;, lwd = 2) hist(phipsi$psi, breaks = seq(-pi, pi, length.out = 15), prob = TRUE) rug(phipsi$psi) density(phipsi$psi, bw = &quot;SJ&quot;, cut = 0) %&gt;% ## Default kernel is &quot;gaussian&quot; lines(col = &quot;red&quot;, lwd = 2) density(phipsi$psi, kernel = &quot;epanechnikov&quot;, bw = &quot;SJ&quot;, cut = 0) %&gt;% lines(col = &quot;blue&quot;, lwd = 2) density(phipsi$psi, kernel = &quot;rectangular&quot;, bw = &quot;SJ&quot;, cut = 0) %&gt;% lines(col = &quot;purple&quot;, lwd = 2) Figure 1.5: Histograms and various density estimates for the \\(\\psi\\)-angles. The colors indicate different choices of bandwidth adjustments using the otherwise default bandwidth selection (left) and different choices of kernels using Sheather-Jones bandwidth selection (right). Figure 1.5 shows examples of several different density estimates that can be obtained by changing the defaults of density. The breaks for the histogram have also been chosen manually to make sure that they match the range of the data. Note, in particular, that Sheather-Jones bandwidth selection appears to work better than the default for this example. This is generally the case for multimodal distributions, where the default tends to oversmooth. Note also that the choice of bandwidth is far more consequential than the choice of kernel, the latter mostly affecting how wiggly the density estimate is locally. It should be noted that defaults arise as a combination of historically sensible choices and backward compatibility. Thought should go into choosing a good, robust default, but once a default is chosen, it should not be changed haphazardly, as this might break existing code. That is why not all defaults used in R are by today’s standards the best known choices. You see this argument made in the documentation of density regarding the default for bandwidth selection, where Sheather-Jones is suggested as a better default than the current, but for compatibility reasons Silverman’s rule-of-thumb is the default and is likely to remain being so. 1.1.4 Large scale smoothing With small data sets of less than 10,000 data points, say, univariate smooth density estimation requires a very modest amount of computation. That is true even with rather naive implementations of the standard methods. The R function density is implemented using a number of computational tricks like binning and the fast Fourier transform, and it can compute density estimates with a million data points (around 8 MB) within a fraction of a second. It is unclear if we ever need truly large scale univariate density estimation with terabytes of data points, say. If we have that amount of (heterogeneous) data it is likely that we are better off breaking the data down into smaller and more homogeneous groups. That is, we should turn a big data computation into a large number of small data computations. That does not remove the computational challenge but it does diminish it somewhat e.g. by parallelization. Deng and Wickham did a review in 2011 on Density estimation in R, where they assessed the performance of a number of R packages including the density function. The KernSmooth package was singled out in terms of speed as well as accuracy for computing smooth density estimates with density performing quite well too. (Histograms are non-smooth density estimates and generally faster to compute). The assessment was based on using defaults for the different packages, which is meaningful in the sense of representing the performance that the occasional user will experience. It is, however, also an evaluation of the combination of default choices and the implementation, and as different packages rely on e.g. different bandwidth selection algorithms, this assessment is not the complete story. The bkde function from the KernSmooth package, as well as density, are solid choices, but the point is that performance assessment is a multifaceted problem. To be a little more specific about the computational complexity of density estimators, suppose that we have \\(n\\) data points and want to evaluate the density in \\(m\\) points. A naive implementation of kernel smoothing, Section 2.2, has \\(O(mn)\\) time complexity, while a naive implementation of the best bandwidth selection algorithms have \\(O(n^2)\\) time complexity. As a simple rule-of-thumb, anything beyond \\(O(n)\\) will not scale to very large data sets. A quadratic time complexity for bandwidth selection will, in particular, be a serious bottleneck. Kernel smoothing illustrates perfectly that a literal implementation of the mathematics behind a statistical method may not always be computationally viable. Even the \\(O(mn)\\) time complexity may be quite a bottleneck as it reflects \\(mn\\) kernel evaluations, each being potentially a computationally relatively expensive operation. The binning trick, with the number of bins set to \\(m\\), is a grouping of the data points into \\(m\\) sets of neighbor points (bins) with each bin representing the points in the bin via a single point and a weight. If \\(m \\ll n\\), this can reduce the time complexity substantially to \\(O(m^2) + O(n)\\). The fast Fourier transform may reduce the \\(O(m^2)\\) term even further to \\(O(m\\log(m))\\). Some approximations are involved, and it is of importance to carefully evaluate the tradeoff between time and memory complexity on one side and accuracy on the other side. Multivariate smoothing is a different story. While it is possible to generalize the basic ideas of univariate density esimation to arbitrary dimensions, the curse-of-dimensionality hits unconstrained smoothing hard – statistically as well as computationally. Multivariate smoothing is therefore still an active research area developing computationally tractable and novel ways of fitting smooth densities or conditional densities to multivariate or even high-dimensional data. A key technique is to make structural assumptions to alleviate the challenge of a large dimension, but there are many different assumptions possible, which makes the body of methods and theory richer and the practical choices much more difficult. "],
["monte-carlo-methods.html", "1.2 Monte Carlo Methods", " 1.2 Monte Carlo Methods 1.2.1 Univariate von Mises distributions The angles considered in Section 1.1 take values in the interval \\((-\\pi, \\pi]\\). The von Mises distribution on this interval is given by the density \\[f(x) = \\frac{1}{\\varphi(\\theta)} e^{\\theta_1 \\cos(x) + \\theta_2 \\sin(x)}\\] for \\(\\theta = (\\theta_1, \\theta_2)^T \\in \\mathbb{R}^2\\). A common alternative parametrization is obtained by introducing \\(\\kappa = \\|\\theta\\|_2 = \\sqrt{\\theta_1^2 + \\theta_2^2}\\), and (whenever \\(\\kappa \\neq 0\\)) \\(\\nu = \\theta / \\kappa = (\\cos(\\mu), \\sin(\\mu))^T\\) for \\(\\mu \\in (-\\pi, \\pi]\\). Using the \\((\\kappa, \\mu)\\)-parametrization the density becomes \\[f(x) = \\frac{1}{\\varphi(\\kappa \\nu)} e^{\\kappa \\cos(x - \\mu)}.\\] The former parametrization in terms of \\(\\theta\\) is, however, the canonical parametrization of the family of distributions as an exponential family, which is particularly useful for various likelihood estimation algorithms. The normalization constant \\[\\begin{align*} \\varphi(\\kappa \\nu) &amp; = \\int_{-\\pi}^\\pi e^{\\kappa \\cos(x - \\mu)}\\mathrm{d} x \\\\ &amp; = 2 \\pi \\int_{0}^{1} e^{\\kappa \\cos(\\pi x)}\\mathrm{d} x = 2 \\pi I_0(\\kappa) \\end{align*}\\] is given in terms of the modified Bessel function \\(I_0\\). We can easily compute and plot the density using R’s besselI implementation of the modified Bessel function. phi &lt;- function(k) 2 * pi * besselI(k, 0) curve(exp(cos(x)) / phi(1), -pi, pi, lwd = 2, ylab = &quot;density&quot;, ylim = c(0, 0.52)) curve(exp(2 * cos(x - 1)) / phi(2), -pi, pi, col = &quot;red&quot;, lwd = 2, add = TRUE) curve(exp(0.5 * cos(x + 1.5)) / phi(0.5), -pi, pi, col = &quot;blue&quot;, lwd = 2, add = TRUE) Figure 1.6: Density for the von Mises distribution with parameters \\(\\kappa = 1\\) and \\(\\nu = 0\\) (black), \\(\\kappa = 2\\) and \\(\\nu = 1\\) (red), and \\(\\kappa = 0.5\\) and \\(\\nu = - 1.5\\) (blue). It is not entirely obvious how we should go about simulating data points from the von Mises distribution. It will be demonstrated in Section 4.3 how to implement a rejection sampler, which is one useful algorithm for simulating samples from a distribution with a density. In this section we simply use the rmovMF function from the movMF package, which implements a few functions for working with (finite mixtures of) von Mises distributions, and even the general von Mises-Fisher distributions that are generalizations of the von Mises distribution to \\(p\\)-dimensional unit spheres. library(&quot;movMF&quot;) xy &lt;- rmovMF(500, 0.5 * c(cos(-1.5), sin(-1.5))) ## rmovMF represents samples as elements on the unit circle x &lt;- acos(xy[, 1]) * sign(xy[, 2]) hist(x, breaks = seq(-pi, pi, length.out = 15), prob = TRUE) rug(x) density(x, bw = &quot;SJ&quot;, cut = 0) %&gt;% lines(col = &quot;red&quot;, lwd = 2) curve(exp(0.5 * cos(x + 1.5)) / phi(0.5), -pi, pi, col = &quot;blue&quot;, lwd = 2, add = TRUE) Figure 1.7: Histogram of 500 simulated data points from a von Mises distribution with parameters \\(\\kappa = 0.5\\) and \\(\\nu = - 1.5\\). A smoothed density estimate (red) and the true density (blue) are added to the plot. 1.2.2 Mixtures of von Mises distributions The von Mises distributions are unimodal distributions on \\((-\\pi, \\pi]\\). Thus to find a good model of the bimodal angle data, say, we have do move beyond these distributions. A standard approach for constructing multimodal distributions is as mixtures of unimodal distributions. A mixture of two von Mises distributions can be constructed by flipping a (biased) coin to decide which of the two distributions to sample from. We will use the exponential family parametrization in the following. thetaA &lt;- c(3.5, -2) thetaB &lt;- c(-4.5, 4) alpha &lt;- 0.55 ## Probability of von Mises distribution A ## The sample function implements the &quot;coin flips&quot; u &lt;- sample(c(1, 0), 500, replace = TRUE, prob = c(alpha, 1 - alpha)) xy &lt;- rmovMF(500, thetaA) * u + rmovMF(500, thetaB) * (1 - u) x &lt;- acos(xy[, 1]) * sign(xy[, 2]) The rmovMF actually implements simulation from a mixture distribution directly, thus there is no need to construct the “coin flips” explicitly. theta &lt;- rbind(thetaA, thetaB) xy &lt;- rmovMF(length(x), theta, c(alpha, 1 - alpha)) x_alt &lt;- acos(xy[, 1]) * sign(xy[, 2]) To compare the simulated data with two mixture components to the model and a smoothed density, we implement an R function that computes the density for an angle argument using the function dmovMF that takes a unit circle argument. dvM &lt;- function(x, theta, alpha) { xx &lt;- cbind(cos(x), sin(x)) dmovMF(xx, theta, c(alpha, 1 - alpha)) / (2 * pi) } Note that dmovMF uses normalized spherical measure on the unit circle as reference measure, thus the need for the \\(2\\pi\\) division if we want the result to be comparable to histograms and density estimates that use Lebesgue measure on \\((-\\pi, \\pi]\\) as the reference measure. hist(x, breaks = seq(-pi, pi, length.out = 15), prob = TRUE, ylim = c(0, 0.5)) rug(x) density(x, bw = &quot;SJ&quot;, cut = 0) %&gt;% lines(col = &quot;red&quot;, lwd = 2) curve(dvM(x, theta, alpha), -pi, pi, col = &quot;blue&quot;, lwd = 2, add = TRUE) hist(x_alt, breaks = seq(-pi, pi, length.out = 15), prob = TRUE, ylim = c(0, 0.5)) rug(x_alt) density(x_alt, bw = &quot;SJ&quot;, cut = 0) %&gt;% lines(col = &quot;red&quot;, lwd = 2) curve(dvM(x, theta, alpha), -pi, pi, col = &quot;blue&quot;, lwd = 2, add = TRUE) Figure 1.8: Histograms of 500 simulated data points from a mixture of two von Mises distributions using either the explicit construction of the mixture (left) or the functionality in rmovMF to simulate mixtures directly (right). A smoothed density estimate (red) and the true density (blue) are added to the plot. Simulation of data from a distribution finds many applications. The technique is widely used whenever we want to investigate a statistical methodology in terms of its frequentistic performance under various data sampling models. As is discussed in the next section, simulation is a tool of fundamental importance for the practical application of Bayesian statistical methods. Another important application is as a tool for computing approximations of integrals. This is usually called Monte Carlo integration and is a form of numerical integration. Computing probabilities or distribution functions, say, are notable examples of integrals, and we consider here the computation of the probability of the interval \\((0, 1)\\) for the above mixture of two von Mises distributions. It is straightforward to compute this probability via Monte Carlo integration as a simple average. Note that we will use a large number of samples, 50,000 in this case, of simulated angles for this computation. Increasing the number even further will make the result more accurate. Chapter 5 deals with the assessment of the accuracy of Monte Carlo integrals, and how this random error can be estimated, bounded and minimized. xy &lt;- rmovMF(50000, theta, c(alpha, 1 - alpha)) x &lt;- acos(xy[, 1]) * sign(xy[, 2]) mean(x &gt; 0 &amp; x &lt; 1) ## Estimate of the probability of the interval (0, 1) ## [1] 0.08676 The probability above could, of course, be expressed using the distribution function of the mixture of von Mises distributions, which in turn can be computed in terms of integrals of von Mises densities. Specifically, the probability is \\[p = \\frac{\\alpha}{\\varphi(\\theta_A)} \\int_0^1 e^{\\theta_{A, 1} \\cos(x) + \\theta_{A, 2} \\sin(x)} \\mathrm{d} x + \\frac{1 - \\alpha}{\\varphi(\\theta_B)} \\int_0^1 e^{\\theta_{B, 1} \\cos(x) + \\theta_{B, 2} \\sin(x)} \\mathrm{d} x,\\] but these integrals do not have a simple analytic representation – just as the distribution function of the von Mises distribution doesn’t have a simple analytic expression. Thus the computation of the probability requires numerical computation of the integrals. The R function integrate can be used for numerical integration of univariate functions using standard numerical integration techniques. We can thus compute the probability by integrating the density of the mixture, as implemented above as the R function dvM. Note the arguments passed to integrate below. The first argument is the density function, then follows the lower and the upper limits of the integration, and then follows additional arguments to the density – in this case parameter values. integrate(dvM, 0, 1, theta = theta, alpha = alpha) ## 0.08635171 with absolute error &lt; 9.6e-16 The integrate function in R is an interface to a couple of classical QUADPACK Fortran routines for numerical integration via adaptive quadrature. Specifically, the computations rely on approximations of the form \\[\\int_a^b f(x) \\mathrm{d} x \\simeq \\sum_i w_i f(x_i)\\] for certain grid points \\(x_i\\) and weights \\(w_i\\), which are computed using Gauss-Kronrod quadrature. This method provides an estimate of the approximation error in addition to the numerical approximation of the integral itself. It is noteworthy that integrate as a function implemented in R takes another function, in this case the density dvM, as an argument. R is a functional programming language and functions are first-class citizens. This implies, for instance, that functions can be passed as arguments to other functions using a variable name – just like any other variable can be passed as an argument to a function. In the parlance of functional programming, integrate is a functional: a higher-order function that takes a function as argument and returns a numerical value. It is a main theme of this book how to make good use of functional (and object oriented) programming features in R to write clear, expressive and modular code without sacrificing computational efficiency. Returning to the specific problem of the computation of an integral, we may ask what the purpose of Monte Carlo integration is? Apparently we can just do numerical integration using e.g. integrate. There are at least two reasons why Monte Carlo integration is sometimes preferable. First, it is straightforward to implement and often works quite well for multivariate and even high-dimensional integrals, whereas grid-based numerical integration schemes scale poorly with the dimension. Second, it does not require that we have an analytic representation of the density. It is common in statistical applications that we are interested in the distribution of a statistic, which is a complicated transformation of data, and whose density is difficult or impossible to find analytically. Yet if we can just simulate data, we can simulate from the distribution of the statistic, and we can then use Monte Carlo integration to compute whatever probability or integral w.r.t. the distribution of the statistic that we are interested in. 1.2.3 Large scale simulation Relations to Complex sampling models. Ex Bayes, MCMC, Stan. "],
["optimization.html", "1.3 Optimization", " 1.3 Optimization Likelihood optimization for mixtures of the von Mises distribution. Direct implementation using optim and the density implementations. Alternatively using the EM-algorithm as implemented in movMF. 1.3.1 The EM-algorithm The movMF function implements the EM-algorithm for mixtures of von Mises distributions. Note again that the movMF package works with data as elements on the unit circle instead of as angles, whence the angle data must be transformed using cos and sin. psi_circle &lt;- cbind(cos(phipsi$psi), sin(phipsi$psi)) vM_fit &lt;- movMF(psi_circle, 2, control = list(verbose = TRUE, maxiter = 10, start = &quot;S&quot;)) ## Iteration: 0 *** L: 56.13 ## Iteration: 1 *** L: 119.228 ## Iteration: 2 *** L: 176.601 ## Iteration: 3 *** L: 192.397 ## Iteration: 4 *** L: 193.087 ## Iteration: 5 *** L: 193.223 ## Iteration: 6 *** L: 193.273 ## Iteration: 7 *** L: 193.291 ## Iteration: 8 *** L: 193.298 ## Iteration: 9 *** L: 193.3 We can compare the fitted model to the data using the density function as implemented above and the parameters estimated by the EM-algorithm. hist(phipsi$psi, breaks = seq(-pi, pi, length.out = 15), prob = TRUE) rug(phipsi$psi) density(phipsi$psi, bw = &quot;SJ&quot;, cut = 0) %&gt;% lines(col = &quot;red&quot;, lwd = 2) curve(dvM(x, vM_fit$theta, vM_fit$alpha[1]), -pi, pi, add = TRUE, col = &quot;blue&quot;, lwd = 2) 1.3.2 Large scale optimization Relations to complex optimization problems. Ex. deep learning, tensorflow. "],
["exercises.html", "1.4 Exercises", " 1.4 Exercises Histograms with non-equidistant breaks This exercise uses the data set infrared, which is also used in Problem 10.1 in the book Computational Statistics. Start by locating this data set from the book homepage and load it into R using the read.table function. As in Problem 10.1 you will throughout study the distribution of the logarithm of the variable called F12. The purpose of this exercise is two-fold. First, you will get familiar with the data and see how different choices of visualizations using histograms can affect your interpretation of the data. Second, you will learn more about how to write functions in R and gain a better understanding of how they work. Exercise 1.1 Plot a histogram of log(F12) using the default value of the argument breaks. Experiment with alternative values of breaks. Exercise 1.2 Write your own function, called myBreaks, which takes two arguments, x (a vector) and h (a positive integer). Let h have default value 5. The function should first sort x into increasing order and then return the vector that: starts with the smallest entry in x; contains every \\(h\\)th unique entry from the sorted x; ends with the largest entry in x. For example, if h = 2 and x = c(1, 3, 2, 5, 10, 11, 1, 1, 3) the function should return c(1, 3, 10, 11). To see this, first sort x, which gives the vector c(1, 1, 1, 2, 3, 3, 5, 10, 11), whose unique values are c(1, 2, 3, 5, 10, 11). Every second unique entry is c(1, 3, 10), and then the largest entry 11 is concatenated. Hint: The functions sort and unique can be useful. Use your function to construct breakpoints for the histogram for different values of h, and compare with the histograms obtained in Exercise ??. Exercise 1.3 If there are no ties in the data set, the function above will produce breakpoints with h observations in the interval between two consecutive breakpoints (except the last two perhaps). If there are ties, the function will by construction return unique breakpoints, but there may be more than h observations in some intervals. The intention is now to rewrite myBreaks so that if possible each interval contains h observations. Modify the myBreaks function with this intention and so that is has the following properties: All breakpoints must be unique. The range of the breakpoints must cover the range of x. For two subsequent breakpoints, \\(a\\) and \\(b\\), there must be at least h observations in the interval \\((a,b]\\), provided h &lt; length(x). (With the exception that for the first two breakpoints, the interval is \\([a,b]\\).) "],
["density.html", "Chapter 2 Density estimation", " Chapter 2 Density estimation Something about density estimation. "],
["unidens.html", "2.1 Univariate density estimation", " 2.1 Univariate density estimation Recall the data on \\(\\phi\\)- and \\(\\psi\\)-angles in polypeptide backbone structures, as considered in Section 1.1.1. Figure 2.1: Histograms equipped with a rug plot of the distribution of \\(\\phi\\)-angles (left) and \\(\\psi\\)-angles (right) of the peptide planes in the protein human protein 1HMP. We will in this section start the treatment of methods for smooth density estimation for univariate data such as data on either the \\(\\phi\\)- or the \\(\\psi\\)-angle. Multivariate methods for estimation of e.g. the bivariate joint density of the angles is postponed to Section 2.4. 2.1.1 Likelihood considerations Let \\(f_0\\) denote the unknown density we want to estimate. If we fit a parametrized statistical model \\((f_{\\theta})_{\\theta}\\) to data using the estimator \\(\\hat{\\theta}\\), then \\(f_{\\hat{\\theta}}\\) is an estimate of \\(f_0\\). The histogram is a nonparametric density estimator, \\(\\hat{f}\\), of \\(f_0\\). We are interested in nonparametric estimators because we want to compare data with the parametric estimate \\(f_{\\hat{\\theta}}\\) we don’t known a suitable parametric model visualization “With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.” — John von Neumann The Normal-inverse Gaussian distribution has four parameters, the generalised hyperbolic distribution is an extension with five, but Neumann was probably thinking more in terms of the spline based expansion in Section 2.1.4 with four or five suitable basis functions. For a parametric family we can use the MLE \\[\\hat{\\theta} = \\text{arg max}_{\\theta} \\sum_{i=1}^n \\log f_{\\theta}(x_i).\\] For nonparametric estimation we can still introduce the log-likelihood: \\[\\ell(f) = \\sum_{i=1}^n \\log f(x_i)\\] Let’s see what happens if \\[f(x) = f_h(x) = \\frac{1}{nh \\sqrt{2 \\pi}} \\sum_{j=1}^n e^{- \\frac{(x - x_j)^2}{2 h^2} }.\\] Figure 2.2: (ref:gausKern) Log-likelihood hseq &lt;- seq(1, 0.001, -0.001) ll &lt;- sapply(hseq, function(h) sum(log(ffun(phipsi$psi, h)))) Log-likelihood If \\(x_i \\neq x_j\\) when \\(i \\neq j\\) \\[\\begin{align*} \\ell(f_h) &amp; = \\sum_{i} \\log\\left(1 + \\sum_{j \\neq i} e^{-(x_i - x_j)^2 / (2 h^2)} \\right) - n \\log(nh\\sqrt{2 \\pi}) \\\\ &amp; \\sim - n \\log(nh\\sqrt{2 \\pi}) \\end{align*}\\] for \\(h \\to 0\\). Hence, \\(\\ell(f_h) \\to \\infty\\) for \\(h \\to 0\\) and there is no MLE in the set of distributions with densities. n &lt;- nrow(phipsi) asympll &lt;- - n * log(n * hseq * sqrt(2 * pi)) p1 &lt;- p1 + geom_line(aes(hseq, asympll)) p2 &lt;- p2 + geom_line(aes(hseq, asympll)) In the sense of weak convergence it actually holds that \\[f_h \\cdot m \\overset{\\mathrm{wk}}{\\longrightarrow} \\varepsilon_n = \\frac{1}{n} \\sum_{i=1}^n \\delta_{x_i}\\] for \\(h \\to 0\\). The empirical measure \\(\\varepsilon_n\\) can sensibly be regarded as the nonparametric MLE of the distribution. But the empirical measure does not have a density, 2.1.2 R digression Alternative, which is faster but more special purpose: diffsq &lt;- outer(phipsi$psi, phipsi$psi, function(x, y) (x - y)^2 / 2) n &lt;- nrow(phipsi) ll2 &lt;- sapply(hseq, function(h) sum(log(colSums(exp(-diffsq / h^2)))) - n * log(n * h * sqrt(2 * pi))) The second implementation reveals the \\(n^2\\)-complexity of the computations by the call to outer. 2.1.3 Method of sieves Nonparametric Maximum Likelihood Estimation by the Method of Sieves Penalized and constraint MLE. 2.1.4 Basis expansions "],
["kernel-density.html", "2.2 Kernel methods", " 2.2 Kernel methods In Section 2 we pursued the principled but also somewhat abstract approach to density estimation via maximum-likelihood estimation over a suitably constrained set of distributions. In this section we will pursue the more basic idea of smooth density estimation relying on the approximation \\[P(X \\in (x-h, x+h)) = \\int_{x-h}^{x+h} f_0(z) \\ dz \\simeq f_0(x) 2h,\\] which is valid for any continuous density \\(f_0\\). Inverting this approximation and using the law of large numbers, \\[\\begin{align*} f_0(x) &amp; \\simeq \\frac{1}{2h}P(X \\in (x-h, x+h)) \\\\ &amp; \\simeq \\frac{1}{2hn} \\sum_{i=1}^n 1_{(x-h, x+h)}(x_i) \\\\ &amp; = \\underbrace{\\frac{1}{2hn} \\sum_{i=1}^n 1_{(-h, h)}(x - x_i)}_{\\hat{f}_h(x)} \\end{align*}\\] for i.i.d. observations \\(x_1, \\ldots, x_n\\) having distribution \\(f_0 \\cdot m\\). The function \\(\\hat{f}_h\\) defined as above is an example of a kernel density estimator with a rectangular kernel. We immediately note that \\(h\\) has to be chosen appropriately. If \\(h\\) is large, \\(\\hat{f}_h\\) will be flat and close to a constant, and for \\(h \\to \\infty\\) the estimated distribution will eventually become uniform distributions over larger and larger intervals. If \\(h\\) is small, \\(\\hat{f}_h\\) will make large jumps close to the observations, and for \\(h \\to 0\\) the estimated distribution will tend to the empirical distribution with point masses in the observations just as in Section 2.1.1. What do we then mean by an “appropriate” choice of \\(h\\) above? Just as for the methods of sieves we must have some prior assumptions about what we expect \\(f_0\\) to look like. Typically, we expect \\(f_0\\) to have few oscillations and to be fairly smooth, and we want \\(\\hat{f}_h\\) to reflect that. A too large \\(h\\) will oversmooth the data relative to \\(f_0\\) by effectively ignoring the data, while a too small \\(h\\) will undersmooth the data relative to \\(f_0\\) by allowing individual data points to have large local effects that make the estimate wiggly. More formally, we can look at the mean and variance of \\(\\hat{f}_h\\). Letting \\(p(x, h) = P(X \\in (x-h, x+h))\\), it follows that \\(f_h(x) = E(\\hat{f}_h(x)) = p(x, h) / (2h)\\) while \\[\\begin{equation} V(\\hat{f}_h(x)) = \\frac{p(x, h) (1 - p(x, h))}{4h^2 n} \\simeq f_h(x) \\frac{1}{2hn}. \\tag{2.1} \\end{equation}\\] We see from these computations that for the \\(\\hat{f}_h(x)\\) to be approximately unbiased for any \\(x\\) we need \\(h\\) to be small – ideally letting \\(h \\to 0\\) since then \\(f_h(x) \\to f_0(x)\\). However, this will make the variance blow up, and to minimize variance we should instead choose \\(h\\) as large as possible. One way to define “appropriate” is then to strike a balance between the bias and the variance as a function of \\(h\\) so as to minimize its mean squared error. We will find the optimal tradeoff for the rectangular kernel in Section 2.3 on bandwidth selection. It’s not difficult, and you are encouraged to try finding it yourself at this point. In this section we will focus on computational aspects of kernel density estimation, but first we will generalize the estimator by allowing for other kernels. The estimate \\(\\hat{f}_h(x)\\) will be unbiased if \\(f_0\\) is constantly equal to \\(f_0(x)\\) in the entire interval \\((x-h, x+h)\\). This is atypical and can only happen for all \\(x\\) if \\(f_0\\) is constant. We expect the typical situation to be that \\(f_0\\) deviates the most from \\(f_0(x)\\) close to \\(x \\pm h\\), and that this causes a bias of \\(\\hat{f}_h(x).\\) Observations falling close to \\(x + h\\), say, should thus perhaps count less than observations falling close to \\(x\\)? The rectangular kernel makes a sharpe cut; either a data point is in or it is out. If we use a smooth weighting function instead of a sharp cut, we might be able to include more data points and lower the variance while keeping the bias small? This is precisely the idea of kernel estimators, defined generally as \\[\\hat{f}_h(x) = \\frac{1}{hn} \\sum_{i=1}^n K\\left(\\frac{x - x_i}{h}\\right)\\] for a kernel \\(K : \\mathbb{R} \\to \\mathbb{R}\\). The uniform or rectangular kernel is \\[K(x) = \\frac{1}{2} 1_{(-1,1)}(x).\\] The Gaussian kernel is \\[K(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}.\\] One direct benefit of considering other kernels than the rectangular is that \\(\\hat{f}_h\\) inherits all smoothness properties from \\(K\\). Whereas the rectangular kernel is not even continuous, the Gaussian kernel is \\(C^{\\infty}\\) and so is the resulting kernel density estimate. 2.2.1 Benchmarking "],
["bandwidth.html", "2.3 Bandwidth selection", " 2.3 Bandwidth selection 2.3.1 Revisiting the rectangular kernel We return to the rectangular kernel and compute the mean squared error. In the analysis it may be helpful to think about \\(n\\) large and \\(h\\) small. Indeed, we will eventually choose \\(h = h_n\\) as a function of \\(n\\) such that as \\(n \\to \\infty\\) we have \\(h_n \\to 0\\). We should also note the \\(f_h(x) = E (\\hat{f}_h(x))\\) is a density, thus \\(\\int f_h(x) \\mathrm{d}x = 1\\). We will assume that \\(f_0\\) is sufficiently differentiable and use a Taylor expansion of the distribution function \\(F_0\\) to get that \\[\\begin{align*} f_h(x) &amp; = \\frac{1}{2h}\\left(F_0(x + h) - F_0(x - h)\\right) \\\\ &amp; = \\frac{1}{2h}\\left(2h f_0(x) + \\frac{h^3}{3} f_0&#39;&#39;(x) + R_0(x,h) \\right) \\\\ &amp; = f_0(x) + \\frac{h^2}{6} f_0&#39;&#39;(x) + R_1(x,h) \\end{align*}\\] where \\(R_1(x, h) = o(h^2)\\). One should note how the quadratic terms in \\(h\\) in the Taylor expansion canceled. This gives the following formula for the bias of \\(\\hat{f}_h\\). \\[\\begin{align*} \\mathrm{bias}(\\hat{f}_h(x)) &amp; = (f_h(x) - f_0(x))^2 \\\\ &amp; = \\left(\\frac{h^2}{6} f_0&#39;&#39;(x) + R_1(x,h) \\right)^2 \\\\ &amp; = \\frac{h^4}{36} f_0&#39;&#39;(x)^2 + R_2(x,h) \\end{align*}\\] where \\(R(x,h) = o(h^4)\\). For the variance we see from (2.1) that \\[V(\\hat{f}_h(x)) = f_h(x)\\frac{1}{2hn} - f_h(x)^2 \\frac{1}{n}.\\] Integrating the sum of the bias and the variance over \\(x\\) gives the integrated mean squared error \\[\\begin{align*} \\mathrm{MISE}(h) &amp; = \\int \\mathrm{bias}(\\hat{f}_h(x)) + V(\\hat{f}_h(x)) \\mathrm{d}x \\\\ &amp; = \\frac{h^4}{36} \\|f_0&#39;&#39;\\|^2_2 + \\frac{1}{2hn} + \\int R(x,h) \\mathrm{d} x - \\frac{1}{n} \\int f_h(x)^2 \\mathrm{d} x. \\end{align*}\\] Since \\(f_h(x) \\leq 1\\), \\[\\int f_h(x)^2 \\mathrm{d} x \\leq \\int f_h(x) \\mathrm{d} x = 1,\\] and the last term is \\(o((nh)^{-1})\\). The second last term is \\(o(h^4)\\) if we can interchange the limit and intergration order. It is concievable that we can do so under suitable assumptions on \\(f_0\\), but we will not pursue those at this place. The two remaining and asymptotically dominating terms in the formula for MISE are \\[\\mathrm{AMISE}(h) = \\frac{h^4}{36} \\|f_0&#39;&#39;\\|^2_2 + \\frac{1}{2hn},\\] which is known as the asymptotic mean integrated squared error. Clearly, for this to be a useful formula, we must assume \\(\\|f_0&#39;&#39;\\|_2^2 &lt; \\infty\\). In this case the formula for AMISE can be used to find the asymptotic optimal tradeoff between (integrated) bias and variance. We find that \\[\\mathrm{AMISE}&#39;(h) = \\frac{h^3}{9} \\|f_0&#39;&#39;\\|^2_2 - \\frac{1}{2h^2n},\\] and solving for \\(\\mathrm{AMISE}&#39;(h) = 0\\) yields \\[h_n = \\left(\\frac{9}{2 \\|f_0&#39;&#39;\\|_2^2}\\right)^{1/5} n^{-1/5}.\\] We conclude that AMISE has a unique stationary point, and as it tends to \\(\\infty\\) for \\(h \\to 0\\) as well as for \\(h \\to \\infty\\), this stationary point is a unique global minimizer. We see how “wiggliness” of \\(f_0\\) enters into the formula for the optimal bandwidth \\(h_n\\) via \\(\\|f_0&#39;&#39;\\|_2\\). This norm of the second derivative is precisely a quantification of how much \\(f_0\\) oscillates. A large value, indicating a wiggly \\(f_0\\), will drive the optimal bandwidth down whereas a small value will drive the optimal bandwidth up. We should also observe that if we plug the optimal bandwidth into the formula for AMISE, we get \\[\\begin{align*} \\mathrm{AMISE}(h_n) &amp; = \\frac{h_n^4}{36} \\|f_0&#39;&#39;\\|^2_2 + \\frac{1}{2h_n n} \\\\ &amp; = C n^{-4/5}, \\end{align*}\\] which indicates that in terms of integrated mean squared error the rate at which we can nonparametrically estimate \\(f_0\\) is \\(n^{-4/5}\\). This should be contrasted to the standard parametric rate of \\(n^{-1}\\) for mean squared error. From a practical viewpoint there is one major problem with the optimal bandwidth \\(h_n\\); it depends via \\(\\|f_0&#39;&#39;\\|^2_2\\) upon the unknown \\(f_0\\) that we are trying to estimate. We therefore refer to \\(h_n\\) as an oracle bandwidth – it is the bandwidth that an oracle that knows \\(f_0\\) would tell us to use. In practice, we will have to come up with an estimate of \\(\\|f_0&#39;&#39;\\|^2_2\\) and plug that estimate into the formula for \\(h_n\\). We pursue a couple of different options for doing so for general kernel density estimators below together with methods that do not rely on the AMISE formula. 2.3.2 ISE, MISE and MSE for kernel estimators Quality of \\(\\hat{f}_h\\) can be quantified by the integrated squared error, \\[\\mathrm{ISE}(\\hat{f}_h) = \\int (\\hat{f}_h(x) - f_0(x))^2 \\ dx = \\|\\hat{f}_h - f_0\\|_2^2.\\] Quality of the estimation procedure producing \\(\\hat{f}_h\\) can be quantified by taking the mean ISE, \\[\\mathrm{MISE}(h) = E(\\mathrm{ISE}(\\hat{f}_h)),\\] where the expectation integral is over the data. \\[\\mathrm{MISE}(h) = \\int \\mathrm{MSE}_x(h) \\ dx\\] where \\(\\mathrm{MSE}_h(x) = \\mathrm{var}(\\hat{f}_h(x)) + (\\mathrm{bias}(\\hat{f}_h(x)))^2\\). If \\(K\\) integrates to 1 and is symmetric about 0 it holds that \\[\\mathrm{MISE}(h) = \\mathrm{AMISE}(h) + o((nh)^{-1} + h^4)\\] where the asymptotic mean integrated squared error is \\[\\mathrm{AMISE}(h) = \\frac{R(K)}{nh} + \\frac{h^4 \\sigma^4_K R(f&#39;&#39;)}{4}\\] with \\[R(g) = \\int g(t)^2 \\ dt = \\|g\\|_2^2 \\quad (\\mathrm{squared } \\ L_2\\mathrm{-norm})\\] and \\(\\sigma_K^2 = \\int t^2 K(t) \\ dt.\\) The derivation of AMISE is presented with a slightly more careful treatment of the error in the Taylor expansion than in the book. It is, however, not completely trivial to rigorously interchange the limit (h -&gt; 0) and the integrations. As a consequence of the AMISE formula it is possible to derive the asymptotically optimal choice of bandwidth. Among several methods discussed in the book (some based on cross validation and some on plug-in estimates using the formula for the optimal h), the Sheather-Jones method is recommended. This is not the default for the density() function in R. The default is a version of Silverman’s rule of thumb. "],
["multivariate-smoothing.html", "2.4 Multivariate methods", " 2.4 Multivariate methods library(MASS) ## kde2d library(KernSmooth) ## bkde2D We compute the density estimate in a grid of size 100 by 100 using a bandwidth of 2 and using the kde2d function that uses a bivariate normal kernel. denshat &lt;- kde2d(phipsi$phi, phipsi$psi, h = 2, n = 100) denshat &lt;- data.frame( cbind(denshat$x, rep(denshat$y, each = length(denshat$x)), as.vector(denshat$z)) ) colnames(denshat) &lt;- c(&quot;phi&quot;, &quot;psi&quot;, &quot;dens&quot;) ggplot(denshat, aes(phi, psi)) + geom_tile(aes(fill = dens), alpha = 0.5) + geom_contour(aes(z = sqrt(dens))) + geom_point(data = phipsi, aes(fill = NULL)) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;darkblue&quot;, trans = &quot;sqrt&quot;) We then recompute the density estimate in the same grid of size using the smaller bandwidth of 0.5. denshat &lt;- kde2d(phipsi$phi, phipsi$psi, h = 0.5, n = 100) To illustrate the use of bkde2D we load a larger data set of angles for 100 proteins. load(&quot;data/top100dih.RData&quot;) phipsi2 &lt;- na.omit(dataset) denshat &lt;- bkde2D(phipsi2[, c(&quot;phi&quot;, &quot;psi&quot;)], bandwidth = 0.3, gridsize = c(100, 100), range.x = list(c(-pi, pi), c(-pi, pi))) denshat &lt;- data.frame( cbind(denshat$x1, rep(denshat$x2, each = length(denshat$x1)), as.vector(denshat$fhat)) ) colnames(denshat) &lt;- c(&quot;phi&quot;, &quot;psi&quot;, &quot;dens&quot;) ggplot(denshat, aes(phi, psi)) + geom_tile(aes(fill = dens), alpha = 0.5) + geom_contour(aes(z = sqrt(dens)), bins = 20) + geom_point(data = phipsi2, aes(fill = NULL), alpha = 0.2) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;darkblue&quot;, trans = &quot;sqrt&quot;) "],
["bivariate.html", "Chapter 3 Bivariate smoothing", " Chapter 3 Bivariate smoothing The focus of this chapter is on estimating how one variable, \\(Y\\), is smoothly related to another, \\(X\\). Thus we are directly aiming for an estimate of the (aspects of) the conditional distribution of \\(Y\\) given \\(X\\). If both variables are real valued, we can get a pretty good idea of their relation by simply looking at a scatter plot, and what we are aiming for is also often referred to as scatter plot smoothing. One of the examples that will be used throughout is the monthly and yearly temperatures in Nuuk, Greenland, see Vinther et al. (2006). The updated data is available from the site SW Greenland temperature data. ggplot(Nuuk_year, aes(x = Year, y = Temperature)) + geom_point() + geom_smooth(se = FALSE) + geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 10), color = &quot;red&quot;, se = FALSE) + geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cr&quot;), color = &quot;purple&quot;, se = FALSE) ggplot(Nuuk, aes(x = Month, y = Temperature)) + geom_line(aes(group = Year), alpha = 0.3) + geom_point(alpha = 0.3) + geom_smooth(color = &quot;red&quot;, se = FALSE) Figure 3.1: Nuuk average yearly temperature in degrees Celsius (left) smoothed using loess (black), a degree 10 polynomial (red) and a smooth spline (purple). Nuuk annual temperature cycles (right) smoothed using a spline. References "],
["running-means.html", "3.1 Running means", " 3.1 Running means Abstract view on running means as linear smoother. Dense linear algebra implementation. 3.1.1 Dense linear algebra Many linear smoothers can be computed much faster than via a matrix multiplication of the smoother matrix and the observation vector, which has time complexity \\(O(n^2)\\). 3.1.2 Direct implementation runMean &lt;- function(y, k) { n &lt;- length(y) m &lt;- floor((k - 1) / 2) k &lt;- 2 * m + 1 y &lt;- y / k s &lt;- rep(NA, n) s[m + 1] &lt;- sum(y[1:k]) for(i in (m + 1):(n - m - 1)) s[i + 1] &lt;- s[i] - y[i - m] + y[i + 1 + m] s } The filter function does the same. y &lt;- rnorm(7) rbind(runMean(y, k = 3), c(stats::filter(y, rep(1/3, 3)))) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] NA -0.1303209 -0.8085272 -0.4411486 -0.9692448 -0.5891618 NA ## [2,] NA -0.1303209 -0.8085272 -0.4411486 -0.9692448 -0.5891618 NA rbind(runMean(y, k = 5), c(stats::filter(y, rep(1/5, 5)))) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] NA NA -0.5762484 -0.723236 -0.4635575 NA NA ## [2,] NA NA -0.5762484 -0.723236 -0.4635575 NA NA A benchmark comparison between runMean and filter gives the following table. expr mean median runMean(y[1:512], k = 11) 86.1 76.4 runMean(y[1:1024], k = 11) 173.2 158.2 runMean(y[1:2048], k = 11) 392.1 316.6 runMean(y[1:4196], k = 11) 680.3 629.7 stats::filter(y[1:512], rep(1/11, 11)) 74.2 61.9 stats::filter(y[1:1024], rep(1/11, 11)) 108.0 98.6 stats::filter(y[1:2048], rep(1/11, 11)) 155.1 118.8 stats::filter(y[1:4196], rep(1/11, 11)) 262.2 216.9 Thus filter is somewhat faster than runMean. "],
["kernel-methods.html", "3.2 Kernel methods", " 3.2 Kernel methods "],
["sparse-linear-algebra.html", "3.3 Sparse linear algebra", " 3.3 Sparse linear algebra library(Matrix) bandSparse(15, 15, seq(-2, 2)) ## 15 x 15 sparse Matrix of class &quot;ngCMatrix&quot; ## ## [1,] | | | . . . . . . . . . . . . ## [2,] | | | | . . . . . . . . . . . ## [3,] | | | | | . . . . . . . . . . ## [4,] . | | | | | . . . . . . . . . ## [5,] . . | | | | | . . . . . . . . ## [6,] . . . | | | | | . . . . . . . ## [7,] . . . . | | | | | . . . . . . ## [8,] . . . . . | | | | | . . . . . ## [9,] . . . . . . | | | | | . . . . ## [10,] . . . . . . . | | | | | . . . ## [11,] . . . . . . . . | | | | | . . ## [12,] . . . . . . . . . | | | | | . ## [13,] . . . . . . . . . . | | | | | ## [14,] . . . . . . . . . . . | | | | ## [15,] . . . . . . . . . . . . | | | Using matrix multiplication K &lt;- bandSparse(n, n, seq(-2, 2)) weights &lt;- c(1/3, 1/4, rep(1/5, n - 4), 1/4, 1/3) weights &lt;- c(NA, NA, rep(1/5, n - 4), NA, NA) p &lt;- ggplot(Nuuk_year, aes(Year, Temperature)) + geom_point() p + geom_line(aes(y = as.numeric(K %*% Nuuk_year$Temperature) * weights), color = &quot;red&quot;) If the smoother matrix is sparse, however, matrix multiplication can be much faster. We will present some benchmark comparisons below. First we compare the runtime for the matrix multiplication as.numeric(K %*% Nuuk_year$Temperature) * weights using a sparse matrix (as above) with the runtime using a dense matrix. The dense matrix is given as Kdense = as.matrix(K). These runtimes are compared to using filter. In all computations, \\(k = 5\\). The difference in slopes between dense and sparse matrix multiplication should be noted. This is the difference between \\(O(n^2)\\) and \\(O(n)\\) runtime. The runtime for the dense matrix multiplication will not change with \\(k\\). For the other two it will increase (linearly) with increasing \\(k\\). For smoothing only once with a given smoother matrix the time to construct the matrix should also be taken into account for fair comparison with filter. We do this next, where we also compare with runMean. The function bandSparse is not optimized for the specific running mean banded matrix, and I have written a faster C++ function for this job. Fast band pattern matrix: #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] List fastBand(int n, int k) { int N = (2 * k + 1) * (n - 2 * k) + 3 * k * k + k; int iter = 0; IntegerVector i(N), p(n + 1); for(int col = 0; col &lt; n; ++col) { p[col] = iter; for(int r = std::max(col - k, 0); r &lt; std::min(col + k + 1, n); ++r) { i[iter] = r; iter++; } } p[n] = N; return List::create(_[&quot;i&quot;] = i, _[&quot;p&quot;] = p); } And the R function bandSparseFast &lt;- function(n, k) { n &lt;- as.integer(n) k &lt;- as.integer(k) tmp &lt;- fastBand(n, k) new(&quot;ngCMatrix&quot;, i = tmp$i, p = tmp$p, Dim = c(n, n)) } Benchmark Benchmark, comments: The construction of the sparse matrix turns out to take up much more time than the matrix-vector multiplication. The runtime is still \\(O(n)\\), but the constant is of the order of a factor 16 larger than for filter. With the faster construction of the sparse matrix, the constant is reduced to being of the order 5 larger than for filter. For small \\(n\\) there is some overhead from the constructor of the sparse matrix object even for the faster algorithm. Take-home points for sparse matrices: If you implement an algorithm (like a smoother) using linear algebra (e.g. a matrix-vector product) then sparse matrix numerical methods can be useful compared to dense matrix numerical methods. The Matrix package for R implements sparse matrices. Always use methods for constructing the sparse matrix that avoid dense intermediates (if possible). Even sparse linear algebra cannot compete with optimized special purpose algorithms like filter or a C++ implementation of runMean. The filter function works for kernels (weights) with equidistant data. For non-equidistant data the sparse matrix approach could be a good solution if the kernel has local support. Comparing output qplot(1:n, as.numeric(K %*% Nuuk_year$Temperature) * weights - c(stats::filter(Nuuk_year$Temperature, rep(1/5, 5)))) + scale_y_continuous(&quot;Difference&quot;) Comparing output all(as.numeric(K %*% Nuuk_year$Temperature) * weights == c(stats::filter(Nuuk_year$Temperature, rep(1/5, 5)))) ## [1] FALSE all.equal(as.numeric(K %*% Nuuk_year$Temperature) * weights, c(stats::filter(Nuuk_year$Temperature, rep(1/5, 5)))) ## [1] TRUE identical(as.numeric(K %*% Nuuk_year$Temperature) * weights, c(stats::filter(Nuuk_year$Temperature, rep(1/5, 5)))) ## [1] FALSE "],
["orthogonal-basis-expansions.html", "3.4 Orthogonal basis expansions", " 3.4 Orthogonal basis expansions 3.4.1 Polynomial expansions Degree 19 polynomial The model matrix intercept &lt;- rep(1/sqrt(n), n) ## To make intercept column have norm one polylm &lt;- lm(Temperature ~ intercept + poly(Year, 19) - 1, data = Nuuk_year) X &lt;- model.matrix(polylm) X[1:5, 1:10] ## intercept poly(Year, 19)1 poly(Year, 19)2 poly(Year, 19)3 ## 1 0.08247861 -0.1418886 0.1807019 -0.2094896 ## 2 0.08247861 -0.1399449 0.1732758 -0.1922712 ## 3 0.08247861 -0.1380012 0.1659521 -0.1756467 ## 4 0.08247861 -0.1360576 0.1587309 -0.1596076 ## 5 0.08247861 -0.1341139 0.1516121 -0.1441457 ## poly(Year, 19)4 poly(Year, 19)5 poly(Year, 19)6 poly(Year, 19)7 ## 1 0.2311608 -0.24700836 0.25778095 -0.26401490 ## 2 0.1994949 -0.19625322 0.18362479 -0.16274891 ## 3 0.1697945 -0.15039857 0.11969706 -0.08033935 ## 4 0.1419959 -0.10917217 0.06514540 -0.01460375 ## 5 0.1160363 -0.07231034 0.01916214 0.03647245 ## poly(Year, 19)8 poly(Year, 19)9 ## 1 0.26616370 -0.26465396 ## 2 0.13490489 -0.10151111 ## 3 0.03532924 0.01212633 ## 4 -0.03740373 0.08588440 ## 5 -0.08762676 0.12807652 Figure 3.2: The model matrix columns as functions The model matrix is (almost) orthogonal image(Matrix(t(X) %*% X)) Estimation with orthogonal design: (t(X) %*% Nuuk_year$Temperature)[1:10, 1] ## intercept poly(Year, 19)1 poly(Year, 19)2 poly(Year, 19)3 ## -17.2469646 4.9002430 -1.7968913 0.8175400 ## poly(Year, 19)4 poly(Year, 19)5 poly(Year, 19)6 poly(Year, 19)7 ## 5.9668689 1.4265091 -1.9258864 -0.2523581 ## poly(Year, 19)8 poly(Year, 19)9 ## -2.1355117 -0.8046267 coef(polylm)[1:10] ## intercept poly(Year, 19)1 poly(Year, 19)2 poly(Year, 19)3 ## -17.2469646 4.9002430 -1.7968913 0.8175400 ## poly(Year, 19)4 poly(Year, 19)5 poly(Year, 19)6 poly(Year, 19)7 ## 5.9668689 1.4265091 -1.9258864 -0.2523581 ## poly(Year, 19)8 poly(Year, 19)9 ## -2.1355117 -0.8046267 With an orthogonal design matrix the normal equation reduces to the estimate \\[\\hat{\\beta} = X^T Y\\] since \\(X^T X = I\\). The predicted (or fitted) values are \\(X X^T Y\\) with smoother matrix \\(S = X X^T.\\) With homogeneous variance \\[\\hat{\\beta}_i \\overset{\\text{approx}}{\\sim} \\mathcal{N}(\\beta_i, \\sigma^2),\\] and for \\(\\beta_i = 0\\) we have \\(P(|\\hat{\\beta}_i| \\geq 1.96\\sigma) \\simeq 0.05.\\) Thresholding: 3.4.2 Fourier expansions Introducing \\[x_{k,m} = \\frac{1}{\\sqrt{n}} e^{2 \\pi i k m / n},\\] then \\[\\sum_{k=0}^{n-1} |x_{k,m}|^2 = 1\\] and for \\(m_1 \\neq m_2\\) \\[\\sum_{k=0}^{n-1} x_{k,m_1}\\overline{x_{k,m_2}} = 0\\] Thus \\(X = (x_{k,m})_{k,m}\\) is an \\(n \\times n\\) unitary matrix; \\[X^*X = I\\] where \\(X^*\\) is the conjugate transposed of \\(X\\). \\(\\hat{\\beta} = X^* Y\\) is the discrete Fourier transform of \\(Y\\). It is the basis coefficients in the orthonormal basis given by \\(X\\); \\[Y_k = \\frac{1}{\\sqrt{n}} \\sum_{m=0}^{n-1} \\hat{\\beta}_m e^{2 \\pi i k m / n}\\] or \\(Y = X \\hat{\\beta}.\\) X &lt;- outer(0:(n - 1), 0:(n - 1), function(k, m) exp(2 * pi * 1i * (k * m) / n) / sqrt(n)) image(Matrix(abs(Conj(t(X)) %*% X)), useAbs = FALSE) The matrix \\(X\\): Columns in the matrix \\(X\\): We can estimate by matrix multiplication betahat &lt;- Conj(t(X)) %*% Nuuk_year$Temperature # t(X) = X for Fourier bases betahat[c(1, 2:4, 73, n:(n - 2))] ## [1] -17.2469646+0.0000000i -2.4642887+2.3871189i 3.5481329+0.9099226i ## [4] 1.6721444+0.7413580i 0.0321232+0.7089991i -2.4642887-2.3871189i ## [7] 3.5481329-0.9099226i 1.6721444-0.7413580i For real \\(Y\\) it holds that \\(\\hat{\\beta}_0\\) is real, and the symmetry \\[\\hat{\\beta}_{n-m} = \\hat{\\beta}_m^*\\] holds for \\(m = 1, \\ldots, n - 1\\). (For \\(n\\) even, \\(\\hat{\\beta}_{n/2}\\) is real too). Modulus distribution: Note that for \\(m \\neq 0, n/2\\), \\(\\beta_m = 0\\) and \\(Y \\sim \\mathcal{N}(X\\beta, \\sigma^2 I_n)\\) then \\[(\\mathrm{Re}(\\hat{\\beta}_m), \\mathrm{Im}(\\hat{\\beta}_m))^T \\sim \\mathcal{N}\\left(0, \\frac{\\sigma^2}{2} I_2\\right),\\] – hence \\[|\\hat{\\beta}_m|^2 = \\mathrm{Re}(\\hat{\\beta}_m)^2 + \\mathrm{Im}(\\hat{\\beta}_m)^2 \\sim \\frac{\\sigma^2}{2} \\chi^2_2,\\] that is, \\(P(|\\hat{\\beta}_m| \\geq 1.73 \\sigma) = 0.05.\\) There is a clear case of multiple testing if we use this threshold at face value, and we would expect around \\(0.05 \\times n/2\\) false positive if there is no signal at all. Lowering the threshold using the Bonferroni correction yields a threshold of around \\(2.7 \\sigma\\) instead. Thresholding Fourier: The coefficients are not independent (remember the symmetry), and one can alternatively consider \\[\\hat{\\gamma}_m = \\sqrt{2} \\mathrm{Re}(\\hat{\\beta}_m) \\quad \\text{and} \\quad \\hat{\\gamma}_{n&#39; + m} = - \\sqrt{2} \\mathrm{Im}(\\hat{\\beta}_m)\\] for \\(1 \\leq m &lt; n / 2\\). Here \\(n&#39; = \\lfloor n / 2 \\rfloor\\). Here, \\(\\hat{\\gamma}_0 = \\hat{\\beta}_0\\), and \\(\\hat{\\gamma}_{n/2} = \\hat{\\beta}_{n/2}\\) for \\(n\\) even. These coefficients are the coefficients in a real cosine, \\(\\sqrt{2} \\cos(2\\pi k m / n)\\), and sine, \\(\\sqrt{2} \\sin(2\\pi k m / n)\\), basis expansion, and they are i.i.d. \\(\\mathcal{N}(0, \\sigma^2)\\) distributed. Thresholding Fourier: Figure 3.3: Fourier based smoother by thresholding (blue) and polynomial fit of degree 4 (red). What is the point using the discrete Fourier transform? The point is that the discrete Fourier transform can be computed via the fast Fourier transform (FFT), which has an \\(O(n\\log(n))\\) time complexity. The FFT works optimally for \\(n = 2^p\\). fft(Nuuk_year$Temperature)[1:4] / sqrt(n) ## [1] -17.246965+0.000000i -2.464289+2.387119i 3.548133+0.909923i ## [4] 1.672144+0.741358i betahat[1:4] ## [1] -17.246965+0.000000i -2.464289+2.387119i 3.548133+0.909923i ## [4] 1.672144+0.741358i 3.4.3 Wavelets "],
["splines.html", "3.5 Splines", " 3.5 Splines "],
["gaussian-processes.html", "3.6 Gaussian processes", " 3.6 Gaussian processes Suppose that \\(X = X_{1:n} \\sim \\mathcal{N}(\\xi_x, \\Sigma_{x})\\) with \\[\\mathrm{cov}(X_i, X_j) = K(t_i - t_j)\\] for a kernel function \\(K\\). With the observation equation \\(Y_i = X_i + \\delta_i\\) for \\(\\delta = \\delta_{1:n} \\sim \\mathcal{N}(0, \\Omega)\\) and \\(\\delta \\perp \\! \\! \\perp X\\) we get \\[(X, Y) \\sim \\mathcal{N}\\left(\\left(\\begin{array}{c} \\xi_x \\\\ \\xi_x \\end{array}\\right), \\left(\\begin{array}{cc} \\Sigma_x &amp; \\Sigma_x \\\\ \\Sigma_x &amp; \\Sigma_x + \\Omega \\end{array} \\right) \\right).\\] Hence \\[E(X \\mid Y) = \\xi_x + \\Sigma_x (\\Sigma_x + \\Omega)^{-1} (Y - \\xi_x).\\] Assuming that \\(\\xi_x = 0\\) the conditional expectation is a linear smoother with smoother matrix \\[S = \\Sigma_x (\\Sigma_x + \\Omega)^{-1}.\\] This is also true if \\(\\Sigma_x (\\Sigma_x + \\Omega)^{-1} \\xi_x = \\xi_x\\). If this identity holds approximately, we can argue that for computing \\(E(X \\mid Y)\\) we don’t need to know \\(\\xi_x\\). If the observation variance is \\(\\Omega = \\sigma^2 I\\) then the smoother matrix is \\[\\Sigma_x (\\Sigma_x + \\sigma^2 I)^{-1} = (I + \\sigma^2 \\Sigma_x^{-1})^{-1}.\\] "],
["the-kalman-filter.html", "3.7 The Kalman filter", " 3.7 The Kalman filter 3.7.1 AR(1)-example Suppose that \\(|\\alpha| &lt; 1\\), \\(X_1 = \\epsilon_1 / \\sqrt{1 - \\alpha^2}\\) and \\[X_i = \\alpha X_{i-1} + \\epsilon_i\\] for \\(i = 2, \\ldots, n\\) with \\(\\epsilon = \\epsilon_{1:n} \\sim \\mathcal{N}(0, I)\\). We have \\(\\mathrm{cov}(X_i, X_j) = \\alpha^{|i-j|} / (1 - \\alpha^2)\\), thus we can find \\(\\Sigma_x\\) and compute \\[E(X_n \\mid Y) = ((I + \\sigma^2 \\Sigma_x^{-1})^{-1} Y)_n\\] Figure 3.4: Gaussian smoother matrix with \\(\\alpha = 0.3, 0.9\\), \\(\\sigma^2 = 2, 20\\) Figure 3.5: Smoothers 3.7.2 The Kalman smoother From the identity \\(\\epsilon_i = X_i - \\alpha X_{i-1}\\) it follows that \\(\\epsilon = A X\\) where \\[A = \\left( \\begin{array}{cccccc} \\sqrt{1 - \\alpha^2} &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ -\\alpha &amp; 1 &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; -\\alpha &amp; 1 &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; -\\alpha &amp; 1 \\\\ \\end{array}\\right),\\] This gives \\(I = V(\\epsilon) = A \\Sigma_x A^T\\), hence \\[\\Sigma_x^{-1} = (A^{-1}(A^T)^{-1})^{-1} = A^T A.\\] We have shown that \\[\\Sigma_x^{-1} = \\left( \\begin{array}{cccccc} 1 &amp; -\\alpha &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ -\\alpha &amp; 1 + \\alpha^2 &amp; -\\alpha &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; -\\alpha &amp; 1 + \\alpha^2 &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 1 + \\alpha^2 &amp; -\\alpha \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; -\\alpha &amp; 1 \\\\ \\end{array}\\right).\\] Hence \\[I + \\sigma^2 \\Sigma_x^{-1} = \\left( \\begin{array}{cccccc} \\gamma_0 &amp; \\rho &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\rho &amp; \\gamma &amp; \\rho &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; \\rho &amp; \\gamma &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; \\gamma &amp; \\rho \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; \\rho &amp; \\gamma_0 \\\\ \\end{array}\\right)\\] with \\(\\gamma_0 = 1 + \\sigma^2\\), \\(\\gamma = 1 + \\sigma^2 (1 + \\alpha^2)\\) and \\(\\rho = -\\sigma^2 \\alpha\\) is a tridiagonal matrix. The equation \\[\\left( \\begin{array}{cccccc} \\gamma_0 &amp; \\rho &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\rho &amp; \\gamma &amp; \\rho &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; \\rho &amp; \\gamma &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; \\gamma &amp; \\rho \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; \\rho &amp; \\gamma_0 \\\\ \\end{array}\\right) \\left( \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\\right) = \\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n \\end{array}\\right)\\] can be solved by a forward and backward sweep. Forward sweep: Set \\(\\rho_1&#39; = \\rho / \\gamma_0\\) and \\(y_1&#39; = y_1 / \\gamma_0\\), then recursively \\[\\rho_i&#39; = \\frac{\\rho}{\\gamma - \\rho \\rho_{i-1}&#39;} \\quad \\text{and} \\quad y_i&#39; = \\frac{y_i - \\rho y_{i-1}&#39;}{\\gamma - \\rho \\rho_{i-1}&#39;}\\] for \\(i = 2, \\ldots, n-1\\) and finally \\[y_n&#39; = \\frac{y_n - \\rho y_{n-1}&#39;}{\\gamma_0 - \\rho \\rho_{n-1}&#39;}.\\] By the forward sweep the equation is transformed to \\[\\left( \\begin{array}{cccccc} 1 &amp; \\rho_1&#39; &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\rho_2&#39; &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 1 &amp; \\rho_{n-1}&#39; \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 &amp; 1 \\\\ \\end{array}\\right) \\left( \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\\right) = \\left(\\begin{array}{c} y_1&#39; \\\\ y_2&#39; \\\\ y_3&#39; \\\\ \\vdots \\\\ y_{n-1}&#39; \\\\ y_n&#39; \\end{array}\\right),\\] which is then solved by backsubstitution from below; \\(x_n = y_n&#39;\\) and \\[x_{i} = y_i&#39; - \\rho_{i}&#39; x_{i+1}, \\quad i = n-1, \\ldots, 1.\\] 3.7.3 Implementation #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] NumericVector KalmanSmooth(NumericVector y, double alpha, double sigmasq) { double tmp, gamma0 = 1 + sigmasq, rho = - sigmasq * alpha; double gamma = 1 + sigmasq * (1 + alpha * alpha); int n = y.size(); NumericVector x(n), rhop(n - 1); rhop[0] = rho / gamma0; x[0] = y[0] / gamma0; for(int i = 1; i &lt; n - 1; ++i) { /* Forward sweep */ tmp = (gamma - rho * rhop[i - 1]); rhop[i] = rho / tmp; x[i] = (y[i] - rho * x[i - 1]) / tmp; } x[n - 1] = (y[n - 1] - rho * x[n - 2]) / (gamma0 - rho * rhop[n - 2]); for(int i = n - 2; i &gt;= 0; --i) { /* Backsubstitution */ x[i] = x[i] - rhop[i] * x[i + 1]; } return x; } Result, \\(\\alpha = 0.95\\), \\(\\sigma^2 = 10\\) Comparing results Sigma &lt;- outer(1:n, 1:n, function(i, j) alpha^(abs(i - j))) / (1 - alpha^2) Smooth &lt;- Sigma %*% solve(Sigma + sigmasq * diag(n)) qplot(1:n, Smooth %*% Nuuk_year$Temperature - ySmooth) + ylab(&quot;Difference&quot;) Note that the forward sweep computes \\(x_n = E(X_n \\mid Y)\\), and from this, the backsubstitution solves the smoothing problem of computing \\(E(X \\mid Y)\\). The Gaussian process used here (the AR(1)-process) is not very smooth and nor is the smoothing of the data. This is related to the kernel function \\(K(s) = \\alpha^{|s|}\\) being non-differentiable in 0. Many smoothers are equivalent to a Gaussian process smoother with an appropriate choice of kernel. Not all have a simple inverse covariance matrix and a Kalman filter algorithm. 3.7.4 The Kalman filter #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] NumericVector KalmanFilt(NumericVector y, double alpha, double sigmasq) { double tmp, gamma0 = 1 + sigmasq, rho = - sigmasq * alpha, yp; double gamma = 1 + sigmasq * (1 + alpha * alpha); int n = y.size(); NumericVector x(n), rhop(n); rhop[0] = rho / gamma0; yp = y[0] / gamma0; x[0] = y[0] / (1 + sigmasq * (1 - alpha * alpha)); for(int i = 1; i &lt; n; ++i) { tmp = (gamma - rho * rhop[i - 1]); rhop[i] = rho / tmp; /* Note differences when compared to smoother */ x[i] = (y[i] - rho * yp) / (gamma0 - rho * rhop[i - 1]); yp = (y[i] - rho * yp) / tmp; } return x; } Result, \\(\\alpha = 0.95\\), \\(\\sigma^2 = 10\\) "],
["univariate-random-variables.html", "Chapter 4 Univariate random variables", " Chapter 4 Univariate random variables This chapter will deal with algorithms for simulating observations from a distribution on \\(\\mathbb{R}\\) or any subset thereof. There can be several purposes of doing so, for instance: We want to investigate properties of the distribution. We want to simulate independent realizations of univariate random variables to investigate the distribution of a transformation. We want to use Monte Carlo integration to compute numerically an integral (which could be a probability). In this chapter the focus is on the simulation of a single random variable or an i.i.d. sequence of random variables primarily via various transformations of pseudo random numbers. The pseudo random numbers themselves being approximate simulations of i.i.d. random variables uniformly distributed on \\((0, 1)\\). "],
["pseudo-random-numbers.html", "4.1 Pseudo random numbers", " 4.1 Pseudo random numbers Most sampling algorithms are based on algorithms for generating pseudo random uniformly distributed samples on \\((0, 1)\\). They arise from deterministic integer sequences initiated by a seed. In R you can get the same sequence by setting the seed. set.seed(27112015) oldseed &lt;- head(.Random.seed, 10) tmp &lt;- runif(1) tmp ## [1] 0.7793288 oldseed ## [1] 403 624 -1660633125 -1167670944 1031453153 ## [6] 815285806 -340883241 546144044 -1074785731 -160910374 head(.Random.seed, 10) ## [1] 403 1 -696993996 -1035426662 -378189083 ## [6] -745352065 -1401016826 -323987056 983360563 297855953 c(tmp, runif(1)) ## [1] 0.7793288 0.5613179 set.seed(27112015) head(.Random.seed, 10) ## [1] 403 624 -1660633125 -1167670944 1031453153 ## [6] 815285806 -340883241 546144044 -1074785731 -160910374 oldseed ## Same as current .Random.seed ## [1] 403 624 -1660633125 -1167670944 1031453153 ## [6] 815285806 -340883241 546144044 -1074785731 -160910374 runif(1) ## Same as tmp ## [1] 0.7793288 It is a research field to develop pseudo random number generators, see ?RNG in R for the algorithms available. The R default (v. 3.4.1) is the 32-bit Mersenne Twister, which generates integers in the range \\[\\{0, 1, \\ldots, 2^{32} -1\\}.\\] It has a long period and all combinations of consecutive integers up to dimension 623 occur equally often in a period. It has good statistical properties. Pseudo random numbers in \\((0, 1)\\) are returned by runif by division with \\(2^{32}\\) and a fix to prevent the algorithm from returning 0. "],
["transformation-techniques.html", "4.2 Transformation techniques", " 4.2 Transformation techniques If \\(T : \\mathcal{Z} \\to \\mathbb{R}\\) is a map and \\(Z \\in \\mathcal{Z}\\) is a random variable we can sample, then we can sample \\(X = T(Z).\\) Classical example: If \\(F^{\\leftarrow} : (0,1) \\mapsto \\mathbb{R}\\) is the (generalized) inverse of a distribution function and \\(U\\) is uniformly distributed on \\((0, 1)\\) then \\[F^{\\leftarrow}(U)\\] has distribution function \\(F\\). This is how R generates samples from \\(\\mathcal{N}(0,1)\\) using a technical approximation of \\(\\Phi^{-1}\\) based on rational functions. 4.2.1 Sampling from a \\(t\\)-distribution Let \\(Z = (Y, W) \\in \\mathbb{R} \\times (0, \\infty)\\) with \\(Z \\sim \\mathcal{N}(0, 1)\\) and \\(W \\sim \\chi^2_k\\) independent. Define \\(T : \\mathbb{R} \\times (0, \\infty) \\to \\mathbb{R}\\) by \\[T(z,w) = \\frac{z}{\\sqrt{w/k}},\\] then \\[X = T(Z, W) = \\frac{Z}{\\sqrt{W/k}} \\sim t_k.\\] This is how R simulates from a \\(t\\)-distribution with \\(W\\) generated from a gamma distribution with shape parameter \\(k / 2\\) and scale parameter \\(2\\). "],
["reject-samp.html", "4.3 Rejection sampling", " 4.3 Rejection sampling Let \\(Y_1, Y_2, \\ldots\\) be i.i.d. with density \\(g\\) on \\(\\mathbb{R}\\) and \\(U_1, U_2, \\ldots\\) be i.i.d. uniformly distributed on \\((0,1)\\) and independent of the \\(Y_i\\)s. Define \\[T(\\mathbf{Y}, \\mathbf{U}) = Y_{\\sigma}\\] with \\[\\sigma = \\inf\\{n \\geq 1 \\mid U_n \\leq \\alpha f(Y_n) / g(Y_n)\\},\\] for \\(\\alpha \\in (0, 1]\\) and \\(f\\) a density. Rejection sampling then consists of sampling independent pairs \\((Y_n, U_n)\\) as long as we reject the proposals \\(Y_n\\) sampled from \\(g\\), that is, as long as \\[U_n &gt; \\alpha f(Y_n) / g(Y_n).\\] The first time we accept a proposal is \\(\\sigma\\), and then we stop the sampling and return the proposal \\(Y_{\\sigma}\\). The result is, indeed, a sample from the distribution with density \\(f\\) as the following theorem states. Theorem 4.1 If \\(\\alpha f(y) \\leq g(y)\\) for all \\(y \\in \\mathbb{R}\\) then the distribution of \\(Y_{\\sigma}\\) has density \\(f\\). Proof. The formal proof decomposes the event \\((Y_{\\sigma} \\leq y)\\) according to the value of \\(\\sigma\\) to reduce the computation to the one on page 155, \\[\\begin{align} P(Y_{\\sigma} \\leq y) &amp; = \\sum_{n = 1}^{\\infty} P(Y_{n} \\leq y, \\ \\sigma = k) \\\\ &amp; = \\sum_{n = 1}^{\\infty} P(Y_{n} \\leq y, \\ U_n \\leq \\alpha f(Y_n) / g(Y_n)) P(\\sigma &gt; n - 1) \\\\ &amp; = \\ldots \\\\ &amp; = \\int_{-\\infty}^y f(z) \\mathrm{d} z. \\end{align}\\] Note that if \\(\\alpha f \\leq g\\) for densities \\(f\\) and \\(g\\), then \\[\\alpha = \\int \\alpha f(x) \\mathrm{d}x \\leq \\int g(x) \\mathrm{d}x = 1,\\] whence it follows automatically that \\(\\alpha \\leq 1\\) whenever \\(\\alpha f\\) is dominated by \\(g\\). The function \\(g/\\alpha\\) is called the envelope of \\(f\\). The tighter the envelope, the smaller is the probability of rejecting a sample from \\(g\\), and this is quantified explicitly by \\(\\alpha\\) as \\(1 - \\alpha\\) is the rejection probability. Thus \\(\\alpha\\) should preferably be as close to one as possible. If \\(f(y) = c q(y)\\) and \\(g(y) = d p(y)\\) for (unknown) normalizing constants \\(c, d &gt; 0\\) and \\(\\alpha&#39; q \\leq p\\) then \\[\\underbrace{\\left(\\frac{\\alpha&#39; d}{c}\\right)}_{= \\alpha} \\ f \\leq g.\\] The constant \\(\\alpha&#39;\\) may be larger than 1, but from the argument above we know that \\(\\alpha \\leq 1\\), and Theorem 4.1 gives that \\(Y_{\\sigma}\\) has distribution with density \\(f\\). It appears that we need to compute the normalizing constants to implement rejection sampling. However, observe that \\[u \\leq \\frac{\\alpha f(y)}{g(y)} \\Leftrightarrow u \\leq \\frac{\\alpha&#39; q(y)}{p(y)},\\] whence rejection sampling can actually be implemented with knowledge of the unnormalized densities and \\(\\alpha&#39;\\) only and without computing \\(c\\) or \\(d\\). This is one great advantages of rejection sampling, though we should note that when we don’t know the normalizing constants, \\(\\alpha&#39;\\) does not tell us anything about how tight the envelope is, and thus how small the rejection probability is. 4.3.1 Gamma distribution It may be possible to find a suitable envelope of the density for the gamma distribution on \\((0, \\infty)\\), but it turns out that there is a very efficient rejection sampler of a non-standard distribution that can be transformed it by a simple transformation. Let \\(t(y) = a(1 + by)^3\\) for \\(y \\in (-b^{-1}, \\infty)\\), then \\(t(Y) \\sim \\Gamma(r,1)\\) if \\(r \\geq 1\\) and \\(Y\\) has density \\[f(y) \\propto t(y)^{r-1}t&#39;(y) e^{-t(y)} = e^{(r-1)\\log t(y) + \\log t&#39;(y) - t(y)}.\\] [Proof: Use univariate density transformation theorem.] The density \\(f\\) will be the target density for a rejection sampler. With \\[f(y) \\propto e^{(r-1)\\log t(y) + \\log t&#39;(y) - t(y)},\\] \\(a = r - 1/3\\) and \\(b = 1/(3 \\sqrt{a})\\) \\[f(y) \\propto e^{a \\log t(y)/a - t(y) + a \\log a} \\propto \\underbrace{e^{a \\log t(y)/a - t(y) + a}}_{q(y)}.\\] An analysis of \\(w(y) := - y^2/2 - \\log q(y)\\) shows that it is convex on \\((-b^{-1}, \\infty)\\) and it attains its minimum in \\(0\\) with \\(w(0) = 0\\), whence \\[q(y) \\leq e^{-y^2/2}.\\] This gives us an envelope expressed in terms of unnormalized densities with \\(\\alpha&#39; = 1\\). The implementation of a rejection sampler based on this analysis is relatively straightforward. The rejection sampler will simulate from the distribution with density \\(f\\) by simulating from the Gaussian distribution (the envelope). For the rejection step we need to implement \\(q\\). Finally, we also need to implement \\(t\\) to transform the result from the rejection sampler to be gamma distributed. ## r &gt;= 1 tfun &lt;- function(y, a) { b &lt;- 1 / (3 * sqrt(a)) (y &gt; -1/b) * a * (1 + b * y)^3 ## 0 when y &lt;= -1/b } qfun &lt;- function(y, r) { a &lt;- r - 1/3 tval &lt;- tfun(y, a) exp(a * log(tval / a) - tval + a) } gammasim &lt;- function(n, r) { y &lt;- numeric(n) for(i in 1:n) { ratio &lt;- 0 u &lt;- 1 while(u &gt; ratio) { y0 &lt;- rnorm(1) ratio &lt;- qfun(y0, r) * exp(y0^2/2) u &lt;- runif(1) } y[i] &lt;- y0 } tfun(y, r - 1/3) } Even if the implementation is not optimized in any way, it can quickly simulate thousands of random variables. We test the implementation by simulating 10,000 values with parameters \\(r = 8\\) as well as \\(r = 1\\) and compare the resulting histograms to the respective theoretical densities. system.time(y &lt;- gammasim(10000, 8)) ## user system elapsed ## 0.057 0.003 0.060 system.time(y &lt;- gammasim(10000, 1)) ## user system elapsed ## 0.052 0.003 0.054 Though this is only a brief and informal test, it indicates that the implementation correctly simulates from the gamma distribution. Rejection sampling can be computationally expensive if many samples are rejected. A very tight envelope will lead to fewer rejections, while a loose envelope will lead to many rejections. We modify the code above to estimate the rejection probability and thus quantify how tight the envelope is. gammasim_trace &lt;- function(n, r) { count &lt;- 0 y &lt;- numeric(n) for(i in 1:n) { ratio &lt;- 0 u &lt;- 1 while(u &gt; ratio) { count &lt;- count + 1 y0 &lt;- rnorm(1) ratio &lt;- qfun(y0, r) * exp(y0^2/2) u &lt;- runif(1) } y[i] &lt;- y0 } cat((count - n) / count) ## Rejection frequency tfun(y, r - 1/3) } y &lt;- gammasim_trace(100000, 16) ## 0.001876472 y &lt;- gammasim_trace(100000, 8) ## 0.003438138 y &lt;- gammasim_trace(100000, 4) ## 0.007975874 y &lt;- gammasim_trace(100000, 1) ## 0.04891434 We observe that the rejection frequencies are small with \\(r = 1\\) being the worst case with around 5% rejections. For the other cases the rejection frequencies are all below 1%, thus rejection is rare. A visual comparison of \\(q\\) to the (unnormalized) Gaussian density also shows that the two (unnormalized) densities are very close except in the tails where there is very little probability mass. "],
["MCI.html", "Chapter 5 Monte Carlo integration", " Chapter 5 Monte Carlo integration The high-dimensional bogus argument. Relations to quasi-Monte Carlo methods A typical usage of sampling is Monte Carlo integration. With \\(X_1, \\ldots, X_n\\) i.i.d. with density \\(f\\) \\[\\hat{\\mu}_{\\textrm{MC}} := \\frac{1}{n} \\sum_{i=1}^n h(X_i) \\rightarrow \\mu := \\mathbb{E} h(X_1) = \\int h(x) f(x) \\ dx\\] for \\(n \\to \\infty\\) by the law of large numbers (LLN). The first question that we deal with below is the assessment of the average as an approximation of the integral. "],
["assessment.html", "5.1 Assessment", " 5.1 Assessment Something general about precision and uncertainty assessment for MC integration. 5.1.1 Using the central limit theorem The CLT gives that \\[\\hat{\\mu}_{\\textrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n h(X_i) \\overset{\\textrm{approx}} \\sim \\mathcal{N}(\\mu, \\sigma^2_{\\textrm{MC}} / n)\\] where \\[\\sigma^2_{\\textrm{MC}} = \\mathbb{V} h(X_1) = \\int (h(x) - \\mu)^2 f(x) \\ dx.\\] We can estimate \\(\\sigma^2_{\\textrm{MC}}\\) using the empirical variance \\[\\hat{\\sigma}^2_{\\textrm{MC}} = \\frac{1}{n - 1} \\sum_{i=1}^n (h(X_i) - \\hat{\\mu}_{\\textrm{MC}})^2,\\] then the variance of \\(\\hat{\\mu}_{\\textrm{MC}}\\) is estimated as \\(\\hat{\\sigma}^2_{\\textrm{MC}} / n\\) and a standard 95% confidence interval for \\(\\mu\\) is \\[\\hat{\\mu}_{\\textrm{MC}} \\pm 1.96 \\frac{\\hat{\\sigma}_{\\textrm{MC}}}{\\sqrt{n}}.\\] x &lt;- gammasim(1000, 8) nn &lt;- seq_along(x) muhat &lt;- cumsum(x) / nn; sigmahat &lt;- sd(x) qplot(nn, muhat) + geom_ribbon(ymin = muhat - 1.96 * sigmahat / sqrt(nn), ymax = muhat + 1.96 * sigmahat / sqrt(nn), fill = &quot;gray&quot;) + geom_line() + geom_point() Figure 5.1: Sample path with confidence band for Monte Carlo integration of the mean of a Gamma distributed random variable. 5.1.2 Concentration inequalities If \\(X\\) is a real valued random variable with finite second moment, \\(\\mu = E(X)\\) and \\(\\sigma^2 = V(X)\\), Chebychev’s inequality holds \\[P(|X - \\mu| &gt; \\varepsilon) \\leq \\frac {\\sigma^2}{\\varepsilon^2}\\] for all \\(\\varepsilon &gt; 0\\). This inequality implies, for instance, that for the simple Monte Carlo average we have the inequality \\[P(|\\hat{\\mu}_{\\textrm{MC}} - \\mu| &gt; \\varepsilon) \\leq \\frac{\\sigma^2_{\\textrm{MC}}}{n\\varepsilon^2}.\\] A common usage of this inequality is for the qualitative statement known as the law of large numbers: for any \\(\\varepsilon &gt; 0\\) \\[P(|\\hat{\\mu}_{\\textrm{MC}} - \\mu| &gt; \\varepsilon) \\rightarrow 0\\] for \\(n \\to \\infty\\). Or \\(\\hat{\\mu}_{\\textrm{MC}}\\) converges in probability towards \\(\\mu\\) as \\(n\\) tends to infinity. However, the inequality actually also provides a quantitative statement about how accurate \\(\\hat{\\mu}_{\\textrm{MC}}\\) is as an approximation of \\(\\mu\\). Chebyshev’s inequality is useful due to its minimal assumption of a finite second moment. However, it typically doesn’t give a very tight bound on the probability \\(P(|X - \\mu| &gt; \\varepsilon)\\). Much better inequalities can be obtained under stronger assumptions, in particular finite exponential moments. Assuming that the moment generating function of \\(X\\) is finite, \\(M(t) = E(e^{tX}) &lt; \\infty\\), for some suitable \\(t \\in \\mathbb{R}\\), it follows from Markov’s inequality that \\[P(X - \\mu &gt; \\varepsilon) = P(e^{tX} &gt; e^{t(\\varepsilon + \\mu)}) \\leq e^{-t(\\varepsilon + \\mu)}M(t),\\] which can provide a very tight upper bound by minimizing the bound over \\(t\\). This requires some knowledge of the moment generating function. We illustrate the usage of this inequality below by considering the Gamma-distribution where the moment generating function is well known. 5.1.3 Exponential tail bound for Gamma distributed variables If \\(X\\) follows a Gamma distribution with shape parameter \\(\\lambda &gt; 0\\) and \\(t &lt;1\\), then \\[M(t) = \\frac{1}{\\Gamma(\\lambda)} \\int_0^{\\infty} x^{\\lambda - 1} e^{-(1-t) x} \\, \\mathrm{d} x = \\frac{1}{(1-t)^{\\lambda}}.\\] Whence \\[P(X-\\lambda &gt; \\varepsilon) \\leq e^{-t(\\varepsilon + \\lambda)} \\frac{1}{(1-t)^{\\lambda}}.\\] Minimization over \\(t\\) of the right hand side gives the minimizer \\(t = \\varepsilon/(\\varepsilon + \\lambda)\\) and the upper bound \\[P(X-\\lambda &gt; \\varepsilon) \\leq e^{-\\varepsilon} \\left(\\frac{\\varepsilon + \\lambda}{\\lambda }\\right)^{\\lambda}.\\] Compare this to the bound \\[P(|X-\\lambda| &gt; \\varepsilon) \\leq \\frac{\\lambda}{\\varepsilon^2}\\] from Chebychev’s inequality. lambda &lt;- 10 curve(pgamma(x, lambda, lower.tail = FALSE), lambda, lambda + 30, ylab = &quot;probability&quot;, main = &quot;Gamma tail&quot;, ylim = c(0, 1)) curve(exp(lambda - x)*(x/lambda)^lambda, lambda, lambda + 30, add = TRUE, col = &quot;red&quot;) curve(lambda/(x-lambda)^2, lambda, lambda + 30, add = TRUE, col = &quot;blue&quot;) curve(pgamma(x, lambda, lower.tail = FALSE, log.p = TRUE), lambda, lambda + 30, ylab = &quot;log-probability&quot;, main = &quot;Logarithm of Gamma tail&quot;, ylim = c(-20, 0)) curve(lambda - x + lambda*log(x/lambda), lambda, lambda + 30, add = TRUE, col = &quot;red&quot;) curve(-2 * log(x-lambda) + log(lambda), lambda, lambda + 30, add = TRUE, col = &quot;blue&quot;) Figure 5.2: Actual tail probabilities (left) for the Gamma distribution, computed via the pgamma function, compared it to the tight bound (red) and the weaker bound from Chebychev’s inequality (blue). The differences in the tail are more clearly seen for the log-probabilities (right) "],
["importance-sampling.html", "5.2 Importance sampling", " 5.2 Importance sampling When we are only interested in Monte Carlo integration, we do not need to sample from the target distribution. Observe that \\[\\begin{align} \\mu = \\int h(x) f(x) \\ dx &amp; = \\int h(x) \\frac{f(x)}{g(x)} g(x) \\ dx \\\\ &amp; = \\int h(x) w^*(x) g(x) \\ dx \\end{align}\\] whenever \\(g\\) is a density fulfilling that \\[g(x) = 0 \\Rightarrow f(x) = 0.\\] With \\(X_1, \\ldots, X_n\\) i.i.d. with density \\(g\\) define the weights \\[w^*(X_i) = f(X_i) / g(X_i).\\] The importance sampling estimator is \\[\\hat{\\mu}_{\\textrm{IS}}^* := \\frac{1}{n} \\sum_{i=1}^n h(X_i)w^*(X_i).\\] It has mean \\(\\mu\\). Again by the LLN \\[\\hat{\\mu}_{\\textrm{IS}}^* \\rightarrow \\mathbb{E} h(X_1) w^*(X_1) = \\mu,\\] To assess the precision of the importance sampling estimate via the CLT we need the variance of the average as for plain Monte Carlo integration. By the CLT \\[\\hat{\\mu}_{\\textrm{IS}}^* \\overset{\\textrm{approx}} \\sim \\mathcal{N}(\\mu, \\sigma^{*2}_{\\textrm{IS}} / n)\\] where \\[\\sigma^{*2}_{\\textrm{IS}} = \\mathbb{V} h(X_1)w^*(X_1) = \\int (h(x) w^*(x) - \\mu)^2 g(x) \\ dx.\\] We may have \\(\\sigma^{*2}_{\\textrm{IS}} &gt; \\sigma^2_{\\textrm{MC}}\\) or \\(\\sigma^{*2}_{\\textrm{IS}} &lt; \\sigma^2_{\\textrm{MC}}\\) depending on \\(h\\) and \\(g\\). By choosing \\(g\\) cleverly so that \\(h(x) w^*(x)\\) becomes as constant as possible, importance sampling can reduce the variance considerably compared to plain MC. The importance sampling variance can be estimated just as the MC variance \\[\\hat{\\sigma}^{*2}_{\\textrm{IS}} = \\frac{1}{n - 1} \\sum_{i=1}^n (h(X_i)w^*(X_i) - \\hat{\\mu}_{\\textrm{IS}}^*)^2,\\] and a 95% standard confidence interval is computed as \\[\\hat{\\mu}^*_{\\textrm{IS}} \\pm 1.96 \\frac{\\hat{\\sigma}^*_{\\textrm{IS}}}{\\sqrt{n}}.\\] 5.2.1 Unknown normalization constants If \\(f = c^{-1} q\\) with \\(c\\) unknown then \\[c = \\int q(x) \\ dx = \\int \\frac{q(x)}{g(x)} g(x) \\ d x,\\] and \\[\\mu = \\frac{\\int h(x) w^*(x) g(x) \\ d x}{\\int w^*(x) g(x) \\ d x},\\] where \\(w^*(x) = q(x) / g(x).\\) An importance sampling estimate of \\(\\mu\\) is thus \\[\\hat{\\mu}_{\\textrm{IS}} = \\frac{\\sum_{i=1}^n h(X_i) w^*(X_i)}{\\sum_{i=1}^n w^*(X_i)} = \\sum_{i=1}^n h(X_i) w(X_i),\\] where \\(w^*(X_i) = q(X_i) / g(X_i)\\) and \\[w(X_i) = \\frac{w^*(X_i)}{\\sum_{i=1}^n w^*(X_i)}\\] are the standardized weights. This works irrespectively of the value of the normalizing constant \\(c\\). The variance of the IS estimator with standardized weights is a little more complicated, because the estimator is a ratio of random variables. From the multivariate CLT \\[\\frac{1}{n} \\sum_{i=1}^n \\left(\\begin{array}{c} h(X_i) w^*(X_i) \\\\ w^*(X_i) \\end{array}\\right) \\overset{\\textrm{approx}}{\\sim} \\mathcal{N}\\left( c \\left(\\begin{array}{c} \\mu \\\\ {1} \\end{array}\\right), \\frac{1}{n} \\left(\\begin{array}{cc} \\sigma^{*2}_{\\textrm{IS}} &amp; \\gamma \\\\ \\gamma &amp; \\sigma^2_{w^*} \\end{array} \\right)\\right),\\] where \\[\\begin{align} \\sigma^{*2}_{\\textrm{IS}} &amp; = \\mathbb{V} h(X_1)w^*(X_1) \\\\ \\gamma &amp; = \\mathrm{cov}(h(X_1)w^*(X_1), w^*(X_1)) \\\\ \\sigma_{w^*}^2 &amp; = \\mathbb{V} w^*(X_1). \\end{align}\\] We can then apply the \\(\\Delta\\)-method with \\(h(x, y) = x / y\\). Note that \\(Dh(x, y) = (1 / y, - x / y^2)\\), whence \\[Dh(c\\mu, c) \\left(\\begin{array}{cc} \\hat{\\sigma}^{*2}_{\\textrm{IS}} &amp; \\gamma \\\\ \\gamma &amp; \\sigma^2_{w^*} \\end{array} \\right) Dh(c\\mu, c)^T = c^{-2} (\\sigma^{*2}_{\\textrm{IS}} + \\mu^2 \\sigma_{w^*}^2 - 2 \\mu \\gamma).\\] By the \\(\\Delta\\)-method \\[\\hat{\\mu}_{\\textrm{IS}} \\overset{\\textrm{approx}}{\\sim} \\mathcal{N}(\\mu, c^{-2} (\\sigma^{*2}_{\\textrm{IS}} + \\mu^2 \\sigma_{w^*}^2 - 2 \\mu \\gamma) / n).\\] Note that for \\(c \\neq 1\\) it is necessary to estimate \\(c\\) as \\(\\hat{c} = \\frac{1}{n} \\sum_{i=1}^n w^*(X_i)\\) to compute an estimate of the variance. 5.2.2 Computing a high-dimensional integral To illustrate the usage and limitations of importance sampling, consider the following \\(p\\)-dimensional integral \\[\\mu := \\int e^{-\\frac{1}{2}\\left(x_1^2 + \\sum_{i=2}^p (x_i - \\alpha x_{i-1})^2\\right)} \\mathrm{d} x.\\] Now this integral is not even expressed as an expectation w.r.t. any distribution in the first place – it is an integral w.r.t. Lebesgue measure in \\(\\mathbb{R}^p\\). To use importance sampling it is therefore necessary to rewrite the integral as an expectation w.r.t. a probability distribution. There might be many ways to do this, and the following is just one. Rewrite the exponent as \\[||x||_2^2 + \\sum_{i = 2}^n \\alpha^2 x_{i-1}^2 - 2\\alpha x_i x_{i-1}\\] so that \\[\\begin{align*} \\mu &amp; = \\int e^{- \\frac{1}{2} \\sum_{i = 2}^n \\alpha^2 x_{i-1}^2 - 2\\alpha x_i x_{i-1}} e^{-\\frac{||x||_2^2}{2}} \\mathrm{d} x \\\\ &amp; = (2 \\pi)^{n/2} \\int e^{- \\frac{1}{2} \\sum_{i = 2}^n \\alpha^2 x_{i-1}^2 - 2\\alpha x_i x_{i-1}} f(x) \\mathrm{d} x \\end{align*}\\] where \\(f\\) is the density for the \\(\\mathcal{N}(0, I_p)\\) distribution. Thus if \\(X \\sim \\mathcal{N}(0, I_p)\\), \\[\\mu = (2 \\pi)^{n/2} E\\left( e^{- \\frac{1}{2} \\sum_{i = 2}^n \\alpha^2 X_{i-1}^2 - 2\\alpha X_i X_{i-1}} \\right)\\] The Monte Carlo integration below computes \\(\\mu\\) by generating \\(p\\)-dimensional random variables from the \\(\\mathcal{N}(0, I_p)\\). First, the function we want to integrate. h &lt;- function(x, alpha = 0.1){ p &lt;- length(x) tmp &lt;- alpha * x[1:(p - 1)] exp( - sum((tmp / 2 - x[2:p]) * tmp)) } Then we specify various parameters. B &lt;- 10000 ## The number of random variables to generate p &lt;- 100 ## The dimension of each random variable alpha &lt;- 0.2 evaluations &lt;- numeric(B) The actual computation is implemented as a simple loop. for(i in 1:B) { x &lt;- rnorm(p) evaluations[i] &lt;- h(x, alpha = alpha) } We can then plot the cumulative average and compare it to the actual value of the integral that we know is 1. plot(cumsum(evaluations) / 1:B, pch = 20) abline(h = 1, col = &quot;red&quot;) If we want to control the error with probability 0.95 we can use Chebychevs inequality and solve for \\(\\varepsilon\\) using the estimated variance. plot(cumsum(evaluations) / 1:B, pch = 20) abline(h = 1, col = &quot;red&quot;) me &lt;- cumsum(evaluations) / 1:B ve &lt;- var(evaluations) epsilon &lt;- sqrt(ve / ((1:B) * 0.05)) lines(1:B, me + epsilon) lines(1:B, me - epsilon) The confidence bands provided by the central limit theorem are typically much more accurate estimates of the actual uncertainty than the upper bounds provided by Chebychev’s inquality. plot(cumsum(evaluations) / 1:B, pch = 20) abline(h = 1, col = &quot;red&quot;) lines(1:B, me + 2*sqrt(ve/(1:B))) lines(1:B, me - 2*sqrt(ve/(1:B))) To see the limits of (naive) Monte Carlo integration try taking \\(\\alpha = 0.4\\). To be fair, it is the choice of standard multivariate normal distribution as the reference distribution for large \\(\\alpha\\) that is problematic rather than the method as such. "],
["network-failure.html", "5.3 Network failure", " 5.3 Network failure Consider the following network consisting of ten nodes and with some of the nodes connected. The network could be a computer network with ten computers. The different connections (edges) may “fail” independently with probability \\(p\\), and the question we will take an interest in is what is the probability that node 1 and node 10 become disconnected? The network of nodes can be represented as a graph adjacency matrix \\(A\\) such that \\(A_{ij} = 1\\) if and only if there is an edge between \\(i\\) and \\(j\\) (and \\(A_{ij} = 0\\) otherwise). A ## Graph adjacency matrix ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0 1 0 1 1 0 0 0 0 0 ## [2,] 1 0 1 0 0 1 0 0 0 0 ## [3,] 0 1 0 0 0 1 1 1 0 1 ## [4,] 1 0 0 0 1 0 0 1 0 0 ## [5,] 1 0 0 1 0 0 0 1 1 0 ## [6,] 0 1 1 0 0 0 1 0 0 1 ## [7,] 0 0 1 0 0 1 0 0 0 1 ## [8,] 0 0 1 1 1 0 0 0 1 0 ## [9,] 0 0 0 0 1 0 0 1 0 1 ## [10,] 0 0 1 0 0 1 1 0 1 0 To compute the probability that 1 and 10 become disconneted by Monte Carlo integration, we need to sample (sub)graphs by randomly removing some of the edges. This is implemented using the upper triangular part of the (symmetric) adjacency matrix. simNet &lt;- function(Aup, p) { ones &lt;- which(Aup == 1) Aup[ones] &lt;- sample(c(0, 1), length(ones), replace = TRUE, prob = c(p, 1 - p)) Aup + t(Aup) } It is fairly fast to sample even a large number of random graphs this way. Aup &lt;- A Aup[lower.tri(Aup)] &lt;- 0 system.time(replicate(1e5, {simNet(Aup, 0.5); NULL})) ## user system elapsed ## 1.972 0.063 2.038 The second function we need to implement checks network connectivity. It relies on the fact that there is a path from node 1 to node 10 consisting of \\(k\\) edges if and only if \\(A^k_{1,10} &gt; 0\\). We see directly that such a path needs to consist of at least \\(k = 3\\) edges. Also, we don’t need to check paths with more than \\(k = 9\\) edges as they will contain the same node multiple times and can thus be shortened. disconAB &lt;- function(A) { k &lt;- 3 Apow &lt;- A %*% A %*% A ## A%^%3 while(Apow[1, 10] == 0 &amp; k &lt; 9) { Apow &lt;- Apow %*% A k &lt;- k + 1 } Apow[1, 10] == 0 ## TRUE if A and B not connected } Estimating probability of nodes 1 and 10 being disconnected using Monte Carlo integration. seed &lt;- 27092016 set.seed(seed) n &lt;- 1e5 tmp &lt;- replicate(n, disconAB(simNet(Aup, 0.05))) muhat &lt;- mean(tmp) As this is a (random) approximation, we should report not only the Monte Carlo estimate but also the confidence interval. Since the estimate is an average of 0-1-variables, we can estimate the variance, \\(\\sigma^2\\), of the individual terms using that \\(\\sigma^2 = \\mu (1 - \\mu)\\). muhat + 1.96 * sqrt(muhat * (1 - muhat) / n) * c(-1, 0, 1) ## [1] 0.0002257328 0.0003400000 0.0004542672 Importance sampling. impw &lt;- function(Aup, A0, p, p0) { w &lt;- disconAB(A0) if (w) { b &lt;- 18 - sum(Aup[A0 == 1]) w &lt;- ((1 - p0) / (1 - p))^18 * (p0 * (1 - p) / (p * (1 - p0)))^b } as.numeric(w) } This uses formula (6.48) in the book for computing the importance weights. For the IS estimator they are multiplied by the indicator that A and B are disconnected. set.seed(seed) tmp &lt;- replicate(n, impw(Aup, simNet(Aup, 0.2), 0.2, 0.05)) muhatIS &lt;- mean(tmp) Confidence interval using empirical variance estimate \\(\\hat{\\sigma}^2\\). muhatIS + 1.96 * sd(tmp) / sqrt(n) * c(-1, 0, 1) ## [1] 0.0002619838 0.0002960900 0.0003301962 c(sd(tmp), sqrt(muhat * (1 - muhat))) ## The two standard deviations ## [1] 0.005502716 0.018435954 The ratio of variances is estimated as muhat * (1 - muhat) / var(tmp) ## [1] 11.22476 We need around 11 times more naive samples when compared to importance sampling to obtain the same precision. A benchmark will show that the extra computing time for importance sampling is small compared to the reduction of variance. It is worth the coding effort if used repeatedly, but not if it is a one-off computation. The graph is small enough for complete enumeration and thus the computation of an exact solution. There are \\(2^{18} = 262,144\\) different networks with any number of the edges failing, so complete enumeration is possible. To systematically walk through each possible combination of edges failing, we use the function intToBits that convert an integer to its binary representation for integers from 0 to 262,143. This is a quick and convenient way of representing all the different fail and non-fail combinations for the edges. ones &lt;- which(Aup == 1) p &lt;- 0.05 prob &lt;- numeric(2^18) for(i in 0:(2^18 - 1)) { on &lt;- as.numeric(intToBits(i)[1:18]) nr &lt;- sum(on) Atmp &lt;- Aup Atmp[ones] &lt;- on if(disconAB(Atmp + t(Atmp))) prob[i + 1] &lt;- p^(18 - nr) * (1 - p)^nr } The probability of nodes 1 and 10 disconnected can then be computed as follows. sum(prob) ## [1] 0.000288295 This number should be compared to the estimates computed above. For a more complete comparison, we have used importance sampling with edge fail probability ranging from 0.1 to 0.4, see Figure 5.3. The results show that a failure probability of 0.2 is close to optimal in terms of giving an importance sampling estimate with minimal variance. For smaller values, the event that 1 and 10 become disconnected is too rare, and for larger values the importance weights become too variable. A choice of 0.2 strikes a good balance. Figure 5.3: Confidence intervals for importance sampling estimates of network nodes 1 and 10 being disconnected under independent edge failures with probability 0.05. The red line is the true probability computed by complete enumeration. 5.3.1 Object oriented implementation network &lt;- function(A, p) { Aup &lt;- A Aup[lower.tri(Aup)] &lt;- 0 ones &lt;- which((Aup == 1)) structure(list(A = A, Aup = Aup, ones = ones, p = p), class = &quot;network&quot;) } myNet &lt;- network(A, p = 0.05) str(myNet) ## List of 4 ## $ A : num [1:10, 1:10] 0 1 0 1 1 0 0 0 0 0 ... ## $ Aup : num [1:10, 1:10] 0 0 0 0 0 0 0 0 0 0 ... ## $ ones: int [1:18] 11 22 31 41 44 52 53 63 66 73 ... ## $ p : num 0.05 ## - attr(*, &quot;class&quot;)= chr &quot;network&quot; Generic functions. sim &lt;- function(x, ...) UseMethod(&quot;sim&quot;) failure &lt;- function(x, ...) UseMethod(&quot;failure&quot;) The first method sim.network &lt;- function(x) { Aup &lt;- x$Aup Aup[x$ones] &lt;- sample(c(0, 1), length(x$ones), replace = TRUE, prob = c(x$p, 1 - x$p)) Aup + t(Aup) } and the second method failure.network &lt;- function(x, n, p = NULL) { if (is.null(p)) { ## Naive simulation tmp &lt;- replicate(n, disconAB(sim(x))) muhat &lt;- mean(tmp) se &lt;- sqrt(muhat * (1 - muhat) / n) } else { ## Importance sampling p0 &lt;- x$p x$p &lt;- p tmp &lt;- replicate(n, impw(x$Aup, sim(x), p, p0)) se &lt;- sd(tmp) / sqrt(n) muhat &lt;- mean(tmp) } value &lt;- muhat + 1.96 * se * c(-1, 0, 1) names(value) &lt;- c(&quot;low&quot;, &quot;estimate&quot;, &quot;high&quot;) value } Test set.seed(seed) ## Resetting seed failure(myNet, n) ## low estimate high ## 0.0002257328 0.0003400000 0.0004542672 set.seed(seed) ## Resetting seed failure(myNet, n, p = 0.2) ## low estimate high ## 0.0002619838 0.0002960900 0.0003301962 Check that the results are the same as on ??. Benchmarking system.time(replicate(1e5, {simNet(Aup, 0.5); NULL})) ## user system elapsed ## 1.961 0.064 2.030 system.time(replicate(1e5, {sim(myNet); NULL})) ## user system elapsed ## 2.181 0.048 2.233 One should expect a small computational overhead due to method dispatching, that is, the procedure that R uses to look up the appropriate sim method for an object of class network. This is done 100,000 times in the benchmark example above. In this case, that overhead is almost negligible. The generic print function already exists. We implement a method for class network. print.network &lt;- function(x) { cat(&quot;#vertices: &quot;, nrow(x$A), &quot;\\n&quot;) cat(&quot;#edges:&quot;, sum(x$Aup), &quot;\\n&quot;) cat(&quot;p = &quot;, x$p, &quot;\\n&quot;) } myNet ## Implicitly calls &#39;print&#39; ## #vertices: 10 ## #edges: 18 ## p = 0.05 Using igraph. library(igraph) net &lt;- graph_from_adjacency_matrix(A, mode = &quot;undirected&quot;) net ## IGRAPH afa7fc6 U--- 10 18 -- ## + edges from afa7fc6: ## [1] 1-- 2 1-- 4 1-- 5 2-- 3 2-- 6 3-- 6 3-- 7 3-- 8 3--10 4-- 5 4-- 8 ## [12] 5-- 8 5-- 9 6-- 7 6--10 7--10 8-- 9 9--10 The igraph package supports a vast number of graph computation, manipulation and visualization tools. Plotting an igraph. ## You can generate a layout ... net_layout &lt;- layout_(net, nicely()) ## ... or you can specify one yourself net_layout &lt;- matrix( c(-20, 1, -4, 3, -4, 1, -4, -1, -4, -3, 4, 3, 4, 1, 4, -1, 4, -3, 20, -1), ncol = 2, nrow = 10, byrow = TRUE) Plotting an igraph plot(net, layout = net_layout, asp = 0) Simulation method for igraph sim.igraph &lt;- function(x, p) { nr &lt;- ecount(x) deledges &lt;- sample(c(TRUE, FALSE), nr, replace = TRUE, prob = c(p, 1 - p)) delete_edges(x, which(deledges)) } A simulated graph plot(sim(net, 0.25), layout = net_layout, asp = 0) This is slower than using the matrix representation alone as in simNet. system.time(replicate(1e5, {sim(net, 0.5); NULL})) ## user system elapsed ## 3.945 1.382 5.334 One could also implement the function for testing if nodes 1 and 10 are disconnected using the shortest_paths function, but this is not faster than the simple matrix multiplications used in disconAB either, though it could be for larger graphs. "],
["design-of-experiments.html", "5.4 Design of experiments", " 5.4 Design of experiments "],
["multivariate-random-variables.html", "Chapter 6 Multivariate random variables ", " Chapter 6 Multivariate random variables "],
["sequential-simulation.html", "6.1 Sequential simulation", " 6.1 Sequential simulation 6.1.1 Sequential MC for the AR(1)-process Recall the update equation \\[X_{t} = \\alpha X_{t-1} + \\epsilon_t.\\] and the observation equation \\[Y_t = X_t + \\delta_t.\\] Simulating a sample path simAR &lt;- function(t, alpha) { x &lt;- numeric(t) e &lt;- rnorm(t, 0, 1) x[1] &lt;- e[1] / sqrt(1 - alpha^2) for(s in seq_len(t)[-1]) { x[s] &lt;- alpha * x[s - 1] + e[s] } x } Simulating a sample path We first generate a realization from the process with \\(\\alpha = 0.9\\) and observation variance \\(\\sigma^2 = 10\\). t &lt;- 100; alpha &lt;- 0.9; sigma &lt;- sqrt(10) set.seed(30) x &lt;- simAR(t, alpha) The simulated sample path Sample path with observations … and observations only Using the Kalman smoother We can use the implemented Kalman smoother and Kalman filter to compute the best prediction of the unobserved process. xSmooth &lt;- KalmanSmooth(y, alpha, sigma^2) xFilt &lt;- KalmanFilt(y, alpha, sigma^2) Using the Kalman smoother Using the Kalman filter only Filter, smoother and process "],
["gaussian-random-variables.html", "6.2 Gaussian random variables", " 6.2 Gaussian random variables "],
["four-examples.html", "Chapter 7 Four Examples ", " Chapter 7 Four Examples "],
["multinomial-models.html", "7.1 Multinomial models", " 7.1 Multinomial models Will be the main example used in all sections. A short general intro to multinomial models 7.1.1 Peppered Moths Three alleles, and six different genotypes (CC, CI, CT, II, IT and TT). Three different phenotypes (black, mottled, light colored). Allele frequencies: \\(p_C\\), \\(p_I\\), \\(p_T\\) with \\(p_C + p_I + p_T = 1.\\) The Peppered Moth is called ‘Birkemåler’ in Danish. There is a nice collection of these in different colors in the Zoological Museum. The alleles are ordered in terms of dominance as C &gt; I &gt; T. Moths with genotype including C are dark. Moths with genotype TT are light colored. Moths with genotypes II and IT are mottled. The peppered moth provided an early demonstration of evolution in the 19th centure England, where the light colored moth was outnumered by the dark colored variety. The dark color became advantageous due to the increased polution, where trees were darkened by soot. According to the Hardy-Weinberg principle, the genotype frequencies are \\[p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2, 2p_Ip_T, p_T^2.\\] The complete multinomial log-likelihood is \\[\\begin{align} &amp; 2n_{CC} \\log(p_C) + n_{CI} \\log (2 p_C p_I) + n_{CT} \\log(2 p_C p_I) \\\\ &amp; \\ \\ + 2 n_{II} \\log(p_I) + n_{IT} \\log(2p_I p_T) + 2 n_{TT} \\log(p_T), \\end{align}\\] We only observe only \\((n_C, n_T, n_I)\\), where \\[n = \\underbrace{n_{CC} + n_{CI} + n_{CT}}_{= n_C} + \\underbrace{n_{IT} + n_{II}}_{=n_I} + \\underbrace{n_{TT}}_{=n_T}.\\] For maksimum-likelihood estimation of the parameters in this model from the observation \\((n_C, n_T, n_I)\\) only, we need the marginal likelihood, that is, the likelihood in the marginal distribution of the observed variables. We will compute this in the section below by considering this particular problem as an example of a more general problem. 7.1.2 Multinomial cell collapsing The Peppered Moth example is an example of cell collapsing in a multinomial model. In general, let \\(A_1 \\cup \\ldots \\cup A_{K_0} = \\{1, \\ldots, K\\}\\) be a partition and let \\[M : \\mathbb{N}_0^K \\to \\mathbb{N}_0^{K_0}\\] be the map given by \\[M((n_1, \\ldots, n_K))_j = \\sum_{i \\in A_j} n_i.\\] If \\(Y \\sim \\textrm{Mult}(p, n)\\) with \\(p = (p_1, \\ldots, p_K)\\) then \\[X = M(Y) \\sim \\textrm{Mult}(M(p), n).\\] The conditional distribution of \\(Y_{A_j} = (Y_i)_{i \\in A_j}\\), conditionally on \\(X\\), can be found too. It will be used for the EM-algorithm the subsequent in Chapter ??. It is easy to see that \\[Y_{A_j} \\mid X = x \\sim \\textrm{Mult}\\left( \\frac{p_{A_j}}{M(p)_j}, x_j \\right).\\] The probability parameters are simply \\(p\\) restricted to \\(A_j\\) and renormalized to a probability vector. Observe that this gives \\[E (Y_k \\mid X = x) = \\frac{x_j p_k}{M(p)_j}\\] for \\(k \\in A_j\\). For the Peppered Moths, \\(K = 6\\) corresponding to the six genotypes, \\(K_0 = 3\\) and the partition corresponding to the phenotypes is \\[\\{1, 2, 3\\} \\cup \\{4, 5\\} \\cup \\{6\\} = \\{1, \\ldots, 6\\},\\] and \\[M(n_1, \\ldots, n_6) = (n_1 + n_2 + n_3, n_4 + n_5, n_6).\\] In terms of the \\((p_C, p_I)\\) parametrization, \\(p_T = 1 - p_C - p_I\\) and \\[p = (p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2, 2p_Ip_T, p_T^2).\\] Hence \\[M(p) = (p_C^2 + 2p_Cp_I + 2p_Cp_T, p_I^2 +2p_Ip_T, p_T^2).\\] The log-likelihood is \\[\\begin{align} \\ell(p_C, p_I) &amp; = n_C \\log(p_C^2 + 2p_Cp_I + 2p_Cp_T) \\\\ &amp; \\ \\ \\ + n_I \\log(p_I^2 +2p_Ip_T) + n_T \\log (p_T^2). \\end{align}\\] "],
["mixtures.html", "7.2 Mixtures", " 7.2 Mixtures 7.2.1 Gaussian mixtures "],
["mixed-effects-models.html", "7.3 Mixed effects models", " 7.3 Mixed effects models "],
["gaussian-state-space-models.html", "7.4 Gaussian state space models", " 7.4 Gaussian state space models "],
["numerical-optimization.html", "Chapter 8 Numerical optimization", " Chapter 8 Numerical optimization Iteration, fixed points, convergence criteria. Ref to Nonlinear Parameter Optimization Using R Tools. "],
["gradient-based-algorithms.html", "8.1 Gradient based algorithms", " 8.1 Gradient based algorithms 8.1.1 Gradient descent and conjugate gradient 8.1.2 Peppered Moths We can code a problem specific version of the negative log-likelihood and use optim to minimize it. ## par = c(pC, pI), pT = 1 - pC - pI ## x is the data vector loglik &lt;- function(par, x) { pT &lt;- 1 - par[1] - par[2] if (par[1] &gt; 1 || par[1] &lt; 0 || par[2] &gt; 1 || par[2] &lt; 0 || pT &lt; 0) return(Inf) PC &lt;- par[1]^2 + 2 * par[1] * par[2] + 2 * par[1] * pT PI &lt;- par[2]^2 + 2 * par[2] * pT PT &lt;- pT^2 - (x[1] * log(PC) + x[2] * log(PI) + x[3]* log(PT)) } optim(c(0.3, 0.3), loglik, x = c(85, 196, 341), method = &quot;CG&quot;) ## $par ## [1] 0.07084109 0.18873718 ## ## $value ## [1] 600.481 ## ## $counts ## function gradient ## 110 15 ## ## $convergence ## [1] 0 ## ## $message ## NULL The computations can beneficially be implemented in greater generality. The function M sums the cells that are collapsed, which has to be specified by the group argument. M &lt;- function(x, group) as.vector(tapply(x, group, sum)) The function prob maps the parameters to the multinomial probability vector. This function will have to be problem specific. prob &lt;- function(p) { p[3] &lt;- 1 - p[1] - p[2] c(p[1]^2, 2 * p[1] * p[2], 2* p[1] * p[3], p[2]^2, 2 * p[2] * p[3], p[3]^2) } loglik &lt;- function(par, x) { pT &lt;- 1 - par[1] - par[2] if (par[1] &gt; 1 || par[1] &lt; 0 || par[2] &gt; 1 || par[2] &lt; 0 || pT &lt; 0) return(Inf) - sum(x * log(M(prob(par), c(1, 1, 1, 2, 2, 3)))) } optim(c(0.3, 0.3), loglik, x = c(85, 196, 341), method = &quot;CG&quot;) ## $par ## [1] 0.07084109 0.18873718 ## ## $value ## [1] 600.481 ## ## $counts ## function gradient ## 107 15 ## ## $convergence ## [1] 0 ## ## $message ## NULL The Peppered Moth example is very simple. The marginal log-likelihood can easily be computed, and we used this problem to illustrate different ways of implementing a likelihood computation in R. One was very problem specific and one was more abstract and general. We demonstrated how to use standard optimization algorithms like conjugate gradient with or without implementing the gradient. If we don’t implement a gradient, a numerical gradient is used by optim This can very well result in a slower algorithm than if the gradient is implemented, but more seriously, in can result in convergence problems. This is because there is a subtle tradeoff between numerical accuracy and accuracy of the finite difference approximation. We did not experience this in the example above, but one way to remedy such problems is to set the parscale or fnscale entries in the control list argument to optim. Below we will illustrate how to use Newton-type methods for optimizing the marginal likelihood, and in the following chapter this same example is used to illustrate the EM-algorithm. It is important to understand that the EM-algorithm does not rely on computations of the marginal likelihood. In many real applications computation of the marginal likelihood is computationally challenging or even impossible, thus most standard optimization algorithms will not be directly applicable, the EM-algorithm will. "],
["newton-type-algorithms.html", "8.2 Newton-type algorithms", " 8.2 Newton-type algorithms "],
["EM.html", "Chapter 9 Expectation Maximization algorithms ", " Chapter 9 Expectation Maximization algorithms "],
["basic-properties.html", "9.1 Basic properties", " 9.1 Basic properties 9.1.1 Incomplete data likelihood Suppose that \\(Y\\) is a random variable and \\(X = M(Y)\\). Suppose that \\(Y\\) has density \\(f(\\cdot \\mid \\theta)\\) and that \\(X\\) has marginal density \\(g(x \\mid \\theta)\\). The marginal density is typically of the form \\[g(x \\mid \\theta) = \\int_{\\{y: M(y) = x\\}} f(y \\mid \\theta) \\ \\mu_x(\\mathrm{d} y)\\] for a suitable measure \\(\\mu_x\\) depending on \\(M\\) and \\(x\\) but not \\(\\theta\\). The general argument for the marginal density relies on the coarea formula. The log-likelihood for observing \\(X = x\\) is \\[\\ell(\\theta) = \\log g(x \\mid \\theta).\\] The marginal likelihood is often impossible to compute analytically and difficult and expensive to compute numerically. The complete log-likelihood, \\(\\log f(y \\mid \\theta)\\), is often easy to compute, but we don’t know \\(Y\\), only that \\(M(Y) = x\\). In some cases it is possible to compute \\[Q(\\theta \\mid \\theta&#39;) := E_{\\theta&#39;}(\\log f(Y \\mid \\theta) \\mid X = x),\\] which is the conditional expectation of the complete log-likelihood given the observed data and computed using the probability measure given by \\(\\theta&#39;\\). Thus for fixed \\(\\theta&#39;\\) this is a computable function of \\(\\theta\\) depending only on the observed data \\(x\\). One could get the following idea: with an initial guess of \\(\\theta&#39; = \\theta^{(0)}\\) compute iteratively \\[\\theta^{(t + 1)} = \\textrm{arg max} \\ Q(\\theta \\mid \\theta^{(t)})\\] for \\(t = 0, 1, 2, \\ldots\\). This idea is the EM-algorithm: E-step: Compute the conditional expectation \\(Q(\\theta \\mid \\theta^{(t)})\\). M-step: Maximize \\(\\theta \\mapsto Q(\\theta \\mid \\theta^{(t)})\\). It is a bit weird to present the algorithm as a two-step algorithm in its abstract formulation. Even though we can regard \\(Q(\\theta \\mid \\theta^{(t)})\\) as something we can compute abstractly for each \\(\\theta\\) for a given \\(\\theta^{(t)}\\), the maximization is in practice not really done using all these evaluations. It is computed either by an analytic formula involving \\(x\\) and \\(\\theta^{(t)}\\), or by a numerical algorithm that computes certain evaluations of \\(Q( \\cdot \\mid \\theta^{(t)})\\) and perhaps its gradient and Hessian. In computing these specific evaluations there is, of course, a need for the computation of conditional expectations, but we would compute these as they are needed and not upfront. However, in some of the most important applications of the EM-algorithm, particularly for exponential families covered in Section 9.2, it makes a lot of sense to regard the algorithm as a two-step algorithm. This is the case whenever \\(Q(\\theta \\mid \\theta^{(t)}) = q(\\theta, t(x, \\theta^{(t)}))\\) is given in terms of \\(\\theta\\) and a function \\(t(x, \\theta^{(t)})\\) of \\(x\\) and \\(\\theta^{(t)}\\) that doesn’t depend on \\(\\theta\\). Then the E-step becomes the computation of \\(t(x, \\theta^{(t)})\\), and in the M-step, \\(Q(\\cdot \\mid \\theta^{(t)})\\) is maximized by maximizing \\(q(\\cdot, t(x, \\theta^{(t)}))\\), and the maximum is a function of \\(t(x, \\theta^{(t)})\\). 9.1.2 The EM-algorithm is ascending We prove below that the algorithm is an ascent algorithm; it (weakly) increases the marginal likelihood in every step. THEOREM AND PROOF It holds in great generality that the conditional distribution of \\(Y\\) given \\(X = x\\) has density \\[h(y \\mid x, \\theta) = \\frac{f(y \\mid \\theta)}{g(x \\mid \\theta)}\\] w.r.t. a suitable measure \\(\\mu_x\\) that does not depend upon \\(\\theta\\). You can verify this for discrete distributions and when \\(Y = (Z, X)\\) with joint density w.r.t. a product measure \\(\\mu \\otimes \\nu\\) that does not depend upon \\(\\theta\\). In the latter case, \\(f(y \\mid \\theta) = f(z, x \\mid \\theta)\\) and \\[g(x \\mid \\theta) = \\int f(z, x \\mid \\theta) \\ \\mu(\\mathrm{d} z)\\] is the marginal density w.r.t. \\(\\nu\\). In general \\[\\log g(x \\mid \\theta) = \\log f(y \\mid \\theta) - \\log h(y \\mid x, \\theta),\\] and under some integrability conditions, this decomposition is used to show that the EM-algorithm increases the likelihood, \\(\\ell(\\theta)\\), in each iteration. 9.1.3 Multinomial cell collapsing The EM-algorithm can be implemented by two simple functions that compute the conditional expectations above (the E-step) and then maximization of the complete observation log-likelihood. EStep0 &lt;- function(p, x, group) { pp &lt;- prob(p) x[group] * pp / M(pp, group)[group] } The MLE of the complete log-likelihood is a linear estimator, as is the case in many examples with explicit MLEs. MStep0 &lt;- function(n, X) as.vector(X %*% n / (sum(n))) The EStep0 and MStep0 functions are abstract implementations. They require specification of the arguments group and X, respectively, to become concrete. 9.1.4 Peppered Moths E- and M-steps Concrete functions for the E- and M-steps are implemented for the particular example. EStep &lt;- function(p, x) EStep0(p, x, c(1, 1, 1, 2, 2, 3)) MStep &lt;- function(n) { X &lt;- matrix( c(2, 1, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0) / 2, 2, 6, byrow = TRUE) MStep0(n, X) } EM &lt;- function(par, x, tmax = 10) { for(i in 1:tmax) par &lt;- MStep(EStep(par, x)) par ## Remember to return the parameter estimate } phat &lt;- EM(c(0.3, 0.3), c(85, 196, 341)) phat ## [1] 0.07083691 0.18873652 9.1.5 Inside the EM Check what is going on in each step of the EM-algorithm. conv_diag &lt;- matrix(nrow = 10, ncol = 5) colnames(conv_diag) &lt;- c(&quot;t&quot;, &quot;pC&quot;, &quot;pI&quot;, &quot;R&quot;, &quot;loglik&quot;) par &lt;- c(0.3, 0.3) for(t in 1:10) { par0 &lt;- par par &lt;- EM(par0, c(85, 196, 341), tmax = 1) R &lt;- par - par0 R &lt;- sqrt(t(R) %*% R / (t(par0) %*% par0)) conv_diag[t, ] &lt;- c(t, par[1], par[2], R, loglik(par[-3], c(85, 196, 341))) } t pC pI R loglik 1 0.0803859 0.2246419 0.5472619 605.7930 2 0.0711893 0.1954696 0.1282007 600.6372 3 0.0708498 0.1899339 0.0266600 600.4859 4 0.0708374 0.1889476 0.0048661 600.4811 5 0.0708369 0.1887737 0.0008619 600.4810 6 0.0708369 0.1887430 0.0001518 600.4810 7 0.0708369 0.1887377 0.0000267 600.4810 8 0.0708369 0.1887367 0.0000047 600.4810 9 0.0708369 0.1887366 0.0000008 600.4810 10 0.0708369 0.1887365 0.0000001 600.4810 Note the log-axis. The EM-algorithm converges rapidly in this example. This is not always the case. If the log-likelihood is flat, the EM-algorithm can become quite slow. "],
["EM-exp.html", "9.2 Exponential families", " 9.2 Exponential families "],
["fisher-information.html", "9.3 Fisher information", " 9.3 Fisher information "],
["two-examples-revisited.html", "9.4 Two examples revisited", " 9.4 Two examples revisited 9.4.1 Gaussian mixtures 9.4.2 Gaussian state space "],
["stochastic-optimization.html", "Chapter 10 Stochastic Optimization ", " Chapter 10 Stochastic Optimization "],
["stochastic-gradient.html", "10.1 Stochastic gradient", " 10.1 Stochastic gradient "],
["stochastic-em.html", "10.2 Stochastic EM", " 10.2 Stochastic EM "],
["references.html", "References", " References "]
]

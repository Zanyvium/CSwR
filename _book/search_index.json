[
["index.html", "Computational Statistics with R Preface", " Computational Statistics with R Niels Richard Hansen 2018-10-29, Git version: 4d288ec Preface This is draft material for a book on computational statistics, which is being developed specifically for the master’s education in statistics at University of Copenhagen. A solid mathematical background is assumed throughout, while the computer science prerequisites are more modest. To be specific, the reader is expected to have a reasonable command of mathematical analysis, linear algebra and mathematical statistics, as exemplified by maximum likelihood estimation of multivariate parameters and asymptotic properties of such a multivariate estimator. The reader is expected to have an understanding of what an algorithm is, how numerical computations differ from symbolic computations, and be able to write small computer programs. The intention of the material is to serve as a pedagogical introduction to computational statistics. No claim is made that the material is comprehensive or even representative, nor does it purport computational statistics as a single coherent field with a unifying theoretical foundation. This introduction is driven by statistical examples with the unifying theme being an experimental approach to solving computational problems in statistics. Contemporary challenges in computational statistics revolve around large scale computations, either because the amount of data is massive or because we want to apply ever more complicated and sophisticated models and methods for the analysis and visualization of data. The examples treated are all of a rather modest complexity compared to these challenges. This is deliberate! A solid understanding of how to solve simpler problems is a prerequisite for solving complex problems. It is the hope that this material provides the reader with a foundation in computational statistics that will subseqently make him or her able to attack and solve novel problems in computational statistics. "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction Computational statistics is about turning theory and methods into algorithms and actual numerical computations with data. It is about solving real computational problems that arise when we visualize, analyze and model data. Computational statistics is not a single coherent topic but rather a large number of vaguely related computational techniques that we use in statistics. This short book is in no way attempting to be comprehensive. Instead, a few selected statistical topics are treated in some detail with the intention that good computational practice can be learned from these topics and transferred to other parts of statistics as needed. Though the topics are arguably fundamental, they reflect the knowledge and interests of the author, and different topics could clearly have been chosen. The demarcation line between statistical methodology and computational statistics is also blurred. Most methodology involves mathematical formulas and even algorithms for computing estimates and other statistics of interest from data, or for evaluating probabilities or integrals numerically or via simulations. The viewpoint taken in this book is that the transition from methodology to computational statistics happens when the methodology is to be implemented, that is, when formulas, algorithms and pseudo code are transformed into actual code and statistical software. It is during this transition that a number of practical challenges reveal themselves, such as actual run time and memory usage, the limitations of finite precision arithmetic, and the practical value of suboptimal or approximate but sufficiently accurate solutions. Statistical software development also requires some basic software engineering skills and knowledge of the most common programming paradigms. Implementing a single algorithm for a specific problem is one thing, but developing a piece of statistical software for others to use is something quite different. This book is not an introduction to statistical software development as such, but the process of developing good software plays a prominent role. Thus solutions are not presented as code that magically manifests itself, but code is developed and analyzed in cycles that resemble how real software development takes place. There is a notable practical and experimental component to software development. However important theoretical considerations are regarding correctness and complexity of algorithms, say, the actual code has to strike a balance between generality, readability, efficiency, accuracy, ease of usage and ease of development among other things. Finding a good balance requires that one is able to reason about benefits and deficiencies of different implementations. It is a major point of this book that such reasoning should rely on experiments and empirical facts and not speculations. R and RStudio is used throughout, and the reader is expected to have some basic knowledge of R programming. While RStudio is not a requirement for most of the book, it is a recommendable IDE (integrated development environment) for R, which offers a convenient framework for developing, benchmarking, profiling, testing, documenting and experimenting with statistical software. The excellent book Advanced R by Hadley Wickham is recommended as a companion book covering R programming in detail. In fact, direct references to that book are given whenever further explanations on e.g. functional programming or object oriented programming in R are required. This book is organized into three parts on smoothing, Monte Carlo methods and optimization. Each part is introduced in the following three sections to give the reader an overview of the topics covered, how they are related to each other and how they are related to some main trends and challenges in contemporary computational statistics. In this introduction, several R functions from various packages are used to illustrate how smoothing, simulation of random variables and optimization play roles in statistics. If you have no intention of moving beyond the role of a data analyst that rely on already implemented solutions, you can stop reading after the introduction. The remaining part of the book is about the development of such solutions at a deeper level and not how to use high-level interfaces to the plethora of already existing implementations. "],
["intro-smooth.html", "1.1 Smoothing", " 1.1 Smoothing Smoothing is a descriptive statistical tool for summarizing data, a practical visualization technique, as well as a nonparametric estimation methodology. The basic idea is that data is representative of an underlying distribution with some smoothness properties, and we would like to approximate or estimate this underlying distribution from data. There are two related but slightly different approaches. Either we attempt to estimate a smooth density of the observed variables, or we attempt to estimate a smooth conditional density of one variable given others. The latter can in principle be done by computing the conditional density from a smooth estimate of the joint distribution. Thus it appears that we really just need a way of computing smooth density estimates. In practice it may, however, be better to solve the conditional smoothing problem directly instead of solving a strictly more complicated problem. This is particularly so, if the conditioning variables are fixed e.g. by a design, or if our main interest is in the conditional mean or median, say, and not the entire conditional distribution. Conditional smoothing is dealt with in Chapter 3. In this introduction we focus on the univariate case, where there really only is one problem: smooth density estimation. Moreover, this is a very basic problem, and one viewpoint is that we simply need to “smooth out the jumps of the histogram”. Indeed, it does not need to be made more sophisticated than that! Humans are able to do this quite well using just a pen and a printed histogram, but it is a bit more complicated to automatize such a smoothing procedure. Moreover, an automatized procedure is likely to need calibration to yield a good tradeoff between smoothness and data fit. This is again something that humans can do quite well by eyeballing visualizations, but that approach does not scale, neither in terms of the number of density estimates we want to consider, nor in terms of going from univariate to multivariate densities. If we want to really discuss how a smoothing procedure works not just as a heuristic but also as an estimator of an underlying density, it is necessary to formalize how to quantify the performance of the procedure. This increases the level of mathematical sophistication, but it allows us to discuss optimality, and it lets us develop fully automatized procedures that do not rely on human calibration. While human inspection of visualizations is always a good idea, computational statistics is also about offloading humans from all computational tasks that can be automatized. This is true for smoothing as well, hence the need for automatic and robust smoothing procedures that produce well calibrated results with a minimum of human effort. 1.1.1 Angle distributions in proteins We will illustrate smoothing using a small data set on angles formed between two subsequent peptide planes in 3D protein structures. This data set is selected because the angle distributions are multimodal and slightly non-standard, and these properties are well suited for illustrating fundamental considerations regarding smooth density estimation in practice. Figure 1.1: The 3D structure of proteins is largely given by the \\(\\phi\\)- and \\(\\psi\\)-angles of the peptide planes. (By Dcrjsr, CC BY 3.0 via Wikimedia Commons.) A protein is a large molecule consisting of a backbone with carbon and nitrogen atoms arranged sequentially: A hydrogen atom binds to each nitrogen (N) and an oxygen atom binds to each carbon without the \\(\\alpha\\) subscript (C), see Figure 1.1, and such four atoms form together what is known as a peptide bond between two alpha-carbon atoms (C\\(_{\\alpha}\\)). Each C\\(_{\\alpha}\\) atom binds a hydrogen atom and an amino acid side chain. There are 20 naturally occurring amino acids in genetically encoded proteins, each having a three letter code (such as Gly for Glycine, Pro for Proline, etc.). The protein will typically form a complicated 3D structure determined by the amino acids, which in turn determine the \\(\\phi\\)- and the \\(\\psi\\)-angles between the peptide planes as shown on Figure 1.1. We will consider a small data set, phipsi, of experimentally determined angles from a single protein, the human protein 1HMP, which is composed of two chains (denoted A and B). Figure 1.2 shows the 3D structure of the protein. head(phipsi) ## chain AA pos phi psi ## 1 A Pro 5 -1.6218794 0.2258685 ## 2 A Gly 6 1.1483709 -2.8314426 ## 3 A Val 7 -1.4160220 2.1190570 ## 4 A Val 8 -1.4926720 2.3941331 ## 5 A Ile 9 -2.1814653 1.4877618 ## 6 A Ser 10 -0.7525375 2.5676186 Figure 1.2: The 3D structure of the atoms constituting the protein 1HMP. The colors indicate the two different chains. We can use base R functions such as hist and density to visualize the marginal distributions of the two angles. hist(phipsi$phi, prob = TRUE) rug(phipsi$phi) density(phipsi$phi) %&gt;% lines(col = &quot;red&quot;, lwd = 2) hist(phipsi$psi, prob = TRUE) rug(phipsi$psi) density(phipsi$psi) %&gt;% lines(col = &quot;red&quot;, lwd = 2) Figure 1.3: Histograms equipped with a rug plot and smoothed density estimate (red line) of the distribution of \\(\\phi\\)-angles (left) and \\(\\psi\\)-angles (right). The smooth red density curve shown in Figure 1.3 can be thought of as a smooth version of a histogram. It is surprisingly difficult to find automatic smoothing procedures that perform uniformly well – it is even quite difficult to automatically select the number and positions of the breaks used for histograms. This is one of the important points that is taken up in this book: how to implement good default choices of various tuning parameters that are required by any smoothing procedure. 1.1.2 Using ggplot2 It is also possible to use ggplot2 to achieve similar results. library(ggplot2) ggplot(phipsi, aes(x = phi)) + geom_histogram(aes(y = ..density..), bins = 13) + geom_density(col = &quot;red&quot;, size = 1) + geom_rug() ggplot(phipsi, aes(x = psi)) + geom_histogram(aes(y = ..density..), bins = 13) + geom_density(col = &quot;red&quot;, size = 1) + geom_rug() Figure 1.4: Histograms and density estimates of \\(\\phi\\)-angles (left) and \\(\\psi\\)-angles (right) made with ggplot2. Histograms produced by ggplot2 have a non adaptive default number of bins equal to 30 (number of breaks equal to 31), which is different from hist that uses Sturges’ formula \\[\\text{number of breaks} = \\lceil \\log_2(n) + 1 \\rceil\\] with \\(n\\) the number of samples in the data set. In addition, this number is further modified by the function pretty that generates “nice” breaks, which results in 14 breaks for the angle data. For easier comparison, the number of bins used by geom_histogram above is set to 13, though it should be noticed that the breaks are not chosen in exactly the same way by geom_histogram and hist. Automatic and data adaptive bin selection is difficult, and geom_histogram implements a simple and fixed, but likely suboptimal, default while notifying the user that this default choice can be improved by setting binwidth. For the density, geom_density actually relies on the density function and its default choices of how and how much to smooth. Thus the figure may have a slightly different appearance, but the estimated density obtained by geom_density is identical to the one obtained by density. 1.1.3 Changing the defaults Note that the range of the angle data is known to be \\((-\\pi, \\pi]\\), which neither the histogram nor the density smoother take advantage of. The pretty function, for instance, chooses breaks in \\(-3\\) and \\(3\\), which results in the two extreme bars in the histogram to be misleading. Note also that for the \\(\\psi\\)-angle it appears that the defaults result in oversmoothing of the density estimate. That is, the density is more smoothed out than the data (and the histogram) appears to support. To obtain different – and perhaps better – results, we can try to change some of the defaults of the histogram and density functions. The two most important defaults to consider are the bandwidth and the kernel. Postponing the mathematics to Chapter 2, the kernel controls how neighboring data points are weighted relatively to each other, and the bandwidth controls the size of neighborhoods. A bandwidth can be specified manually as a specific numerical value, but for a fully automatic procedure, it is selected by a bandwidth selection algorithm. The density default is a rather simplistic algorithm known as Silverman’s rule-of-thumb. hist(phipsi$psi, breaks = seq(-pi, pi, length.out = 15), prob = TRUE) rug(phipsi$psi) density(phipsi$psi, adjust = 1, cut = 0) %&gt;% lines(col = &quot;red&quot;, lwd = 2) density(phipsi$psi, adjust = 0.5, cut = 0) %&gt;% lines(col = &quot;blue&quot;, lwd = 2) density(phipsi$psi, adjust = 2, cut = 0) %&gt;% lines(col = &quot;purple&quot;, lwd = 2) hist(phipsi$psi, breaks = seq(-pi, pi, length.out = 15), prob = TRUE) rug(phipsi$psi) density(phipsi$psi, bw = &quot;SJ&quot;, cut = 0) %&gt;% ## Default kernel is &quot;gaussian&quot; lines(col = &quot;red&quot;, lwd = 2) density(phipsi$psi, kernel = &quot;epanechnikov&quot;, bw = &quot;SJ&quot;, cut = 0) %&gt;% lines(col = &quot;blue&quot;, lwd = 2) density(phipsi$psi, kernel = &quot;rectangular&quot;, bw = &quot;SJ&quot;, cut = 0) %&gt;% lines(col = &quot;purple&quot;, lwd = 2) Figure 1.5: Histograms and various density estimates for the \\(\\psi\\)-angles. The colors indicate different choices of bandwidth adjustments using the otherwise default bandwidth selection (left) and different choices of kernels using Sheather-Jones bandwidth selection (right). Figure 1.5 shows examples of several different density estimates that can be obtained by changing the defaults of density. The breaks for the histogram have also been chosen manually to make sure that they match the range of the data. Note, in particular, that Sheather-Jones bandwidth selection appears to work better than the default for this example. This is generally the case for multimodal distributions, where the default tends to oversmooth. Note also that the choice of bandwidth is far more consequential than the choice of kernel, the latter mostly affecting how wiggly the density estimate is locally. It should be noted that defaults arise as a combination of historically sensible choices and backward compatibility. Thought should go into choosing a good, robust default, but once a default is chosen, it should not be changed haphazardly, as this might break existing code. That is why not all defaults used in R are by today’s standards the best known choices. You see this argument made in the documentation of density regarding the default for bandwidth selection, where Sheather-Jones is suggested as a better default than the current, but for compatibility reasons Silverman’s rule-of-thumb is the default and is likely to remain being so. 1.1.4 Multivariate methods This section provides a single illustration of how to use the bivariate kernel smoother kde2d from the MASS package for bivariate density estimation of the \\((\\phi, \\psi)\\)-angle distribution. A scatter plot of \\(\\phi\\) and \\(\\psi\\) angles is known as a Ramachandran plot, and it provides a classical and important way of visualizing local structural constraints of proteins in structural biochemistry. The density estimate can be understood as an estimate of the distribution of \\((\\phi, \\psi)\\)-angles in naturally occuring proteins from the small sample of angles in our data set. library(MASS) ## kde2d We compute the density estimate in a grid of size 100 by 100 using a bandwidth of 2 and using the kde2d function that uses a bivariate normal kernel. denshat &lt;- kde2d(phi, psi, h = 2, n = 100) denshat &lt;- data.frame( cbind(denshat$x, rep(denshat$y, each = length(denshat$x)), as.vector(denshat$z)) ) colnames(denshat) &lt;- c(&quot;phi&quot;, &quot;psi&quot;, &quot;dens&quot;) p &lt;- ggplot(denshat, aes(phi, psi)) + geom_tile(aes(fill = dens), alpha = 0.5) + geom_contour(aes(z = sqrt(dens))) + geom_point(data = phipsi, aes(fill = NULL)) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;darkblue&quot;, trans = &quot;sqrt&quot;) We then recompute the density estimate in the same grid of size using the smaller bandwidth of 0.5. denshat &lt;- kde2d(phi, psi, h = 0.5, n = 100) Figure 1.6: Bivariate density estimates of protein backbone angles using a bivariate Gaussian kernel with bandwiths \\(2\\) (left) and \\(0.5\\) (right). The Ramachandran plot in Figure 1.6 shows how structural constraints of a protein, such as steric effects, induces a non-standard bivariate distribution of \\((\\phi, \\psi)\\)-angles. 1.1.5 Large scale smoothing With small data sets of less than 10,000 data points, say, univariate smooth density estimation requires a very modest amount of computation. That is true even with rather naive implementations of the standard methods. The R function density is implemented using a number of computational tricks like binning and the fast Fourier transform, and it can compute density estimates with a million data points (around 8 MB) within a fraction of a second. It is unclear if we ever need truly large scale univariate density estimation with terabytes of data points, say. If we have that amount of (heterogeneous) data it is likely that we are better off breaking the data down into smaller and more homogeneous groups. That is, we should turn a big data computation into a large number of small data computations. That does not remove the computational challenge but it does diminish it somewhat e.g. by parallelization. Deng and Wickham did a review in 2011 on Density estimation in R, where they assessed the performance of a number of R packages including the density function. The KernSmooth package was singled out in terms of speed as well as accuracy for computing smooth density estimates with density performing quite well too. (Histograms are non-smooth density estimates and generally faster to compute). The assessment was based on using defaults for the different packages, which is meaningful in the sense of representing the performance that the occasional user will experience. It is, however, also an evaluation of the combination of default choices and the implementation, and as different packages rely on e.g. different bandwidth selection algorithms, this assessment is not the complete story. The bkde function from the KernSmooth package, as well as density, are solid choices, but the point is that performance assessment is a multifaceted problem. To be a little more specific about the computational complexity of density estimators, suppose that we have \\(n\\) data points and want to evaluate the density in \\(m\\) points. A naive implementation of kernel smoothing, Section 2.2, has \\(O(mn)\\) time complexity, while a naive implementation of the best bandwidth selection algorithms have \\(O(n^2)\\) time complexity. As a simple rule-of-thumb, anything beyond \\(O(n)\\) will not scale to very large data sets. A quadratic time complexity for bandwidth selection will, in particular, be a serious bottleneck. Kernel smoothing illustrates perfectly that a literal implementation of the mathematics behind a statistical method may not always be computationally viable. Even the \\(O(mn)\\) time complexity may be quite a bottleneck as it reflects \\(mn\\) kernel evaluations, each being potentially a computationally relatively expensive operation. The binning trick, with the number of bins set to \\(m\\), is a grouping of the data points into \\(m\\) sets of neighbor points (bins) with each bin representing the points in the bin via a single point and a weight. If \\(m \\ll n\\), this can reduce the time complexity substantially to \\(O(m^2) + O(n)\\). The fast Fourier transform may reduce the \\(O(m^2)\\) term even further to \\(O(m\\log(m))\\). Some approximations are involved, and it is of importance to carefully evaluate the tradeoff between time and memory complexity on one side and accuracy on the other side. Multivariate smoothing is a different story. While it is possible to generalize the basic ideas of univariate density esimation to arbitrary dimensions, the curse-of-dimensionality hits unconstrained smoothing hard – statistically as well as computationally. Multivariate smoothing is therefore still an active research area developing computationally tractable and novel ways of fitting smooth densities or conditional densities to multivariate or even high-dimensional data. A key technique is to make structural assumptions to alleviate the challenge of a large dimension, but there are many different assumptions possible, which makes the body of methods and theory richer and the practical choices much more difficult. "],
["monte-carlo-methods.html", "1.2 Monte Carlo Methods", " 1.2 Monte Carlo Methods 1.2.1 Univariate von Mises distributions The angles considered in Section 1.1 take values in the interval \\((-\\pi, \\pi]\\). The von Mises distribution on this interval is given by the density \\[f(x) = \\frac{1}{\\varphi(\\theta)} e^{\\theta_1 \\cos(x) + \\theta_2 \\sin(x)}\\] for \\(\\theta = (\\theta_1, \\theta_2)^T \\in \\mathbb{R}^2\\). A common alternative parametrization is obtained by introducing \\(\\kappa = \\|\\theta\\|_2 = \\sqrt{\\theta_1^2 + \\theta_2^2}\\), and (whenever \\(\\kappa \\neq 0\\)) \\(\\nu = \\theta / \\kappa = (\\cos(\\mu), \\sin(\\mu))^T\\) for \\(\\mu \\in (-\\pi, \\pi]\\). Using the \\((\\kappa, \\mu)\\)-parametrization the density becomes \\[f(x) = \\frac{1}{\\varphi(\\kappa \\nu)} e^{\\kappa \\cos(x - \\mu)}.\\] The former parametrization in terms of \\(\\theta\\) is, however, the canonical parametrization of the family of distributions as an exponential family, which is particularly useful for various likelihood estimation algorithms. The normalization constant \\[\\begin{align*} \\varphi(\\kappa \\nu) &amp; = \\int_{-\\pi}^\\pi e^{\\kappa \\cos(x - \\mu)}\\mathrm{d} x \\\\ &amp; = 2 \\pi \\int_{0}^{1} e^{\\kappa \\cos(\\pi x)}\\mathrm{d} x = 2 \\pi I_0(\\kappa) \\end{align*}\\] is given in terms of the modified Bessel function \\(I_0\\). We can easily compute and plot the density using R’s besselI implementation of the modified Bessel function. phi &lt;- function(k) 2 * pi * besselI(k, 0) curve(exp(cos(x)) / phi(1), -pi, pi, lwd = 2, ylab = &quot;density&quot;, ylim = c(0, 0.52)) curve(exp(2 * cos(x - 1)) / phi(2), -pi, pi, col = &quot;red&quot;, lwd = 2, add = TRUE) curve(exp(0.5 * cos(x + 1.5)) / phi(0.5), -pi, pi, col = &quot;blue&quot;, lwd = 2, add = TRUE) Figure 1.7: Density for the von Mises distribution with parameters \\(\\kappa = 1\\) and \\(\\nu = 0\\) (black), \\(\\kappa = 2\\) and \\(\\nu = 1\\) (red), and \\(\\kappa = 0.5\\) and \\(\\nu = - 1.5\\) (blue). It is not entirely obvious how we should go about simulating data points from the von Mises distribution. It will be demonstrated in Section 4.3 how to implement a rejection sampler, which is one useful algorithm for simulating samples from a distribution with a density. In this section we simply use the rmovMF function from the movMF package, which implements a few functions for working with (finite mixtures of) von Mises distributions, and even the general von Mises-Fisher distributions that are generalizations of the von Mises distribution to \\(p\\)-dimensional unit spheres. library(&quot;movMF&quot;) xy &lt;- rmovMF(500, 0.5 * c(cos(-1.5), sin(-1.5))) ## rmovMF represents samples as elements on the unit circle x &lt;- acos(xy[, 1]) * sign(xy[, 2]) hist(x, breaks = seq(-pi, pi, length.out = 15), prob = TRUE) rug(x) density(x, bw = &quot;SJ&quot;, cut = 0) %&gt;% lines(col = &quot;red&quot;, lwd = 2) curve(exp(0.5 * cos(x + 1.5)) / phi(0.5), -pi, pi, col = &quot;blue&quot;, lwd = 2, add = TRUE) Figure 1.8: Histogram of 500 simulated data points from a von Mises distribution with parameters \\(\\kappa = 0.5\\) and \\(\\nu = - 1.5\\). A smoothed density estimate (red) and the true density (blue) are added to the plot. 1.2.2 Mixtures of von Mises distributions The von Mises distributions are unimodal distributions on \\((-\\pi, \\pi]\\). Thus to find a good model of the bimodal angle data, say, we have do move beyond these distributions. A standard approach for constructing multimodal distributions is as mixtures of unimodal distributions. A mixture of two von Mises distributions can be constructed by flipping a (biased) coin to decide which of the two distributions to sample from. We will use the exponential family parametrization in the following. thetaA &lt;- c(3.5, -2) thetaB &lt;- c(-4.5, 4) alpha &lt;- 0.55 ## Probability of von Mises distribution A ## The sample function implements the &quot;coin flips&quot; u &lt;- sample(c(1, 0), 500, replace = TRUE, prob = c(alpha, 1 - alpha)) xy &lt;- rmovMF(500, thetaA) * u + rmovMF(500, thetaB) * (1 - u) x &lt;- acos(xy[, 1]) * sign(xy[, 2]) The rmovMF actually implements simulation from a mixture distribution directly, thus there is no need to construct the “coin flips” explicitly. theta &lt;- rbind(thetaA, thetaB) xy &lt;- rmovMF(length(x), theta, c(alpha, 1 - alpha)) x_alt &lt;- acos(xy[, 1]) * sign(xy[, 2]) To compare the simulated data with two mixture components to the model and a smoothed density, we implement an R function that computes the density for an angle argument using the function dmovMF that takes a unit circle argument. dvM &lt;- function(x, theta, alpha) { xx &lt;- cbind(cos(x), sin(x)) dmovMF(xx, theta, c(alpha, 1 - alpha)) / (2 * pi) } Note that dmovMF uses normalized spherical measure on the unit circle as reference measure, thus the need for the \\(2\\pi\\) division if we want the result to be comparable to histograms and density estimates that use Lebesgue measure on \\((-\\pi, \\pi]\\) as the reference measure. hist(x, breaks = seq(-pi, pi, length.out = 15), prob = TRUE, ylim = c(0, 0.5)) rug(x) density(x, bw = &quot;SJ&quot;, cut = 0) %&gt;% lines(col = &quot;red&quot;, lwd = 2) curve(dvM(x, theta, alpha), -pi, pi, col = &quot;blue&quot;, lwd = 2, add = TRUE) hist(x_alt, breaks = seq(-pi, pi, length.out = 15), prob = TRUE, ylim = c(0, 0.5)) rug(x_alt) density(x_alt, bw = &quot;SJ&quot;, cut = 0) %&gt;% lines(col = &quot;red&quot;, lwd = 2) curve(dvM(x, theta, alpha), -pi, pi, col = &quot;blue&quot;, lwd = 2, add = TRUE) Figure 1.9: Histograms of 500 simulated data points from a mixture of two von Mises distributions using either the explicit construction of the mixture (left) or the functionality in rmovMF to simulate mixtures directly (right). A smoothed density estimate (red) and the true density (blue) are added to the plot. Simulation of data from a distribution finds many applications. The technique is widely used whenever we want to investigate a statistical methodology in terms of its frequentistic performance under various data sampling models, and simulation is a tool of fundamental importance for the practical application of Bayesian statistical methods. Another important application is as a tool for computing approximations of integrals. This is usually called Monte Carlo integration and is a form of numerical integration. Computing probabilities or distribution functions, say, are notable examples of integrals, and we consider here the computation of the probability of the interval \\((0, 1)\\) for the above mixture of two von Mises distributions. It is straightforward to compute this probability via Monte Carlo integration as a simple average. Note that we will use a large number of samples, 50,000 in this case, of simulated angles for this computation. Increasing the number even further will make the result more accurate. Chapter 5 deals with the assessment of the accuracy of Monte Carlo integrals, and how this random error can be estimated, bounded and minimized. xy &lt;- rmovMF(50000, theta, c(alpha, 1 - alpha)) x &lt;- acos(xy[, 1]) * sign(xy[, 2]) mean(x &gt; 0 &amp; x &lt; 1) ## Estimate of the probability of the interval (0, 1) ## [1] 0.08676 The probability above could, of course, be expressed using the distribution function of the mixture of von Mises distributions, which in turn can be computed in terms of integrals of von Mises densities. Specifically, the probability is \\[p = \\frac{\\alpha}{\\varphi(\\theta_A)} \\int_0^1 e^{\\theta_{A, 1} \\cos(x) + \\theta_{A, 2} \\sin(x)} \\mathrm{d} x + \\frac{1 - \\alpha}{\\varphi(\\theta_B)} \\int_0^1 e^{\\theta_{B, 1} \\cos(x) + \\theta_{B, 2} \\sin(x)} \\mathrm{d} x,\\] but these integrals do not have a simple analytic representation – just as the distribution function of the von Mises distribution doesn’t have a simple analytic expression. Thus the computation of the probability requires numerical computation of the integrals. The R function integrate can be used for numerical integration of univariate functions using standard numerical integration techniques. We can thus compute the probability by integrating the density of the mixture, as implemented above as the R function dvM. Note the arguments passed to integrate below. The first argument is the density function, then follows the lower and the upper limits of the integration, and then follows additional arguments to the density – in this case parameter values. integrate(dvM, 0, 1, theta = theta, alpha = alpha) ## 0.08635171 with absolute error &lt; 9.6e-16 The integrate function in R is an interface to a couple of classical QUADPACK Fortran routines for numerical integration via adaptive quadrature. Specifically, the computations rely on approximations of the form \\[\\int_a^b f(x) \\mathrm{d} x \\simeq \\sum_i w_i f(x_i)\\] for certain grid points \\(x_i\\) and weights \\(w_i\\), which are computed using Gauss-Kronrod quadrature. This method provides an estimate of the approximation error in addition to the numerical approximation of the integral itself. It is noteworthy that integrate as a function implemented in R takes another function, in this case the density dvM, as an argument. R is a functional programming language and functions are first-class citizens. This implies, for instance, that functions can be passed as arguments to other functions using a variable name – just like any other variable can be passed as an argument to a function. In the parlance of functional programming, integrate is a functional: a higher-order function that takes a function as argument and returns a numerical value. It is a main theme of this book how to make good use of functional (and object oriented) programming features in R to write clear, expressive and modular code without sacrificing computational efficiency. Returning to the specific problem of the computation of an integral, we may ask what the purpose of Monte Carlo integration is? Apparently we can just do numerical integration using e.g. integrate. There are at least two reasons why Monte Carlo integration is sometimes preferable. First, it is straightforward to implement and often works quite well for multivariate and even high-dimensional integrals, whereas grid-based numerical integration schemes scale poorly with the dimension. Second, it does not require that we have an analytic representation of the density. It is common in statistical applications that we are interested in the distribution of a statistic, which is a complicated transformation of data, and whose density is difficult or impossible to find analytically. Yet if we can just simulate data, we can simulate from the distribution of the statistic, and we can then use Monte Carlo integration to compute whatever probability or integral w.r.t. the distribution of the statistic that we are interested in. 1.2.3 Large scale simulation Relations to Complex sampling models. Ex Bayes, MCMC, Stan. "],
["optimization.html", "1.3 Optimization", " 1.3 Optimization Likelihood optimization for mixtures of the von Mises distribution. Direct implementation using optim and the density implementations. Alternatively using the EM-algorithm as implemented in movMF. 1.3.1 The EM-algorithm The movMF function implements the EM-algorithm for mixtures of von Mises distributions. Note again that the movMF package works with data as elements on the unit circle instead of as angles, whence the angle data must be transformed using cos and sin. psi_circle &lt;- cbind(cos(phipsi$psi), sin(phipsi$psi)) vM_fit &lt;- movMF(psi_circle, 2, control = list(verbose = TRUE, maxiter = 10, start = &quot;S&quot;)) ## Iteration: 0 *** L: 56.13 ## Iteration: 1 *** L: 119.228 ## Iteration: 2 *** L: 176.601 ## Iteration: 3 *** L: 192.397 ## Iteration: 4 *** L: 193.087 ## Iteration: 5 *** L: 193.223 ## Iteration: 6 *** L: 193.273 ## Iteration: 7 *** L: 193.291 ## Iteration: 8 *** L: 193.298 ## Iteration: 9 *** L: 193.3 We can compare the fitted model to the data using the density function as implemented above and the parameters estimated by the EM-algorithm. hist(phipsi$psi, breaks = seq(-pi, pi, length.out = 15), prob = TRUE) rug(phipsi$psi) density(phipsi$psi, bw = &quot;SJ&quot;, cut = 0) %&gt;% lines(col = &quot;red&quot;, lwd = 2) curve(dvM(x, vM_fit$theta, vM_fit$alpha[1]), -pi, pi, add = TRUE, col = &quot;blue&quot;, lwd = 2) 1.3.2 Large scale optimization Relations to complex optimization problems. Ex. deep learning, tensorflow. "],
["exercises.html", "1.4 Exercises", " 1.4 Exercises R training exercises Exercise 1.1 Explain the result of evaluating the following R expression. (0.1 + 0.1 + 0.1) &gt; 0.3 ## [1] TRUE Exercise 1.2 Write a function that takes a numeric vector x and a threshold value h as arguments and returns the vector of all values in x greater than h. Test the function on seq(0, 1, 0.1) with threshold 0.3. Have the example from Exercise 1.1 in mind. Exercise 1.3 Investigate how your function from Exercise 1.2 treats missing values (NA), infinite values (Inf and -Inf) and the special value “Not a Number” (NaN). Rewrite your function (if necessary) to exclude all or some of such values from x. Hint: The functions is.na, is.nan and is.finite are useful. Histograms with non-equidistant breaks The following three exercises will use a data set consisting of measurements from objects outside of our galaxy including infered emissions. We will focus on the variable F12, which is the total 12 micron band flux density. infrared &lt;- read.table(&quot;data/infrared.txt&quot;, header = TRUE) F12 &lt;- infrared$F12 The purpose of this exercise is two-fold. First, you will get familiar with the data and see how different choices of visualizations using histograms can affect your interpretation of the data. Second, you will learn more about how to write functions in R and gain a better understanding of how they work. Exercise 1.4 Plot a histogram of log(F12) using the default value of the argument breaks. Experiment with alternative values of breaks. Exercise 1.5 Write your own function, called myBreaks, which takes two arguments, x (a vector) and h (a positive integer). Let h have default value 5. The function should first sort x into increasing order and then return the vector that: starts with the smallest entry in x; contains every \\(h\\)th unique entry from the sorted x; ends with the largest entry in x. For example, if h = 2 and x = c(1, 3, 2, 5, 10, 11, 1, 1, 3) the function should return c(1, 3, 10, 11). To see this, first sort x, which gives the vector c(1, 1, 1, 2, 3, 3, 5, 10, 11), whose unique values are c(1, 2, 3, 5, 10, 11). Every second unique entry is c(1, 3, 10), and then the largest entry 11 is concatenated. Hint: The functions sort and unique can be useful. Use your function to construct breakpoints for the histogram for different values of h, and compare with the histograms obtained in Exercise 1.4. Exercise 1.6 If there are no ties in the data set, the function above will produce breakpoints with h observations in the interval between two consecutive breakpoints (except the last two perhaps). If there are ties, the function will by construction return unique breakpoints, but there may be more than h observations in some intervals. The intention is now to rewrite myBreaks so that if possible each interval contains h observations. Modify the myBreaks function with this intention and so that is has the following properties: All breakpoints must be unique. The range of the breakpoints must cover the range of x. For two subsequent breakpoints, \\(a\\) and \\(b\\), there must be at least h observations in the interval \\((a,b]\\), provided h &lt; length(x). (With the exception that for the first two breakpoints, the interval is \\([a,b]\\).) Functions and functional programming The following four exercises build on having implemented a function that computes breakpoints for a histogram either as in Exercise 1.5 or as in Exercise 1.6. The purpose of the exercises is to explore. Exercise 1.7 Write a function called myHist, which takes a single argument h and plots a histogram of log(F12) using your function. Extend the implementation so that any additional argument specified when calling myHist is passed on to hist. Investigate and explain what happens when executing the following function calls. myHist() myHist(h = 5, freq = TRUE) myHist(h = 0) Finally, what happens if you remove that data from the global environment and call myHist subsequently? Exercise 1.8 What is the environment of myHist? Change it to a new environment, and assign (using the function assign) the data to a variable with an appropriate name in that environment. Once this is done, check what now happens when calling myHist after the data is removed from the global environment. Exercise 1.9 Write a function that takes an argument x (the data) and returns a function, where the returned function takes an argument h (just as myHist) and plots a histogram (just as myHist). Because the return value is a function, we may refer to the function as a function factory. What is the environment of the function created by the function factory? What is in the environment? Does it have any effect when calling the function whether the data is altered or removed from the global environment? Exercise 1.10 Evaluate the following function call: tmp &lt;- myHist(10, plot = FALSE) What is the type and class of tmp? What happens when plot(tmp, col = &quot;red&quot;) is executed? How can you find help on what plot does with an object of this class? Specifically, how do you find the documentation for the argument col, which is not an argument of plot? "],
["density.html", "Chapter 2 Density estimation", " Chapter 2 Density estimation This chapter is on nonparametric density estimation. A classical nonparametric estimator of a density is the histogram, which provides discontinuous and piecewise constant estimates. The focus in this chapter is on some of the alternatives that provide continuous or even smooth estimates instead. Kernel methods form an important class of smooth density estimators as implemented by the R function density. These estimators are essentially just locally weighted averages, and their computation is relatively straightforward in theory. In practice, different choices of how to implement the computations can, however, have a big effect on the actual computation time, and the implementation of kernel density estimators will illustrate three points: if possible, choose vectorized implementations in R, if a small loss in accuracy is acceptable, an approximate solution can be orders of magnitude faster than a literal implementation, and the time it takes to numerically evaluate different elementary functions can depend a lot on the function and how you implement the computation. The first point is emphasized because it results in implementations that are short, expressive and easier to understand just as much as it typically results in computationally more efficient implementations. Note also that not every computation can be vectorized in a beneficial way, and one should never go through hoops to vectorize a computation at all costs. Before the chapter turns to the main topic on kernel methods a brief likelihood analysis is carried out. This is done to illustrate that in nonparametric statistics the estimators must be regularized to not overfit the data. The estimators need to achieve the right balance of adapting to data without adapting too much to the random variation in the data. With the likelihood approach regularization can be achieved by constraining the density estimates to belong to a family of increasingly flexible parametric densities that are fitted to data. This is known as the method of sieves. For the kernel methods the regularization is controlled by the bandwidth tuning parameter. In either case, methods are developed in this chapter for automatically choosing the amount of regularization. Choosing the right amount of regularization is just as important as choosing the method to use in the first place. It may, in fact, be more important. We actually don’t have a complete implementation of a nonparametric estimator until we have implemented a data driven and automatic way of choosing the amount of regularization. Implementing only the computations for evaluating a kernel estimator, say, and leaving it completely to the user to choose the bandwidth is a job half done. Methods and implementations for choosing the bandwidth are therefore treated in some detail in this chapter. The chapter concludes by a section on multivariate nonparametric density estimation via kernel methods. However, the treatment is currently incomplete and only scratches the surface by showcasing a few R function. "],
["unidens.html", "2.1 Univariate density estimation", " 2.1 Univariate density estimation Recall the data on \\(\\phi\\)- and \\(\\psi\\)-angles in polypeptide backbone structures, as considered in Section 1.1.1. Figure 2.1: Histograms equipped with a rug plot of the distribution of \\(\\phi\\)-angles (left) and \\(\\psi\\)-angles (right) of the peptide planes in the protein human protein 1HMP. We will in this section start the treatment of methods for smooth density estimation for univariate data such as data on either the \\(\\phi\\)- or the \\(\\psi\\)-angle. Multivariate methods for estimation of e.g. the bivariate joint density of the angles is postponed to Section 1.1.4. 2.1.1 Likelihood considerations Let \\(f_0\\) denote the unknown density that we want to estimate. That is, we imagine that the data points \\(x_1, \\ldots, x_n\\) are all observations drawn from the probability measure with density \\(f_0\\) w.r.t. Lebesgue measure on \\(\\mathbb{R}\\). Note that whenever we have a parametrized statistical model \\((f_{\\theta})_{\\theta}\\), where \\(f_{\\theta}\\) is a density w.r.t. Lebesgue measure on \\(\\mathbb{R}\\), and an estimate \\(\\hat{\\theta}\\) of the parameter, then \\(f_{\\hat{\\theta}}\\) is an estimate of the unknown density \\(f_0\\). For a parametric family we can always try to use the MLE \\[\\hat{\\theta} = \\text{arg max}_{\\theta} \\sum_{i=1}^n \\log f_{\\theta}(x_i)\\] as an estimate of \\(\\theta\\). Likewise, we might compute the empirical mean and variance for the data and plug those numbers into the density for the Gaussian distribution, and in this way obtain a Gaussian density estimate of \\(f_0\\). psi_mean &lt;- mean(psi) psi_sd &lt;- sd(psi) hist(psi, prob = TRUE) rug(psi) curve(dnorm(x, psi_mean, psi_sd), add = TRUE, col = &quot;red&quot;) Figure 2.2: Gaussian density (red) fitted to the \\(\\psi\\)-angles. As Figure 2.2 shows, if we fit a Gaussian distribution to the \\(\\psi\\)-angle data we get a density estimate that clearly doesn’t match the histogram. The Gaussian density matches the data on the first and second moments, but the histogram shows a clear bimodality that the Gaussian distribution by definition cannot match. Thus we need a more flexible parametric model than the Gaussian if we want to fit a density to this data set. It is natural to ask why we don’t go all-in and simply try to find the maximum likelihood estimate of the density among all possible densities? Why restrict attention to some subset of densities in a particular parametric family? The log-likelihood \\[\\ell(f) = \\sum_{i=1}^n \\log f(x_i)\\] is well defined as a function of the density \\(f\\) – even when \\(f\\) isn’t restricted to belong to a finite-dimensional parametric model. To investigate if a nonparametric MLE is meaningful we consider how the likelihood behaves for the densities \\[\\overline{f}_h(x) = \\frac{1}{nh \\sqrt{2 \\pi}} \\sum_{j=1}^n e^{- \\frac{(x - x_j)^2}{2 h^2} }\\] for different choices of \\(h\\). ## The densities can easily be implemented using the density implementation ## of the Gaussian density in R f_h &lt;- function(x, h) mean(dnorm(x, psi, h)) ## This function does not work as a vectorized function as it is, but there is ## a convenience function, &#39;Vectorize&#39;, in R that turns the function into a ## function that can actually be applied correctly to a vector. f_h &lt;- Vectorize(f_h) Figure 2.3 shows what some of these densities look like compared to the histogram of the \\(\\psi\\)-angle data. Clearly, for large \\(h\\) these densities are smooth and slowly oscillating, while as \\(h\\) gets smaller the densities become more and more wiggly. As \\(h \\to 0\\) the densities become increasingly dominated by tall narrow peaks around the individual data points. Figure 2.3: The densities \\(\\overline{f}_h\\) for different choices of \\(h\\). The way that these densities adapt to the data points is reflected in the log-likelihood as well. ## To plot the log-likelihood we need to evaluate it in a grid of h-values. hseq &lt;- seq(1, 0.001, -0.001) ## For the following computation of the log-likelihood it is necessary ## that f_h is vectorized. There are other ways to implement this computation ## in R, and some are more efficient, but the computation of the log-likelihood ## for each h scales as O(n^2) with n the number of data points. ll &lt;- sapply(hseq, function(h) sum(log(f_h(psi, h)))) qplot(hseq, ll, geom = &quot;line&quot;) + xlab(&quot;h&quot;) + ylab(&quot;Log-likelihood&quot;) qplot(hseq, ll, geom = &quot;line&quot;) + scale_x_log10(&quot;h&quot;) + ylab(&quot;&quot;) Figure 2.4: Log-likelihood, \\(\\ell(\\overline{f}_h)\\), for the densities \\(\\overline{f}_h\\) as a function of \\(h\\). Note the log-scale on the right. From Figure 2.4 it is clear that the likelihood is decreasing in \\(h\\), and it appears that it is unbounded as \\(h \\to 0\\). This is most clearly seen on the figure when \\(h\\) is plotted on the log-scale because then it appears that the log-likelihood approximately behaves as \\(-\\log(h)\\) for \\(h \\to 0\\). We can show that that is, indeed, the case. If \\(x_i \\neq x_j\\) when \\(i \\neq j\\) \\[\\begin{align*} \\ell(\\overline{f}_h) &amp; = \\sum_{i} \\log\\left(1 + \\sum_{j \\neq i} e^{-(x_i - x_j)^2 / (2 h^2)} \\right) - n \\log(nh\\sqrt{2 \\pi}) \\\\ &amp; \\sim - n \\log(nh\\sqrt{2 \\pi}) \\end{align*}\\] for \\(h \\to 0\\). Hence, \\(\\ell(\\overline{f}_h) \\to \\infty\\) for \\(h \\to 0\\). This demonstrates that the MLE of the density doesn’t exist in the set of all distributions with densities. In the sense of weak convergence it actually holds that \\[\\overline{f}_h \\cdot m \\overset{\\mathrm{wk}}{\\longrightarrow} \\varepsilon_n = \\frac{1}{n} \\sum_{i=1}^n \\delta_{x_i}\\] for \\(h \\to 0\\). The empirical measure \\(\\varepsilon_n\\) can sensibly be regarded as the nonparametric MLE of the distribution, but the empirical measure does not have a density. We conclude that we cannot directly define a sensible density estimator as a maximum-likelihood estimator. 2.1.2 Method of sieves A sieve is a family of models, \\(\\Theta_{h}\\), indexed by a real valued parameter, \\(h \\in \\mathbb{R}\\), such that \\(\\Theta_{h_1} \\subseteq \\Theta_{h_2}\\) for \\(h_1 \\leq h_2\\). In this chapter \\(\\Theta_{h}\\) will denote a set of probability densities. If the increasing family of models is chosen in a sensible way, we may be able to compute the MLE \\[\\hat{f}_h = \\text{arg max}_{f \\in \\Theta_h} \\ell(f),\\] and we may even be able to choose \\(h = h_n\\) as a function of the sample size \\(n\\) such that \\(\\hat{f}_{h_n}\\) becomes a consistent estimator of \\(f_0\\). It is possible to take \\[\\Theta_h = \\{ \\overline{f}_{h&#39;} \\mid h&#39; \\leq h \\}\\] with \\(\\overline{f}_{h&#39;}\\) as defined above, in which case \\(\\hat{f}_h = \\overline{f}_h\\). We will see in the following section that this is simply a kernel estimator. A more interesting example is obtained by letting \\[\\Theta_h = \\left\\{ x \\mapsto \\frac{1}{h \\sqrt{2 \\pi}} \\int e^{- \\frac{(x - z)^2}{2 h^2} } \\mathrm{d}\\mu(z) \\Biggm| \\mu \\textrm{ a probability measure} \\right\\},\\] which is known as the convolution sieve. We note that \\(\\overline{f}_h \\in \\Theta_h\\) by taking \\(\\mu = \\varepsilon_n\\), but generally \\(\\hat{f}_h\\) will be different from \\(\\overline{f}_h\\). We will not pursue the general theory of sieve estimators, but refer to the paper Nonparametric Maximum Likelihood Estimation by the Method of Sieves by Geman and Hwang. In the following section we will work out some more practical details for a particular sieve estimator based on a basis expansion of the log-density. 2.1.3 Basis expansions We suppose in this section that the data points are all contained in the interval \\([a,b]\\) for \\(a, b \\in \\mathbb{R}\\). This is true for the angle data with \\(a = -\\pi\\) and \\(b = \\pi\\) no matter the size of the data set, but if \\(f_0\\) does not have a bounded support it may be necessary to let \\(a\\) and \\(b\\) change with the data. However, for any fixed data set we can choose some sufficiently large \\(a\\) and \\(b\\). In this section the sieve will be indexed by integers, and for \\(h \\in \\mathbb{N}\\) we suppose that we have chosen continuous functions \\(b_1, \\ldots, b_h : [a,b] \\to \\mathbb{R}\\). These will be called basis functions. We then define \\[\\Theta_h = \\left\\{ x \\mapsto \\varphi(\\boldsymbol{\\beta})^{-1} \\exp\\left(\\sum_{k=1}^h \\beta_k b_k(x)\\right) \\Biggm| \\boldsymbol{\\beta} = (\\beta_1, \\ldots, \\beta_h)^T \\in \\mathbb{R}^h \\right\\},\\] where \\[\\varphi(\\boldsymbol{\\beta}) = \\int_a^b \\exp\\left(\\sum_{k=1}^h \\beta_k b_k(x)\\right) \\mathrm{d} x.\\] The MLE over \\(\\Theta_h\\) is then given as \\[\\hat{f}_h(x) = \\varphi(\\hat{\\boldsymbol{\\beta}})^{-1} \\exp\\left(\\sum_{k=1}^h \\hat{\\beta}_k b_k(x)\\right),\\] where \\[\\begin{align*} \\hat{\\boldsymbol{\\beta}} &amp; = \\text{arg max}_{\\boldsymbol{\\beta} \\in \\mathbb{R}^h} \\ \\sum_{i=1}^n \\sum_{k=1}^h \\beta_k b_k(x_i) - \\log \\varphi(\\boldsymbol{\\beta}) \\\\ &amp; = \\text{arg max}_{\\boldsymbol{\\beta} \\in \\mathbb{R}^h} \\ \\ \\mathbf{1}^T \\mathbf{B} \\boldsymbol{\\beta} - \\log \\varphi(\\boldsymbol{\\beta}). \\end{align*}\\] Here \\(\\mathbf{B}\\) is the \\(n \\times h\\) matrix with \\(B_{ik} = b_k(x_i)\\). Thus for any fixed \\(h\\) the model is, in fact, just an ordinary parametric exponential family, though it may not be entirely straightforward how to compute \\(\\varphi(\\boldsymbol{\\beta})\\). Many basis functions are possible. Polynomials may be used, but splines are often preferred. An alternative is a selection of trigonometric functions, for instance \\[b_1(x) = \\cos(x), b_2(x) = \\sin(x), \\ldots, b_{2h-1}(x) = \\cos(hx), b_{2h}(x) = \\sin(hx)\\] on the interval \\([-\\pi, \\pi]\\). In Section 1.2.1 a simple special case was actually treated corresponding to \\(h = 2\\), where the normalization constant was identified in terms of a modified Bessel function. It is worth remembering the following: “With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.” — John von Neumann The Normal-inverse Gaussian distribution has four parameters and the generalized hyperbolic distribution is an extension with five, but von Neumann was probably thinking more in terms of a spline or a polynomial expansion as above with four or five suitably chosen basis functions. The quote is not a mathematical statement but an empirical observation. With a handful of parameters you already have a quite flexible class of densities that will fit many real data sets well. But remember that a reasonably good fit doesn’t mean that you have found the “true” data generating model. Though data is in some situations not as scarce a resource today as when von Neumann made elephants wiggle their trunks, the quote still suggests that \\(h\\) should grow rather slowly with \\(n\\) to avoid overfitting. "],
["kernel-density.html", "2.2 Kernel methods", " 2.2 Kernel methods In Section 2.1 we pursued the principled but also somewhat abstract approach to density estimation via maximum-likelihood estimation over a suitably constrained set of distributions. In this section we will pursue the more basic idea of smooth density estimation relying on the approximation \\[P(X \\in (x-h, x+h)) = \\int_{x-h}^{x+h} f_0(z) \\ dz \\simeq f_0(x) 2h,\\] which is valid for any continuous density \\(f_0\\). Inverting this approximation and using the law of large numbers, \\[\\begin{align*} f_0(x) &amp; \\simeq \\frac{1}{2h}P(X \\in (x-h, x+h)) \\\\ &amp; \\simeq \\frac{1}{2hn} \\sum_{i=1}^n 1_{(x-h, x+h)}(x_i) \\\\ &amp; = \\underbrace{\\frac{1}{2hn} \\sum_{i=1}^n 1_{(-h, h)}(x - x_i)}_{\\hat{f}_h(x)} \\end{align*}\\] for i.i.d. observations \\(x_1, \\ldots, x_n\\) from the distribution \\(f_0 \\cdot m\\). The function \\(\\hat{f}_h\\) defined as above is an example of a kernel density estimator with a rectangular kernel. We immediately note that \\(h\\) has to be chosen appropriately. If \\(h\\) is large, \\(\\hat{f}_h\\) will be flat and close to a constant. If \\(h\\) is small, \\(\\hat{f}_h\\) will make large jumps close to the observations. What do we then mean by an “appropriate” choice of \\(h\\) above? Just as for the methods of sieves we must have some prior assumptions about what we expect \\(f_0\\) to look like. Typically, we expect \\(f_0\\) to have few oscillations and to be fairly smooth, and we want \\(\\hat{f}_h\\) to reflect that. A too large \\(h\\) will oversmooth the data relative to \\(f_0\\) by effectively ignoring the data, while a too small \\(h\\) will undersmooth the data relative to \\(f_0\\) by allowing individual data points to have large local effects that make the estimate wiggly. More formally, we can look at the mean and variance of \\(\\hat{f}_h\\). Letting \\(p(x, h) = P(X \\in (x-h, x+h))\\), it follows that \\(f_h(x) = E(\\hat{f}_h(x)) = p(x, h) / (2h)\\) while \\[\\begin{equation} V(\\hat{f}_h(x)) = \\frac{p(x, h) (1 - p(x, h))}{4h^2 n} \\simeq f_h(x) \\frac{1}{2hn}. \\tag{2.1} \\end{equation}\\] We see from these computations that for the \\(\\hat{f}_h(x)\\) to be approximately unbiased for any \\(x\\) we need \\(h\\) to be small – ideally letting \\(h \\to 0\\) since then \\(f_h(x) \\to f_0(x)\\). However, this will make the variance blow up, and to minimize variance we should instead choose \\(h\\) as large as possible. One way to define “appropriate” is then to strike a balance between the bias and the variance as a function of \\(h\\) so as to minimize the mean squared error of \\(\\hat{f}_h(x)\\). We will find the optimal tradeoff for the rectangular kernel in Section 2.3 on bandwidth selection. It’s not difficult, and you are encouraged to try finding it yourself at this point. In this section we will focus on computational aspects of kernel density estimation, but first we will generalize the estimator by allowing for other kernels. The estimate \\(\\hat{f}_h(x)\\) will be unbiased if \\(f_0\\) is constantly equal to \\(f_0(x)\\) in the entire interval \\((x-h, x+h)\\). This is atypical and can only happen for all \\(x\\) if \\(f_0\\) is constant. We expect the typical situation to be that \\(f_0\\) deviates the most from \\(f_0(x)\\) close to \\(x \\pm h\\), and that this causes a bias of \\(\\hat{f}_h(x).\\) Observations falling close to \\(x + h\\), say, should thus count less than observations falling close to \\(x\\)? The rectangular kernel makes a sharp cut; either a data point is in or it is out. If we use a smooth weighting function instead of a sharp cut, we might be able to include more data points and lower the variance while keeping the bias small. This is precisely the idea of kernel estimators, defined generally as \\[\\begin{equation} \\hat{f}_h(x) = \\frac{1}{hn} \\sum_{i=1}^n K\\left(\\frac{x - x_i}{h}\\right) \\tag{2.2} \\end{equation}\\] for a kernel \\(K : \\mathbb{R} \\to \\mathbb{R}\\). The parameter \\(h &gt; 0\\) is known as the bandwidth. Examples of kernels include the uniform or rectangular kernel \\[K(x) = \\frac{1}{2} 1_{(-1,1)}(x),\\] and the Gaussian kernel \\[K(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}.\\] One direct benefit of considering other kernels than the rectangular is that \\(\\hat{f}_h\\) inherits all smoothness properties from \\(K\\). Whereas the rectangular kernel is not even continuous, the Gaussian kernel is \\(C^{\\infty}\\) and so is the resulting kernel density estimate. We may note that \\(\\overline{f}_h\\) considered in Section 2.1.1 simply is the kernel density estimator with the Gaussian kernel and bandwidth \\(h\\). 2.2.1 Implementation What should be computed to compute a kernel density estimate? That is, in fact, a good question, because the definition actually just specifies how to evaluate \\(\\hat{f}_h\\) in any given point \\(x\\), but there is really not anything to compute until we need to evaluate \\(\\hat{f}_h\\). Thus when we implement kernel density estimation we really implement algorithms for evaluating a density estimate in a finite number of points. Our first implementation is a fairly low-level implementation that returns the evaluation of the density estimate in a given number of equidistant points. The function mimics some of the defaults of density so that it actually evaluates the estimate in the same points as density. ## This is an implementation of the function &#39;kernDens&#39; that computes ## evaluations of Gaussian kernel density estimates in a grid of points. ## ## The function has three formal arguments: &#39;x&#39; is the numeric vector of data ## points, &#39;h&#39; is the bandwidth and &#39;m&#39; is the number of grid points. ## The default value of 512 is chosen to match the default of &#39;density&#39;. kernDens &lt;- function (x, h, m = 512) { rg &lt;- range(x) ## xx is equivalent to grid points in &#39;density&#39; xx &lt;- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m) y &lt;- numeric(m) ## The evaluations, initialized as a vector of zeroes ## The actual computation is done using nested for-loops. The outer loop ## is over the grid points, and the inner loop is over the data points. for (i in seq_along(xx)) for (j in seq_along(x)) y[i] &lt;- y[i] + exp(- (xx[i] - x[j])^2 / (2 * h^2)) y &lt;- y / (sqrt(2 * pi) * h * length(x)) list(x = xx, y = y) } Note that the function returns a list containing the grid points (x) where the density estimate is evaluated as well as the estimated density evaluations (y). Note also that the argument m above sets the number of grid points, whereas density uses the argument n for that. The latter can be a bit confusing as \\(n\\) is often used to denote the number of data points. We will immediately test if the implementation works as expected – in this case by comparing it to our reference implementation density. f_hat &lt;- kernDens(psi, 0.2) f_hat_dens &lt;- density(psi, 0.2) plot(f_hat, type = &quot;l&quot;, lwd = 4, xlab = &quot;x&quot;, ylab = &quot;Density&quot;) lines(f_hat_dens, col = &quot;red&quot;, lwd = 2) plot(f_hat$x, f_hat$y - f_hat_dens$y, type = &quot;l&quot;, lwd = 2, xlab = &quot;x&quot;, ylab = &quot;Difference&quot;) Figure 2.5: Kernel density estimates with the Gaussian kernel (left) using R’s implementation (black) and our implementation (red) together with differences of the estimates (right). Figure 2.5 suggests that the estimates computed by our implementation and by density are the same when we just visually compare the plotted densities. However, if we look at the differences instead, we see that they are as large as \\(4 \\times 10^{-4}\\) in absolute value. This is way above what we should expect from rounding errors alone when using double precision arithmetic. Thus the two implementations only compute approximately the same, which is, in fact, because density relies on certain approximations for run time efficiency. In R we can often beneficially implement computations in a vectorized way instead of using an explicit loop. It is fairly easy to change the implementation to be more vectorized by computing each evaluation in one single line using the sum function and the fact that exp and squaring are vectorized. kernDens_vec &lt;- function (x, h, m = 512) { rg &lt;- range(x) xx &lt;- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m) y &lt;- numeric(m) ## The inner loop from &#39;kernDens&#39; has been vectorized, and only the ## outer loop over the grid points remains. const &lt;- (sqrt(2 * pi) * h * length(x)) for (i in seq_along(xx)) y[i] &lt;- sum(exp(-(xx[i] - x)^2 / (2 * h^2))) / const list(x = xx, y = y) } We test this new implementation by comparing it to our previous implementation. range(kernDens(psi, 0.2)$y - kernDens_vec(psi, 0.2)$y) ## [1] -5.551115e-16 3.885781e-16 The magnitude of the differences are of order at most \\(10^{-16}\\), which is what can be expected due to rounding errors. Thus we conclude that up to rounding errors, kernDens and kernDens_vec return the same on this data set. This is, of course, not a comprehensive test, but it is an example of one among a number of tests that should be considered. There are several ways to get completely rid of the explicit loops and write an entirely vectorized implementation in R. One of the solutions will use the sapply function, which belongs to the family of *apply functions that apply a function to each element in a vector or a list. In the parlance of functional programming the *apply functions are variations of the functional, or higher-order-function, known as map. kernDens_apply &lt;- function (x, h, m = 512) { rg &lt;- range(x) xx &lt;- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m) const &lt;- sqrt(2 * pi) * h * length(x) y &lt;- sapply(xx, function(z) sum(exp(-(z - x)^2 / (2 * h^2))) / const) list(x = xx, y = y) } The sapply call above will apply the function function(z) sum(dnorm(... to every element in the vector xx and return the result as a vector. The function is an example of an anonymous function that doesn’t get a name and exists only during the sapply evaluation. Instead of sapply it is possible to use lapply that returns a list. In fact, sapply is a simple wrapper around lapply that attempts to “simplify” the result from a list to an array (and in this case to a vector). An alternative, and also completely vectorized, solution can be based on the functions outer and rowMeans. kernDens_outer &lt;- function (x, h, m = 512) { rg &lt;- range(x) xx &lt;- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m) y &lt;- outer(xx, x, function(zz, z) exp(-(zz - z)^2 / (2 * h^2))) y &lt;- rowMeans(y) / (sqrt(2 * pi) * h) list(x = xx, y = y) } The outer function evaluates the kernel in all combinations of the grid and data points and returns a matrix of dimensions \\(m \\times n\\). The function rowMeans computes the means of each row and returns a vector of length \\(m\\). We should, of course, also remember to test these two last implementations. range(kernDens(psi, 0.2)$y - kernDens_apply(psi, 0.2)$y) ## [1] -5.551115e-16 3.885781e-16 range(kernDens(psi, 0.2)$y - kernDens_outer(psi, 0.2)$y) ## [1] -4.996004e-16 3.885781e-16 The natural question is then how to choose between the different implementations? Besides being correct it is important that the code is easy to read and understand. Which of the four implementations above that is best in this respect may depend a lot on the background of the reader. If you strip the implementations for comments, all four are arguably quite readable, but kernDens with the double loop might appeal a bit more to people used to imperative programming, while kernDens_apply might appeal more to people with a preference for functional programming. This functional and vectorized solution is also a bit closer to the mathematical notation with e.g. the sum sign \\(\\Sigma\\) being mapped directly to the sum function instead of the incremental addition in the for-loop. For these specific implementations these differences are nuances and preferences may be more subjective and aesthetic than substantial. To make a qualified choice between the implementations we should investigate if they differ in terms of run time and memory consumption, and this is precisely the topic of the next section. 2.2.2 Benchmarking Benchmarking is about measuring and comparing performance. For software this often means measuring run time and memory usage, though there are clearly many other aspects of software that should be benchmarked in general. This includes user experience, energy consumption and implementation and maintenance time. In this section we focus on benchmarking run time. The function system.time in R provides a simple way of benchmarking run time measured in seconds. system.time(kernDens(psi, 0.2)) system.time(kernDens_vec(psi, 0.2)) system.time(kernDens_apply(psi, 0.2)) system.time(kernDens_outer(psi, 0.2)) ## kernDens: ## user system elapsed ## 0.042 0.000 0.044 ## kernDens_vec: ## user system elapsed ## 0.003 0.000 0.003 ## kernDens_apply: ## user system elapsed ## 0.003 0.000 0.004 ## kernDens_outer: ## user system elapsed ## 0.004 0.000 0.005 The “elapsed” time is the total run time as experienced, while the “user” and “system” times are how long the CPU spent on executing your code and operating system code on behalf of your code, respectively. From this simple benchmark, kernDens is clearly substantially slower than the three other implementations. For more systematic benchmarking of run time, the R package microbenchmark is useful. library(microbenchmark) kern_bench &lt;- microbenchmark( kernDens(psi, 0.2), kernDens_vec(psi, 0.2), kernDens_apply(psi, 0.2), kernDens_outer(psi, 0.2) ) The result stored in kern_bench is a data frame with two columns. The first contains the R expressions evaluated, and the second is the evaluation time measured in nanoseconds. The data frame has 100 rows as each expression by default is evaluated 100 times, but this can be changed using the times argument to microbenchmark. It may not be immediately obvious that kern_bench is a data frame, because when you print it the results are automatically summarized, but the actual data structure is revealed by the R function str. str(kern_bench) ## Classes &#39;microbenchmark&#39; and &#39;data.frame&#39;: 400 obs. of 2 variables: ## $ expr: Factor w/ 4 levels &quot;kernDens(psi, 0.2)&quot;,..: 3 2 3 2 1 1 3 2 4 3 ... ## $ time: num 39532277 19657577 3106378 2756675 51821521 ... A total of 400 evaluations are done for the above benchmark, and these evaluations are done in a random order by default. Measuring evaluation time on a complex system like a modern computer is an empirical science, and the order of evaluation can potentially affect the results as the conditions for the evaluation change over time. The purpose of the randomization is to avoid that the ordering causes systematically misleading results. The microbenchmark package implements some methods for summarizing and printing the results such as the following summary table with times in milliseconds. ## Unit: milliseconds ## expr min lq mean median uq max neval ## kernDens(psi, 0.2) 34.49 35.57 37.54 37.36 38.55 51.8 100 ## kernDens_vec(psi, 0.2) 2.28 2.47 3.16 2.60 3.14 19.7 100 ## kernDens_apply(psi, 0.2) 2.51 2.71 3.82 2.83 3.57 39.5 100 ## kernDens_outer(psi, 0.2) 2.90 3.20 5.68 3.65 5.14 119.8 100 The summary table shows some key statistics like median and mean evaluation times but also extremes and upper and lower quartiles. The distributions of run times can be investigated further using the autoplot function, which is based on ggplot2 and thus easy to modify. autoplot(kern_bench) + geom_jitter(position = position_jitter(0.2, 0), aes(color = expr), alpha = 0.4) + aes(fill = I(&quot;gray&quot;)) + theme(legend.position = &quot;none&quot;) This more refined benchmark study doesn’t change our initial impression from using system.time substantially. The function kernDens is notably slower than the three vectorized implementations, but we are now able to more clearly see the minor differences among them. For instance, kernDens_vec and kernDens_apply have very similar run time distributions, while kernDens_outer clearly has a larger median run time and also a run time distribution that is more spread out to the right. In many cases when we benchmark run time it is of interest to investigate how run time depends on various parameters. This is so for kernel density estimation, where we want to understand how changes in the number of data points, \\(n\\), and the number of grid points, \\(m\\), affect run time. We can still use microbenchmark for running the benchmark experiment, but we will typically process and plot the benchmark data afterwards in a customized way. Figure 2.6: Median run times for the four different implementations of kernel density estimation. The dashed gray line is a reference line with slope 1. Figure 2.6 shows median run times for an experiment with 28 combinations of parameters for each of the four different implementations yielding a total of 112 different R expressions being benchmarked. The number of replications for each expression was set to 40. Again we are confirmed that kernDens is substantially slower than the vectorized implementations for all combinations of \\(n\\) and \\(m\\). However, Figure 2.6 also reveals a new pattern; kernDens_outer appears to scale with \\(n\\) in a slightly different way than the two other vectorized implementations for small \\(n\\). It is comparable to or even a bit faster than kernDens_vec and kernDens_apply for very small data sets, while it becomes slower for the larger data sets. Run time for many algorithms have to a good approximation a dominating power law behavior as a function of typical size parameters, that is, the run time will scale approximately like \\(n \\mapsto C n^a\\) for constants \\(C\\) and \\(a\\) and with \\(n\\) denoting a generic size parameter. Therefore it is beneficial to plot run time using log-log scales and to design benchmark studies with size parameters being equidistant on a log-scale. With approximate power law scaling, the log run time behaves like \\[\\log(C) + a \\log(n),\\] that is, on a log-log scale we see approximate straight lines. The slope reveals the exponent \\(a\\), and two different algorithms for solving the same problem might have different exponents and thus different slopes on the log-log-scale. Two different implementations of the same algorithm should have approximately the same slope but may differ in the constant \\(C\\) depending upon how efficient the particular implementation is in the particular programming language used. Differences in \\(C\\) correspond to vertical translations on the log-log scale. In practice, we will see some deviations from straight lines on the log-log plot for a number of reasons. Writing the run time as \\(C n^a + R(n)\\), the residual term \\(R(n)\\) will often be noticeable or even dominating and positive for small \\(n\\). It is only for large enough \\(n\\) that the power law term, \\(C n^a\\), will dominate. In addition, run time can be affected by hardware constraints such as cache and memory sizes, which can cause abrupt jumps in run time. Using microbenchmark over system.time has two main benefits. First, it handles the replication and randomization automatically, which is convenient. Second, it attempts to provide more accurate timings. The latter is mostly important when we benchmark very fast computations. It can be debated if a median summary of randomly ordered evaluations is the best way to summarize run time. This is due to the way R does memory management. R allocates and deallocates memory automatically and uses garbage collection for the deallocation. This means that computations occasionally, and in a somewhat unpredictable manner, trigger the garbage collector, and as a result a small fraction of the evaluations may take substantially longer time than the rest. The median will typically be almost unaffected, and memory deallocation is thus effectively (and wrongly) disregarded from run time when the median summary is used. This is an argument for using the mean instead of the median, but due to the randomization the computation that triggered the garbage collector might not be the one that caused the memory allocation in the first place. Using the mean instead of the median will therefore smear out the garbage collection run time on all benchmarked expressions. Setting the argument control = list(order = &quot;block&quot;) for microbenchmark will evaluate the expressions in blocks, which in combination with a mean summary more correctly accounts for memory allocation and deallocation in the run time. The downside is that without the randomization the results might suffer from other artefacts. This book will use randomization and median summaries throughout, but we keep in mind that this could underestimate actual average run time depending upon how much memory a given computation requires. Memory usage and how it affects run time by triggering garbage collection will be dealt with via code profiling tools instead. "],
["bandwidth.html", "2.3 Bandwidth selection", " 2.3 Bandwidth selection 2.3.1 Revisiting the rectangular kernel We return to the rectangular kernel and compute the mean squared error. In the analysis it may be helpful to think about \\(n\\) large and \\(h\\) small. Indeed, we will eventually choose \\(h = h_n\\) as a function of \\(n\\) such that as \\(n \\to \\infty\\) we have \\(h_n \\to 0\\). We should also note the \\(f_h(x) = E (\\hat{f}_h(x))\\) is a density, thus \\(\\int f_h(x) \\mathrm{d}x = 1\\). We will assume that \\(f_0\\) is sufficiently differentiable and use a Taylor expansion of the distribution function \\(F_0\\) to get that \\[\\begin{align*} f_h(x) &amp; = \\frac{1}{2h}\\left(F_0(x + h) - F_0(x - h)\\right) \\\\ &amp; = \\frac{1}{2h}\\left(2h f_0(x) + \\frac{h^3}{3} f_0&#39;&#39;(x) + R_0(x,h) \\right) \\\\ &amp; = f_0(x) + \\frac{h^2}{6} f_0&#39;&#39;(x) + R_1(x,h) \\end{align*}\\] where \\(R_1(x, h) = o(h^2)\\). One should note how the quadratic terms in \\(h\\) in the Taylor expansion canceled. This gives the following formula for the squared bias of \\(\\hat{f}_h\\). \\[\\begin{align*} \\mathrm{bias}(\\hat{f}_h(x))^2 &amp; = (f_h(x) - f_0(x))^2 \\\\ &amp; = \\left(\\frac{h^2}{6} f_0&#39;&#39;(x) + R_1(x,h) \\right)^2 \\\\ &amp; = \\frac{h^4}{36} f_0&#39;&#39;(x)^2 + R_2(x,h) \\end{align*}\\] where \\(R(x,h) = o(h^4)\\). For the variance we see from (2.1) that \\[V(\\hat{f}_h(x)) = f_h(x)\\frac{1}{2hn} - f_h(x)^2 \\frac{1}{n}.\\] Integrating the sum of the bias and the variance over \\(x\\) gives the integrated mean squared error \\[\\begin{align*} \\mathrm{MISE}(h) &amp; = \\int \\mathrm{bias}(\\hat{f}_h(x))^2 + V(\\hat{f}_h(x)) \\mathrm{d}x \\\\ &amp; = \\frac{h^4}{36} \\|f_0&#39;&#39;\\|^2_2 + \\frac{1}{2hn} + \\int R(x,h) \\mathrm{d} x - \\frac{1}{n} \\int f_h(x)^2 \\mathrm{d} x. \\end{align*}\\] Since \\(f_h(x) \\leq 1\\), \\[\\int f_h(x)^2 \\mathrm{d} x \\leq \\int f_h(x) \\mathrm{d} x = 1,\\] and the last term is \\(o((nh)^{-1})\\). The second last term is \\(o(h^4)\\) if we can interchange the limit and integration order. It is conceivable that we can do so under suitable assumptions on \\(f_0\\), but we will not pursue those at this place. The sum of the two remaining and asymptotically dominating terms in the formula for MISE is \\[\\mathrm{AMISE}(h) = \\frac{h^4}{36} \\|f_0&#39;&#39;\\|^2_2 + \\frac{1}{2hn},\\] which is known as the asymptotic mean integrated squared error. Clearly, for this to be a useful formula, we must assume \\(\\|f_0&#39;&#39;\\|_2^2 &lt; \\infty\\). In this case the formula for AMISE can be used to find the asymptotic optimal tradeoff between (integrated) bias and variance. We find that \\[\\mathrm{AMISE}&#39;(h) = \\frac{h^3}{9} \\|f_0&#39;&#39;\\|^2_2 - \\frac{1}{2h^2n},\\] and solving for \\(\\mathrm{AMISE}&#39;(h) = 0\\) yields \\[h_n = \\left(\\frac{9}{2 \\|f_0&#39;&#39;\\|_2^2}\\right)^{1/5} n^{-1/5}.\\] We conclude that AMISE has a unique stationary point, and as it tends to \\(\\infty\\) for \\(h \\to 0\\) as well as for \\(h \\to \\infty\\), this stationary point is a unique global minimizer. We see how “wiggliness” of \\(f_0\\) enters into the formula for the optimal bandwidth \\(h_n\\) via \\(\\|f_0&#39;&#39;\\|_2\\). This norm of the second derivative is precisely a quantification of how much \\(f_0\\) oscillates. A large value, indicating a wiggly \\(f_0\\), will drive the optimal bandwidth down whereas a small value will drive the optimal bandwidth up. We should also observe that if we plug the optimal bandwidth into the formula for AMISE, we get \\[\\begin{align*} \\mathrm{AMISE}(h_n) &amp; = \\frac{h_n^4}{36} \\|f_0&#39;&#39;\\|^2_2 + \\frac{1}{2h_n n} \\\\ &amp; = C n^{-4/5}, \\end{align*}\\] which indicates that in terms of integrated mean squared error the rate at which we can nonparametrically estimate \\(f_0\\) is \\(n^{-4/5}\\). This should be contrasted to the common parametric rate of \\(n^{-1}\\) for mean squared error. From a practical viewpoint there is one major problem with the optimal bandwidth \\(h_n\\); it depends via \\(\\|f_0&#39;&#39;\\|^2_2\\) upon the unknown \\(f_0\\) that we are trying to estimate. We therefore refer to \\(h_n\\) as an oracle bandwidth – it is the bandwidth that an oracle that knows \\(f_0\\) would tell us to use. In practice, we will have to come up with an estimate of \\(\\|f_0&#39;&#39;\\|^2_2\\) and plug that estimate into the formula for \\(h_n\\). We pursue a couple of different options for doing so for general kernel density estimators below together with methods that do not rely on the AMISE formula. 2.3.2 ISE, MISE and MSE for kernel estimators Bandwidth selection for general kernel estimators can be studied asymptotically just as above. To this end it is useful to formalize how we quantify the quality of an estimate \\(\\hat{f}_h\\). One natural quantification is the integrated squared error, \\[\\mathrm{ISE}(\\hat{f}_h) = \\int (\\hat{f}_h(x) - f_0(x))^2 \\ \\mathrm{d}x = \\|\\hat{f}_h - f_0\\|_2^2.\\] The quality of the estimation procedure producing \\(\\hat{f}_h\\) from data can then be quantified by taking the mean ISE, \\[\\mathrm{MISE}(h) = E(\\mathrm{ISE}(\\hat{f}_h)),\\] where the expectation integral is over the data. Using Tonelli’s theorem we may interchange the expectation and the integration over \\(x\\) to get \\[\\mathrm{MISE}(h) = \\int \\mathrm{MSE}_x(h) \\ \\mathrm{d}x\\] where \\[\\mathrm{MSE}_h(x) = \\mathrm{var}(\\hat{f}_h(x)) + \\mathrm{bias}(\\hat{f}_h(x))^2.\\] is the pointwise mean squared error. Using the same kind of Taylor expansion argument as above we can show that if \\(K\\) is a square integrable probability density with mean 0 and \\[\\sigma_K^2 = \\int z^2 K(z) \\ \\mathrm{d}z &gt; 0,\\] then \\[\\mathrm{MISE}(h) = \\mathrm{AMISE}(h) + o((nh)^{-1} + h^4)\\] where the asymptotic mean integrated squared error is \\[\\mathrm{AMISE}(h) = \\frac{\\|K\\|_2^2}{nh} + \\frac{h^4 \\sigma^4_K \\|f_0&#39;&#39;\\|_2^2}{4}\\] with \\[\\|g\\|_2^2 = \\int g(z)^2 \\ \\mathrm{d}z \\quad (\\mathrm{squared } \\ L_2\\mathrm{-norm}).\\] Some regularity assumptions on \\(f_0\\) are necessary, and from the result we clearly need to require that \\(f_0&#39;&#39;\\) is meaningful and square integrable. However, that is also enough. See Proposition A.1 in Tsybakov (2009) for a rigorous proof. It is noteworthy that the result and the proof holds even if \\(K\\) is not assumed to be nonnegative, though the resulting estimate may then take negative values and thus not be a valid density. Tsybakov (2009) contains an extensive treatment of results that improve on the rate \\(n^{-4/5}\\) by allowing for kernels that take negative values. By minimizing \\(\\mathrm{AMISE}(h)\\) we derive the optimal oracle bandwidth \\[\\begin{equation} \\tag{2.3} h_n = \\left( \\frac{\\|K\\|_2^2}{\\sigma_K^4 \\|f_0&#39;&#39;\\|_2^2} \\right)^{1/5} n^{-1/5}. \\end{equation}\\] We observe that for the rectangular kernel, \\[\\sigma_K^4 = \\left(\\frac{1}{2} \\int_{-1}^1 z^2 \\ \\mathrm{d} z\\right)^2 = \\frac{1}{9}\\] and \\[\\|K\\|_2^2 = \\frac{1}{2^2} \\int_{-1}^1 \\ \\mathrm{d} z = \\frac{1}{2}.\\] Plugging these numbers into (2.3) we find the oracle bandwidth for the rectangular kernel as derived in Section 2.3.1. For the Gaussian kernel we find that \\(\\sigma_K^4 = 1\\), while \\[\\|K\\|_2^2 = \\frac{1}{2 \\pi} \\int e^{-x^2} \\ \\mathrm{d} x = \\frac{1}{2 \\sqrt{\\pi}}.\\] 2.3.3 Plug-in estimation of the oracle bandwidth Suppose that \\(f_0\\) is Gaussian with mean 0 and variance \\(\\sigma^2\\). Then \\[\\begin{align*} \\|f_0&#39;&#39;\\|^2_2 &amp; = \\frac{1}{2 \\pi \\sigma^2} \\int \\left(\\frac{x^2}{\\sigma^4} - \\frac{1}{\\sigma^2}\\right)^2 e^{-x^2/\\sigma^2} \\ \\mathrm{d}x \\\\ &amp; = \\frac{1}{2 \\sigma^9 \\sqrt{\\pi}} \\frac{1}{\\sqrt{\\pi \\sigma^2}} \\int (x^4 - 2 \\sigma^2 x^2 + \\sigma^4) e^{-x^2/\\sigma^2} \\ \\mathrm{d}x \\\\ &amp; = \\frac{1}{2 \\sigma^9 \\sqrt{\\pi}} (\\frac{3}{4} \\sigma^4 - \\sigma^4 + \\sigma^4) \\\\ &amp; = \\frac{1}{2 \\sqrt{\\pi}} \\frac{3}{4 \\sigma^5}. \\end{align*}\\] If we use the Gaussian kernel as well and plug this formula for the squared 2-norm of the second derivative of the density into the formula for the oracle bandwidth we get \\[h_n = \\left(\\frac{4}{3n}\\right)^{1/5} \\sigma.\\] We can now simply estimate \\(\\sigma\\), using e.g. the empirical standard deviation \\(\\hat{\\sigma}\\), and plug this estimate into the formula above to get an estimate of the oracle bandwidth. It is well known that the empirical standard deviation is sensitive to outliers, and to get a more robust bandwidth estimator, Silverman suggested using the interquartile range to estimate \\(\\sigma\\). In fact, he suggested estimating \\(\\sigma\\) by \\[\\tilde{\\sigma} = \\min\\{\\hat{\\sigma}, \\mathrm{IQR} / 1.35\\}\\] where \\(1.35\\) to two decimals accuracy is the interquartile range, \\(\\Phi^{-1}(0.75) - \\Phi^{-1}(0.25)\\), of the standard Gaussian distribution and IQR is the empirical interquartile range. The bandwidth is then estimated as \\[\\hat{h}_n = \\left(\\frac{4}{3n}\\right)^{1/5} \\tilde{\\sigma},\\] and this estimator has become known as Silverman’s rule of thumb. Even if the derivation assumes that \\(f_0\\) is Gaussian, it could still be a reasonable bandwidth estimator – at least if \\(f_0\\) is unimodal. If \\(f_0\\) is multimodal, Silverman’s rule of thumb is known to oversmooth the density estimate. Silverman’s rule of thumb is the default bandwidth estimator for density as implemented by the function bw.nrd0, though this particular implementation follows an ad hoc suggestion by Silverman to use a factor of 0.9 instead of \\((4/3)^{1/5} \\simeq 1.06\\). The bw.nrd function implements the rule using the factor 1.06. Curiously, Silverman used 1.34 instead of 1.35, and the implementation in R does the same. Instead of computing \\(\\|f_0&#39;&#39;\\|^2_2\\) assuming that the distribution is Gaussian, we can compute the norm for a pilot estimate, \\(\\tilde{f}\\), and plug the result into the formula for \\(h_n\\). If the pilot estimate is a kernel estimate with kernel \\(H\\) and bandwidth \\(r\\) we get \\[\\|\\tilde{f}&#39;&#39;\\|^2_2 = \\frac{1}{n^2r^6} \\sum_{i = 1}^n \\sum_{j=1}^n \\int H&#39;&#39;\\left( \\frac{x - x_i}{r} \\right) H&#39;&#39;\\left( \\frac{x - x_j}{r} \\right) \\mathrm{d} x.\\] The problem is, of course, that now we have to choose the pilot bandwidth \\(r\\). But doing so using a simple method like Silverman’s rule of thumb at this stage is typically not too bad an idea. Thus we arrive at the following plug-in procedure using the Gaussian kernel for the pilot estimate: Compute an estimate, \\(\\hat{r}\\), of the pilot bandwidth using Silverman’s rule of thumb. Compute \\(\\|\\tilde{f}&#39;&#39;\\|^2_2\\) using the Gaussian kernel as pilot kernel \\(H\\) and using the estimated pilot bandwidth \\(\\hat{r}\\). Plug \\(\\|\\tilde{f}&#39;&#39;\\|^2_2\\) into the oracle bandwidth formula (2.3) to compute \\(\\hat{h}_n\\) for the kernel \\(K\\). Note that if we want to use a pilot kernel different from the Gaussian we have to adjust the formulas above for Silverman’s rule of thumb accordingly. Sheather and Jones (1991) took these plug-in ideas a step further and analyzed in detail how to choose the pilot bandwidth in a good and data adaptive way. The resulting method is somewhat complicated but implementable. We skip the details but simply observe that their method is implemented in R in the function bw.SJ, and it can be selected when using density by setting the argument bw = &quot;SJ&quot;. This plug-in method is regarded as a solid default that performs well for many different data generating densities \\(f_0\\). 2.3.4 Cross-validation An alternative to relying on the asymptotic optimality arguments for integrated mean squared error and the corresponding plug-in estimates of the bandwidth is known as cross-validation. The method mimics the idea of setting aside a subset of the data set, which is then not used for computing an estimate but only for validating the estimator’s performance. The benefit of cross-validation over simply setting aside a validation data set is that we don’t “waste” any of the data points on validation only. All data points are used for the ultimate computation of the estimate. The deficit is that cross-validation is usually computationally more demanding. Suppose that \\(I_1, \\ldots, I_k\\) form a partition of the index set \\(\\{1, \\ldots, n\\}\\) and define \\[I^{-i} = \\bigcup_{l: i \\not \\in I_l} I_l.\\] That is, \\(I^{-i}\\) contains all indices but those that belong to the set \\(I_l\\) containing \\(i\\). In particular, \\(i \\not \\in I^{-i}\\). Define also \\(n_i = |I^{-i}|\\) and \\[\\hat{f}^{-i}_h = \\frac{1}{h n_i} \\sum_{j \\in I^{-i}} K\\left(\\frac{x_i - x_j}{h}\\right).\\] That is, \\(\\hat{f}^{-i}_h\\) is the kernel density estimate based on data with indices in \\(I^{-i}\\) and evaluated in \\(x_i\\). Since the density estimate evaluated in \\(x_i\\) is not based on \\(x_i\\), the quantity \\(\\hat{f}^{-i}_h\\) can be used to assess how well the density estimates computed using a bandwidth \\(h\\) concur with the data point \\(x_i\\). This can be summarized using the log-likelihood \\[\\ell_{\\mathrm{CV}}(h) = \\sum_{i=1}^n \\log (\\hat{f}^{-i}_h),\\] that we will refer to as the cross-validated log-likelihood. Contrary to the log-likelihood considered in Section 2.1.1, the cross-validated log-likelihood will not just grow towards \\(\\infty\\) for \\(h \\to 0\\), and we define \\[\\hat{h}_{\\mathrm{CV}} = \\textrm{arg max}_h \\ \\ \\ell_{\\mathrm{CV}}(h).\\] This cross-validation based bandwidth can then be used for computing kernel density estimates using the entire data set. If the partition of indices consists of \\(k\\) subsets we usually talk about \\(k\\)-fold cross-validation. If \\(k = n\\) so that all subsets consist of just a single index we talk about leave-one-out cross-validation. For leave-one-out cross-validation there is only one possible partition, while for \\(k &lt; n\\) there are many possible partitions. Which should be chosen then? In practice, we choose the partition by sampling indices randomly without replacement into \\(k\\) sets of size roughly \\(n / k\\). It is also possible to use cross-validation in combination with MISE. Rewriting we find that \\[\\mathrm{MISE}(h) = E (\\| \\hat{f}_h\\|_2^2) - 2 E (\\hat{f}_h(X)) + E(\\|f_0^2\\|_2^2)\\] for \\(X\\) a random variable independent of the data and with distribution having density \\(f_0\\). The last term does not depend upon \\(h\\) and we can ignore it from the point of view of minimizing MISE. For the first term we have an unbiased estimate in \\(\\| \\hat{f}_h\\|_2^2\\). The middle term can be estimated without bias by \\[\\frac{2}{n} \\sum_{i=1}^n \\hat{f}^{-i}_h,\\] and this leads to the statistic \\[\\mathrm{UCV}(h) = \\| \\hat{f}_h\\|_2^2 - \\frac{2}{n} \\sum_{i=1}^n \\hat{f}^{-i}_h\\] known as the unbiased cross-validation criterion. The corresponding bandwidth estimator is \\[\\hat{h}_{\\mathrm{UCV}} = \\textrm{arg min}_h \\ \\ \\mathrm{UCV}(h).\\] Contrary to the log-likelihood based criterion, this criterion requires the computation of \\(\\| \\hat{f}_h\\|_2^2\\). Bandwidth selection using UCV is implemented in R in the function bw.ucv and can be used with density by setting the argument bw = &quot;ucv&quot;. References "],
["exercises-1.html", "2.4 Exercises", " 2.4 Exercises Kernel density estimation Exercise 2.1 The Epanechnikov kernel is given by \\[K(x) = \\frac{3}{4}(1 - x^2)\\] for \\(x \\in [-1, 1]\\) and 0 elsewhere. Show that this is a probability density with mean zero and compute \\(\\sigma_K^2\\) as well as \\(\\|K\\|_2^2\\). For the following exercises use the log(F12) variable as considered in Exercise 1.4. Exercise 2.2 Use density to compute the kernel density estimate with the Epanechnikov kernel to the log(F12) data. Try different bandwidths. Exercise 2.3 Implement kernel density estimation yourself using the Epanechnikov kernel. Test your implementation by comparing it to density using the log(F12) data. Benchmarking Exercise 2.4 Construct the following vector x &lt;- rnorm(2^13) Then use microbenchmark to benchmark the computation of density(x[1:k], 0.2) for k ranging from \\(2^5\\) to \\(2^{13}\\). Summarize the benchmarking results. Exercise 2.5 Benchmark your own implementation of kernel density estimation using the Epanechnikov kernel. Compare the results to those obtained for density. Exercise 2.6 Experiment with different implementations of kernel evaluation in R using the Gaussian kernel and the Epanechnikov kernel. Use microbenchmark to compare the different implementations. "],
["bivariate.html", "Chapter 3 Bivariate smoothing", " Chapter 3 Bivariate smoothing The focus of this chapter is on estimating how one variable, \\(Y\\), is smoothly related to another, \\(X\\). Thus we are directly aiming for an estimate of (aspects of) the conditional distribution of \\(Y\\) given \\(X\\). If both variables are real valued, we can get a pretty good idea of their relation by simply looking at a scatter plot, and what we are aiming for is also often referred to as scatter plot smoothing. In some cases \\(X\\) represents a random variable, while in other cases, as the temperature example below, \\(X\\) represents a deterministic variable. In the example below \\(X\\) is time, and in other applications \\(X\\) could be fixed by an experimental design. One of the examples that will be used throughout is the monthly and yearly temperatures in Nuuk, Greenland, see Vinther et al. (2006). The updated data is available from the site SW Greenland temperature data. p_Nuuk &lt;- ggplot(Nuuk_year, aes(x = Year, y = Temperature)) + geom_point() p_Nuuk + geom_smooth(se = FALSE) + geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 10), color = &quot;red&quot;, se = FALSE) + geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x, bs = &quot;cr&quot;), color = &quot;purple&quot;, se = FALSE) ggplot(Nuuk, aes(x = Month, y = Temperature)) + geom_line(aes(group = Year), alpha = 0.3) + geom_point(alpha = 0.3) + geom_smooth(color = &quot;red&quot;, se = FALSE) Figure 3.1: Nuuk average yearly temperature in degrees Celsius (left) smoothed using loess (black), a degree 10 polynomial (red) and a smooth spline (purple). Nuuk annual temperature cycles (right) smoothed using a spline. References "],
["nearest-neighbor-smoothers.html", "3.1 Nearest neighbor smoothers", " 3.1 Nearest neighbor smoothers One of the most basic ideas on smoothing bivariate data is to do use a running mean or moving average. This is particularly sensible when the \\(x\\)-values are equidistant, e.g. when the observations constitute a time series such as the Nuuk temperature data. The running mean is an example of the more general nearest neighbor smoothers. Mathematically, the \\(k\\) nearest neighbor smoother in \\(x_i\\) is defined as \\[\\hat{f}_i = \\frac{1}{k} \\sum_{j \\in N_i} y_j\\] where \\(N_i\\) is the set of indices for the \\(k\\) nearest neighbors of \\(x_i\\). This simple idea is actually very general and powerful. It works as long as the \\(x\\)-values lie in a metric space, and by letting \\(k\\) grow with \\(n\\) it is possible to construct consistent nonparametric estimators of regression functions, \\(f(x) = E(Y \\mid X = x)\\), under minimal assumptions. The practical problem is that \\(k\\) must grow slowly in high dimensions, and the estimator is not a panacea. In this chapter we focus exclusively on \\(x\\) being real valued with the ordinary metric used to define the nearest neighbors. The total ordering of the real line adds a couple of extra possibilities to the definition of \\(N_i\\). When \\(k\\) is odd, the symmetric nearest neighbor smoother takes \\(N_i\\) to consist of \\(x_i\\) together with the \\((k-1)/2\\) smaller \\(x_j\\)s closest to \\(x_i\\) and the \\((k-1)/2\\) larger \\(x_j\\)s closest to \\(x_i\\). It is also possible to choose a one-sided smoother with \\(N_i\\) corresponding to the \\(k\\) smaller \\(x_j\\)s closest to \\(x_i\\), in which case the smoother would be known as a causal filter. The symmetric definition of neighbors makes it very easy to handle the neighbors computationally; we don’t need to compute and keep track of the \\(n^2\\) pairwise distances between the \\(x_i\\)s, we only need to sort data according to the \\(x\\)-values. Once data is sorted, \\[N_i = \\{i - (k - 1) / 2, i - (k - 1) / 2 + 1, \\ldots, i - 1 , i, i + 1, \\ldots, i + (k - 1) / 2\\}\\] for \\((k - 1) / 2 \\leq i \\leq n - (k - 1) / 2\\). The symmetric \\(k\\) nearest neighbor smoother is thus a running mean of the \\(y\\)-values when sorted according to the \\(x\\)-values. There are a couple of possibilities for handling the boundaries, one being simply to not define a value of \\(\\hat{f}_i\\) outside of the interval above. With \\(\\hat{\\mathbf{f}}\\) denoting the vector of smoothed values by a nearest neighbor smoother we can observe that it is always possible to write \\(\\hat{\\mathbf{f}} = \\mathbf{S}\\mathbf{y}\\) for a matrix \\(\\mathbf{S}\\). For the symmetric nearest neighbor smoother and with data sorted according to the \\(x\\)-values, the matrix has the following band diagonal form \\[ \\mathbf{S} = \\left( \\begin{array}{cccccccccc} \\frac{1}{5} &amp; \\frac{1}{5} &amp; \\frac{1}{5} &amp; \\frac{1}{5} &amp; \\frac{1}{5} &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{5} &amp; \\frac{1}{5} &amp; \\frac{1}{5} &amp; \\frac{1}{5} &amp; \\frac{1}{5} &amp; 0 &amp; \\ldots &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; \\frac{1}{5} &amp; \\frac{1}{5} &amp; \\frac{1}{5} &amp; \\frac{1}{5} &amp; \\frac{1}{5} &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ldots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; \\frac{1}{5} &amp; \\frac{1}{5} \\\\ \\end{array} \\right) \\] here given for \\(k = 5\\) and with dimensions \\((n - 4) \\times n\\) due to the undefined boundary values. 3.1.1 Linear smoothers A smoother of the form \\(\\hat{\\mathbf{f}} = \\mathbf{S}\\mathbf{y}\\) for a smoother matrix \\(\\mathbf{S}\\), such as the nearest neighbor smoother, is known as a linear smoother. The linear form is often beneficial for theoretical arguments, and many smoothers considered in this chapter will be linear smoothers. For computing \\(\\mathbf{f}\\) there may, however, be many alternatives to forming the matrix \\(\\mathbf{S}\\) and computing the matrix-vector product. Indeed, this is often not the best way to compute the smoothed values. It is, on the other hand, useful to see how \\(\\mathbf{S}\\) can be constructed for the symmetric nearest neighbor smoother. w &lt;- c(rep(1/11, 11), rep(0, 147 - 10)) S &lt;- matrix(w, 147 - 10, 147, byrow = TRUE) ## Warning in matrix(w, 147 - 10, 147, byrow = TRUE): data length [148] is not ## a sub-multiple or multiple of the number of rows [137] The construction above relies on vector recycling of w in the construction of S and the fact that w has length \\(147 + 1\\), which will effectively cause w to be translated by one to the right every time it is recycled for a new row. As seen, the code triggers a warning by R, but in this case we get what we want. S[1:20, 1:20] We can use the matrix to smooth the annual average temperature in Nuuk using a running mean with a window of \\(k = 11\\) years. That is, the smoothed temperature at a given year is the average of the temperatures in the period from five years before to five years after. Note that to add the smoothed values to the previous plot we need to pad the values at the boundaries with NAs to get a vector of length 147. ## Check first if data is sorted correctly. ## The test is backwards, but confirms that data isn&#39;t unsorted :-) is.unsorted(Nuuk_year$Year) ## [1] FALSE f_hat &lt;- c(rep(NA, 5), S %*% Nuuk_year$Temperature, rep(NA, 5)) p_Nuuk + geom_line(aes(y = f_hat), color = &quot;blue&quot;) Figure 3.2: Annual average temperature in Nuuk smoothed using the running mean with \\(k = 11\\) neighbors. 3.1.2 Implementing the running mean The running mean smoother fulfills the following identity \\[\\hat{f}_{i+1} = \\hat{f}_{i} - y_{i - (k-1)/2} + y_{i + (k + 1)/2},\\] which can be used for much more efficient implementation than the matrix-vector multiplication. It should be emphasized again that the identity above and the implementation below assume that data is sorted according to \\(x\\)-values. ## The vector &#39;y&#39; must be sorted according to the x-values runMean &lt;- function(y, k) { n &lt;- length(y) m &lt;- floor((k - 1) / 2) k &lt;- 2 * m + 1 y &lt;- y / k s &lt;- rep(NA, n) s[m + 1] &lt;- sum(y[1:k]) for(i in (m + 1):(n - m - 1)) s[i + 1] &lt;- s[i] - y[i - m] + y[i + 1 + m] s } p_Nuuk + geom_line(aes(y = runMean(Nuuk_year$Temperature, 11)), color = &quot;blue&quot;) Figure 3.3: Annual average temperature in Nuuk smoothed using the running mean with \\(k = 11\\) neighbors. This time using a different implementation than in Figure 3.2. The R function filter (from the stats package) can be used to compute running means and general moving averages using any weight vector. We compare our two implementations to filter. f_hat_filter &lt;- stats::filter(Nuuk_year$Temperature, rep(1/11, 11)) range(f_hat_filter - f_hat, na.rm = TRUE) ## [1] -4.440892e-16 4.440892e-16 range(f_hat_filter - runMean(Nuuk_year$Temperature, 11), na.rm = TRUE) ## [1] -1.332268e-15 4.440892e-16 Note that filter uses the same boundary convention as used in runMean. A benchmark comparison between matrix-vector multiplication, runMean and filter gives the following table with median run times in microseconds. ## expr median ## 1 S1 %*% y[1:512] 326.7990 ## 2 S2 %*% y[1:1024] 1319.8865 ## 3 S3 %*% y[1:2048] 5326.7215 ## 4 S4 %*% y[1:4196] 22840.1370 ## 5 runMean(y[1:512], k = 11) 80.9945 ## 6 runMean(y[1:1024], k = 11) 151.0775 ## 7 runMean(y[1:2048], k = 11) 301.7670 ## 8 runMean(y[1:4196], k = 11) 606.5745 ## 9 stats::filter(y[1:512], rep(1/11, 11)) 87.2600 ## 10 stats::filter(y[1:1024], rep(1/11, 11)) 113.6075 ## 11 stats::filter(y[1:2048], rep(1/11, 11)) 156.5690 ## 12 stats::filter(y[1:4196], rep(1/11, 11)) 255.6385 The matrix-vector computation is clearly much slower than the two alternatives, and the time to construct the \\(\\mathbf{S}\\)-matrix has not even been included in the benchmark above. There is also a difference in how the matrix-vector multiplication scales with the size of data compared to the alternatives. Whenever the data size doubles the run time approximately doubles for both filter and runMean, while it quadruples for the matrix-vector multiplication. This shows the difference between an algorithm that scales like \\(O(n)\\) and an algorithm that scales like \\(O(n^2)\\) as the matrix-vector product does. Despite the fact that filter is actually implementing a more general algorithm than runMean, it is still faster. This reflects the fact that it is implemented in C and compiled. 3.1.3 Choose \\(k\\) by cross-validation Many (linear) smoothers have a natural definition of an “out-of-sample” prediction, that is, how \\(\\hat{f}(x)\\) is computed for \\(x\\) not in the data. If so, it becomes possible to define \\[\\hat{f}^{-i}_i = \\hat{f}^{-i}(x_i)\\] as the prediction at \\(x_i\\) using the smoother computed from data with \\((x_i, y_i)\\) excluded. However, instead of relying on this, we define \\[\\hat{f}^{-i}_i = \\sum_{j \\neq i} \\frac{S_{ij}y_j}{1 - S_{ii}}\\] for any linear smoother. This definition will concur with \\(\\hat{f}^{-i}(x_i)\\) for most smoothers, but this has to be verified case-by-case. The running mean is a little speciel in this respect. In the previous section, the running mean was only considered for odd \\(k\\) and using a symmetric neighbor definition. This is convenient when considering the running mean in the observations \\(x_i\\). When considering the running mean in any other point, a symmetric neighbor definition works better with an even \\(k\\). This is exactly what the definition of \\(\\hat{f}^{-i}_i\\) above amounts to. If \\(\\mathbf{S}\\) is the running mean smoother matrix for an odd \\(k\\), then \\(\\hat{f}^{-i}_i\\) corresponds to symmetric \\((k-1)\\)-nearest neighbor smoothing excluding \\((x_i, y_i)\\) from the data. Using the definition above, we get that the leave-one-out cross-validation squared error criterion becomes \\[\\mathrm{LOOCV} = \\sum_{i=1}^n (y_i - \\hat{f}^{-i}_i)^2 = \\sum_{i=1}^n \\left(\\frac{y_i - \\hat{f}_i}{1 - S_{ii}}\\right)^2.\\] What is important to observe from the identity above is that LOOCV can be computed without actually computing all the \\(\\hat{f}^{-i}_i\\). In some cases the diagonal elements, \\(S_{ii}\\), of the smoother matrix are not computed as part of computing \\(\\hat{f}\\), but it is possible to compute the trace \\[\\mathrm{df} = \\mathrm{trace}(\\mathbf{S}) = \\sum_{i=1}^n S_{ii}\\] easily. If we then replace \\(S_{ii}\\) in the formula by \\(\\mathrm{df} / n\\) we get the generalized cross-validation criterion \\[\\mathrm{GCV} = \\sum_{i=1}^n \\left(\\frac{y_i - \\hat{f}_i}{1 - \\mathrm{df} / n}\\right)^2.\\] For the running mean, LOOCV and GCV are identical as all diagonal elements of the smoother matrix are identical. We disregard boundary values, and to get a comparable quantity across different choices of \\(k\\) we use mean instead of sum in the implementation. loocv &lt;- function(k, y) { f_hat &lt;- runMean(y, k) mean(((y - f_hat) / (1 - 1/k))^2, na.rm = TRUE) } k &lt;- seq(3, 40, 2) CV &lt;- sapply(k, function(kk) loocv(kk, Nuuk_year$Temperature)) k_opt &lt;- k[which.min(CV)] qplot(k, CV) + geom_line() + geom_vline(xintercept = k_opt, color = &quot;red&quot;) Figure 3.4: The leave-one-out cross-validation criterion for the running mean as a function of the number of neighbors \\(k\\). The optimal choice of \\(k\\) is 15, but the LOOCV criterion jumps quite a lot up and down with changing neighbor size, and \\(k = 25\\) gives a rather low value as well. p_Nuuk + geom_line(aes(y = runMean(Nuuk_year$Temperature, 25)), color = &quot;red&quot;) + geom_line(aes(y = runMean(Nuuk_year$Temperature, k_opt)), color = &quot;blue&quot;) Figure 3.5: The \\(k\\)-nearest neighbor smoother with the optimal choice of \\(k\\) based on LOOCV (blue) and with \\(k = 25\\) (red). "],
["kernel-methods.html", "3.2 Kernel methods", " 3.2 Kernel methods "],
["sparse-linear-algebra.html", "3.3 Sparse linear algebra", " 3.3 Sparse linear algebra library(Matrix) bandSparse(15, 15, seq(-2, 2)) ## 15 x 15 sparse Matrix of class &quot;ngCMatrix&quot; ## ## [1,] | | | . . . . . . . . . . . . ## [2,] | | | | . . . . . . . . . . . ## [3,] | | | | | . . . . . . . . . . ## [4,] . | | | | | . . . . . . . . . ## [5,] . . | | | | | . . . . . . . . ## [6,] . . . | | | | | . . . . . . . ## [7,] . . . . | | | | | . . . . . . ## [8,] . . . . . | | | | | . . . . . ## [9,] . . . . . . | | | | | . . . . ## [10,] . . . . . . . | | | | | . . . ## [11,] . . . . . . . . | | | | | . . ## [12,] . . . . . . . . . | | | | | . ## [13,] . . . . . . . . . . | | | | | ## [14,] . . . . . . . . . . . | | | | ## [15,] . . . . . . . . . . . . | | | K &lt;- bandSparse(n, n, seq(-2, 2)) weights &lt;- c(1/3, 1/4, rep(1/5, n - 4), 1/4, 1/3) weights &lt;- c(NA, NA, rep(1/5, n - 4), NA, NA) p_Nuuk &lt;- ggplot(Nuuk_year, aes(Year, Temperature)) + geom_point() p_Nuuk + geom_line(aes(y = as.numeric(K %*% Nuuk_year$Temperature) * weights), color = &quot;red&quot;) When the smoother matrix is sparse, matrix multiplication can be much faster. We will present some benchmark comparisons below. First we compare the run time for the matrix multiplication as.numeric(K %*% Nuuk_year$Temperature) * weights using a sparse matrix (as above) with the run time using a dense matrix. The dense matrix is given as Kdense = as.matrix(K). These run times are compared to using filter. In all computations, \\(k = 5\\). The difference in slopes between dense and sparse matrix multiplication should be noted. This is the difference between \\(O(n^2)\\) and \\(O(n)\\) run time. The run time for the dense matrix multiplication will not change with \\(k\\). For the other two it will increase (linearly) with increasing \\(k\\). For smoothing only once with a given smoother matrix the time to construct the matrix should also be taken into account for fair comparison with filter. It turns out that the function bandSparse is not optimized for the specific running mean banded matrix, and a faster C++ function for this job is given below. #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] List fastBand(int n, int k) { int N = (2 * k + 1) * (n - 2 * k) + 3 * k * k + k; int iter = 0; IntegerVector i(N), p(n + 1); for(int col = 0; col &lt; n; ++col) { p[col] = iter; for(int r = std::max(col - k, 0); r &lt; std::min(col + k + 1, n); ++r) { i[iter] = r; ++iter; } } p[n] = N; return List::create(_[&quot;i&quot;] = i, _[&quot;p&quot;] = p); } And then R function. bandSparseFast &lt;- function(n, k) { n &lt;- as.integer(n) k &lt;- as.integer(k) tmp &lt;- fastBand(n, k) new(&quot;ngCMatrix&quot;, i = tmp$i, p = tmp$p, Dim = c(n, n)) } The construction of the sparse matrix turns out to take up much more time than the matrix-vector multiplication. The run time is still \\(O(n)\\), but the constant is of the order of a factor 16 larger than for filter. With the faster construction of the sparse matrix, the constant is reduced to being of the order 5 larger than for filter. For small \\(n\\) there is some overhead from the constructor of the sparse matrix object even for the faster algorithm. If you implement an algorithm (like a smoother) using linear algebra (e.g. a matrix-vector product) then sparse matrix numerical methods can be useful compared to dense matrix numerical methods. The Matrix package for R implements sparse matrices, and you should always attempt to use methods for constructing the sparse matrix that avoid dense intermediates. But even with a special purpose constructor of a sparse band matrix, sparse linear algebra cannot compete with optimized special purpose algorithms like filter or a C++ implementation of runMean. The filter function even works more generally for kernels (weights) with equidistant data. We conclude this section by verifying that filter actually computes the running mean up to numerical errors. qplot(1:n, as.numeric(K %*% Nuuk_year$Temperature) * weights - c(stats::filter(Nuuk_year$Temperature, rep(1/5, 5)))) + scale_y_continuous(&quot;Difference&quot;) all(as.numeric(K %*% Nuuk_year$Temperature) * weights == c(stats::filter(Nuuk_year$Temperature, rep(1/5, 5)))) ## [1] FALSE all.equal(as.numeric(K %*% Nuuk_year$Temperature) * weights, c(stats::filter(Nuuk_year$Temperature, rep(1/5, 5)))) ## [1] TRUE identical(as.numeric(K %*% Nuuk_year$Temperature) * weights, c(stats::filter(Nuuk_year$Temperature, rep(1/5, 5)))) ## [1] FALSE "],
["onb.html", "3.4 Orthogonal basis expansions", " 3.4 Orthogonal basis expansions 3.4.1 Polynomial expansions Degree 19 polynomial fitted to the temperature data. We can extract the model matrix from the lm-object. intercept &lt;- rep(1/sqrt(n), n) ## To make intercept column have norm one polylm &lt;- lm(Temperature ~ intercept + poly(Year, 19) - 1, data = Nuuk_year) Phi &lt;- model.matrix(polylm) Figure 3.6: The model matrix columns as functions The model matrix is (almost) orthogonal, and estimation becomes quite simple. With an orthogonal model matrix the normal equation reduces to the estimate \\[\\hat{\\beta} = \\Phi^T Y\\] since \\(\\Phi^T \\Phi = I\\). The predicted (or fitted) values are \\(\\Phi \\Phi^T Y\\) with smoother matrix \\(\\mathbf{S} = \\Phi \\Phi^T\\) being a projection. (t(Phi) %*% Nuuk_year$Temperature)[1:10, 1] ## intercept poly(Year, 19)1 poly(Year, 19)2 poly(Year, 19)3 ## -17.2469646 4.9002430 -1.7968913 0.8175400 ## poly(Year, 19)4 poly(Year, 19)5 poly(Year, 19)6 poly(Year, 19)7 ## 5.9668689 1.4265091 -1.9258864 -0.2523581 ## poly(Year, 19)8 poly(Year, 19)9 ## -2.1355117 -0.8046267 coef(polylm)[1:10] ## intercept poly(Year, 19)1 poly(Year, 19)2 poly(Year, 19)3 ## -17.2469646 4.9002430 -1.7968913 0.8175400 ## poly(Year, 19)4 poly(Year, 19)5 poly(Year, 19)6 poly(Year, 19)7 ## 5.9668689 1.4265091 -1.9258864 -0.2523581 ## poly(Year, 19)8 poly(Year, 19)9 ## -2.1355117 -0.8046267 With homogeneous variance \\[\\hat{\\beta}_i \\overset{\\text{approx}}{\\sim} \\mathcal{N}(\\beta_i, \\sigma^2),\\] and for \\(\\beta_i = 0\\) we have \\(P(|\\hat{\\beta}_i| \\geq 1.96\\sigma) \\simeq 0.05.\\) Thresholding: Figure 3.7: Polynomial fit using all 19 basis functions (blue) and using a degree 5 polynomial (red). 3.4.2 Fourier expansions Introducing \\[x_{k,m} = \\frac{1}{\\sqrt{n}} e^{2 \\pi i k m / n},\\] then \\[\\sum_{k=0}^{n-1} |x_{k,m}|^2 = 1\\] and for \\(m_1 \\neq m_2\\) \\[\\sum_{k=0}^{n-1} x_{k,m_1}\\overline{x_{k,m_2}} = 0\\] Thus \\(\\Phi = (x_{k,m})_{k,m}\\) is an \\(n \\times n\\) unitary matrix; \\[\\Phi^*\\Phi = I\\] where \\(\\Phi^*\\) is the conjugate transposed of \\(\\Phi\\). \\(\\hat{\\beta} = \\Phi^* y\\) is the discrete Fourier transform of \\(y\\). It is the basis coefficients in the orthonormal basis given by \\(\\Phi\\); \\[y_k = \\frac{1}{\\sqrt{n}} \\sum_{m=0}^{n-1} \\hat{\\beta}_m e^{2 \\pi i k m / n}\\] or \\(y = \\Phi \\hat{\\beta}.\\) Phi &lt;- outer(0:(n - 1), 0:(n - 1), function(k, m) exp(2 * pi * 1i * (k * m) / n) / sqrt(n)) The matrix \\(\\Phi\\) generates an interesting pattern. Columns in the matrix \\(\\Phi\\): We can estimate by matrix multiplication betahat &lt;- Conj(t(Phi)) %*% Nuuk_year$Temperature # t(Phi) = Phi for Fourier bases betahat[c(1, 2:4, 73, n:(n - 2))] ## [1] -17.2469646+0.0000000i -2.4642887+2.3871189i 3.5481329+0.9099226i ## [4] 1.6721444+0.7413580i 0.0321232+0.7089991i -2.4642887-2.3871189i ## [7] 3.5481329-0.9099226i 1.6721444-0.7413580i For real \\(y\\) it holds that \\(\\hat{\\beta}_0\\) is real, and the symmetry \\[\\hat{\\beta}_{n-m} = \\hat{\\beta}_m^*\\] holds for \\(m = 1, \\ldots, n - 1\\). (For \\(n\\) even, \\(\\hat{\\beta}_{n/2}\\) is real too). Modulus distribution: Note that for \\(m \\neq 0, n/2\\), \\(\\beta_m = 0\\) and \\(y \\sim \\mathcal{N}(\\Phi\\beta, \\sigma^2 I_n)\\) then \\[(\\mathrm{Re}(\\hat{\\beta}_m), \\mathrm{Im}(\\hat{\\beta}_m))^T \\sim \\mathcal{N}\\left(0, \\frac{\\sigma^2}{2} I_2\\right),\\] hence \\[|\\hat{\\beta}_m|^2 = \\mathrm{Re}(\\hat{\\beta}_m)^2 + \\mathrm{Im}(\\hat{\\beta}_m)^2 \\sim \\frac{\\sigma^2}{2} \\chi^2_2,\\] that is, \\(P(|\\hat{\\beta}_m| \\geq 1.73 \\sigma) = 0.05.\\) There is a clear case of multiple testing if we use this threshold at face value, and we would expect around \\(0.05 \\times n/2\\) false positive if there is no signal at all. Lowering the probability using the Bonferroni correction yields a threshold of around \\(2.7 \\sigma\\) instead. Thresholding Fourier: The coefficients are not independent (remember the symmetry), and one can alternatively consider \\[\\hat{\\gamma}_m = \\sqrt{2} \\mathrm{Re}(\\hat{\\beta}_m) \\quad \\text{and} \\quad \\hat{\\gamma}_{n&#39; + m} = - \\sqrt{2} \\mathrm{Im}(\\hat{\\beta}_m)\\] for \\(1 \\leq m &lt; n / 2\\). Here \\(n&#39; = \\lfloor n / 2 \\rfloor\\). Here, \\(\\hat{\\gamma}_0 = \\hat{\\beta}_0\\), and \\(\\hat{\\gamma}_{n/2} = \\hat{\\beta}_{n/2}\\) for \\(n\\) even. These coefficients are the coefficients in a real cosine, \\(\\sqrt{2} \\cos(2\\pi k m / n)\\), and sine, \\(\\sqrt{2} \\sin(2\\pi k m / n)\\), basis expansion, and they are i.i.d. \\(\\mathcal{N}(0, \\sigma^2)\\) distributed. Thresholding Fourier: Figure 3.8: Fourier based smoother by thresholding (blue) and polynomial fit of degree 5 (red). What is the point using the discrete Fourier transform? The point is that the discrete Fourier transform can be computed via the fast Fourier transform (FFT), which has an \\(O(n\\log(n))\\) time complexity. The FFT works optimally for \\(n = 2^p\\). fft(Nuuk_year$Temperature)[1:4] / sqrt(n) ## [1] -17.246965+0.000000i -2.464289+2.387119i 3.548133+0.909923i ## [4] 1.672144+0.741358i betahat[1:4] ## [1] -17.246965+0.000000i -2.464289+2.387119i 3.548133+0.909923i ## [4] 1.672144+0.741358i 3.4.3 Wavelets "],
["splines.html", "3.5 Splines", " 3.5 Splines In the previous section orthogonality of basis functions played an important role for computing basis function expansions efficiently as well as for the statistical assessment of estimated coefficients. This section will deal with bivariate smoothing via basis functions that are not necessarily orthogonal. Though some of the material of this section will apply to any choice of basis, we restrict attention to splines and consider almost exclusively the widely used B-splines (the “B” is for basis). 3.5.1 Smoothing splines To motivate splines we briefly consider the following penalized least squares criterion for finding a smooth approximation to bivariate data: minimize \\[\\begin{equation} L(f) = \\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\|f&#39;&#39;\\|_2^2 \\tag{3.1} \\end{equation}\\] over all twice differentiable functions \\(f\\). The first term is the standard squared error, and we can easily find a smooth function interpolating the \\(y\\)-values (if all the \\(x\\)-values are different), which will thus drive the squared error to 0. The squared 2-norm regularizes the minimization problem so that the minimizer finds a balance between interpolation and having a small second derivative (note that \\(\\|f&#39;&#39;\\|_2 = 0\\) if and only if \\(f\\) is an affine function). The tuning parameter \\(\\lambda\\) controls this balance. It is possible to show that the minimizer of (3.1) is a natural cubic spline with knots in the data points \\(x_i\\). That is, the spline is a \\(C^2\\)-function that equals a third degree polynomial in between the knots. At the knots, the two polynomials that meet fit together up to the second derivative, but they may differ on the third derivative. That the solution is natural means that it has zero second and third derivative at and beyond the two boundary knots. It is not particularly difficult to show that the space of natural cubic splines is a vector space of dimension \\(n\\) if all the \\(x\\)-values are different. It is therefore possible to find a basis of splines, \\(\\varphi_1, \\ldots, \\varphi_n\\), such that the \\(f\\) that minimizes (3.1) is of the form \\[f = \\sum_{i=1}^n \\beta_i \\varphi_i.\\] What is remarkable about this is that the basis (and the finite dimensional vector space it spans) doesn’t depend upon the \\(y\\)-values. Though the optimization is over an infinite dimensional space, the penalization ensures that the minimizer is always in the same finite dimensional space nomatter what \\(y_1, \\ldots, y_n\\) are. Moreover, since (3.1) is a quite natural criterion to minimize to find a smooth function fitting the bivariate data, splines appear as good candidates for producing such smooth fits. On top of that, splines have several computational advantages and are widely used. If we let \\(\\hat{f}_i = \\hat{f}(x_i)\\) with \\(\\hat{f}\\) the minimizer of (3.1), we have in vector notation that \\[\\hat{\\mathbf{f}} = \\boldsymbol{\\Phi}\\hat{\\beta}\\] with \\(\\boldsymbol{\\Phi}_{ij} = \\varphi_j(x_i)\\). The minimizer can be found by observing that \\[\\begin{align} L(\\mathbf{f}) &amp; = (\\mathbf{y} - \\mathbf{f})^T (\\mathbf{y} - \\mathbf{f}) + \\lambda \\| f&#39;&#39; \\|_2^2 \\\\ &amp; = ( \\mathbf{y} - \\boldsymbol{\\Phi}\\beta)^T (\\mathbf{y} - \\boldsymbol{\\Phi}\\beta) + \\lambda \\beta^T \\mathbf{\\Omega} \\beta \\end{align}\\] where \\[\\mathbf{\\Omega}_{ij} = \\langle \\varphi_i&#39;&#39;, \\varphi_j&#39;&#39; \\rangle = \\int \\varphi_i&#39;&#39;(z) \\varphi_j&#39;&#39;(z) \\mathrm{d}z.\\] The matrix \\(\\mathbf{\\Omega}\\) is positive semidefinite by construction, and we refer to it as the penalty matrix. It induces a seminorm on \\(\\mathbb{R}^n\\) so that we can express the seminorm, \\(\\|f&#39;&#39;\\|_2\\), of \\(f\\) in terms of the parameters in the basis expansion using \\(\\varphi_i\\). This is a standard penalized least squares problem, whose solution is \\[\\hat{\\beta} = (\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} + \\lambda \\mathbf{\\Omega})^{-1}\\boldsymbol{\\Phi}^T \\mathbf{y}\\] and with resulting smoother \\[\\hat{\\mathbf{f}} = \\underbrace{\\boldsymbol{\\Phi} ((\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} + \\lambda \\mathbf{\\Omega})^{-1}\\boldsymbol{\\Phi}^T}_{\\mathbf{S}_{\\lambda}} \\mathbf{y}.\\] This linear smoother with smoothing matrix \\(\\mathbf{S}_{\\lambda}\\) based on natural cubic splines gives what is known as a smoothing spline that minimizes (3.1). We will pursue spline based smoothing by minimizing (3.1) but using various B-spline bases that may have more or less than \\(n\\) elements. For the linear algebra, it doesn’t matter what basis we choose as long as \\(\\boldsymbol{\\Phi}_{ij} = \\varphi_j(x_i)\\) and \\(\\mathbf{\\Omega}\\) is given in terms of \\(\\varphi&#39;&#39;_i\\) as above. 3.5.2 Splines in R The splines package in R implements some of the basic functions needed to work with splines. In particular, the splineDesign function that computes evaluations of B-splines and their derivatives. library(splines) ## Note the specification of repeated boundary knots knots &lt;- c(0, 0, 0, seq(0, 1, 0.2), 1, 1, 1) xx &lt;- seq(0, 1, 0.005) B_splines &lt;- splineDesign(knots, xx) matplot(xx, B_splines, type = &quot;l&quot;, lty = 1) Figure 3.9: B-spline basis as computed by splineDesign. The basis shown in Figure 3.9 is an example of a cubic B-spline basis with 11 inner knots in \\(0, 0.1, \\ldots, 0.9, 1\\). The repeated boundary knots control how the spline basis behaves close to the boundaries of the interval. This basis has 13 basis functions, not 11, and spans a larger space than the space of natural cubic splines. It is possible to compute a basis based on B-splines for the natural cubic splines using the function ns, but for all practical purposes this is not important, and we will work exclusively with the B-spline basis itself. The computation of the penalty matrix \\(\\mathbf{\\Omega}\\) constitutes a practical problem, but observing that \\(\\varphi&#39;&#39;_i\\) is an affine function in between knots leads to a simple way of computing \\(\\mathbf{\\Omega}_{ij}\\). Letting \\(g_{ij} = \\varphi&#39;&#39;_i \\varphi&#39;&#39;_j\\) it holds that \\(g_{ij}\\) is quadratic between two consecutive knots \\(a\\) and \\(b\\), in which case \\[\\int_a^b g_{ij}(z) \\mathrm{d}z = \\frac{b - a}{6}\\left(g_{ij}(a) + 4 g_{ij}\\left(\\frac{b-a}{2}\\right) + g_{ij}(b)\\right).\\] This identity is behind Simpson’s rule for numerical integration, and the fact that this is an identity for quadratic polynomials, and not an approximation, means that Simpson’s rule applied appropriately leads to exact computation of \\(\\mathbf{\\Omega}_{ij}\\). All we need is the ability to evaluate \\(\\varphi&#39;&#39;_i\\) at certain points, and splineDesign can be used for that. pen_mat &lt;- function(inner_knots) { knots &lt;- sort(c(rep(range(inner_knots), 3), inner_knots)) d &lt;- diff(inner_knots) ## the vector of knot differences; b - a g_ab &lt;- splineDesign(knots, inner_knots, derivs = 2) knots_mid &lt;- inner_knots[-length(inner_knots)] + d / 2 g_ab_mid &lt;- splineDesign(knots, knots_mid, derivs = 2) g_a &lt;- g_ab[-nrow(g_ab), ] g_b &lt;- g_ab[-1, ] (crossprod(d * g_a, g_a) + 4 * crossprod(d * g_ab_mid, g_ab_mid) + crossprod(d * g_b, g_b)) / 6 } It is laborious to write good tests of pen_mat. We would have to work out a set of example matrices by other means, e.g. by hand. Alternatively, we can compare to a simpler numerical integration technique using Riemann sums. tmp_deriv &lt;- splineDesign(c(0, 0, 0, 0, 0.5, 1, 1, 1, 1), seq(0, 1, 1e-5), derivs = 2) Omega_numeric &lt;- crossprod(tmp_deriv[-1, ]) * 1e-5 ## Right Riemann sums Omega &lt;- pen_mat(c(0, 0.5, 1)) Omega_numeric / Omega ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.9999700 0.9999673 0.999940 1.000000 NaN ## [2,] 0.9999673 0.9999663 0.999955 1.000000 1.000000 ## [3,] 0.9999400 0.9999550 1.000000 1.000045 1.000060 ## [4,] 1.0000000 1.0000000 1.000045 1.000034 1.000033 ## [5,] NaN 1.0000000 1.000060 1.000033 1.000030 range((Omega_numeric - Omega) / (Omega + 0.001)) ## Relative error ## [1] -5.99967e-05 5.99983e-05 And we should also test an example with non-equidistant knots. tmp_deriv &lt;- splineDesign( c(0, 0, 0, 0, 0.2, 0.3, 0.5, 0.6, 0.65, 0.7, 1, 1, 1, 1), seq(0, 1, 1e-5), derivs = 2) Omega_numeric &lt;- crossprod(tmp_deriv[-1, ]) * 1e-5 ## Right Riemann sums Omega &lt;- pen_mat(c(0, 0.2, 0.3, 0.5, 0.6, 0.65, 0.7, 1)) range((Omega_numeric - Omega) / (Omega + 0.001)) ## Relative error ## [1] -0.0001607084 0.0002545494 These examples indicate that pen_mat computes \\(\\mathbf{\\Omega}\\) correctly, in particular as increasing the Riemann sum precision by lowering the number \\(10^{-5}\\) will decrease the relative error (not shown). Of course, correctness ultimately depends on splineDesign computing the correct second derivatives, which hasn’t been tested here. We can also test how our implementation of smoothing splines works on data. We do this here by implementing the matrix-algebra directly for computing \\(\\mathbf{S}_{\\lambda} \\mathbf{y}\\). inner_knots &lt;- Nuuk_year$Year Phi &lt;- splineDesign(c(rep(range(inner_knots), 3), inner_knots), inner_knots) Omega &lt;- pen_mat(inner_knots) smoother &lt;- function(lambda) Phi %*% solve(crossprod(Phi) + lambda * Omega, t(Phi) %*% Nuuk_year$Temperature) p_Nuuk + geom_line(aes(y = smoother(10)), color = &quot;blue&quot;) + ## Undersmooth geom_line(aes(y = smoother(1000)), color = &quot;red&quot;) + ## Smooth geom_line(aes(y = smoother(100000)), color = &quot;purple&quot;) ## Oversmooth Smoothing splines can be computed using the R function smooth.spline from the stats package. It uses generalized cross validation (GCV) by default for automatically choosing the tuning parameter \\(\\lambda\\), but one can also manually specify the amount of smoothing using one of the arguments lambda, spar or df (the latter being the trace of the smoother matrix). Note, however, that due to internal differences from the splineDesign basis above, the lambda argument to smooth.spline does not match the \\(\\lambda\\) parameter above. To compare our results to smooth.spline we therefore optimize the GCV criterion. First we implement a function that computes GCV for a fixed value of \\(\\lambda\\). gcv &lt;- function(lambda, y) { S &lt;- Phi %*% solve(crossprod(Phi) + lambda * Omega, t(Phi)) df &lt;- sum(diag(S)) ## The trace of the smoother matrix sum(((y - S %*% y) / (1 - df / length(y)))^2, na.rm = TRUE) } Then we apply this function to a grid of \\(\\lambda\\)-values and choose the value of \\(\\lambda\\) that minimizes GCV. lambda &lt;- seq(50, 250, 2) GCV &lt;- sapply(lambda, gcv, y = Nuuk_year$Temperature) lambda_opt &lt;- lambda[which.min(GCV)] qplot(lambda, GCV) + geom_vline(xintercept = lambda_opt, color = &quot;red&quot;) Figure 3.10: The generalized cross-validation criterion for smoothing splines as a function of the tuning parameter \\(\\lambda\\). Finally, we can visualize the resulting smoothing spline. temp_smooth_opt &lt;- Phi %*% solve(crossprod(Phi) + lambda_opt * Omega, t(Phi) %*% Nuuk_year$Temperature) p_Nuuk + geom_line(aes(y = temp_smooth_opt), color = &quot;blue&quot;) Figure 3.11: The smoothing spline that minimizes GCV over the tuning parameter \\(\\lambda\\) The smoothing spline that we found by minimizing GCV can be compared to the smoothing spline that smooth.spline computes by minimizing GCV as well. temp_smooth_splines &lt;- smooth.spline(Nuuk_year$Year, Nuuk_year$Temperature, all.knots = TRUE) ## Don&#39;t use heuristic range(temp_smooth_splines$y - temp_smooth_opt) ## [1] -0.000775662 0.001072587 p_Nuuk + geom_line(aes(y = temp_smooth_splines$y), color = &quot;blue&quot;) Figure 3.12: The smoothing spline that minimizes GCV as computed by smooth.spline. The differences between the smoothing spline computed by our implementation and by smooth.spline is hardly detectable visually, and they are at most of the order \\(10^{-3}\\) as computed above. It is possible to further decrease the differences by finding the optimal value of \\(\\lambda\\) with a higher precision, but we will not pursue this here. 3.5.3 Efficient computation with splines Using the full B-spline basis with knots in every observation is computationally heavy and from a practical viewpoint unnecessary. Smoothing using B-splines is therefore often done using a knot-selection heuristic that selects much fewer knots than \\(n\\), in particular if \\(n\\) is large. This is also what smooth.spline does unless all.knots = TRUE. The heuristic for selecting the number of knots is a bit complicated, but it is implemented in the function .nknots.smspl, which can be inspected for details. Once the number of knots gets above 200 it grows extremely slowly with \\(n\\). With the number of knots selected, a common heuristic for selecting their position is to use the quantiles of the distribution of the \\(x\\)-values. That is, with 9 knots, say, the knots are positioned in the deciles (0.1-quantile, 0.2-quantile etc.). This is effectively also what smooth.spline does, and this heuristic places most of the knots where we have most of the data points. Having implemented a knot-selection heuristic that results in \\(p\\) B-spline basis functions, the matrix \\(\\Phi\\) will be \\(n \\times p\\), typically with \\(p &lt; n\\) and with \\(\\Phi\\) of full rank \\(p\\). In this case we derive a way of computing the smoothing spline that is computationally more efficient and numerically more stable than relying on the matrix-algebraic solution above. This is particularly so when we need to compute the smoother for many different \\(\\lambda\\)s to optimize the smoother. As we will show, we are effectively computing a simultaneous diagonalization of the (symmetric) smoother matrix \\(\\mathbf{S}_{\\lambda}\\) for all values of \\(\\lambda\\). The matrix \\(\\Phi\\) has a singular value decomposition \\[\\Phi = \\mathbf{U} D \\mathbf{V}^T\\] where \\(D\\) is diagonal with entries \\(d_1 \\geq d_2 \\geq \\ldots \\geq d_p &gt; 0\\), \\(\\mathbf{U}\\) is \\(n \\times p\\), \\(\\mathbf{V}\\) is \\(p \\times p\\) and both are orthogonal matrices. This means that \\[\\mathbf{U}^T \\mathbf{U} = \\mathbf{V}^T \\mathbf{V} = \\mathbf{V} \\mathbf{V}^T = I\\] is the \\(p \\times p\\) dimensional identity matrix. We find that \\[\\begin{align} \\mathbf{S}_{\\lambda} &amp; = \\mathbf{U}D\\mathbf{V}^T(\\mathbf{V}D^2\\mathbf{V}^T + \\lambda \\mathbf{\\Omega})^{-1} \\mathbf{V}D\\mathbf{U}^T \\\\ &amp; = \\mathbf{U}D (D^2 + \\lambda \\mathbf{V}^T \\mathbf{\\Omega} \\mathbf{V})^{-1} D \\mathbf{U}^T \\\\ &amp; = \\mathbf{U} (I + \\lambda D^{-1} \\mathbf{V}^T \\mathbf{\\Omega} \\mathbf{V} D^{-1})^{-1} \\mathbf{U}^T \\\\ &amp; = \\mathbf{U}(I + \\lambda \\widetilde{\\mathbf{\\Omega}})^{-1} \\mathbf{U}^T, \\end{align}\\] where \\(\\widetilde{\\mathbf{\\Omega}} = D^{-1} \\mathbf{V}^T \\mathbf{\\Omega} \\mathbf{V} D^{-1}\\) is a positive semidefinite \\(p \\times p\\) matrix. By diagonalization, \\[\\widetilde{\\mathbf{\\Omega}} = \\mathbf{W} \\Gamma \\mathbf{W}^T,\\] where \\(\\mathbf{W}\\) is orthogonal and \\(\\Gamma\\) is a diagonal matrix with nonnegative values in the diagonal, we find that \\[\\begin{align} \\mathbf{S}_{\\lambda} &amp; = \\mathbf{U} \\mathbf{W} (I + \\lambda \\Gamma)^{-1} \\mathbf{W}^T \\mathbf{U}^T \\\\ &amp; = \\widetilde{\\mathbf{U}} (I + \\lambda \\Gamma)^{-1} \\widetilde{\\mathbf{U}}^T \\end{align}\\] where \\(\\widetilde{\\mathbf{U}} = \\mathbf{U} \\mathbf{W}\\) is an orthogonal \\(n \\times p\\) matrix. The interpretation of this representation is as follows. First, the coefficients, \\(\\hat{\\beta} = \\widetilde{\\mathbf{U}}^Ty\\), are computed for expanding \\(y\\) in the basis given by the columns of \\(\\mathbf{U}\\). Second, the \\(i\\)th coefficient is shrunk towards 0, \\[\\hat{\\beta}_i(\\lambda) = \\frac{\\hat{\\beta}_i}{1 + \\lambda \\gamma_i}.\\] Third, the smoothed values, \\(\\widetilde{\\mathbf{U}} \\hat{\\beta}(\\lambda)\\), are computed as an expansion using the shrunken coefficients. Thus the smoother works by shrinking the coefficients in the orthonormal basis given by \\(\\widetilde{\\mathbf{U}}\\) toward zero. The coefficients corresponding to the largest eigenvalues \\(\\gamma_i\\) are shrunk relatively more toward zero than those corresponding to the small eigenvalues. We implement the computation of the diagonalization for the Nuuk temperature data using \\(p = 20\\) basis functions (18 inner knots) equidistantly distributed over the range of the years for which we have data. inner_knots &lt;- seq(1867, 2013, length.out = 18) Phi &lt;- splineDesign(c(rep(range(inner_knots), 3), inner_knots), Nuuk_year$Year) Omega &lt;- pen_mat(inner_knots) Phi_svd &lt;- svd(Phi) Omega_tilde &lt;- t(crossprod(Phi_svd$v, Omega %*% Phi_svd$v)) / Phi_svd$d Omega_tilde &lt;- t(Omega_tilde) / Phi_svd$d ## It is safer to use the numerical singular value decomposition (&#39;svd&#39;) ## for diagonalizing a positive semidefinite matrix than to use a ## more general numerical diagonalization implementation such as &#39;eigen&#39;. Omega_tilde_svd &lt;- svd(Omega_tilde) U_tilde &lt;- Phi_svd$u %*% Omega_tilde_svd$u Figure 3.13: The eigenvalues \\(\\gamma_i\\) that determine how much the different basis coefficients in the orthonormal spline expansion are shrunk toward zero. Left plot shows the eigenvalues untransformed, while the right plot shows the eigenvalues log-transformed. Figure 3.14: The columns of \\(\\widetilde{\\mathbf{U}}\\) that consitute an orthonormal basis for computing the spline based smoother. We observe from Figures 3.13 and 3.14 that there are two relatively large eigenvalues corresponding to the two basis functions with erratic behavior close to the boundaries, and there are two eigenvalues that are effectively zero corresponding to the two affine basis functions. In addition, the more oscillating the basis function is, the larger is the corresponding eigenvalue, and the more is the corresponding coefficient shrunk toward zero by the spline smoother. Observe also that \\[\\mathrm{df}(\\lambda) = \\mathrm{trace}(\\mathbf{S}_\\lambda) = \\sum_{i=1}^n \\frac{1}{1 + \\lambda \\gamma_i},\\] which makes it possible to implement GCV without even computing the diagonal entries of \\(\\mathbf{S}_{\\lambda}.\\) "],
["gaussian-processes.html", "3.6 Gaussian processes", " 3.6 Gaussian processes Suppose that \\(X = X_{1:n} \\sim \\mathcal{N}(\\xi_x, \\Sigma_{x})\\) with \\[\\mathrm{cov}(X_i, X_j) = K(t_i - t_j)\\] for a kernel function \\(K\\). With the observation equation \\(Y_i = X_i + \\delta_i\\) for \\(\\delta = \\delta_{1:n} \\sim \\mathcal{N}(0, \\Omega)\\) and \\(\\delta \\perp \\! \\! \\perp X\\) we get \\[(X, Y) \\sim \\mathcal{N}\\left(\\left(\\begin{array}{c} \\xi_x \\\\ \\xi_x \\end{array}\\right), \\left(\\begin{array}{cc} \\Sigma_x &amp; \\Sigma_x \\\\ \\Sigma_x &amp; \\Sigma_x + \\Omega \\end{array} \\right) \\right).\\] Hence \\[E(X \\mid Y) = \\xi_x + \\Sigma_x (\\Sigma_x + \\Omega)^{-1} (Y - \\xi_x).\\] Assuming that \\(\\xi_x = 0\\) the conditional expectation is a linear smoother with smoother matrix \\[S = \\Sigma_x (\\Sigma_x + \\Omega)^{-1}.\\] This is also true if \\(\\Sigma_x (\\Sigma_x + \\Omega)^{-1} \\xi_x = \\xi_x\\). If this identity holds approximately, we can argue that for computing \\(E(X \\mid Y)\\) we don’t need to know \\(\\xi_x\\). If the observation variance is \\(\\Omega = \\sigma^2 I\\) then the smoother matrix is \\[\\Sigma_x (\\Sigma_x + \\sigma^2 I)^{-1} = (I + \\sigma^2 \\Sigma_x^{-1})^{-1}.\\] "],
["the-kalman-filter.html", "3.7 The Kalman filter", " 3.7 The Kalman filter 3.7.1 AR(1)-example Suppose that \\(|\\alpha| &lt; 1\\), \\(X_1 = \\epsilon_1 / \\sqrt{1 - \\alpha^2}\\) and \\[X_i = \\alpha X_{i-1} + \\epsilon_i\\] for \\(i = 2, \\ldots, n\\) with \\(\\epsilon = \\epsilon_{1:n} \\sim \\mathcal{N}(0, \\sigma^2 I)\\). We have \\(\\mathrm{cov}(X_i, X_j) = \\alpha^{|i-j|} / (1 - \\alpha^2)\\), thus we can find \\(\\Sigma_x\\) and compute \\[E(X_n \\mid Y) = ((I + \\sigma^2 \\Sigma_x^{-1})^{-1} Y)_n\\] Figure 3.15: Gaussian smoother matrix with \\(\\alpha = 0.3, 0.9\\), \\(\\sigma^2 = 2, 20\\) Figure 3.16: Smoothers 3.7.2 The Kalman smoother From the identity \\(\\epsilon_i = X_i - \\alpha X_{i-1}\\) it follows that \\(\\epsilon = A X\\) where \\[A = \\left( \\begin{array}{cccccc} \\sqrt{1 - \\alpha^2} &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ -\\alpha &amp; 1 &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; -\\alpha &amp; 1 &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; -\\alpha &amp; 1 \\\\ \\end{array}\\right),\\] This gives \\(I = V(\\epsilon) = A \\Sigma_x A^T\\), hence \\[\\Sigma_x^{-1} = (A^{-1}(A^T)^{-1})^{-1} = A^T A.\\] We have shown that \\[\\Sigma_x^{-1} = \\left( \\begin{array}{cccccc} 1 &amp; -\\alpha &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ -\\alpha &amp; 1 + \\alpha^2 &amp; -\\alpha &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; -\\alpha &amp; 1 + \\alpha^2 &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 1 + \\alpha^2 &amp; -\\alpha \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; -\\alpha &amp; 1 \\\\ \\end{array}\\right).\\] Hence \\[I + \\sigma^2 \\Sigma_x^{-1} = \\left( \\begin{array}{cccccc} \\gamma_0 &amp; \\rho &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\rho &amp; \\gamma &amp; \\rho &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; \\rho &amp; \\gamma &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; \\gamma &amp; \\rho \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; \\rho &amp; \\gamma_0 \\\\ \\end{array}\\right)\\] with \\(\\gamma_0 = 1 + \\sigma^2\\), \\(\\gamma = 1 + \\sigma^2 (1 + \\alpha^2)\\) and \\(\\rho = -\\sigma^2 \\alpha\\) is a tridiagonal matrix. The equation \\[\\left( \\begin{array}{cccccc} \\gamma_0 &amp; \\rho &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\rho &amp; \\gamma &amp; \\rho &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; \\rho &amp; \\gamma &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; \\gamma &amp; \\rho \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; \\rho &amp; \\gamma_0 \\\\ \\end{array}\\right) \\left( \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\\right) = \\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n \\end{array}\\right)\\] can be solved by a forward and backward sweep. Forward sweep: Set \\(\\rho_1&#39; = \\rho / \\gamma_0\\) and \\(y_1&#39; = y_1 / \\gamma_0\\), then recursively \\[\\rho_i&#39; = \\frac{\\rho}{\\gamma - \\rho \\rho_{i-1}&#39;} \\quad \\text{and} \\quad y_i&#39; = \\frac{y_i - \\rho y_{i-1}&#39;}{\\gamma - \\rho \\rho_{i-1}&#39;}\\] for \\(i = 2, \\ldots, n-1\\) and finally \\[y_n&#39; = \\frac{y_n - \\rho y_{n-1}&#39;}{\\gamma_0 - \\rho \\rho_{n-1}&#39;}.\\] By the forward sweep the equation is transformed to \\[\\left( \\begin{array}{cccccc} 1 &amp; \\rho_1&#39; &amp; 0 &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\rho_2&#39; &amp; \\ldots &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\ldots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 1 &amp; \\rho_{n-1}&#39; \\\\ 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 &amp; 1 \\\\ \\end{array}\\right) \\left( \\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \\end{array}\\right) = \\left(\\begin{array}{c} y_1&#39; \\\\ y_2&#39; \\\\ y_3&#39; \\\\ \\vdots \\\\ y_{n-1}&#39; \\\\ y_n&#39; \\end{array}\\right),\\] which is then solved by backsubstitution from below; \\(x_n = y_n&#39;\\) and \\[x_{i} = y_i&#39; - \\rho_{i}&#39; x_{i+1}, \\quad i = n-1, \\ldots, 1.\\] 3.7.3 Implementation #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] NumericVector KalmanSmooth(NumericVector y, double alpha, double sigmasq) { double tmp, gamma0 = 1 + sigmasq, rho = - sigmasq * alpha; double gamma = 1 + sigmasq * (1 + alpha * alpha); int n = y.size(); NumericVector x(n), rhop(n - 1); rhop[0] = rho / gamma0; x[0] = y[0] / gamma0; for(int i = 1; i &lt; n - 1; ++i) { /* Forward sweep */ tmp = (gamma - rho * rhop[i - 1]); rhop[i] = rho / tmp; x[i] = (y[i] - rho * x[i - 1]) / tmp; } x[n - 1] = (y[n - 1] - rho * x[n - 2]) / (gamma0 - rho * rhop[n - 2]); for(int i = n - 2; i &gt;= 0; --i) { /* Backsubstitution */ x[i] = x[i] - rhop[i] * x[i + 1]; } return x; } Result, \\(\\alpha = 0.95\\), \\(\\sigma^2 = 10\\) Comparing results Sigma &lt;- outer(1:n, 1:n, function(i, j) alpha^(abs(i - j))) / (1 - alpha^2) Smooth &lt;- Sigma %*% solve(Sigma + sigmasq * diag(n)) qplot(1:n, Smooth %*% Nuuk_year$Temperature - ySmooth) + ylab(&quot;Difference&quot;) Note that the forward sweep computes \\(x_n = E(X_n \\mid Y)\\), and from this, the backsubstitution solves the smoothing problem of computing \\(E(X \\mid Y)\\). The Gaussian process used here (the AR(1)-process) is not very smooth and nor is the smoothing of the data. This is related to the kernel function \\(K(s) = \\alpha^{|s|}\\) being non-differentiable in 0. Many smoothers are equivalent to a Gaussian process smoother with an appropriate choice of kernel. Not all have a simple inverse covariance matrix and a Kalman filter algorithm. 3.7.4 The Kalman filter #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] NumericVector KalmanFilt(NumericVector y, double alpha, double sigmasq) { double tmp, gamma0 = 1 + sigmasq, rho = - sigmasq * alpha, yp; double gamma = 1 + sigmasq * (1 + alpha * alpha); int n = y.size(); NumericVector x(n), rhop(n); rhop[0] = rho / gamma0; yp = y[0] / gamma0; x[0] = y[0] / (1 + sigmasq * (1 - alpha * alpha)); for(int i = 1; i &lt; n; ++i) { tmp = (gamma - rho * rhop[i - 1]); rhop[i] = rho / tmp; /* Note differences when compared to smoother */ x[i] = (y[i] - rho * yp) / (gamma0 - rho * rhop[i - 1]); yp = (y[i] - rho * yp) / tmp; } return x; } Result, \\(\\alpha = 0.95\\), \\(\\sigma^2 = 10\\) "],
["univariate-random-variables.html", "Chapter 4 Univariate random variables", " Chapter 4 Univariate random variables This chapter will deal with algorithms for simulating observations from a distribution on \\(\\mathbb{R}\\) or any subset thereof. There can be several purposes of doing so, for instance: We want to investigate properties of the distribution. We want to simulate independent realizations of univariate random variables to investigate the distribution of a transformation. We want to use Monte Carlo integration to compute numerically an integral (which could be a probability). In this chapter the focus is on the simulation of a single random variable or an i.i.d. sequence of random variables primarily via various transformations of pseudo random numbers. The pseudo random numbers themselves being approximate simulations of i.i.d. random variables uniformly distributed on \\((0, 1)\\). "],
["pseudo-random-numbers.html", "4.1 Pseudo random numbers", " 4.1 Pseudo random numbers Most simulation algorithms are based on algorithms for generating pseudo random uniformly distributed variables on \\((0, 1)\\). They arise from deterministic integer sequences initiated by a seed. A classical, but now largely obsolete, class of pseudo random integer generators are known as linear congruential generators given by the recursion \\[x_{n+1} = (a x_n + c) \\text{ mod m}.\\] The seed is a value of \\(x_1\\) between \\(0\\) and \\(m - 1\\). The ANSI C standard specifies the choices \\(m = 2^{31}\\), \\(a = 1,103,515,245\\) and \\(c = 12,345\\). However, the linear congruential generators have been superseded by better but more complicated generators. In R you can set the seed and get the same sequence using set.seed that takes and integer argument. The argument will not necessarily be the actual seed, but it will generate a seed from which the sequence of pseudo random numbers will be generated. set.seed(27112015) ## Computes a new seed from an integer oldseed &lt;- .Random.seed ## The actual seed tmp &lt;- runif(1) tmp ## [1] 0.7793288 head(oldseed, 10) ## [1] 403 624 -1660633125 -1167670944 1031453153 ## [6] 815285806 -340883241 546144044 -1074785731 -160910374 head(.Random.seed, 10) ## [1] 403 1 -696993996 -1035426662 -378189083 ## [6] -745352065 -1401016826 -323987056 983360563 297855953 c(tmp, runif(1)) ## [1] 0.7793288 0.5613179 set.seed(27112015) head(.Random.seed, 10) ## [1] 403 624 -1660633125 -1167670944 1031453153 ## [6] 815285806 -340883241 546144044 -1074785731 -160910374 head(oldseed, 10) ## Same as current .Random.seed ## [1] 403 624 -1660633125 -1167670944 1031453153 ## [6] 815285806 -340883241 546144044 -1074785731 -160910374 identical(oldseed, .Random.seed) ## [1] TRUE runif(1) ## Same as tmp ## [1] 0.7793288 It is a research field to develop pseudo random number generators, see ?RNG in R for the algorithms available. The R default (v. 3.5.0) is the 32-bit Mersenne Twister, which generates integers in the range \\[\\{0, 1, \\ldots, 2^{32} -1\\}.\\] It has a long period and all combinations of consecutive integers up to dimension 623 occur equally often in a period. It also has good statistical properties. Pseudo random numbers in \\((0, 1)\\) are returned by runif by division with \\(2^{32}\\) and a fix to prevent the algorithm from returning 0. "],
["transformation-techniques.html", "4.2 Transformation techniques", " 4.2 Transformation techniques If \\(T : \\mathcal{Z} \\to \\mathbb{R}\\) is a map and \\(Z \\in \\mathcal{Z}\\) is a random variable we can simulate, then we can simulate \\(X = T(Z).\\) Theorem 4.1 If \\(F^{\\leftarrow} : (0,1) \\mapsto \\mathbb{R}\\) is the (generalized) inverse of a distribution function and \\(U\\) is uniformly distributed on \\((0, 1)\\) then the distribution of \\[F^{\\leftarrow}(U)\\] has distribution function \\(F\\). The proof of Theorem 4.1 can be found in many textbooks and will be skipped. It is easiest to use this theorem if we have an analytic formula for the inverse distribution function. But even in cases where we don’t it might be useful for simulation anyway if we have a very accurate approximation that is fast to evaluate. The default way R generates samples from \\(\\mathcal{N}(0,1)\\) is, in fact, based on Theorem 4.1 using a technical approximation of \\(\\Phi^{-1}\\) via rational functions. 4.2.1 Sampling from a \\(t\\)-distribution Let \\(Z = (Y, W) \\in \\mathbb{R} \\times (0, \\infty)\\) with \\(Z \\sim \\mathcal{N}(0, 1)\\) and \\(W \\sim \\chi^2_k\\) independent. Define \\(T : \\mathbb{R} \\times (0, \\infty) \\to \\mathbb{R}\\) by \\[T(z,w) = \\frac{z}{\\sqrt{w/k}},\\] then \\[X = T(Z, W) = \\frac{Z}{\\sqrt{W/k}} \\sim t_k.\\] This is how R simulates from a \\(t\\)-distribution with \\(W\\) generated from a gamma distribution with shape parameter \\(k / 2\\) and scale parameter \\(2\\). "],
["reject-samp.html", "4.3 Rejection sampling", " 4.3 Rejection sampling This section deals with a general algorithm for simulating variables from a distribution with density \\(f\\). We call \\(f\\) the target density and the corresponding distribution is called the target distribution. The idea is to simulate proposals from a different distribution with density \\(g\\) (the proposal distribution) and then according to a criterion decide to accept or reject the proposals. It is assumed throughout that the proposal density \\(g\\) is a density fulfilling that \\[\\begin{equation} g(x) = 0 \\Rightarrow f(x) = 0. \\tag{4.1} \\end{equation}\\] Let \\(Y_1, Y_2, \\ldots\\) be i.i.d. with density \\(g\\) on \\(\\mathbb{R}\\) and \\(U_1, U_2, \\ldots\\) be i.i.d. uniformly distributed on \\((0,1)\\) and independent of the \\(Y_i\\)s. Define \\[T(\\mathbf{Y}, \\mathbf{U}) = Y_{\\sigma}\\] with \\[\\sigma = \\inf\\{n \\geq 1 \\mid U_n \\leq \\alpha f(Y_n) / g(Y_n)\\},\\] for \\(\\alpha \\in (0, 1]\\) and \\(f\\) a density. Rejection sampling then consists of simulating independent pairs \\((Y_n, U_n)\\) as long as we reject the proposals \\(Y_n\\) sampled from \\(g\\), that is, as long as \\[U_n &gt; \\alpha f(Y_n) / g(Y_n).\\] The first time we accept a proposal is \\(\\sigma\\), and then we stop the sampling and return the proposal \\(Y_{\\sigma}\\). The result is, indeed, a sample from the distribution with density \\(f\\) as the following theorem states. Theorem 4.2 If \\(\\alpha f(y) \\leq g(y)\\) for all \\(y \\in \\mathbb{R}\\) and \\(\\alpha &gt; 0\\) then the distribution of \\(Y_{\\sigma}\\) has density \\(f\\). Proof. Note that \\(g\\) automatically fulfills (4.1). The formal proof decomposes the event \\((Y_{\\sigma} \\leq y)\\) according to the value of \\(\\sigma\\) as follows \\[\\begin{align} P(Y_{\\sigma} \\leq y) &amp; = \\sum_{n = 1}^{\\infty} P(Y_{n} \\leq y, \\ \\sigma = n) \\\\ &amp; = \\sum_{n = 1}^{\\infty} P(Y_{n} \\leq y, \\ U_n \\leq \\alpha f(Y_n) / g(Y_n)) P(\\sigma &gt; n - 1) \\\\ &amp; = P(Y_{1} \\leq y, \\ U_1 \\leq \\alpha f(Y_1) / g(Y_1)) \\sum_{n = 1}^{\\infty} P(\\sigma &gt; n - 1). \\end{align}\\] By independence of the pairs \\((Y_n, U_n)\\) we find that \\[P(\\sigma &gt; n - 1) = p^{(n-1)}\\] where \\(p = P(U_1 &gt; \\alpha f(Y_1) / g(Y_1))\\), and \\[\\sum_{n = 1}^{\\infty} P(\\sigma &gt; n - 1) = \\sum_{n = 1}^{\\infty} p^{(n-1)} = \\frac{1}{1 - p}.\\] We further find using Tonelli’s theorem that \\[\\begin{align} P(Y_{1} \\leq y, \\ U_1 \\leq \\alpha f(Y_1) / g(Y_1)) &amp; = \\int_{-\\infty}^y \\alpha \\frac{f(z)}{g(z)} g(z) \\mathrm{d}z \\\\ &amp; = \\alpha \\int_{-\\infty}^y f(z) \\mathrm{d} z. \\end{align}\\] It also follows from this, by taking \\(y = \\infty\\), that \\(1 - p = \\alpha\\), and we conclude that \\[P(Y_{\\sigma} \\leq y) = \\int_{-\\infty}^y f(z) \\mathrm{d} z,\\] and the density for the distribution of \\(Y_{\\sigma}\\) is, indeed, \\(f\\). Note that if \\(\\alpha f \\leq g\\) for densities \\(f\\) and \\(g\\), then \\[\\alpha = \\int \\alpha f(x) \\mathrm{d}x \\leq \\int g(x) \\mathrm{d}x = 1,\\] whence it follows automatically that \\(\\alpha \\leq 1\\) whenever \\(\\alpha f\\) is dominated by \\(g\\). The function \\(g/\\alpha\\) is called the envelope of \\(f\\). The tighter the envelope, the smaller is the probability of rejecting a sample from \\(g\\), and this is quantified explicitly by \\(\\alpha\\) as \\(1 - \\alpha\\) is the rejection probability. Thus \\(\\alpha\\) should preferably be as close to one as possible. If \\(f(y) = c q(y)\\) and \\(g(y) = d p(y)\\) for (unknown) normalizing constants \\(c, d &gt; 0\\) and \\(\\alpha&#39; q \\leq p\\) for \\(\\alpha&#39; &gt; 0\\) then \\[\\underbrace{\\left(\\frac{\\alpha&#39; d}{c}\\right)}_{= \\alpha} \\ f \\leq g.\\] The constant \\(\\alpha&#39;\\) may be larger than 1, but from the argument above we know that \\(\\alpha \\leq 1\\), and Theorem 4.2 gives that \\(Y_{\\sigma}\\) has distribution with density \\(f\\). It appears that we need to compute the normalizing constants to implement rejection sampling. However, observe that \\[u \\leq \\frac{\\alpha f(y)}{g(y)} \\Leftrightarrow u \\leq \\frac{\\alpha&#39; q(y)}{p(y)},\\] whence rejection sampling can actually be implemented with knowledge of the unnormalized densities and \\(\\alpha&#39;\\) only and without computing \\(c\\) or \\(d\\). This is one great advantage of rejection sampling. We should note, though, that when we don’t know the normalizing constants, \\(\\alpha&#39;\\) does not tell us anything about how tight the envelope is, and thus how small the rejection probability is. Given two functions \\(q\\) and \\(p\\), how do we then find \\(\\alpha&#39;\\) so that \\(\\alpha&#39; q \\leq p\\)? Consider the function \\[y \\mapsto \\frac{p(y)}{q(y)}\\] for \\(q(y) &gt; 0\\). If this function is lower bounded by a value strictly larger than zero, we can take \\[\\alpha&#39; = \\inf_{y: q(y) &gt; 0} \\frac{p(y)}{q(y)} &gt; 0.\\] We can in practice often find this value by minimizing \\(p(y)/q(y)\\). If the minimum is zero, there is no \\(\\alpha&#39;\\), and \\(p\\) cannot be used to construct an envelope. If the minimum is strictly positive it is the best possible choice of \\(\\alpha&#39;\\). 4.3.1 von Mises distribution Recall the von Mises distribution from Section 1.2.1. It is a distribution on \\((-\\pi, \\pi]\\) with density \\[f(x) \\propto e^{\\kappa \\cos(x - \\mu)}\\] for parameters \\(\\kappa &gt; 0\\) and \\(\\mu \\in (-\\pi, \\pi]\\). Clearly, \\(\\mu\\) is a location parameter, and we fix \\(\\mu = 0\\) in the following. Simulating random variables with \\(\\mu \\neq 0\\) can be achieved by (wrapped) translation of variables with \\(\\mu = 0\\). Thus the target density is \\(f(x) \\propto e^{\\kappa \\cos(x)}\\). In this section we will use the uniform distribution on \\((-\\pi, \\pi)\\) as proposal distribution. It has constant density \\(g(x) = (2\\pi)^{-1}\\), but all we need is, in fact, that \\(g(x) \\propto 1\\). Since \\(x \\mapsto 1 / \\exp(\\kappa \\cos(x)) = \\exp(-\\kappa \\cos(x))\\) attains its minimum \\(\\exp(-\\kappa)\\) for \\(x = 0\\), we find that \\[\\alpha&#39; e^{\\kappa \\cos(x)} = e^{\\kappa(\\cos(x) - 1)} \\leq 1,\\] with \\(\\alpha&#39; = \\exp(-\\kappa)\\). The rejection test of the proposal \\(Y \\sim g\\) can therefore be carried out by testing if a uniformly distributed random variable \\(U\\) on \\((0,1)\\) satisfies \\[U &gt; e^{\\kappa(\\cos(Y) - 1)}.\\] vMsim_slow &lt;- function(n, kappa) { y &lt;- numeric(n) for(i in 1:n) { reject &lt;- TRUE while(reject) { y0 &lt;- runif(1, - pi, pi) u &lt;- runif(1) reject &lt;- u &gt; exp(kappa * (cos(y0) - 1)) } y[i] &lt;- y0 } y } f &lt;- function(x, k) exp(k * cos(x)) / (2 * pi * besselI(k, 0)) x &lt;- vMsim_slow(100000, 0.5) hist(x, breaks = seq(-pi, pi, length.out = 20), prob = TRUE) curve(f(x, 0.5), -pi, pi, col = &quot;blue&quot;, lwd = 2, add = TRUE) x &lt;- vMsim_slow(100000, 2) hist(x, breaks = seq(-pi, pi, length.out = 20), prob = TRUE) curve(f(x, 2), -pi, pi, col = &quot;blue&quot;, lwd = 2, add = TRUE) Figure 4.1: Histograms of 100,000 simulated data points from von Mises distributions with parameters \\(\\kappa = 0.5\\) (left) and \\(\\kappa = 2\\) (right). The true densities (blue) are added to the plots. Figure 4.1 confirms that the implementation simulates from the von Mises distribution. system.time(vMsim_slow(100000, kappa = 5)) ## user system elapsed ## 2.490 0.388 2.939 Though the implementation can easily simulate 100,000 variables in a couple of seconds, it might still be possible to improve it. To investigate what most of the run time is spent on we use the line profiling tool as implemented in the profvis package. library(profvis) profvis(vMsim_slow(10000, 5)) The profiling result shows that almost all the time is spent on simulating uniformly distributed random variables. It is, perhaps, expected that this should take some time, but that it takes so much more time than computing the ratio, say, used for the rejection test is a bit surprising. What might be even more surprising is the large amount of memory allocation and deallocation associated with the simulation of the variables. The culprit is runif that has some overhead associated with each call. The function performs much better if called once to return a vector than if called repeatedly as above to return just single numbers. We could rewrite the rejection sampler to make better use of runif, but it would make the code a bit more complicated because we don’t know upfront how many uniform variables we need. This will introduce some bookkeeping that it is possible to abstract away from the implementation of any rejection sampler. Therefore we implement a generic wrapper of the random number generator that will cache a suitable amount of random variables. This function will take care of some bookkeeping and variables can then be extracted as needed. This also nicely illustrates a use of closures and a function factory. rng_stream &lt;- function(m, rng, ...) { args &lt;- list(...) cache &lt;- do.call(rng, c(m, args)) j &lt;- 0 fact &lt;- 1 next_rn &lt;- function(r = m) { j &lt;&lt;- j + 1 if(j &gt; m) { if(fact == 1 &amp;&amp; r &lt; m) fact &lt;&lt;- m / (m - r) m &lt;&lt;- floor(fact * (r + 1)) cache &lt;&lt;- do.call(rng, c(m, args)) j &lt;&lt;- 1 } cache[j] } next_rn } The implementation above is a function that returns a function. The returned function, next_rn comes with its own environment, where it stores the cached variables and extracts and returns one variable whenever called. It generates a new vector of random variables whenever it “runs out”. The first time it does so, the function estimates a factor of how many variables is needed in total based on the argument r, and then it generates the estimated number of variables needed. This may be repeated a couple of times. We can then reimplement vMsim using rng_stream. For later usage we add the possibility of printing out some tracing information. vMsim &lt;- function(n, kappa, trace = FALSE) { count &lt;- 0 y &lt;- numeric(n) y0 &lt;- rng_stream(n, runif, - pi, pi) u &lt;- rng_stream(n, runif) for(i in 1:n) { reject &lt;- TRUE while(reject) { count &lt;- count + 1 z &lt;- y0(n - i) reject &lt;- u(n - i) &gt; exp(kappa * (cos(z) - 1)) } y[i] &lt;- z } if(trace) cat(&quot;kappa =&quot;, kappa, &quot;:&quot;, (count - n)/ count, &quot;\\n&quot;) ## Rejection frequency y } We should, of course, remember to test that the new implementation still generates variables from the von Mises distribution. x &lt;- vMsim(100000, 0.5) hist(x, breaks = seq(-pi, pi, length.out = 20), prob = TRUE) curve(f(x, 0.5), -pi, pi, col = &quot;blue&quot;, lwd = 2, add = TRUE) x &lt;- vMsim(100000, 2) hist(x, breaks = seq(-pi, pi, length.out = 20), prob = TRUE) curve(f(x, 2), -pi, pi, col = &quot;blue&quot;, lwd = 2, add = TRUE) Figure 4.2: Histograms of 100,000 simulated data points from von Mises distributions with parameters \\(\\kappa = 0.5\\) (left) and \\(\\kappa = 2\\) (right), simulated using vectorized generation of random variables. Then we can compare the run time of this new implementation to the run time of the first implementation. system.time(vMsim(100000, kappa = 5)) ## user system elapsed ## 0.573 0.008 0.581 As we see from the time estimate above, using a vectorized call of runif reduces the run time by a factor 4-5. It is possible to get a further factor 2-3 run time improvement (not shown) by implementing the computations done by rng_stream directly inside vMsim. However, we prioritize here to have modular code so that we can reuse rng_stream for other rejection samplers without repeating code. A pure R implementation based on a loop will never be able to compete with a C++ implementation anyway when the accept-reject step is such a simple computation. In fact, to write a pure R function that is run time efficient, we need to turn the entire rejection sampler into a vectorized computation. That is, it is not just the generation of random numbers that need to be vectorized. There is no way around some form of loop as we don’t known upfront how many rejections there will be. We can, however, benefit from the ideas in rng_stream on how to estimate the fraction of acceptances from a first round, which can be used for subsequent simulations. This is done in the following fully vectorized R implementation. vMsim_vec &lt;- function(n, kappa) { fact &lt;- 1 j &lt;- 1 l &lt;- 0 ## The number of accepted samples y &lt;- list() while(l &lt; n) { m &lt;- floor(fact * (n - l)) ## is n the first time y0 &lt;- runif(m, - pi, pi) u &lt;- runif(m) accept &lt;- u &lt;= exp(kappa * (cos(y0) - 1)) l &lt;- l + sum(accept) y[[j]] &lt;- y0[accept] j &lt;- j + 1 if(fact == 1) fact &lt;- n / l } unlist(y)[1:n] } The implementation above incrementally grows a list, whose entries contain vectors of accepted samples. It is usually not advisable to dynamically grow objects (vectors or list), as this will lead to a lot of memory allocation, copying and deallocation. Thus it is better to initialize a vector of the correct size upfront. In this particular case the list will only contain few entries, and it is inconsequential that it is grown dynamically. Finally, a C++ implementation via Rcpp is given below where the random variables are then again generated one at a time via the C-interface to R’s random number generators. There is no (substantial) overhead of doing so in C++. #include &lt;Rcpp.h&gt; using namespace Rcpp; // [[Rcpp::export]] NumericVector vMsim_cpp(int n, double kappa) { NumericVector y(n); double y0; bool reject; for(int i = 0; i &lt; n; ++i) { do { y0 = R::runif(- M_PI, M_PI); reject = R::runif(0, 1) &gt; exp(kappa * (cos(y0) - 1)); } while(reject); y[i] = y0; } return y; } Figure 4.3: Histograms of 100,000 simulated data points from von Mises distributions with parameters \\(\\kappa = 0.5\\) (left) and \\(\\kappa = 2\\) (right), simulated using the Rcpp implementation (top) and the fully vectorized R implementation (bottom). Figure 4.3 shows the results from testing the C++ implementation and the fast R implementation, and confirms that the implementations do simulate from the von Mises distribution. We conclude by measurering the run time of the implementations using system.time and a combined microbenchmark of all four different implementations. system.time(vMsim_cpp(100000, kappa = 5)) ## user system elapsed ## 0.043 0.000 0.048 microbenchmark( vMsim_slow(1000, kappa = 5), vMsim(1000, kappa = 5), vMsim_vec(1000, kappa = 5), vMsim_cpp(1000, kappa = 5) ) ## Unit: microseconds ## expr min lq mean median uq max neval ## vMsim_slow(1000, kappa = 5) 17271 22592 25166 25428 27258 51018 100 ## vMsim(1000, kappa = 5) 5284 5797 6597 6027 6471 40792 100 ## vMsim_vec(1000, kappa = 5) 447 480 807 506 575 20069 100 ## vMsim_cpp(1000, kappa = 5) 312 342 395 350 376 2147 100 The C++ implementation is only a factor 1.5 faster than the fully vectorized R implementation, while it is around a factor 15 faster than the loop-based vMsim and a factor 85 or so faster than the first implementation vMsim_slow. Rejection sampling is a good example of an algorithm for which a naive loop-based R implementation performs rather poorly in terms of run time, while a completely vectorized implementation is competitive with an Rcpp implementation. 4.3.2 Gamma distribution It may be possible to find a suitable envelope of the density for the gamma distribution on \\((0, \\infty)\\), but it turns out that there is a very efficient rejection sampler of a non-standard distribution that can be transformed into a gamma distribution by a simple transformation. Let \\(t(y) = a(1 + by)^3\\) for \\(y \\in (-b^{-1}, \\infty)\\), then \\(t(Y) \\sim \\Gamma(r,1)\\) if \\(r \\geq 1\\) and \\(Y\\) has density \\[f(y) \\propto t(y)^{r-1}t&#39;(y) e^{-t(y)} = e^{(r-1)\\log t(y) + \\log t&#39;(y) - t(y)}.\\] The proof of this follows from a simple univariate density transformation theorem, but see also the original paper Marsaglia and Tsang (2000) that proposed the rejection sampler discussed in this section. The density \\(f\\) will be the target density for a rejection sampler. With \\[f(y) \\propto e^{(r-1)\\log t(y) + \\log t&#39;(y) - t(y)},\\] \\(a = r - 1/3\\) and \\(b = 1/(3 \\sqrt{a})\\) \\[f(y) \\propto e^{a \\log t(y)/a - t(y) + a \\log a} \\propto \\underbrace{e^{a \\log t(y)/a - t(y) + a}}_{q(y)}.\\] An analysis of \\(w(y) := - y^2/2 - \\log q(y)\\) shows that it is convex on \\((-b^{-1}, \\infty)\\) and it attains its minimum in \\(0\\) with \\(w(0) = 0\\), whence \\[q(y) \\leq e^{-y^2/2}.\\] This gives us an envelope expressed in terms of unnormalized densities with \\(\\alpha&#39; = 1\\). The implementation of a rejection sampler based on this analysis is relatively straightforward. The rejection sampler will simulate from the distribution with density \\(f\\) by simulating from the Gaussian distribution (the envelope). For the rejection step we need to implement \\(q\\). Finally, we also need to implement \\(t\\) to transform the result from the rejection sampler to be gamma distributed. The rejection sampler is otherwise implemented as for the non-vectorized von Mises distribution. To investigate rejection probabilities below we additionally implement the possibility of printing out some tracing information. ## r &gt;= 1 tfun &lt;- function(y, a) { b &lt;- 1 / (3 * sqrt(a)) (y &gt; -1/b) * a * (1 + b * y)^3 ## 0 when y &lt;= -1/b } qfun &lt;- function(y, r) { a &lt;- r - 1/3 tval &lt;- tfun(y, a) exp(a * log(tval / a) - tval + a) } gammasim &lt;- function(n, r, trace = FALSE) { count &lt;- 0 y &lt;- numeric(n) y0 &lt;- rng_stream(n, rnorm) u &lt;- rng_stream(n, runif) for(i in 1:n) { reject &lt;- TRUE while(reject) { count &lt;- count + 1 z &lt;- y0(n - i) reject &lt;- u(n - i) &gt; qfun(z, r) * exp(z^2/2) } y[i] &lt;- z } if(trace) cat(&quot;r =&quot;, r, &quot;:&quot;, (count - n)/ count, &quot;\\n&quot;) ## Rejection frequency tfun(y, r - 1/3) } We test the implementation by simulating \\(100,000\\) values with parameters \\(r = 8\\) as well as \\(r = 1\\) and compare the resulting histograms to the respective theoretical densities. Figure 4.4: Histograms of simulated gamma distributed variables with shape parameters \\(r = 8\\) (left) and \\(r = 1\\) (right) with corresponding theoretical densities (blue). Though this is only a simple and informal test, it indicates that the implementation correctly simulates from the gamma distribution. Rejection sampling can be computationally expensive if many samples are rejected. A very tight envelope will lead to fewer rejections, while a loose envelope will lead to many rejections. Using the tracing option as implemented we obtain estimates of the rejection probability and thus a quantification of how tight the envelope is. y &lt;- gammasim(100000, 16, trace = TRUE) y &lt;- gammasim(100000, 8, trace = TRUE) y &lt;- gammasim(100000, 4, trace = TRUE) y &lt;- gammasim(100000, 1, trace = TRUE) ## r = 16 : 0.001826657 ## r = 8 : 0.003716139 ## r = 4 : 0.008064436 ## r = 1 : 0.04866099 We observe that the rejection frequencies are small with \\(r = 1\\) being the worst case with around 5% rejections. For the other cases the rejection frequencies are all below 1%, thus rejection is rare. A visual comparison of \\(q\\) to the (unnormalized) Gaussian density also shows that the two (unnormalized) densities are very close except in the tails where there is very little probability mass. Figure 4.5: Comparisons of the Gaussian proposal (red) and the target density (blue) used for eventually simulating gamma distributed variables via a transformation. References "],
["adaptive.html", "4.4 Adaptive envelopes", " 4.4 Adaptive envelopes A good envelope should be tight, meaning that \\(\\alpha\\) is close to one, it should be fast to simulate from and have a density that is fast to evaluate. It is not obvious how to find such an envelope for an arbitrary target density \\(f\\). This section develops a general scheme for the construction of envelopes for all log-concave target densities. This is a special class of densities, but it is not uncommon in practice. The scheme can also be extended to work for some densities that have combinations of log-concave and log-convex behaviors. The same idea used for constructing envelopes can be used to bound \\(f\\) from below. The accept-reject step can then avoid many evaluations of \\(f\\), which is beneficial if \\(f\\) is computationally expensive to evaluate. The key idea of the scheme is to bound the log-density by piecewise affine functions. This is particularly easy to do if the density is log-concave. The scheme leads to analytically manageable formulas for the envelope, its corresponding distribution function and its inverse, and as a result it is fast to simulate proposals and compute the envelope as needed in the accept-reject step. The scheme requires the choice of a finite number of points to determine the affine bounds. For any given choice of points the scheme adapts the envelope to the target density automatically. It is possible to implement a fully adaptive scheme that doesn’t even require the choice of points but initializes and updates the points dynamically as more and more rejection samples are computed. In this section the focus is on the scheme with a given and fixed number of points. For a continuously differentiable, strictly positive and log-concave target on an open interval \\(I \\subseteq \\mathbb{R}\\) it holds that \\[\\log(f(x)) \\leq \\frac{f&#39;(x_0)}{f(x_0)}(x - x_0) + \\log(f(x_0))\\] for any \\(x, x_0 \\in I\\). Let \\(x_1 &lt; x_2 &lt; \\ldots &lt; x_{m} \\in I\\) and let \\(I_1, \\ldots, I_m \\subseteq I\\) be intervals that form a partition of \\(I\\) such that \\(x_i \\in I_i\\). Defining \\[a_i = (\\log(f(x_i)))&#39; = \\frac{f&#39;(x_i)}{f(x_i)} \\quad \\text{and} \\quad b_i = \\log(f(x_i)) - \\alpha_i x_i\\] we find the upper bound \\[\\log(f(x)) \\leq V(x) = \\sum_{i=1}^m (a_i x + b_i) 1_{I_i}(x),\\] or \\[f(x) \\leq e^{V(x)}.\\] Note that by the log-concavity of \\(f\\), \\(a_1 \\geq a_2 \\geq \\ldots \\geq a_m\\). The upper bound is integrable over \\(I\\) if either \\(a_1 &gt; 0\\) and \\(a_m &lt; 0\\), or \\(a_m &lt; 0\\) and \\(I\\) is bounded to the left, or \\(a_1 &gt; 0\\) and \\(I\\) is bounded to the right. In any of these cases we define \\[c = \\int_I e^{V(x)} \\mathrm{d} x &lt; \\infty\\] and \\(g(x) = c^{-1} \\exp(V(x))\\), and we find that with \\(\\alpha = c^{-1}\\) then \\(\\alpha f \\leq g\\) and \\(g\\) is an envelope of \\(f\\). Note that it is actually not necessary to compute \\(c\\) (or \\(\\alpha\\)) to implement the rejection step in the rejection sampler, but that \\(c\\) is needed for the simulating from \\(g\\) as described below. We will assume in the following that \\(c &lt; \\infty\\). The intervals \\(I_i\\) have not been specified, and we could, in fact, implement rejection sampling with any choice of intervals fulfilling the conditions above. But in the interest of maximizing \\(\\alpha\\) (minimizing \\(c\\)) and thus minimizing the rejection frequency, we should choose \\(I_i\\) so that \\(a_i x + b_i\\) is minimal over \\(I_i\\) among all the affine upper bounds. This will result in the tightest envelope. This means that for \\(i = 1, \\ldots, m - 1\\), \\(I_i = (z_{i-1}, z_i]\\) with \\(z_i\\) the point where \\(a_i x + b_i\\) and \\(a_{i+1} x + b_{i+1}\\) intersect. We find that the solution of \\[a_i x + b_i = a_{i+1} x + b_{i+1}\\] is \\[z_i = \\frac{b_{i+1} - b_i}{a_i - a_{i+1}}\\] provided that \\(a_{i+1} &gt; a_i\\). The two extremes, \\(z_0\\) and \\(z_m\\), are chosen as the endpoints of \\(I\\) and may be \\(- \\infty\\) and \\(+ \\infty\\), respectively. One way to simulate from such envelopes is by transformation of uniform random variables by the inverse distribution function. It requires a little bookkeeping, but is otherwise straightforward. Define for \\(x \\in I_i\\) \\[F_i(x) = \\int_{z_{i-1}}^x e^{a_i z + b_i} \\mathrm{d} z,\\] and let \\(R_i = F_i(z_i)\\). Then \\(c = \\sum_{i=1}^m R_i\\), and if we define \\(Q_i = \\sum_{k=1}^{i} R_k\\) for \\(i = 0, \\ldots, m\\) the inverse of the distribution function in \\(q\\) is given as the solution of the equation \\[F_i(x) = cq - Q_{i-1}, \\qquad Q_{i-1} &lt; cq \\leq Q_{i}.\\] That is, for a given \\(q \\in (0, 1)\\), first determine which interval \\((Q_{i-1}, Q_{i}]\\) that \\(c q\\) falls into, and then solve the corresponding equation. Observe that when \\(a_i \\neq 0\\), \\[F_i(x) = \\frac{1}{a_i}e^{b_i}\\left(e^{a_i x} - e^{a_i z_{i-1}}\\right).\\] 4.4.1 Beta distribution To illustrate the envelope construction above for a simple log-concave density we consider the Beta distribution on \\((0, 1)\\) with shape parameters \\(\\geq 1\\). This distribution has density \\[f(x) \\propto x^{\\alpha - 1}(1-x)^{\\beta - 1},\\] which is log-concave (when the shape parameters are greater than one). We implement the rejection sampling algorithm for this density with the adaptive envelope using two points. Betasim &lt;- function(n, x1, x2, alpha, beta) { lf &lt;- function(x) (alpha - 1) * log(x) + (beta - 1) * log(1 - x) lf_deriv &lt;- function(x) (alpha - 1)/x - (beta - 1)/(1 - x) a1 &lt;- lf_deriv(x1) a2 &lt;- lf_deriv(x2) if(a1 == 0 || a2 == 0 || a1 - a2 == 0) stop(&quot;\\nThe implementation requires a_1 and a_2 different and both different from zero. Choose different values of x_1 and x_2.&quot;) b1 &lt;- lf(x1) - a1 * x1 b2 &lt;- lf(x2) - a2 * x2 z1 &lt;- (b2 - b1) / (a1 - a2) Q1 &lt;- exp(b1) * (exp(a1 * z1) - 1) / a1 c &lt;- Q1 + exp(b2) * (exp(a2 * 1) - exp(a2 * z1)) / a2 y &lt;- numeric(n) uy &lt;- rng_stream(n, runif) u &lt;- rng_stream(n, runif) for(i in 1:n) { reject &lt;- TRUE while(reject) { u0 &lt;- c * uy(n - i) if(u0 &lt; Q1) { z &lt;- log(a1 * exp(-b1) * u0 + 1) / a1 reject &lt;- u(n - i) &gt; exp(lf(z) - a1 * z - b1) } else { z &lt;- log(a2 * exp(-b2) * (u0 - Q1) + exp(a2 * z1)) / a2 reject &lt;- u(n - i) &gt; exp(lf(z) - a2 * z - b2) } } y[i] &lt;- z } y } Figure 4.6: Histograms of simulated variables from Beta distributions using the rejection sampler with the adaptive envelope based on log-concavity. The true density (blue) and the envelope (red) are added to the plots. Note that as a safeguard we implemented a test on the \\(a_i\\)s to check that the formulas used are actually meaningful, specifically that there are no divisions by zero. Betasim(1, x1 = 0.25, x2 = 0.75, alpha = 4, beta = 2) ## Error in Betasim(1, x1 = 0.25, x2 = 0.75, alpha = 4, beta = 2): ## This implementation requires a_1 and a_2 different ## and both different from zero. Choose different values of x_1 and x_2. Betasim(1, x1 = 0.2, x2 = 0.75, alpha = 4, beta = 2) ## Error in Betasim(1, x1 = 0.2, x2 = 0.75, alpha = 4, beta = 2): ## This implementation requires a_1 and a_2 different ## and both different from zero. Choose different values of x_1 and x_2. Betasim(1, x1 = 0.2, x2 = 0.8, alpha = 4, beta = 2) ## [1] 0.7668483 4.4.2 von Mises distribution The von Mises rejection sampler in Section 4.3.1 used the uniform distribution as proposal distribution. As it turns out, the uniform density is not a particularly tight envelope. We illustrate this by studying the proportion of rejections for our previous implementation. y &lt;- vMsim(10000, 0.1, trace = TRUE) y &lt;- vMsim(10000, 0.5, trace = TRUE) y &lt;- vMsim(10000, 2, trace = TRUE) y &lt;- vMsim(10000, 5, trace = TRUE) ## kappa = 0.1 : 0.09453097 ## kappa = 0.5 : 0.3473437 ## kappa = 2 : 0.6954562 ## kappa = 5 : 0.8169939 The rejection frequency is high and increases with \\(\\kappa\\). For \\(\\kappa = 5\\) more than 80% of the proposals are rejected, and simulating \\(n = 10,000\\) von Mises distributed variables thus requires the simulation of around \\(50,000\\) variables from the proposal. The von Mises density is, unfortunately, not log-concave on \\((-\\pi, \\pi)\\), but it is on \\((-\\pi/2, \\pi/2)\\). It is, furthermore, log-convex on \\((-\\pi, -\\pi/2)\\) as well as \\((\\pi/2, \\pi)\\), which implies that on these two intervals the log-density is below the corresponding chords. These chords can be pieced together with tangents to give an envelope. vMsim_adapt &lt;- function(n, x1, x2, kappa, trace = FALSE) { lf &lt;- function(x) kappa * cos(x) lf_deriv &lt;- function(x) - kappa * sin(x) a1 &lt;- 2 * kappa / pi a2 &lt;- lf_deriv(x1) a3 &lt;- lf_deriv(x2) a4 &lt;- - a1 b1 &lt;- kappa b2 &lt;- lf(x1) - a2 * x1 b3 &lt;- lf(x2) - a3 * x2 b4 &lt;- kappa z0 &lt;- -pi z1 &lt;- -pi/2 z2 &lt;- (b3 - b2) / (a2 - a3) z3 &lt;- pi/2 z4 &lt;- pi Q1 &lt;- exp(b1) * (exp(a1 * z1) - exp(a1 * z0)) / a1 Q2 &lt;- Q1 + exp(b2) * (exp(a2 * z2) - exp(a2 * z1)) / a2 Q3 &lt;- Q2 + exp(b3) * (exp(a3 * z3) - exp(a3 * z2)) / a3 c &lt;- Q3 + exp(b4) * (exp(a4 * z4) - exp(a4 * z3)) / a4 count &lt;- 0 y &lt;- numeric(n) uy &lt;- rng_stream(n, runif) u &lt;- rng_stream(n, runif) for(i in 1:n) { reject &lt;- TRUE while(reject) { count &lt;- count + 1 u0 &lt;- c * uy(n - i) if(u0 &lt; Q1) { z &lt;- log(a1 * exp(-b1) * u0 + exp(a1 * z0)) / a1 reject &lt;- u(n - i) &gt; exp(lf(z) - a1 * z - b1) } else if(u0 &lt; Q2) { z &lt;- log(a2 * exp(-b2) * (u0 - Q1) + exp(a2 * z1)) / a2 reject &lt;- u(n - i) &gt; exp(lf(z) - a2 * z - b2) } else if(u0 &lt; Q3) { z &lt;- log(a3 * exp(-b3) * (u0 - Q2) + exp(a3 * z2)) / a3 reject &lt;- u(n - i) &gt; exp(lf(z) - a3 * z - b3) } else { z &lt;- log(a4 * exp(-b4) * (u0 - Q3) + exp(a4 * z3)) / a4 reject &lt;- u(n - i) &gt; exp(lf(z) - a4 * z - b4) } } y[i] &lt;- z } if(trace) cat(&quot;kappa =&quot;, kappa, &quot;, x1 =&quot;, x1, &quot;, x2 =&quot;, x2, &quot;:&quot;, (count - n) / count, &quot;\\n&quot;) y } Figure 4.7: Histograms of simulated variables from von Mises distributions using the rejection sampler with the adaptive envelope based on a combination of log-concavity and log-convexity. The true density (blue) and the envelope (red) are added to the plots. y &lt;- vMsim_adapt(100000, -0.4, 0.4, 5, trace = TRUE) y &lt;- vMsim_adapt(100000, -1, 1, 2, trace = TRUE) y &lt;- vMsim_adapt(100000, -0.1, 0.1, 5, trace = TRUE) y &lt;- vMsim_adapt(100000, -0.4, 0.4, 2, trace = TRUE) ## kappa = 5 , x1 = -0.4 , x2 = 0.4 : 0.2027743 ## kappa = 2 , x1 = -1 , x2 = 1 : 0.2416889 ## kappa = 5 , x1 = -0.1 , x2 = 0.1 : 0.4845547 ## kappa = 2 , x1 = -0.4 , x2 = 0.4 : 0.1554981 We see that compared to using the uniform density as envelope, these adaptive envelopes are generally tighter and leads to fewer rejections. Even tighter envelopes are possible by using more than four intervals, but it is, of course, always a good question how the added complexity and bookkeeping induced by using more advanced and adaptive envelopes affect run time. It is even a good question if our current adaptive implementation will outperform our first, and much simpler, implementation that used the uniform envelope. microbenchmark(vMsim_adapt(100, -1, 1, 5), vMsim_adapt(100, -0.4, 0.4, 5), vMsim_adapt(100, -0.2, 0.2, 5), vMsim_adapt(100, -0.1, 0.1, 5), vMsim(100, 5), vMsim_vec(100, 5) ) ## Unit: microseconds ## expr min lq mean median uq max neval ## vMsim_adapt(100, -1, 1, 5) 470 542 602 587 650 901 100 ## vMsim_adapt(100, -0.4, 0.4, 5) 236 262 295 279 323 475 100 ## vMsim_adapt(100, -0.2, 0.2, 5) 264 314 348 329 371 714 100 ## vMsim_adapt(100, -0.1, 0.1, 5) 314 379 422 403 462 591 100 ## vMsim(100, 5) 468 551 614 596 662 840 100 ## vMsim_vec(100, 5) 56 67 80 74 89 182 100 The results from the benchmark show that the adaptive implementation has run time comparable to using the uniform proposal. With \\(x_1 = -0.4\\) and \\(x_2 = 0.4\\) and \\(\\kappa = 5\\) we found above that the rejection frequency was about 20% with the adaptive envelope, while it was about 80% when using the uniform envelope. A naive computation would thus suggest a speedup of a factor 4, but using the adaptive envelope there is actually only a speedup of a factor 2. And the vectorized solution is still considerably faster. A completely vectorized solution using the adaptive envelope is possible, but it is not entirely straightforward how to implement the more complicated envelope efficiently, and it may be a better option in this case to implement it using Rcpp. Even if either implementation can be improved further in terms of run time, it is an important point when comparing algorithms that we don’t get too focused on surrogate performance quantities. The probability of rejection is a surrogate for actual run time, and it might be conceptually of interest to bring this probability down. But if it is at the expense of additional computations it might not be worth the effort in terms of real run time. "],
["exercises-2.html", "4.5 Exercises", " 4.5 Exercises Rejection sampling of Gaussian random variables This exercise is on rejection sampling from the Gaussian distribution by using the Laplace distribution as an envelope. Recall that the Laplace distribution has density \\[g(x) = \\frac{1}{2} e^{-|x|}\\] for \\(x \\in \\mathbb{R}\\). Note that if \\(X\\) and \\(Y\\) are independent and exponentially distributed with mean one, then \\(X - Y\\) has a Laplace distribution. This gives a way to easily sample from the Laplace distribution. Exercise 4.1 Implement rejection sampling from the standard Gaussian distribution with density \\[f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{- x^2 / 2}\\] by simulating Laplace random variables as differences of exponentially distributed random variables. Test the implementation by computing the variance of the Gaussian distribution as an MC estimate and by comparing directly with the Gaussian distribution using histograms and QQ-plots. Exercise 4.2 Implement simulation from the Laplace distribution by transforming a uniform random variable by the inverse distribution function. Use this method together with the rejection sampler you implemented in Exercise 4.1 Note: The Laplace distribution can be seen as a simple version of the adaptive envelopes suggested in Section 4.4. "],
["mci.html", "Chapter 5 Monte Carlo integration", " Chapter 5 Monte Carlo integration A typical usage of simulation is Monte Carlo integration. With \\(X_1, \\ldots, X_n\\) i.i.d. with density \\(f\\) \\[\\hat{\\mu}_{\\textrm{MC}} := \\frac{1}{n} \\sum_{i=1}^n h(X_i) \\rightarrow \\mu := E(h(X_1)) = \\int h(x) f(x) \\ \\mathrm{d}x\\] for \\(n \\to \\infty\\) by the law of large numbers (LLN). The first question that we deal with below is the assessment of the average as an approximation of the integral. "],
["assessment.html", "5.1 Assessment", " 5.1 Assessment The error analysis of Monte Carlo integration differs from that of ordinary (deterministic) numerical integration methods. For the latter, error analysis provides bounds on the error of the computable approximation in terms of properties of the function to be integrated. Such bounds provide a guarantee on what the error at most can be. It is generally impossible to provide such a guarantee when using Monte Carlo integration because the computed approximation is by construction random. Thus the error analysis and assessment of the accuracy of \\(\\hat{\\mu}_{\\textrm{MC}}\\) as an approximation of \\(\\mu\\) has to be probabilistic. There are two main approaches. We can use approximations of the distribution of \\(\\hat{\\mu}_{\\textrm{MC}}\\) to assess the accuracy by computing a confidence interval, say. Or we can provide finite sample upper bounds, known as concentration inequalities, on the probability that the error of \\(\\hat{\\mu}_{\\textrm{MC}}\\) is larger than a given \\(\\varepsilon\\). A concentration inequality can be turned into a confidence interval, if needed, or it can be used directly to answer a question such as: if I want the approximation to have an error smaller than \\(\\varepsilon = 10^{-3}\\), how large does \\(n\\) need to be to guarantee this error bound with probability at least \\(99.99\\)%. Confidence intervals are typically computed using the central limit theorem and an estimated value of the asymptotic variance. The most notable practical problem is the estimation of that asymptotic variance, but otherwise the method is straightforward to use. A major deficit of this method is that the central limit theorem does not provide bounds – only approximations of unknown precision for a finite \\(n\\). Thus without further analysis, we cannot really be certain that the results from the central limit theorem reflect the accuracy of \\(\\hat{\\mu}_{\\textrm{MC}}\\). Concentration inequalities provide actual guarantees, albeit probabilistic. They are, however, typically problem specific and much harder to derive, they involve constants that are difficult to compute or estimate, and they tend to be somewhat pessimistic in real applications. The focus in this chapter is therefore on using the central limit theorem, but we do emphasize the example in Section 5.2.2 that shows how potentially misleading the confidence intervals can be when the convergence is slow. 5.1.1 Using the central limit theorem The CLT gives that \\[\\hat{\\mu}_{\\textrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n h(X_i) \\overset{\\textrm{approx}} \\sim \\mathcal{N}(\\mu, \\sigma^2_{\\textrm{MC}} / n)\\] where \\[\\sigma^2_{\\textrm{MC}} = V(h(X_1)) = \\int (h(x) - \\mu)^2 f(x) \\ \\mathrm{d}x.\\] We can estimate \\(\\sigma^2_{\\textrm{MC}}\\) using the empirical variance \\[\\hat{\\sigma}^2_{\\textrm{MC}} = \\frac{1}{n - 1} \\sum_{i=1}^n (h(X_i) - \\hat{\\mu}_{\\textrm{MC}})^2,\\] then the variance of \\(\\hat{\\mu}_{\\textrm{MC}}\\) is estimated as \\(\\hat{\\sigma}^2_{\\textrm{MC}} / n\\) and a standard 95% confidence interval for \\(\\mu\\) is \\[\\hat{\\mu}_{\\textrm{MC}} \\pm 1.96 \\frac{\\hat{\\sigma}_{\\textrm{MC}}}{\\sqrt{n}}.\\] x &lt;- gammasim(1000, 8) nn &lt;- seq_along(x) muhat &lt;- cumsum(x) / nn; sigmahat &lt;- sd(x) qplot(nn, muhat) + geom_ribbon(ymin = muhat - 1.96 * sigmahat / sqrt(nn), ymax = muhat + 1.96 * sigmahat / sqrt(nn), fill = &quot;gray&quot;) + geom_line() + geom_point() Figure 5.1: Sample path with confidence band for Monte Carlo integration of the mean of a gamma distributed random variable. 5.1.2 Concentration inequalities If \\(X\\) is a real valued random variable with finite second moment, \\(\\mu = E(X)\\) and \\(\\sigma^2 = V(X)\\), Chebychev’s inequality holds \\[P(|X - \\mu| &gt; \\varepsilon) \\leq \\frac {\\sigma^2}{\\varepsilon^2}\\] for all \\(\\varepsilon &gt; 0\\). This inequality implies, for instance, that for the simple Monte Carlo average we have the inequality \\[P(|\\hat{\\mu}_{\\textrm{MC}} - \\mu| &gt; \\varepsilon) \\leq \\frac{\\sigma^2_{\\textrm{MC}}}{n\\varepsilon^2}.\\] A common usage of this inequality is for the qualitative statement known as the law of large numbers: for any \\(\\varepsilon &gt; 0\\) \\[P(|\\hat{\\mu}_{\\textrm{MC}} - \\mu| &gt; \\varepsilon) \\rightarrow 0\\] for \\(n \\to \\infty\\). Or \\(\\hat{\\mu}_{\\textrm{MC}}\\) converges in probability towards \\(\\mu\\) as \\(n\\) tends to infinity. However, the inequality actually also provides a quantitative statement about how accurate \\(\\hat{\\mu}_{\\textrm{MC}}\\) is as an approximation of \\(\\mu\\). Chebyshev’s inequality is useful due to its minimal assumption of a finite second moment. However, it typically doesn’t give a very tight bound on the probability \\(P(|X - \\mu| &gt; \\varepsilon)\\). Much better inequalities can be obtained under stronger assumptions, in particular finite exponential moments. Assuming that the moment generating function of \\(X\\) is finite, \\(M(t) = E(e^{tX}) &lt; \\infty\\), for some suitable \\(t \\in \\mathbb{R}\\), it follows from Markov’s inequality that \\[P(X - \\mu &gt; \\varepsilon) = P(e^{tX} &gt; e^{t(\\varepsilon + \\mu)}) \\leq e^{-t(\\varepsilon + \\mu)}M(t),\\] which can provide a very tight upper bound by minimizing the bound over \\(t\\). This requires some knowledge of the moment generating function. We illustrate the usage of this inequality below by considering the gamma distribution where the moment generating function is well known. 5.1.3 Exponential tail bound for Gamma distributed variables If \\(X\\) follows a Gamma distribution with shape parameter \\(\\lambda &gt; 0\\) and \\(t &lt;1\\), then \\[M(t) = \\frac{1}{\\Gamma(\\lambda)} \\int_0^{\\infty} x^{\\lambda - 1} e^{-(1-t) x} \\, \\mathrm{d} x = \\frac{1}{(1-t)^{\\lambda}}.\\] Whence \\[P(X-\\lambda &gt; \\varepsilon) \\leq e^{-t(\\varepsilon + \\lambda)} \\frac{1}{(1-t)^{\\lambda}}.\\] Minimization over \\(t\\) of the right hand side gives the minimizer \\(t = \\varepsilon/(\\varepsilon + \\lambda)\\) and the upper bound \\[P(X-\\lambda &gt; \\varepsilon) \\leq e^{-\\varepsilon} \\left(\\frac{\\varepsilon + \\lambda}{\\lambda }\\right)^{\\lambda}.\\] Compare this to the bound \\[P(|X-\\lambda| &gt; \\varepsilon) \\leq \\frac{\\lambda}{\\varepsilon^2}\\] from Chebychev’s inequality. lambda &lt;- 10 curve(pgamma(x, lambda, lower.tail = FALSE), lambda, lambda + 30, ylab = &quot;probability&quot;, main = &quot;Gamma tail&quot;, ylim = c(0, 1)) curve(exp(lambda - x)*(x/lambda)^lambda, lambda, lambda + 30, add = TRUE, col = &quot;red&quot;) curve(lambda/(x-lambda)^2, lambda, lambda + 30, add = TRUE, col = &quot;blue&quot;) curve(pgamma(x, lambda, lower.tail = FALSE, log.p = TRUE), lambda, lambda + 30, ylab = &quot;log-probability&quot;, main = &quot;Logarithm of Gamma tail&quot;, ylim = c(-20, 0)) curve(lambda - x + lambda*log(x/lambda), lambda, lambda + 30, add = TRUE, col = &quot;red&quot;) curve(-2 * log(x-lambda) + log(lambda), lambda, lambda + 30, add = TRUE, col = &quot;blue&quot;) Figure 5.2: Actual tail probabilities (left) for the gamma distribution, computed via the pgamma function, compared it to the tight bound (red) and the weaker bound from Chebychev’s inequality (blue). The differences in the tail are more clearly seen for the log-probabilities (right) "],
["importance-sampling.html", "5.2 Importance sampling", " 5.2 Importance sampling When we are only interested in Monte Carlo integration, we do not need to sample from the target distribution. Observe that \\[\\begin{align} \\mu = \\int h(x) f(x) \\ \\mathrm{d}x &amp; = \\int h(x) \\frac{f(x)}{g(x)} g(x) \\ \\mathrm{d}x \\\\ &amp; = \\int h(x) w^*(x) g(x) \\ \\mathrm{d}x \\end{align}\\] whenever \\(g\\) is a density fulfilling that \\[g(x) = 0 \\Rightarrow f(x) = 0.\\] With \\(X_1, \\ldots, X_n\\) i.i.d. with density \\(g\\) define the weights \\[w^*(X_i) = f(X_i) / g(X_i).\\] The importance sampling estimator is \\[\\hat{\\mu}_{\\textrm{IS}}^* := \\frac{1}{n} \\sum_{i=1}^n h(X_i)w^*(X_i).\\] It has mean \\(\\mu\\). Again by the LLN \\[\\hat{\\mu}_{\\textrm{IS}}^* \\rightarrow E(h(X_1) w^*(X_1)) = \\mu.\\] To assess the precision of the importance sampling estimate via the CLT we need the variance of the average as for plain Monte Carlo integration. By the CLT \\[\\hat{\\mu}_{\\textrm{IS}}^* \\overset{\\textrm{approx}} \\sim \\mathcal{N}(\\mu, \\sigma^{*2}_{\\textrm{IS}} / n)\\] where \\[\\sigma^{*2}_{\\textrm{IS}} = V (h(X_1)w^*(X_1)) = \\int (h(x) w^*(x) - \\mu)^2 g(x) \\ \\mathrm{d}x.\\] We may have \\(\\sigma^{*2}_{\\textrm{IS}} &gt; \\sigma^2_{\\textrm{MC}}\\) or \\(\\sigma^{*2}_{\\textrm{IS}} &lt; \\sigma^2_{\\textrm{MC}}\\) depending on \\(h\\) and \\(g\\). By choosing \\(g\\) cleverly so that \\(h(x) w^*(x)\\) becomes as constant as possible, importance sampling can reduce the variance considerably compared to plain MC. The importance sampling variance can be estimated just as the MC variance \\[\\hat{\\sigma}^{*2}_{\\textrm{IS}} = \\frac{1}{n - 1} \\sum_{i=1}^n (h(X_i)w^*(X_i) - \\hat{\\mu}_{\\textrm{IS}}^*)^2,\\] and a 95% standard confidence interval is computed as \\[\\hat{\\mu}^*_{\\textrm{IS}} \\pm 1.96 \\frac{\\hat{\\sigma}^*_{\\textrm{IS}}}{\\sqrt{n}}.\\] 5.2.1 Unknown normalization constants If \\(f = c^{-1} q\\) with \\(c\\) unknown then \\[c = \\int q(x) \\ \\mathrm{d}x = \\int \\frac{q(x)}{g(x)} g(x) \\ d x,\\] and \\[\\mu = \\frac{\\int h(x) w^*(x) g(x) \\ d x}{\\int w^*(x) g(x) \\ d x},\\] where \\(w^*(x) = q(x) / g(x).\\) An importance sampling estimate of \\(\\mu\\) is thus \\[\\hat{\\mu}_{\\textrm{IS}} = \\frac{\\sum_{i=1}^n h(X_i) w^*(X_i)}{\\sum_{i=1}^n w^*(X_i)} = \\sum_{i=1}^n h(X_i) w(X_i),\\] where \\(w^*(X_i) = q(X_i) / g(X_i)\\) and \\[w(X_i) = \\frac{w^*(X_i)}{\\sum_{i=1}^n w^*(X_i)}\\] are the standardized weights. This works irrespectively of the value of the normalizing constant \\(c\\). The variance of the IS estimator with standardized weights is a little more complicated, because the estimator is a ratio of random variables. From the multivariate CLT \\[\\frac{1}{n} \\sum_{i=1}^n \\left(\\begin{array}{c} h(X_i) w^*(X_i) \\\\ w^*(X_i) \\end{array}\\right) \\overset{\\textrm{approx}}{\\sim} \\mathcal{N}\\left( c \\left(\\begin{array}{c} \\mu \\\\ {1} \\end{array}\\right), \\frac{1}{n} \\left(\\begin{array}{cc} \\sigma^{*2}_{\\textrm{IS}} &amp; \\gamma \\\\ \\gamma &amp; \\sigma^2_{w^*} \\end{array} \\right)\\right),\\] where \\[\\begin{align} \\sigma^{*2}_{\\textrm{IS}} &amp; = V(h(X_1)w^*(X_1)) \\\\ \\gamma &amp; = \\mathrm{cov}(h(X_1)w^*(X_1), w^*(X_1)) \\\\ \\sigma_{w^*}^2 &amp; = V (w^*(X_1)). \\end{align}\\] We can then apply the \\(\\Delta\\)-method with \\(h(x, y) = x / y\\). Note that \\(Dh(x, y) = (1 / y, - x / y^2)\\), whence \\[Dh(c\\mu, c) \\left(\\begin{array}{cc} \\hat{\\sigma}^{*2}_{\\textrm{IS}} &amp; \\gamma \\\\ \\gamma &amp; \\sigma^2_{w^*} \\end{array} \\right) Dh(c\\mu, c)^T = c^{-2} (\\sigma^{*2}_{\\textrm{IS}} + \\mu^2 \\sigma_{w^*}^2 - 2 \\mu \\gamma).\\] By the \\(\\Delta\\)-method \\[\\hat{\\mu}_{\\textrm{IS}} \\overset{\\textrm{approx}}{\\sim} \\mathcal{N}(\\mu, c^{-2} (\\sigma^{*2}_{\\textrm{IS}} + \\mu^2 \\sigma_{w^*}^2 - 2 \\mu \\gamma) / n).\\] Note that for \\(c \\neq 1\\) it is necessary to estimate \\(c\\) as \\(\\hat{c} = \\frac{1}{n} \\sum_{i=1}^n w^*(X_i)\\) to compute an estimate of the variance. 5.2.2 Computing a high-dimensional integral To illustrate the usage and limitations of importance sampling, consider the following \\(p\\)-dimensional integral \\[\\int e^{-\\frac{1}{2}\\left(x_1^2 + \\sum_{i=2}^p (x_i - \\alpha x_{i-1})^2\\right)} \\mathrm{d} x.\\] Now this integral is not even expressed as an expectation w.r.t. any distribution in the first place – it is an integral w.r.t. Lebesgue measure in \\(\\mathbb{R}^p\\). To use importance sampling it is therefore necessary to rewrite the integral as an expectation w.r.t. a probability distribution. There might be many ways to do this, and the following is just one. Rewrite the exponent as \\[||x||_2^2 + \\sum_{i = 2}^p \\alpha^2 x_{i-1}^2 - 2\\alpha x_i x_{i-1}\\] so that \\[\\begin{align*} \\int e^{-\\frac{1}{2}\\left(x_1^2 + \\sum_{i=2}^p (x_i - \\alpha x_{i-1})^2\\right)} \\mathrm{d} x &amp; = \\int e^{- \\frac{1}{2} \\sum_{i = 2}^n \\alpha^2 x_{i-1}^2 - 2\\alpha x_i x_{i-1}} e^{-\\frac{||x||_2^2}{2}} \\mathrm{d} x \\\\ &amp; = (2 \\pi)^{p/2} \\int e^{- \\frac{1}{2} \\sum_{i = 2}^p \\alpha^2 x_{i-1}^2 - 2\\alpha x_i x_{i-1}} f(x) \\mathrm{d} x \\end{align*}\\] where \\(f\\) is the density for the \\(\\mathcal{N}(0, I_p)\\) distribution. Thus if \\(X \\sim \\mathcal{N}(0, I_p)\\), \\[\\int e^{-\\frac{1}{2}\\left(x_1^2 + \\sum_{i=2}^p (x_i - \\alpha x_{i-1})^2\\right)} \\mathrm{d} x = (2 \\pi)^{n/2} E\\left( e^{- \\frac{1}{2} \\sum_{i = 2}^p \\alpha^2 X_{i-1}^2 - 2\\alpha X_i X_{i-1}} \\right).\\] The Monte Carlo integration below computes \\[\\mu = E\\left( e^{- \\frac{1}{2} \\sum_{i = 2}^p \\alpha^2 X_{i-1}^2 - 2\\alpha X_i X_{i-1}} \\right)\\] by generating \\(p\\)-dimensional random variables from \\(\\mathcal{N}(0, I_p)\\). It can actually be shown that \\(\\mu = 1\\), but we skip the proof of that. We can view this example as an example of plain Monte Carlo integration, but it is also a variation of importance sampling. The initial integral was not an expectation but an integral w.r.t. Lebesgue measure. By changing the integration measure to the Gaussian distribution and reweighting the integrand we represented the integral in terms of an expectation. This is exactly the idea of importance sampling as well. First, we implement the function we want to integrate. h &lt;- function(x, alpha = 0.1){ p &lt;- length(x) tmp &lt;- alpha * x[1:(p - 1)] exp( - sum((tmp / 2 - x[2:p]) * tmp)) } Then we specify various parameters. B &lt;- 10000 ## The number of random variables to generate p &lt;- 100 ## The dimension of each random variable The actual computation is implemented using the apply function. We first look at the case with \\(\\alpha = 0.2\\). x &lt;- matrix(rnorm(B * p), B, p) evaluations &lt;- apply(x, 1, h, alpha = 0.2) We can then plot the cumulative average and compare it to the actual value of the integral that we know is 1. plot(cumsum(evaluations) / 1:B, pch = 20, xlab = &quot;n&quot;) abline(h = 1, col = &quot;red&quot;) If we want to control the error with probability 0.95 we can use Chebychev’s inequality and solve for \\(\\varepsilon\\) using the estimated variance. plot(cumsum(evaluations) / 1:B, pch = 20, xlab = &quot;n&quot;) abline(h = 1, col = &quot;red&quot;) me &lt;- cumsum(evaluations) / 1:B ve &lt;- var(evaluations) epsilon &lt;- sqrt(ve / ((1:B) * 0.05)) lines(1:B, me + epsilon) lines(1:B, me - epsilon) The confidence bands provided by the central limit theorem are typically much more accurate estimates of the actual uncertainty than the upper bounds provided by Chebychev’s inequality. plot(cumsum(evaluations) / 1:B, pch = 20, xlab = &quot;n&quot;) abline(h = 1, col = &quot;red&quot;) lines(1:B, me + 2*sqrt(ve/(1:B))) lines(1:B, me - 2*sqrt(ve/(1:B))) To illustrate the limits of Monte Carlo integration we increase \\(\\alpha\\) to \\(\\alpha = 0.4\\). set.seed(123) x &lt;- matrix(rnorm(B * p), B, p) evaluations &lt;- apply(x, 1, h, alpha = 0.4) The sample path above is not carefully selected to be pathological. Due to occasional large values, the typical sample path will show occasional large jumps, and the variance may easily be grossly underestimated. Figure 5.3: Four sample paths of the cumulative average for \\(\\alpha = 0.4\\). To be fair, it is the choice of standard multivariate normal distribution as the reference distribution for large \\(\\alpha\\) that is problematic rather than Monte Carlo integration and importance sampling as such. However, in high dimensions it an be quite difficult to choose a suitable distribution to sample from. "],
["network-failure.html", "5.3 Network failure", " 5.3 Network failure Consider the following network consisting of ten nodes and with some of the nodes connected. The network could be a computer network with ten computers. The different connections (edges) may “fail” independently with probability \\(p\\), and the question we will take an interest in is what is the probability that node 1 and node 10 become disconnected? The network of nodes can be represented as a graph adjacency matrix \\(A\\) such that \\(A_{ij} = 1\\) if and only if there is an edge between \\(i\\) and \\(j\\) (and \\(A_{ij} = 0\\) otherwise). A ## Graph adjacency matrix ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0 1 0 1 1 0 0 0 0 0 ## [2,] 1 0 1 0 0 1 0 0 0 0 ## [3,] 0 1 0 0 0 1 1 1 0 1 ## [4,] 1 0 0 0 1 0 0 1 0 0 ## [5,] 1 0 0 1 0 0 0 1 1 0 ## [6,] 0 1 1 0 0 0 1 0 0 1 ## [7,] 0 0 1 0 0 1 0 0 0 1 ## [8,] 0 0 1 1 1 0 0 0 1 0 ## [9,] 0 0 0 0 1 0 0 1 0 1 ## [10,] 0 0 1 0 0 1 1 0 1 0 To compute the probability that 1 and 10 become disconnected by Monte Carlo integration, we need to sample (sub)graphs by randomly removing some of the edges. This is implemented using the upper triangular part of the (symmetric) adjacency matrix. simNet &lt;- function(Aup, p) { ones &lt;- which(Aup == 1) Aup[ones] &lt;- sample(c(0, 1), length(ones), replace = TRUE, prob = c(p, 1 - p)) Aup + t(Aup) } It is fairly fast to sample even a large number of random graphs this way. Aup &lt;- A Aup[lower.tri(Aup)] &lt;- 0 system.time(replicate(1e5, {simNet(Aup, 0.5); NULL})) ## user system elapsed ## 1.972 0.063 2.038 The second function we need to implement checks network connectivity. It relies on the fact that there is a path from node 1 to node 10 consisting of \\(k\\) edges if and only if \\(A^k_{1,10} &gt; 0\\). We see directly that such a path needs to consist of at least \\(k = 3\\) edges. Also, we don’t need to check paths with more than \\(k = 9\\) edges as they will contain the same node multiple times and can thus be shortened. disconAB &lt;- function(A) { k &lt;- 3 Apow &lt;- A %*% A %*% A ## A%^%3 while(Apow[1, 10] == 0 &amp; k &lt; 9) { Apow &lt;- Apow %*% A k &lt;- k + 1 } Apow[1, 10] == 0 ## TRUE if A and B not connected } Estimating probability of nodes 1 and 10 being disconnected using Monte Carlo integration. seed &lt;- 27092016 set.seed(seed) n &lt;- 1e5 tmp &lt;- replicate(n, disconAB(simNet(Aup, 0.05))) muhat &lt;- mean(tmp) As this is a (random) approximation, we should report not only the Monte Carlo estimate but also the confidence interval. Since the estimate is an average of 0-1-variables, we can estimate the variance, \\(\\sigma^2\\), of the individual terms using that \\(\\sigma^2 = \\mu (1 - \\mu)\\). muhat + 1.96 * sqrt(muhat * (1 - muhat) / n) * c(-1, 0, 1) ## [1] 0.0002257328 0.0003400000 0.0004542672 To implement importance sampling we note that the point probabilities (density w.r.t. counting measure) for sampling 18 independent 0-1-variables \\(x = (x_1, \\ldots, x_{18})\\) with \\(P(X_i = 0) = p\\) is \\[f_p(x) = p^b (1- p)^{18-b}\\] where \\(b = 18 - \\sum_{i=1}^{18} x_i\\). The implementation computes the weights when sampling using probability \\(p_0\\) (density \\(g = f_{p_0}\\)) instead of \\(p\\), and the weights are only computed if the graph is disconnected. impw &lt;- function(Aup, A0, p0, p) { w &lt;- disconAB(A0) if (w) { b &lt;- 18 - sum(Aup[A0 == 1]) w &lt;- ((1 - p) / (1 - p0))^18 * (p * (1 - p0) / (p0 * (1 - p)))^b } as.numeric(w) } The implementation uses the formula \\[ w(x) = \\frac{f_p(x)}{f_{p_0}(x)} = \\frac{p^b (1- p)^{18-b}}{p_0^b (1- p_0)^{18-b}} = \\left(\\frac{1- p}{1- p_0}\\right)^{18} \\left(\\frac{p (1- p_0)}{p_0 (1- p)}\\right)^b. \\] set.seed(seed) tmp &lt;- replicate(n, impw(Aup, simNet(Aup, 0.2), 0.2, 0.05)) muhatIS &lt;- mean(tmp) Confidence interval using empirical variance estimate \\(\\hat{\\sigma}^2\\). muhatIS + 1.96 * sd(tmp) / sqrt(n) * c(-1, 0, 1) ## [1] 0.0002619838 0.0002960900 0.0003301962 c(sd(tmp), sqrt(muhat * (1 - muhat))) ## The two standard deviations ## [1] 0.005502716 0.018435954 The ratio of variances is estimated as muhat * (1 - muhat) / var(tmp) ## [1] 11.22476 We need around 11 times more naive samples when compared to importance sampling to obtain the same precision. A benchmark will show that the extra computing time for importance sampling is small compared to the reduction of variance. It is worth the coding effort if used repeatedly, but not if it is a one-off computation. The graph is small enough for complete enumeration and thus the computation of an exact solution. There are \\(2^{18} = 262,144\\) different networks with any number of the edges failing, so complete enumeration is possible. To systematically walk through each possible combinations of edges failing, we use the function intToBits that convert an integer to its binary representation for integers from 0 to 262,143. This is a quick and convenient way of representing all the different fail and non-fail combinations for the edges. ones &lt;- which(Aup == 1) p &lt;- 0.05 prob &lt;- numeric(2^18) for(i in 0:(2^18 - 1)) { on &lt;- as.numeric(intToBits(i)[1:18]) nr &lt;- sum(on) Atmp &lt;- Aup Atmp[ones] &lt;- on if(disconAB(Atmp + t(Atmp))) prob[i + 1] &lt;- p^(18 - nr) * (1 - p)^nr } The probability of nodes 1 and 10 disconnected can then be computed as follows. sum(prob) ## [1] 0.000288295 This number should be compared to the estimates computed above. For a more complete comparison, we have used importance sampling with edge fail probability ranging from 0.1 to 0.4, see Figure 5.4. The results show that a failure probability of 0.2 is close to optimal in terms of giving an importance sampling estimate with minimal variance. For smaller values, the event that 1 and 10 become disconnected is too rare, and for larger values the importance weights become too variable. A choice of 0.2 strikes a good balance. Figure 5.4: Confidence intervals for importance sampling estimates of network nodes 1 and 10 being disconnected under independent edge failures with probability 0.05. The red line is the true probability computed by complete enumeration. 5.3.1 Object oriented implementation network &lt;- function(A, p) { Aup &lt;- A Aup[lower.tri(Aup)] &lt;- 0 ones &lt;- which((Aup == 1)) structure(list(A = A, Aup = Aup, ones = ones, p = p), class = &quot;network&quot;) } myNet &lt;- network(A, p = 0.05) str(myNet) ## List of 4 ## $ A : num [1:10, 1:10] 0 1 0 1 1 0 0 0 0 0 ... ## $ Aup : num [1:10, 1:10] 0 0 0 0 0 0 0 0 0 0 ... ## $ ones: int [1:18] 11 22 31 41 44 52 53 63 66 73 ... ## $ p : num 0.05 ## - attr(*, &quot;class&quot;)= chr &quot;network&quot; Generic functions. sim &lt;- function(x, ...) UseMethod(&quot;sim&quot;) failure &lt;- function(x, ...) UseMethod(&quot;failure&quot;) The first method. sim.network &lt;- function(x) { Aup &lt;- x$Aup Aup[x$ones] &lt;- sample(c(0, 1), length(x$ones), replace = TRUE, prob = c(x$p, 1 - x$p)) Aup + t(Aup) } And the second method. failure.network &lt;- function(x, n, p0 = NULL) { if (is.null(p0)) { ## Naive simulation tmp &lt;- replicate(n, disconAB(sim(x))) muhat &lt;- mean(tmp) se &lt;- sqrt(muhat * (1 - muhat) / n) } else { ## Importance sampling p &lt;- x$p x$p &lt;- p0 tmp &lt;- replicate(n, impw(x$Aup, sim(x), p0, p)) se &lt;- sd(tmp) / sqrt(n) muhat &lt;- mean(tmp) } value &lt;- muhat + 1.96 * se * c(-1, 0, 1) names(value) &lt;- c(&quot;low&quot;, &quot;estimate&quot;, &quot;high&quot;) value } We test the implementation against the previously computed results. set.seed(seed) ## Resetting seed failure(myNet, n) ## low estimate high ## 0.0002257328 0.0003400000 0.0004542672 set.seed(seed) ## Resetting seed failure(myNet, n, p0 = 0.2) ## low estimate high ## 0.0002619838 0.0002960900 0.0003301962 We find that these are the same numbers as computed above, thus the object oriented implementation concurs with the non-object oriented on this example. We benchmark the object oriented implementation. microbenchmark( simNet(Aup, 0.05), sim(myNet), times = 1e4 ) ## Unit: microseconds ## expr min lq mean median uq max neval ## simNet(Aup, 0.05) 14.2 16.1 24.6 20.4 26.7 9103 10000 ## sim(myNet) 17.3 19.2 27.3 23.1 28.9 9244 10000 One should expect a small computational overhead due to method dispatching, that is, the procedure that R uses to look up the appropriate sim method for an object of class network. The generic print function already exists. We implement a method for class network. print.network &lt;- function(x) { cat(&quot;#vertices: &quot;, nrow(x$A), &quot;\\n&quot;) cat(&quot;#edges:&quot;, sum(x$Aup), &quot;\\n&quot;) cat(&quot;p = &quot;, x$p, &quot;\\n&quot;) } myNet ## Implicitly calls &#39;print&#39; ## #vertices: 10 ## #edges: 18 ## p = 0.05 Using igraph. library(igraph) net &lt;- graph_from_adjacency_matrix(A, mode = &quot;undirected&quot;) net ## IGRAPH afa7fc6 U--- 10 18 -- ## + edges from afa7fc6: ## [1] 1-- 2 1-- 4 1-- 5 2-- 3 2-- 6 3-- 6 3-- 7 3-- 8 3--10 4-- 5 4-- 8 ## [12] 5-- 8 5-- 9 6-- 7 6--10 7--10 8-- 9 9--10 The igraph package supports a vast number of graph computation, manipulation and visualization tools. Plotting an igraph. ## You can generate a layout ... net_layout &lt;- layout_(net, nicely()) ## ... or you can specify one yourself net_layout &lt;- matrix( c(-20, 1, -4, 3, -4, 1, -4, -1, -4, -3, 4, 3, 4, 1, 4, -1, 4, -3, 20, -1), ncol = 2, nrow = 10, byrow = TRUE) Plotting an igraph plot(net, layout = net_layout, asp = 0) Simulation method for igraph sim.igraph &lt;- function(x, p) { nr &lt;- ecount(x) deledges &lt;- sample(c(TRUE, FALSE), nr, replace = TRUE, prob = c(p, 1 - p)) delete_edges(x, which(deledges)) } A simulated graph plot(sim(net, 0.25), layout = net_layout, asp = 0) This is slower than using the matrix representation alone as in simNet. system.time(replicate(1e5, {sim(net, 0.05); NULL})) ## user system elapsed ## 4.103 1.508 5.628 One could also implement the function for testing if nodes 1 and 10 are disconnected using the shortest_paths function, but this is not faster than the simple matrix multiplications used in disconAB either, though it could be for larger graphs. "],
["design-of-experiments.html", "5.4 Design of experiments", " 5.4 Design of experiments "],
["multivariate-random-variables.html", "Chapter 6 Multivariate random variables ", " Chapter 6 Multivariate random variables "],
["sequential-simulation.html", "6.1 Sequential simulation", " 6.1 Sequential simulation 6.1.1 Sequential MC for the AR(1)-process Recall the update equation \\[X_{t} = \\alpha X_{t-1} + \\epsilon_t.\\] and the observation equation \\[Y_t = X_t + \\delta_t.\\] Simulating a sample path simAR &lt;- function(t, alpha) { x &lt;- numeric(t) e &lt;- rnorm(t, 0, 1) x[1] &lt;- e[1] / sqrt(1 - alpha^2) for(s in seq_len(t)[-1]) { x[s] &lt;- alpha * x[s - 1] + e[s] } x } Simulating a sample path We first generate a realization from the process with \\(\\alpha = 0.9\\) and observation variance \\(\\sigma^2 = 10\\). t &lt;- 100; alpha &lt;- 0.9; sigma &lt;- sqrt(10) set.seed(30) x &lt;- simAR(t, alpha) The simulated sample path Sample path with observations … and observations only Using the Kalman smoother We can use the implemented Kalman smoother and Kalman filter to compute the best prediction of the unobserved process. xSmooth &lt;- KalmanSmooth(y, alpha, sigma^2) xFilt &lt;- KalmanFilt(y, alpha, sigma^2) Using the Kalman smoother Using the Kalman filter only Filter, smoother and process "],
["gaussian-random-variables.html", "6.2 Gaussian random variables", " 6.2 Gaussian random variables "],
["five-examples.html", "Chapter 7 Five Examples", " Chapter 7 Five Examples This chapter treats five examples of non-trivial statistical models in some detail. These are all parametric models, and a central computational challenge is to fit the models to data via (penalized) likelihood maximization. The actual optimization algorithms and implementations are the topics of Chapters 8, 9, and 10. The focus of this chapter is on the structure of the statistical models themselves to provide the necessary background for the later chapters. Statistical models come in all forms and shapes, and it is possible to take a very general and abstract mathematical approach; statistical models are parametrized families of probability distributions. To say anything of interest, we need more structure such as structure on the parameter set, properties of the parametrized distributions, and properties of the mapping from the parameter set to the distributions. For any specific model we have ample of structure but often also an overwhelming amount of irrelevant details that will be more distracting than clarifying. The intention is that the five examples treated will illustrate the breath of statistical models that share important structures without getting lost in a wasteland of abstractions. If one should emphasize a single abstract idea that is of theoretical value as well as of practical importance, it is the idea of exponential families. Statistical models that are exponential families have so much structure that the general theory provides a number of results and details of practical value for individual models. Exponential families are exemplary statistical models, that are widely used as models of data, or as central building blocks of more complicated models of data. For this reason, the treatment of the examples is preceded by a treatment of exponential families. "],
["exp-fam.html", "7.1 Exponential families", " 7.1 Exponential families This section introduces exponential families in a concise way. The crucial observation is that the log-likelihood is concave, and that we can derive general formulas for derivatives. This will be important for the optimization algorithms developed later for computing maximum-likelihood estimates and for answering standard asymptotic inference questions. The exponential families are extremely well behaved from a mathematical as well as a computational viewpoint, but they may be inadequate for modeling data in some cases. A typical practical problem is that there is heterogeneous variation in data beyond what can be captured by any single exponential family. A fairly common technique is then to build an exponential family model of the observed variables as well as some latent variables. The latent variables then serve the purpose of modeling the heterogeneity. The resulting model of the observed variables is consequently the marginalization of an exponential family, which is generally not an exponential family and in many ways less well behaved. It is nevertheless possible to exploit the exponential family structure underlying the marginalized model for many computations of statistical importance. The EM-algorithm as treated in Chapter 9 is one particularly good example, but Bayesian computations can in similar ways exploit the structure. 7.1.1 Full exponential families In this section we consider statistical models on an abstract product sample space \\[\\mathcal{Y} = \\mathcal{Y}_1 \\times \\ldots \\times \\mathcal{Y}_m.\\] We will be interested in models of observations \\(y_1 \\in \\mathcal{Y}_1, \\ldots, y_m \\in \\mathcal{Y}_m\\) that are independent but not necessarily identically distributed. An exponential family is defined in terms of two ingredients: maps \\(t_j : \\mathcal{Y}_j \\to \\mathbb{R}^p\\) for \\(j = 1, \\ldots, m\\), and non-trivial \\(\\sigma\\)-finite measures \\(\\nu_j\\) on \\(\\mathcal{Y}_j\\) for \\(j = 1, \\ldots, m\\). The maps \\(t_j\\) are called sufficient statistics, and in terms of these and the base measures \\(\\nu_j\\) we define \\[\\varphi_j(\\theta) = \\int e^{\\theta^T t_j(u)} \\nu_j(\\mathrm{d}u).\\] These functions are well defined as functions \\[\\varphi_j : \\mathbb{R}^p \\to (0,\\infty].\\] We define \\[\\Theta_j = \\mathrm{int}(\\{ \\theta \\in \\mathbb{R}^p \\mid \\varphi_j(\\theta) &lt; \\infty \\}),\\] which by definition is an open set as. It can be shown that \\(\\Theta_j\\) is convex and that \\(\\varphi_j\\) is a log-convex function. Defining \\[\\Theta = \\bigcap_{j=1}^m \\Theta_j,\\] then \\(\\Theta\\) is likewise open and convex, and we define the exponential family as the distributions parametrized by \\(\\theta \\in \\Theta\\) that have densities \\[\\begin{equation} f(\\mathbf{y} \\mid \\theta) = \\prod_{j=1}^m \\frac{1}{\\varphi_j(\\theta)} e^{\\theta^T t_j(y_j)} = e^{\\theta^T \\sum_{j=1}^m t_j(y_j) - \\sum_{j=1}^m \\log \\varphi_j(\\theta)}, \\quad \\mathbf{y} \\in \\mathcal{Y}, \\tag{7.1} \\end{equation}\\] w.r.t. \\(\\otimes_{j=1}^m \\nu_j\\). The case where \\(\\Theta = \\emptyset\\) is of no interest, and we will thus assume that the parameter set \\(\\Theta\\) is non-empty. The parameter \\(\\theta\\) is called the canonical parameter and \\(\\Theta\\) is the canonical parameter space. We may also say that the exponential family is canonically parametrized by \\(\\theta\\). It is important to realize that an exponential family may come with a non-canonical parametrization that doesn’t reveal right away that it is an exponential family. Thus a bit of work is then needed to show that the parametrized family of distributions can, indeed, be reparametrized as an exponential family. In the non-canonical parametrization, the family is then an example of a curved exponential family as defined below. Example 7.1 The von Mises distributions on \\(\\mathcal{Y} = (-\\pi, \\pi]\\) form an exponential family with \\(m = 1\\). The sufficient statistic \\(t_1 : (-\\pi, \\pi] \\mapsto \\mathbb{R}^2\\) is \\[t_1(y) = \\left(\\begin{array}{c} \\cos(y) \\\\ \\sin(y) \\end{array}\\right),\\] and \\[\\varphi(\\theta) = \\int_{-\\pi}^{\\pi} e^{\\theta_1 \\cos(u) + \\theta_2 \\sin(u)} \\mathrm{d}u &lt; \\infty\\] for all \\(\\theta = (\\theta_1, \\theta_2)^T \\in \\mathbb{R}^2\\). Thus the canonical parameter space is \\(\\Theta = \\mathbb{R}^2\\). As mentioned in Section 1.2.1, the function \\(\\varphi(\\theta)\\) can be expressed in terms of a modified Bessel function, but it doesn’t have an expression in terms of elementary functions. Likewise in Section 1.2.1, an alternative parametrization (polar coordinates) was given; \\[(\\kappa, \\mu) \\mapsto \\theta = \\kappa \\left(\\begin{array}{c} \\cos(\\mu) \\\\ \\sin(\\mu) \\end{array}\\right)\\] that maps \\([0,\\infty) \\times (-\\pi, \\pi]\\) onto \\(\\Theta\\). The von Mises distributions form a curved exponential family in the \\((\\kappa, \\mu)\\)-parametrization, but this parametrization has several problems. First, the \\(\\mu\\) parameter is not identifiable if \\(\\kappa = 0\\), which is reflected by the fact that the reparametrization is not a one-to-one map. Second, the parameter space is not open, which can be quite a nuisance for e.g. maxmimum-likelihood estimation. We could circumvent these problems by restricting attention to \\((\\kappa, \\mu) \\in (0,\\infty) \\times (-\\pi, \\pi)\\), but we would then miss some of the distributions in the exponential family – notably the uniform distribution corresponding to \\(\\theta = 0\\). In conclusion, the canonical parametrization of the family of distributions as an exponential family is preferable for mathematical and computational reasons. Example 7.2 The family of Gaussian distributions on \\(\\mathbb{R}\\) is an example of an exponential family as defined above with \\(m = 1\\) and \\(\\mathcal{Y} = \\mathbb{R}\\). The density of the \\(\\mathcal{N}(\\mu, \\sigma^2)\\) distribution is \\[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right) = \\frac{1}{\\sqrt{\\pi}} \\exp\\left(\\frac{\\mu}{\\sigma^2} y - \\frac{1}{2\\sigma^2} y^2 - \\frac{\\mu^2}{2\\sigma^2} - \\frac{1}{2}\\log (2\\sigma^2) \\right).\\] Letting the base measure \\(\\nu_1\\) be Lebesgue measure scaled by \\(1/\\sqrt{\\pi}\\), and \\(t_1 : \\mathbb{R} \\mapsto \\mathbb{R}^2\\) be \\[t_1(y) = \\left(\\begin{array}{c} y \\\\ - y^2 \\end{array}\\right)\\] we identify this family of distributions as an exponential family with canonical parameter \\[\\theta = \\left(\\begin{array}{c} \\frac{\\mu}{\\sigma^2} \\\\ \\frac{1}{2 \\sigma^2} \\end{array}\\right).\\] We can express the mean and variance in terms of \\(\\theta\\) as \\[\\sigma^2 = \\frac{1}{2\\theta_2} \\quad \\text{and} \\quad \\mu = \\frac{\\theta_1}{2\\theta_2},\\] and we find that \\[\\log \\varphi_1(\\theta) = \\frac{\\mu^2}{2\\sigma^2} + \\frac{1}{2} \\log(2 \\sigma^2) = \\frac{\\theta_1^2}{4\\theta_2} - \\frac{1}{2} \\log \\theta_2.\\] We note that the reparametrization \\((\\mu, \\sigma^2) \\mapsto \\theta\\) maps \\(\\mathbb{R} \\times (0,\\infty)\\) bijectively onto the open set \\(\\mathbb{R} \\times (0,\\infty)\\), and that the formula above for \\(\\log \\varphi_1(\\theta)\\) only holds on this set. It is natural to ask if the canonical parameter space is actually larger for this exponential family. That is, is \\(\\varphi_1(\\theta) &lt; \\infty\\) for \\(\\theta_2 \\leq 0\\)? To this end observe that if \\(\\theta_2 \\leq 0\\) \\[\\varphi_1(\\theta) = \\frac{1}{\\sqrt{2\\pi}} \\int e^{\\theta_1 u - \\theta_2 \\frac{u^2}{2}} \\mathrm{d}u \\geq \\frac{1}{\\sqrt{2\\pi}} \\int e^{\\theta_1 u} \\mathrm{d} u = \\infty,\\] and we conclude that \\[\\Theta = \\mathbb{R} \\times (0,\\infty).\\] The family of Gaussian distributions is an example of a family of distributions whose commonly used parametrization in terms of mean and variance differs from the canonical parametrization as an exponential family. The mean and variance are easy to interpret, but in terms of mean and variance, the Gaussian distributions form a curved exponential family. For mathematical and computational purposes the canonical parametrization is preferable. For general exponential families it may seem restrictive that all the sufficient statistics, \\(t_j\\), take values in the same \\(p\\)-dimensional space, and that all marginal distributions share a common parameter vector \\(\\theta\\). This is, however, not a restriction. Say we have two distributions with sufficient statistics \\(\\tilde{t}_1 : \\mathcal{Y}_1 \\to \\mathbb{R}^{p_1}\\) and \\(\\tilde{t}_2 : \\mathcal{Y}_2 \\to \\mathbb{R}^{p_2}\\) and corresponding parameters \\(\\theta_1\\) and \\(\\theta_2\\), then we construct \\[t_1(y_1) = \\left(\\begin{array}{c} \\tilde{t}_1(y_1) \\\\ 0 \\end{array}\\right) \\quad \\text{and} \\quad t_2(y_2) = \\left(\\begin{array}{c} 0 \\\\ \\tilde{t}_2(y_2) \\end{array}\\right).\\] Now \\(t_1\\) and \\(t_2\\) both map into \\(\\mathbb{R}^{p}\\) with \\(p = p_1 + p_2\\), and we can bundle the parameters together into the vector \\[\\theta = \\left(\\begin{array}{c} \\theta_1 \\\\ \\theta_2 \\end{array}\\right) \\in \\mathbb{R}^p.\\] Clearly, this construction can be generalized to any number of distributions and allows us to always assume a common parameter space. The sufficient statistics then take care of selecting which of the coordinates in the parameter vector that is used for any particular marginal distribution. 7.1.2 Exponential family Bayesian networks An exponential family is defined above as a parametrized family of distributions on \\(\\mathcal{Y}\\) of independent variables. That is, the joint density in (7.1) factorizes w.r.t. a product measure. Without really changing the notation this assumption can be relaxed considerably. If the sufficient statistic \\(t_j\\), instead of being a fixed map, is allowed to depend on \\(y_1, \\ldots, y_{j-1}\\), \\(f(\\mathbf{y} \\mid \\theta)\\) as defined in (7.1) is still a joint density. The only difference is that the factor \\[f_j(y_j \\mid y_1, \\ldots, y_{j-1}, \\theta) = e^{\\theta^T t_j(y_j) - \\log \\varphi_j(\\theta)}\\] is now the conditional density of \\(y_j\\) given \\(y_1, \\ldots, y_{j-1}\\). In the notation we let the dependence of \\(t_j\\) on \\(y_1, \\ldots, y_{j-1}\\) be implicit as this will not affect the abstract theory. In any concrete example though, it will be clear how \\(t_j\\) actually depends upon all, some or none of these variables. Note that \\(\\varphi_j\\) may now also depend upon the data through \\(y_1, \\ldots, y_{j-1}\\). The observation above is powerful. It allows us to develop a unified approach based on exponential families for a majority of all statistical models that are applied in practice. The models we consider make two essential assumptions the variables that we model can be ordered such that \\(y_j\\) only depends on \\(y_1, \\ldots, y_{j-1}\\) for \\(j = 1, \\ldots, m\\), all the conditional distributions form exponential families themselves, with the conditioning variables entering through the \\(t_j\\)-maps. The first of these assumptions is equivalent to the joint distribution being a Bayesian network, that is, a distribution whose density factorizes according to a directed acyclic graph. This includes time series models and hierarchical models. The second assumption is more restrictive, but is a common practice in applied work. Moreover, as many standard statistical models of univariate discrete and continuous variables are, in fact, exponential families, building up a joint distribution as a Bayesian network via conditional binomial, Poisson, beta, Gamma and Gaussian distributions among others is a rather flexible framework, and yet it will result in a model with a density that factorizes as in (7.1). Bayesian networks is a large and interesting subject in itself, and it is unfair to gloss over all the details. One of the main points is that for many computations it is possible to develop efficient algorithms by exploiting the graph structure. The seminal paper by Lauritzen and Spiegelhalter (1988) demonstrated this for the computation of conditional distributions. The mere fact that the variables can be ordered in a way that aligns with how the variables depend on each other is useful, but there can be many ways to do this, and just specifying an ordering ignores important details of the graph. It is, however, beyond the scope of this book to get into the graph algorithms required for a thorough general treatment of Bayesian networks. 7.1.3 Likelihood computations To simplify the notation we introduce \\[t(\\mathbf{y}) = \\sum_{j=1}^m t_j(y_j),\\] which we refer to as the sufficient statistic, and \\[\\kappa(\\theta) = \\sum_{j=1}^m \\log \\varphi_j(\\theta),\\] which is a convex \\(C^{\\infty}\\)-function on \\(\\Theta\\). Having observed \\(\\mathbf{y} \\in \\mathcal{Y}\\) it is evident that the log-likelihood for the exponential family specified by (7.1) is \\[\\ell(\\theta) = \\log f(\\mathbf{y} \\mid \\theta) = \\theta^T t(\\mathbf{y}) - \\kappa(\\theta).\\] From this it follows that the gradient of the log-likelihood is \\[\\nabla \\ell(\\theta) = t(\\mathbf{y}) - \\nabla \\kappa(\\theta)\\] and the Hessian is \\[D^2 \\ell(\\theta) = - D^2 \\kappa(\\theta),\\] which is always negative semidefinite. The maximum-likelihood estimator exists if and only if there is a solution to the score equation \\[t(\\mathbf{y}) = \\nabla \\kappa(\\theta),\\] and it is unique if there is such a solution, \\(\\hat{\\theta}\\), for which \\(I(\\hat{\\theta}) = D^2 \\kappa(\\hat{\\theta})\\) is positive definite. We call \\(I(\\hat{\\theta})\\) the observed Fisher information. Note that under the independence assumption, \\[\\nabla \\log \\varphi_j(\\theta) = \\frac{1}{\\varphi_j(\\theta)} \\int t_j(u) e^{\\theta^T t_j(u)} \\nu_i(\\mathrm{d} u) = E_{\\theta}(t_j(Y)), \\] which means that the score equation can be expressed as \\[t(\\mathbf{y}) = \\sum_{j=1}^m E_{\\theta}(t_j(Y)).\\] In the Bayesian network setup \\(\\nabla \\log \\varphi_j(\\theta) = E_{\\theta}(t_j(Y) \\mid Y_1, \\ldots, Y_{j-1}),\\) and the score equation is \\[t(\\mathbf{y}) = \\sum_{j=1}^m E_{\\theta}(t_j(Y) \\mid y_1, \\ldots, y_{j-1}),\\] which is a bit more complicated as the right-hand-side depends on the observations. The definition of an exponential family in Section 7.1 encompasses the situation where \\(y_1, \\ldots, y_m\\) are i.i.d. by taking \\(t_j\\) to be independent of \\(j\\). In that case, \\(\\kappa(\\theta) = m \\kappa_1(\\theta)\\), the score equation can be rewritten as \\[\\frac{1}{m} \\sum_{j=1}^m t_1(y_j) = \\kappa_1(\\theta),\\] and the Fisher information becomes \\[I(\\hat{\\theta}) = m D^2 \\kappa_1(\\hat{\\theta}).\\] However, the point of the general formulation is that it includes regression models, and, via the Bayesian networks extension above, models with dependence structures. We could, of course, then have i.i.d. replications \\(\\mathbf{y}_1, \\ldots, \\mathbf{y}_n\\) of the entire \\(m\\)-dimensional vector \\(\\mathbf{y}\\), and we would get the score equation \\[\\frac{1}{n} \\sum_{i=1}^n t(\\mathbf{y}_i) = \\kappa(\\theta),\\] and corresponding Fisher information \\[I(\\hat{\\theta}) = n D^2 \\kappa(\\hat{\\theta}).\\] 7.1.4 Curved exponential families A curved exponential family consists of an exponential family together with a \\(C^2\\)-map \\(\\theta : \\Psi \\to \\Theta\\) from a set \\(\\Psi \\subseteq \\mathbb{R}^q\\). The set \\(\\Psi\\) provides a parametrization of the subset \\(\\theta(\\Psi) \\subseteq \\Theta\\) of the full exponential family, and the log-likelihood in the \\(\\psi\\)-parameter is \\[\\ell(\\psi) = \\theta(\\psi)^T t(\\mathbf{y}) - \\kappa(\\theta(\\psi)).\\] It has gradient \\[\\nabla \\ell(\\psi) = D \\theta(\\psi)^T t(\\mathbf{y}) - \\nabla \\kappa(\\theta(\\psi)) D \\theta(\\psi)\\] and Hessian \\[D^2 \\ell(\\psi) = \\sum_{k=1}^p D^2\\theta_k(\\psi) (t(\\mathbf{y})_k - \\partial_k \\kappa(\\theta(\\psi))) - D \\theta(\\psi)^T D^2 \\kappa(\\theta(\\psi)) D \\theta(\\psi).\\] References "],
["multinomial-models.html", "7.2 Multinomial models", " 7.2 Multinomial models The multinomial model is the model of all probability distributions on a finite set \\(\\mathcal{Y} = \\{1, \\ldots, K\\}\\). The model is parametrized by the simplex \\[\\Delta_K = \\left\\{(p_1, \\ldots, p_K)^T \\in \\mathbb{R}^K \\mid p_k \\geq 0, \\sum_{k=1}^K p_k = 1\\right\\}.\\] The distributions parametrized by the relative interior of \\(\\Delta_K\\) form an exponential family by the parametrization \\[p_k = \\frac{e^{\\theta_k}}{\\sum_{l=1}^K e^{\\theta_l}} \\in (0,1)\\] for \\((\\theta_1, \\ldots, \\theta_K)^T \\in \\mathbb{R}^K.\\) That is, the sufficient statistic is \\(k \\mapsto (\\delta_{1k}, \\ldots, \\delta_{Kk})^T \\in \\mathbb{R}^{K-1}\\) (where \\(\\delta_{lk} \\in \\{0, 1\\}\\) is the Kronecker delta being 1 if and only if \\(l = k\\)), and \\[\\varphi(\\theta_1, \\ldots, \\theta_K) = \\sum_{l=1}^K e^{\\theta_l}.\\] We call this exponential family parametrization the symmetric parametrization. The canonical parameter in the symmetric parametrization is not identifiable, and to resolve the lack of identifiability there is a tradition of fixing the last parameter as \\(\\theta_K = 0\\). This results in a canonical parameter \\(\\theta = (\\theta_1, \\ldots, \\theta_{K-1})^T \\in \\mathbb{R}^{K-1},\\) a sufficient statistic \\(t_1(k) = (\\delta_{1k}, \\ldots, \\delta_{(K-1)k})^T \\in \\mathbb{R}^p\\), \\[\\varphi(\\theta) = 1 + \\sum_{l=1}^{K-1} e^{\\theta_l}\\] and probabilities \\[p_k = \\left\\{\\begin{array}{ll} \\frac{e^{\\theta_k}}{1 + \\sum_{l=1}^{K-1} e^{\\theta_l}} &amp; \\quad \\text{if } k = 1, \\ldots, K-1 \\\\ \\frac{1}{1 + \\sum_{l=1}^{K-1} e^{\\theta_l}} &amp; \\quad \\text{if } k = K. \\end{array}\\right.\\] We see that in this parametrization \\(p_k = e^{\\theta_k}p_K\\) for \\(k = 1, \\ldots, K-1\\), where the probability of the last element \\(K\\) acts as a baseline or reference probability, and the factors \\(e^{\\theta_k}\\) act as multiplicative modifications of this baseline. Moreover, \\[\\frac{p_k}{p_l} = e^{\\theta_k - \\theta_l},\\] which is independent of the chosen baseline. In the special case of \\(K = 2\\) the two elements \\(\\mathcal{Y}\\) are often given other labels than \\(1\\) and \\(2\\). The most common labels are \\(\\{0, 1\\}\\) and \\(\\{-1, 1\\}\\). If we use the \\(0\\)-\\(1\\)-labels the convention is to use \\(p_0\\) as the baseline and thus \\[p_1 = \\frac{e^{\\theta}}{1 + e^{\\theta}} = e^{\\theta} p_0 = e^{\\theta} (1 - p_1).\\] parametrized by \\(\\theta \\in \\mathbb{R}\\). As this function of \\(\\theta\\) is known as the logistic function, this parametrization of the probability distributions on \\(\\{0,1\\}\\) is often referred to as the logistic model. From the above we see directly that \\[\\theta = \\log \\frac{p_1}{1 - p_1}\\] is the log-odds. If we use the \\(\\pm 1\\)-labels, an alternative exponential family parametrization is \\[p_k = \\frac{e^{k\\theta}}{e^\\theta + e^{-\\theta}}\\] for \\(\\theta \\in \\mathbb{R}\\) and \\(k \\in \\{-1, 1\\}\\). This gives a symmetric treatment of the two labels while retaining the identifiability. With i.i.d. observations from a multinomial distribution we find that the log-likelihood is \\[\\begin{align*} \\ell(\\theta) &amp; = \\sum_{i=1}^n \\sum_{k=1}^K \\delta_{k y_i} \\log(p_k(\\theta)) \\\\ &amp; = \\sum_{k=1}^K \\underbrace{\\sum_{i=1}^n \\delta_{k y_i}}_{= n_k} \\log(p_k(\\theta)) \\\\ &amp; = \\theta^T \\mathbf{n} - n \\log \\left(1 + \\sum_{l=1}^{K-1} e^{\\theta_l} \\right). \\end{align*}\\] where \\(\\mathbf{n} = (n_1, \\ldots, n_K)^T = \\sum_{i=1}^n t(y_i)\\) is the sufficient statistic, which is simply a tabulation of the times the different elements in \\(\\{1, \\ldots, K\\}\\) were observed. 7.2.1 Peppered Moths This example is on the color of the peppered Moth (Birkemåler in Danish). The color of the moth is primarily determined by one gene that occur in three different alleles denoted C, I and T. The alleles are ordered in terms of dominance as C &gt; I &gt; T. Moths with genotype including C are dark. Moths with genotype TT are light colored. Moths with genotypes II and IT are mottled. Thus there a total of six different genotypes (CC, CI, CT, II, IT and TT) and three different phenotypes (black, mottled, light colored). The peppered moth provided an early demonstration of evolution in the 19th century England, where the light colored moth was outnumbered by the dark colored variety. The dark color became dominant due to the increased pollution, where trees were darkened by soot, and had for that reason a selective advantage. There is a nice collection of moth in different colors at the Danish Zoological Museum, and further explanation of the role it played in understanding evolution. We denote the allele frequencies \\(p_C\\), \\(p_I\\), \\(p_T\\) with \\(p_C + p_I + p_T = 1.\\) According to the Hardy-Weinberg principle, the genotype frequencies are then \\[p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2, 2p_Ip_T, p_T^2.\\] If we could observe the genotypes, the complete multinomial log-likelihood would be \\[\\begin{align*} &amp; 2n_{CC} \\log(p_C) + n_{CI} \\log (2 p_C p_I) + n_{CT} \\log(2 p_C p_I) \\\\ &amp; \\ \\ + 2 n_{II} \\log(p_I) + n_{IT} \\log(2p_I p_T) + 2 n_{TT} \\log(p_T) \\\\ &amp; = 2n_{CC} \\log(p_C) + n_{CI} \\log (2 p_C p_I) + n_{CT} \\log(2 p_C p_I) \\\\ &amp; \\ \\ + 2 n_{II} \\log(p_I) + n_{IT} \\log(2p_I (1 - p_C - p_I)) + 2 n_{TT} \\log(1 - p_C - p_I). \\end{align*}\\] The log-likelihood is given in terms of the genotype counts and the two probability parameters \\(p_C, p_I \\geq 0\\) with \\(p_C + p_I \\leq 1\\), and on the interior of this parameter set the model is a curved exponential family. Collecting moths and determining their color will, however, only identify their phenotype, not their genotype. Thus we observe \\((n_C, n_T, n_I)\\), where \\[n = \\underbrace{n_{CC} + n_{CI} + n_{CT}}_{= n_C} + \\underbrace{n_{IT} + n_{II}}_{=n_I} + \\underbrace{n_{TT}}_{=n_T}.\\] For maximum-likelihood estimation of the parameters in this model from the observation \\((n_C, n_T, n_I)\\), we need the likelihood, that is, the likelihood in the marginal distribution of the observed variables. The peppered Moth example is an example of cell collapsing in a multinomial model. In general, let \\(A_1 \\cup \\ldots \\cup A_{K_0} = \\{1, \\ldots, K\\}\\) be a partition and let \\[M : \\mathbb{N}_0^K \\to \\mathbb{N}_0^{K_0}\\] be the map given by \\[M((n_1, \\ldots, n_K))_j = \\sum_{k \\in A_j} n_k.\\] If \\(Y \\sim \\textrm{Mult}(p, n)\\) with \\(p = (p_1, \\ldots, p_K)\\) then \\[X = M(Y) \\sim \\textrm{Mult}(M(p), n).\\] If \\(\\theta \\mapsto p(\\theta)\\) is a parametrization of the cell probabilities the log-likelihood under the collapsed multinomial model is generally \\[\\begin{equation} \\ell(\\theta) = \\sum_{j = 1}^{K_0} x_j \\log (M(p(\\theta))_j) = \\sum_{j = 1}^{K_0} x_j \\log \\left(\\sum_{k \\in A_j} p_k(\\theta)\\right). \\tag{7.2} \\end{equation}\\] For the peppered Moths, \\(K = 6\\) corresponding to the six genotypes, \\(K_0 = 3\\) and the partition corresponding to the phenotypes is \\[\\{1, 2, 3\\} \\cup \\{4, 5\\} \\cup \\{6\\} = \\{1, \\ldots, 6\\},\\] and \\[M(n_1, \\ldots, n_6) = (n_1 + n_2 + n_3, n_4 + n_5, n_6).\\] In terms of the \\((p_C, p_I)\\) parametrization, \\(p_T = 1 - p_C - p_I\\) and \\[p = (p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2, 2p_Ip_T, p_T^2).\\] Hence \\[M(p) = (p_C^2 + 2p_Cp_I + 2p_Cp_T, p_I^2 +2p_Ip_T, p_T^2).\\] The log-likelihood is \\[\\begin{align} \\ell(p_C, p_I) &amp; = n_C \\log(p_C^2 + 2p_Cp_I + 2p_Cp_T) + n_I \\log(p_I^2 +2p_Ip_T) + n_T \\log (p_T^2). \\end{align}\\] We can implement the log-likelihood in a very problem specific way. Note how the parameter constraints are encoded via the return value \\(\\infty\\). ## par = c(pC, pI), pT = 1 - pC - pI ## x is the data vector of length 3 of counts loglik &lt;- function(par, x) { pT &lt;- 1 - par[1] - par[2] if (par[1] &gt; 1 || par[1] &lt; 0 || par[2] &gt; 1 || par[2] &lt; 0 || pT &lt; 0) return(Inf) PC &lt;- par[1]^2 + 2 * par[1] * par[2] + 2 * par[1] * pT PI &lt;- par[2]^2 + 2 * par[2] * pT PT &lt;- pT^2 ## The function returns the negative log-likelihood - (x[1] * log(PC) + x[2] * log(PI) + x[3]* log(PT)) } It is possible to use optim in R with just this implementation to compute the maximum-likelihood estimate of the allele parameters. optim(c(0.3, 0.3), loglik, x = c(85, 196, 341)) ## $par ## [1] 0.07084643 0.18871900 ## ## $value ## [1] 600.481 ## ## $counts ## function gradient ## 71 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL The optim function uses an algorithm called Nelder-Mead as the default algorithm that relies on log-likelihood evaluations only. It is slow but fairly robust, though a bit of thought has to go into the initial parameter choice. optim(c(0, 0), loglik, x = c(85, 196, 341)) ## Error in optim(c(0, 0), loglik, x = c(85, 196, 341)): function cannot be evaluated at initial parameters Starting the algorithm in a boundary value where the negative log-likelihood attains the value \\(\\infty\\) does not work. The computations can beneficially be implemented in greater generality. The function M sums the cells that are collapsed, which has to be specified by the group argument. M &lt;- function(x, group) as.vector(tapply(x, group, sum)) The log-likelihood is then implemented for multinomial cell collapsing via M and two problem specific arguments to the loglik function. One of these is a vector specifying the grouping structure of the collapsing. The other is a function that determines the parametrization that maps the parameter that we are optimizing over to the cell probabilities. In the implementation it is assumed that this prob function in addition encodes parameter constraints by returning NULL if parameter constraints are violated. loglik &lt;- function(par, x, prob, group, ...) { p &lt;- prob(par) if(is.null(p)) return(Inf) - sum(x * log(M(prob(par), group))) } The function prob is implemented specifically for the peppered moths as follows. prob &lt;- function(p) { p[3] &lt;- 1 - p[1] - p[2] if (p[1] &gt; 1 || p[1] &lt; 0 || p[2] &gt; 1 || p[2] &lt; 0 || p[3] &lt; 0) return(NULL) c(p[1]^2, 2 * p[1] * p[2], 2* p[1] * p[3], p[2]^2, 2 * p[2] * p[3], p[3]^2) } We test that the new implementation gives the same result when optimized as using the more problem specific implementation. optim(c(0.3, 0.3), loglik, x = c(85, 196, 341), prob = prob, group = c(1, 1, 1, 2, 2, 3)) ## $par ## [1] 0.07084643 0.18871900 ## ## $value ## [1] 600.481 ## ## $counts ## function gradient ## 71 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Once we have found an estimate of the parameters, we can compute a prediction of the unobserved genotype counts from the phenotype counts using the conditional distribution of the genotypes given the phenotypes. This is straightforward as the conditional distribution of \\(Y_{A_j} = (Y_k)_{k \\in A_j}\\), conditionally on \\(X\\) is also a multinomial distribution; \\[Y_{A_j} \\mid X = x \\sim \\textrm{Mult}\\left( \\frac{p_{A_j}}{M(p)_j}, x_j \\right).\\] The probability parameters are simply \\(p\\) restricted to \\(A_j\\) and renormalized to a probability vector. Observe that this gives \\[E (Y_k \\mid X = x) = \\frac{x_j p_k}{M(p)_j}\\] for \\(k \\in A_j\\). Using the estimated parameters and the M function implemented above, we can compute a prediction of the genotype counts as follows. x &lt;- c(85, 196, 341) group &lt;- c(1, 1, 1, 2, 2, 3) p &lt;- prob(c(0.07084643, 0.18871900)) x[group] * p / M(p, group)[group] ## [1] 3.121549 16.630211 65.248241 22.154520 173.845480 341.000000 This is of interest in itself, but computing these conditional expectations will also be central for the EM algorithm in Chapter 9. "],
["regression.html", "7.3 Regression models", " 7.3 Regression models Regression models are models of one variable, called the response, conditionally on one or more other variables, called the predictors. Typically we use models that assume conditional independence of the responses given the predictors, and a particularly convenient class of regression models is based on exponential families. If we let \\(y_i \\in \\mathbb{R}\\) denote the \\(i\\)th response and \\(x_i \\in \\mathbb{R}^p\\) the \\(i\\)th predictor we can consider the exponential family of models with joint density \\[f(\\mathbf{y} \\mid \\mathbf{X}, \\beta) = \\prod_{i=1}^n e^{\\theta^T t_i(y_i \\mid x_i) - \\log \\varphi_i(\\theta)}\\] for suitably chosen base measures. Here \\[\\mathbf{X} = \\left( \\begin{array}{c} x_1^T \\\\ x_2^T \\\\ \\vdots \\\\ x_{n-1}^T \\\\ x_n^T \\end{array} \\right)\\] is called the model matrix and is an \\(n \\times p\\) matrix. Example 7.3 If \\(y_i \\in \\mathbb{N}_0\\) are counts we often use a Poisson regression model with point probabilities (density w.r.t. counting measure) \\[p_i(y_i \\mid x_i) = e^{-\\mu(x_i)} \\frac{\\mu(x_i)^{y_i}}{y_i!}.\\] If the mean depends on the predictors in a log-linear way, \\(\\log(\\mu(x_i)) = x_i^T \\beta\\), then \\[p_i(y_i \\mid x_i) = e^{\\beta^T x_i y_i - \\exp( x_i^T \\beta)} \\frac{1}{y_i!}.\\] The factor \\(1/y_i!\\) can be absorbed into the base measure, and we recognize this Poisson regression model as an exponential family with sufficient statistics \\[t_i(y_i) = x_i y_i\\] and \\[\\log \\varphi_i(\\beta) = \\exp( x_i^T \\beta).\\] To implement numerical optimization algorithms for computing the maximum-likelihood estimate we note that \\[t(\\mathbf{y}) = \\sum_{i=1}^n x_i y_i = \\mathbf{X}^T \\mathbf{y} \\quad \\text{and} \\quad \\kappa(\\beta) = \\sum_{i=1}^n e^{x_i^T \\beta},\\] that \\[\\nabla \\kappa(\\beta) = \\sum_{i=1}^n x_i e^{x_i^T \\beta},\\] and that \\[D^2 \\kappa(\\beta) = \\sum_{i=1}^n x_i x_i^T e^{x_i^T \\beta} = \\mathbf{X}^T \\mathbf{W}(\\beta) \\mathbf{X}\\] where \\(\\mathbf{W}(\\beta)\\) is a diagonal matrix with \\(\\mathbf{W}(\\beta)_{ii} = \\exp(x_i^T \\beta).\\) All these formulas follow directly from the general formulas for exponential families. To illustrate the use of a Poisson regression model we consider the following data set from a major Swedish supermarket chain. It contains the number of bags of frozen vegetables sold in a given week in a given store under a marketing campaign. A predicted number of items sold in a normal week (without a campaign) based on historic sales is included. It is of interest to recalibrate this number to give a good prediction of the number of items actually sold. vegetables &lt;- read_csv(&quot;data/vegetables.csv&quot;, col_types = cols(store = &quot;c&quot;)) summary(vegetables) ## sale normalSale store ## Min. : 1.00 Min. : 0.20 Length:1066 ## 1st Qu.: 12.00 1st Qu.: 4.20 Class :character ## Median : 21.00 Median : 7.25 Mode :character ## Mean : 40.29 Mean : 11.72 ## 3rd Qu.: 40.00 3rd Qu.: 12.25 ## Max. :571.00 Max. :102.00 It is natural to model the number of sold items using a Poisson regression model, and we will consider the following simple log-linear model: \\[\\log(E(\\text{sale} \\mid \\text{normalSale})) = \\beta_0 + \\beta_1 \\log(\\text{normalSale}).\\] This is an example of an exponential family regression model as above with a Poisson response distribution. It is a standard regression model that can be fitted to data using the glm function and using the formula interface to specify how the response should depend on the normal sale. The model is fitted by computing the maximum-likelihood estimate of the two parameters \\(\\beta_0\\) and \\(\\beta_1\\). pois_model_null &lt;- glm(sale ~ log(normalSale), data = vegetables, family = poisson) ## A Poisson regression model summary(pois_model_null) ## ## Call: ## glm(formula = sale ~ log(normalSale), family = poisson, data = vegetables) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -16.218 -2.677 -0.864 1.324 21.730 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.461 0.015 97.2 &lt;2e-16 *** ## log(normalSale) 0.922 0.005 184.4 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 51177 on 1065 degrees of freedom ## Residual deviance: 17502 on 1064 degrees of freedom ## AIC: 22823 ## ## Number of Fisher Scoring iterations: 5 The fitted model of the expected sale as a function of the normal sale, \\(x\\), is \\[x \\mapsto e^{1.46 + 0.92 \\times \\log(x)} = (4.31 \\times x^{-0.08}) \\times x.\\] This model roughly predicts a four-fold increase of the sale during a campaign, though the effect decays with increasing \\(x\\). If the normal sale is 10 items the factor is \\(4.31 \\times 10^{-0.08} = 3.58\\), and if the normal sale is 100 items the factor reduces to \\(4.31 \\times 100^{-0.08} = 2.98\\). We are not entirely satisfied with this model. It is fitted across all stores in the data set, and it is not obvious that the same effect should apply across all stores. Thus we would like to fit a model of the form \\[\\log(E(\\text{sale})) = \\beta_{\\text{store}} + \\beta_1 \\log(\\text{normalSale})\\] instead. Fortunately, this is also straightforward using glm. ## Note, variable store is a factor! The &#39;intercept&#39; is removed ## in the formula to obtain a parametrization as above. pois_model &lt;- glm(sale ~ store + log(normalSale) - 1, data = vegetables, family = poisson) We will not print all the individual store effects as there are 352 individual stores. summary(pois_model)$coefficients[c(1, 2, 3, 4, 353), ] ## Estimate Std. Error z value Pr(&gt;|z|) ## store1 2.7182 0.12756 21.309 9.416e-101 ## store10 4.2954 0.12043 35.668 1.254e-278 ## store100 3.4866 0.12426 28.060 3.055e-173 ## store101 3.3007 0.11791 27.993 1.989e-172 ## log(normalSale) 0.2025 0.03127 6.474 9.536e-11 We should note that the coefficient of \\(\\log(\\text{normalSale})\\) has changed considerably, and the model is a somewhat different model now for the individual stores. From a computational viewpoint the most important thing that has changed is that the number of parameters has increased a lot. In the first and simple model there are two parameters. In the second model there are 353 parameters. Computing the maximum-likelihood estimate is a considerably more difficult problem in the second case. "],
["finite-mixture-models.html", "7.4 Finite mixture models", " 7.4 Finite mixture models A finite mixture model is a model of a pair of random variables \\((Y, Z)\\) with \\(Z \\in \\{1, \\ldots, K\\}\\), \\(P(Z = k) = p_k\\), and the conditional distribution of \\(Y\\) given \\(Z = k\\) having density \\(f_k( \\cdot \\mid \\theta_k)\\). The joint density is then \\[(y, k) \\mapsto f_k(y \\mid \\theta_k) p_k,\\] and the marginal density for the distribution of \\(Y\\) is \\[f(y \\mid \\theta) = \\sum_{k=1}^K f_k(y \\mid \\theta_k) p_k\\] where \\(\\theta\\) is the vector of all parameters. We say that the model has \\(K\\) mixture components and call \\(f_k( \\cdot \\mid \\theta_k)\\) the mixture distributions and \\(p_k\\) the mixture weights. The main usage of mixture models is to situations where \\(Z\\) is not observed. In practice, only \\(Y\\) is observed, and parameter estimation has to be based on the marginal distribution of \\(Y\\) with density \\(f(\\cdot \\mid \\theta)\\), which is a weighted sum of the mixture distributions. The set of all probability measures on \\(\\{1, \\ldots, K\\}\\) is an exponential family with sufficient statistic \\[\\tilde{t}_0(k) = (\\delta_{1k}, \\delta_{2k}, \\ldots, \\delta_{(K-1)k}) \\in \\mathbb{R}^{K-1},\\] canonical parameter \\(\\alpha = (\\alpha_1, \\ldots, \\alpha_{K-1}) \\in \\mathbb{R}^{K-1}\\) and \\[p_k = \\frac{e^{\\alpha_k}}{1 + \\sum_{l=1}^{K-1} e^{\\alpha_l}}.\\] When \\(f_k\\) is an exponential family as well with sufficient statistic \\(\\tilde{t}_k : \\mathcal{Y} \\to \\mathbb{R}^{p_k}\\) and \\(\\theta_k\\) the canonical parameter, we bundle \\(\\alpha, \\theta_1, \\ldots, \\theta_K\\) into \\(\\theta\\) and define \\[t_1(y) = \\left(\\begin{array}{c} \\tilde{t}_0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right)\\] together with \\[t_2(y \\mid k) = \\left(\\begin{array}{c} 0 \\\\ \\delta_{1k} \\tilde{t}_1(y) \\\\ \\delta_{2k} \\tilde{t}_2(y) \\\\ \\vdots \\\\ \\delta_{Kk} \\tilde{t}_K(y) \\end{array} \\right)\\] we see that we have an exponential family of the joint distribution of \\((Y, Z)\\) with the \\(p = K-1 + p_1 + \\ldots + p_K\\)-dimensional canonical parameter \\(\\theta\\), with the sufficient statistic \\(t_1\\) determining the marginal distribution of \\(Z\\) and with the sufficient statistic \\(t_2\\) determining the conditional distribution of \\(Y\\) given \\(Z\\). We have here made the conditioning variable explicit. The marginal density of \\(Y\\) in the exponential family parametrization then becomes \\[f(y \\mid \\theta) = \\sum_{k=1}^K e^{\\theta^T (t_1(k) + t_2(y \\mid k)) - \\log \\varphi_1(\\theta) - \\log \\varphi_2(\\theta \\mid k)}.\\] For small \\(K\\) it is usually unproblematic to implement the computation of the marginal density using the formula above, and the computation of derivatives can likewise be implemented based on the formulas derived in Section 7.1.3. 7.4.1 Gaussian mixtures A Gaussian mixture model is a mixture model where all the mixture distributions are Gaussian distributions but potentially with different means and variances. In this section we focus on the simplest Gaussian mixture model with \\(K = 2\\) mixture components. When \\(K = 2\\), the Gaussian mixture model is parametrized by the five parameters: the mixture weight \\(p = P(Z = 1) \\in (0, 1)\\), the two means \\(\\mu_1, \\mu_2 \\in \\mathbb{R}\\), and the two variances \\(\\sigma_1, \\sigma_2 &gt; 0\\). This is not the parametrization using canonical exponential family parameters, which we return to below. First we will simply simulate random variables from this model. sigma1 &lt;- 1 sigma2 &lt;- 2 mu1 &lt;- -0.5 mu2 &lt;- 4 p &lt;- 0.5 n &lt;- 5000 z &lt;- sample(c(TRUE, FALSE), n, replace = TRUE, prob = c(p, 1 - p)) ## Conditional simulation from the mixture components y &lt;- numeric(n) n1 &lt;- sum(z) y[z] &lt;- rnorm(n1, mu1, sigma1) y[!z] &lt;- rnorm(n - n1, mu2, sigma2) The simulation above generated 5000 samples from a two-component Gaussian mixture model with mixture distributions \\(\\mathcal{N}(-0.5, 1)\\) and \\(\\mathcal{N}(4, 4)\\), and with each component having weight \\(0.5\\). This gives a bimodal distribution as illustrated by the histogram on Figure 7.1. yy &lt;- seq(-3, 11, 0.1) dens &lt;- p * dnorm(yy, mu1, sigma1) + (1 - p) * dnorm(yy, mu2, sigma2) ggplot(mapping = aes(y, ..density..)) + geom_histogram(bins = 20) + geom_line(aes(yy, dens), color = &quot;blue&quot;, size = 1) + stat_density(bw = &quot;SJ&quot;, geom=&quot;line&quot;, color = &quot;red&quot;, size = 1) Figure 7.1: Histogram and density estimate (red) of data simulated from a two-component Gaussian mixture distribution. The true mixture distribution has It is possible to give a mathematically different representation of the marginal distribution of \\(Y\\) that is sometimes useful. Though it gives the same marginal distribution from the same components, it does provide a different interpretation of what a mixture model is a model of, and it does provide a different way of simulating from a mixture distribution. If we let \\(Y_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)\\) and \\(Y_2 \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)\\) be independent, and independent of \\(Z\\), we can define \\[\\begin{equation} Y = 1(Z = 1) Y_1 + 1(Z = 2) Y_2 = Y_{Z}. \\tag{7.3} \\end{equation}\\] Clearly, the conditional distribution of \\(Y\\) given \\(Z = k\\) is \\(f_k\\). From (7.3) it follows directly that \\[E(Y^n) = P(Z = 1) E(Y_1^n) + P(Z = 2) E(Y_2^n) = p m_1(n) + (1 - p) m_2(n)\\] where \\(m_k(n)\\) denotes the \\(n\\)th non-central moment of the \\(k\\)th mixture component. In particular, \\[E(Y) = p \\mu_1 + (1 - p) \\mu_2.\\] The variance can be found from the second moment as \\[\\begin{align} V(Y) &amp; = p(\\mu_1^2 + \\sigma_1^2) + (1-p)(\\mu_2^2 + \\sigma_2^2) - (p \\mu_1 + (1 - p) \\mu_2)^2 \\\\ &amp; = p\\sigma_1^2 + (1 - p) \\sigma_2^2 + p(1-p)(\\mu_1^2 + \\mu_2^2 - 2 \\mu_1 \\mu_2). \\end{align}\\] While it is certainly possible to derive this formula by other means, using (7.3) gives a simple argument based on elementary properties of expectation and independence of \\((Y_1, Y_2)\\) and \\(Z\\). The construction via (7.3) has an interpretation that differs from how a mixture model was defined in the first place. Though \\((Y, Z)\\) has the correct joint distribution, \\(Y\\) is by (7.3) the result of \\(Z\\) selecting one out of two possible observations. The difference can best be illustrated by an example. Suppose that we have a large population consisting of married couples entirely. We can draw a sample of individuals (ignoring the marriage relations completely) from this population and let \\(Z\\) denote the sex and \\(Y\\) the height of the individual. Then \\(Y\\) follows a mixture distribution with two components corresponding to males and females according to the definition. At the risk of being heteronormative, suppose that all couples consist of one male and one female. We could then also draw a sample of married couples, and for each couple flip a coin to decide whether to report the male’s or the female’s height. This corresponds to the construction of \\(Y\\) by (7.3). We get the same marginal mixture distribution of heights though. Arguably the heights of individuals in a marriage are not independent, but this is actually immaterial. Any dependence structure between \\(Y_1\\) and \\(Y_2\\) is lost in the transformation (7.3), and we can just as well assume them independent for mathematical convenience. We won’t be able to tell the difference from observing only \\(Y\\) (and \\(Z\\)) anyway. We illustrate below how (7.3) can be used for alternative implementations of ways to simulate from the mixture model. We compare empirical means and variances to the theoretical values to test if all the implementations actually simulate from the Gaussian mixture model. ## Mean mu &lt;- p * mu1 + (1 - p) * mu2 ## Variance sigmasq &lt;- p * sigma1^2 + (1 - p) * sigma2^2 + p * (1-p)*(mu1^2 + mu2^2 - 2 * mu1 * mu2) ## Simulation using the selection formulation via &#39;ifelse&#39; y2 &lt;- ifelse(z, rnorm(n, mu1, sigma1), rnorm(n, mu2, sigma2)) ## Yet another way of simulating from a mixture model ## using arithmetic instead of &#39;ifelse&#39; for the selection ## and with Y_1 and Y_2 actually being dependent y3 &lt;- rnorm(n) y3 &lt;- z * (sigma1 * y3 + mu1) + (!z) * (sigma2 * y3 + mu2) ## Returning to the definition again, this last method simulates conditionally ## from the mixture components via transformation of an underlying Gaussian ## variable with mean 0 and variance 1 y4 &lt;- rnorm(n) y4[z] &lt;- sigma1 * y4[z] + mu1 y4[!z] &lt;- sigma2 * y4[!z] + mu2 ## Tests data.frame(mean = c(mu, mean(y), mean(y2), mean(y3), mean(y4)), variance = c(sigmasq, var(y), var(y2), var(y3), var(y4)), row.names = c(&quot;true&quot;, &quot;conditional&quot;, &quot;ifelse&quot;, &quot;arithmetic&quot;, &quot;conditional2&quot; )) ## mean variance ## true 1.750000 7.562500 ## conditional 1.782107 7.438091 ## ifelse 1.768700 7.463544 ## arithmetic 1.799331 7.462198 ## conditional2 1.772048 7.626938 In terms of run time there is not a big difference between three of the ways of simulating from a mixture model. A benchmark study (not shown) will reveal that the first and third methods are comparable in terms of run time and slightly faster than the fourth, while the second using ifelse takes more than twice as much time as the others. This is unsurprising as the ifelse method takes (7.3) very literally and generates twice the number of Gaussian variables actually needed. The marginal density of \\(Y\\) is \\[f(y) = p \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} e^{-\\frac{(y - \\mu_1)^2}{2 \\sigma_1^2}} + (1 - p)\\frac{1}{\\sqrt{2 \\pi \\sigma_2^2}}e^{-\\frac{(y - \\mu_2)^2}{2 \\sigma_2^2}}\\] as given in terms of the parameters \\(p\\), \\(\\mu_1, \\mu_2\\), \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\). Returning to the canonical parameters we see that they are given as follows: \\[\\theta_1 = \\log \\frac{p}{1 - p}, \\quad \\theta_2 = \\frac{\\mu_1}{2\\sigma_1^2}, \\quad \\theta_3 = \\frac{1}{2\\sigma_1^2}, \\quad \\theta_4 = \\frac{\\mu_2}{\\sigma_2^2}, \\quad \\theta_5 = \\frac{1}{2\\sigma_2^2}.\\] The joint density in this parametrization then becomes \\[(y,k) \\mapsto \\left\\{ \\begin{array}{ll} \\exp\\left(\\theta_1 + \\theta_2 y - \\theta_3 y^2 - \\log (1 + e^{\\theta_1}) - \\frac{ \\theta_2^2}{4\\theta_3}+ \\frac{1}{2} \\log(\\theta_3) \\right) &amp; \\quad \\text{if } k = 1 \\\\ \\exp\\left(\\theta_4 y - \\theta_5 y^2 - \\log (1 + e^{\\theta_1}) - \\frac{\\theta_4^2}{4\\theta_5} + \\frac{1}{2}\\log(\\theta_5) \\right) &amp; \\quad \\text{if } k = 2 \\end{array} \\right. \\] and the marginal density for \\(Y\\) is \\[\\begin{align} f(y \\mid \\theta) &amp; = \\exp\\left(\\theta_1 + \\theta_2 y - \\theta_3 y^2 - \\log (1 + e^{\\theta_1}) - \\frac{ \\theta_2^2}{4\\theta_3}+ \\frac{1}{2} \\log(\\theta_3) \\right) \\\\ &amp; + \\exp\\left(\\theta_4 y - \\theta_5 y^2 - \\log (1 + e^{\\theta_1}) - \\frac{\\theta_4^2}{4\\theta_5} + \\frac{1}{2}\\log(\\theta_5) \\right). \\end{align}\\] There is no apparent benefit to the canonical parametrization when considering the marginal density. It is, however, of value when we need the logarithm of the joint density as we will for the EM-algorithm. 7.4.2 von Mises mixtures "],
["mixed-models.html", "7.5 Mixed models", " 7.5 Mixed models A mixed model is a regression model of observations that allows for random variation at two different levels. In this section we will focus on mixed models in an exponential family context. Mixed models can be considered in greater generality but there will then be little shared structure and one has to deal with the models much more on a case-by-case manner. In a mixed model we have observations \\(y_j \\in \\mathcal{Y}\\) and \\(z \\in \\mathcal{Z}\\) such that: the distribution of \\(z\\) is an exponential family with canonical parameter \\(\\theta_0\\) conditionally on \\(z\\) the \\(y_j\\)s are independent with a distribution from an exponential family with sufficient statistics \\(t_j( \\cdot \\mid z)\\). This definition emphasizes how the \\(y_j\\)s have variation at two levels. There is variation in the underlying \\(z\\), which is the first level of variation (often called the random effect), and then there is variation among the \\(y_j\\)s given the \\(z\\), which is the second level of variation. It is a special case of hierarchical models (Bayesian networks with tree graphs) also known as multilevel models, with the mixed model having only two levels. When we observe data from such a model we typically observe independent replications, \\((y_{ij})_{j=1, \\ldots, m_i}\\) for \\(i = 1, \\ldots, n\\), of the \\(y_j\\)s only. Note that we allow for a different number, \\(m_i\\), of \\(y_j\\)s for each \\(i\\). The simplest class of mixed models is obtained by \\(t_j = t\\) not depending on \\(j\\), and \\[t(y_j \\mid z) = \\left(\\begin{array}{cc} t_1(y_j) \\\\ t_2(y_j, z) \\end{array} \\right)\\] for some fixed maps \\(t_1 : \\mathcal{Y} \\mapsto \\mathbb{R}^{p_1}\\) and \\(t_2 : \\mathcal{Y} \\times \\mathcal{Z} \\mapsto \\mathbb{R}^{p_2}\\). This is called a random effects model (this is a model where there are no fixed effects in the sense that \\(t_j\\) does not depend on \\(j\\), and given the random effect \\(z\\) the \\(y_j\\)s are i.i.d.). The canonical parameters associated with such a model are \\(\\theta_0\\) that enters into the distribution of the random effect, \\(\\theta_1 \\in \\mathbb{R}^{p_1}\\) and \\(\\theta_2 \\in \\mathbb{R}^{p_2}\\) that enter into the conditional distribution of \\(y_j\\) given \\(z\\). Example 7.4 The special case of a Gaussian, linear random effects model is the model where \\(z\\) is \\(\\mathcal{N}(0, 1)\\) distributed, \\(\\mathcal{Y} = \\mathbb{R}\\) (with base measure proportional to Lebesgue measure) and the sufficient statistic is \\[t(y_j \\mid z) = \\left(\\begin{array}{cc} y_j \\\\ - y_j^2 \\\\ zy_j \\end{array}\\right).\\] There are no free parameters in the distribution of \\(z\\). From Example 7.2 it follows that the conditional variance of \\(y\\) given \\(z\\) is \\[\\sigma^2 = \\frac{1}{2\\theta_2}\\] and the conditional mean of \\(y\\) given \\(z\\) is \\[\\frac{\\theta_1 + \\theta_3 z}{2 \\theta_2} = \\sigma^2 \\theta_1 + \\sigma^2 \\theta_3 z.\\] Reparametrizing in terms of \\(\\sigma^2\\), \\(\\beta_0 = \\sigma^2 \\theta_1\\) and \\(\\nu = \\sigma^2 \\theta_3\\) we see how the conditional distribution of \\(y_j\\) given \\(z\\) is \\(\\mathcal{N}(\\beta_0 + \\nu z, \\sigma^2)\\). From this it is clear how the mixed model of \\(y_j\\) conditionally on \\(z\\) is a regression model. However, we do not observe \\(z\\) in practice. Using the above distributional result we can see that the Gaussian random effects model of observations \\(y_{ij}\\) can equivalently be stated as \\[Y_{ij} = \\beta_0 + \\nu Z_i + \\varepsilon_{ij}\\] for \\(i = 1, \\ldots, n\\) and \\(j = 1, \\ldots, m_i\\) where \\(Z_1, \\ldots, Z_n\\) are i.i.d. \\(\\mathcal{N}(0, 1)\\)-distributed and independent of \\(\\varepsilon_{11}, \\varepsilon_{12}, \\ldots, \\varepsilon_{1n_1}, \\ldots, \\varepsilon_{mn_m}\\) that are themselves i.i.d. \\(\\mathcal{N}(0, \\sigma^2)\\)-distributed. "],
["state-space-models.html", "7.6 State space models", " 7.6 State space models "],
["numopt.html", "Chapter 8 Numerical optimization", " Chapter 8 Numerical optimization The main application of numerical optimization in statistics is for the computation of parameter estimates. Typically by maximizing the likelihood function or by maximizing or minimizing another estimation criterion. The focus of this chapter is on optimization algorithms for (penalized) maximum likelihood estimation, in particular for exponential families. Out of tradition we formulate all results and algorithms in terms of minimization and not maximization. The generic optimization problem considered is the minimization of \\(H : \\Theta \\to \\mathbb{R}\\) for \\(\\Theta \\subseteq \\mathbb{R}^p\\) an open set and \\(H\\) twice differentiable. In applications, \\(H = -\\ell\\), the negative log-likelihood function, or \\(H = -\\ell + J\\), where \\(J : \\Theta \\to \\mathbb{R}\\) is a penalty function, likewise twice differentiable, that does not depend upon data. There are a couple of observations regarding statistical optimization problems that are worthwhile to make once and for all. The \\(-\\ell\\) term in the objective function to be minimized is a sum over the data, and the more data we have the more computationally demanding it is to evaluate \\(H\\) and its derivatives. There are exceptions, though, when a sufficient statistic can be computed upfront. Additionally, high precision in the computed (local) minimizer is rarely needed. If the numerical precision is already orders of magnitudes smaller than the statistical uncertainty of the parameter estimate being computed, further optimization will not make a difference in any relevant way. In fact, blindly pursuing the global maximum of the likelihood can very well lead you astray if your model is not as well behaved as an exponential family is. In situations where \\(H\\) is not convex, as is the case for finite mixtures, there are common examples where the likelihood is unbounded, yet there will be a local minimizer that is a good estimate. Though we typically phrase our algorithms as optimization algorithms, what we are really pursuing when \\(H\\) is not convex are stationary points; solutions to \\(\\nabla H(\\theta) = 0\\). And rather than being fixated on computing the global minimizer of \\(H\\) to the greatest numerical precision, we should find as many approximately stationary points as possible, and more generally explore the likelihood surface. In this respect, optimization in statistics differs from certain other applications of optimization. Finding the optimum is not “the thing”. It is a surrogate for “the thing”, which is to fit models to data and understand the statistical precision of our fitted model. That is, as a minimum we should quantify the uncertainty of (relevant aspects of) the fitted model that is due to the finite sample size. For the generic minimization problem considered in this chapter, the practical challenge when implementing algorithms in R is typically to implement efficient evaluation of \\(H\\) and its derivatives. In particular, efficient evaluations of \\(-\\ell\\). Several choices of standard optimization algorithms are possible and some are already implemented and available in R. For many practical purposes the BFGS-algorithms as implemented via the optim function work well and require only the computation of gradients. It is, of course, paramount that \\(H\\) and \\(\\nabla H\\) are correctly implemented, and efficiency of the algorithms is largely determined by the efficiency of the implementation of \\(H\\) and \\(\\nabla H\\) but also by the choice of parametrization. Newton-type algorithms are available through nlm and nls but with different interfaces. "],
["algorithms-and-convergence.html", "8.1 Algorithms and convergence", " 8.1 Algorithms and convergence A numerical optimization algorithm computes from an initial value \\(\\theta_0 \\in \\Theta\\) a sequence \\(\\theta_1, \\theta_2, \\ldots \\in \\Theta\\). One could hope for \\[\\theta_n \\rightarrow \\text{arg min}_{\\theta} H(\\theta)\\] for \\(n \\to \\infty\\), but much less can typically be guaranteed. First, the global minimizer may not exist or it may not be unique, in which case the convergence itself is ambiguous. Second, \\(\\theta_n\\) can in general only be shown to converge to a local minimizer if anything. Third, \\(\\theta_n\\) may not even converge, but \\(H(\\theta_n)\\) may still converge to a local minimum. This section will give a brief introduction to convergence analysis of optimization algorithms. We will see what kind of conditions on \\(H\\) can be used to show convergence results and some of the basic proof techniques. We will only scratch the surface here with the hope that it can motivate the algorithms that will be introduced in subsequent sections and chapters as well as the empirical techniques introduced below for practical assessment of convergence. 8.1.1 Descent algorithms Example 8.1 Suppose that \\(D^2H(\\theta)\\) has numerical radius uniformly bounded by \\(L\\), that is, \\[|\\gamma^T D^2H(\\theta) \\gamma| \\leq L \\|\\gamma\\|_2^2\\] for all \\(\\theta \\in \\Theta\\) and \\(\\gamma \\in \\mathbb{R}^p\\). Define an algorithm by \\[\\theta_{n} = \\theta_{n-1} - \\frac{1}{L + 1} \\nabla H(\\theta_{n-1}).\\] Fixing \\(n\\) there is by Taylor’s theorem a \\(\\tilde{\\theta} = \\alpha \\theta_{n} + (1- \\alpha)\\theta_{n-1}\\) (where \\(\\alpha \\in [0,1]\\)) on the line between \\(\\theta_n\\) and \\(\\theta_{n-1}\\) such that \\[\\begin{align*} H(\\theta_n) &amp; = H(\\theta_{n-1}) - \\frac{1}{L+1} \\|\\nabla H(\\theta_{n-1})\\|_2^2 + \\frac{1}{(L+1)^2} \\nabla H(\\theta_{n-1})^T D^2H(\\tilde{\\theta}) \\nabla H(\\theta_{n-1}) \\\\ &amp; \\leq H(\\theta_{n-1}) - \\frac{1}{L+1} \\|\\nabla H(\\theta_{n-1})\\|_2^2 + \\frac{L}{(L+1)^2} \\|\\nabla H(\\theta_{n-1})\\|_2^2 \\\\ &amp; = H(\\theta_{n-1}) - \\frac{1}{(L+1)^2} \\|\\nabla H(\\theta_{n-1})\\|_2^2. \\end{align*}\\] This shows that \\(H(\\theta_n) \\leq H(\\theta_{n-1})\\), and if \\(\\theta_{n-1}\\) is not a stationary point, \\(H(\\theta_n) &lt; H(\\theta_{n-1})\\). That is, the algorithm will produce a sequence with non-increasing \\(H\\)-values, and unless it hits a stationary point the \\(H\\)-values will be strictly decreasing. The algorithm is an example of a gradient descent algorithm. In general, we define a descent algorithm to be an algorithm for which \\[H(\\theta_0) \\geq H(\\theta_1) \\geq H(\\theta_2) \\geq \\ldots.\\] If all inequalities are sharp, unless if some \\(\\theta_i\\) is a local minimizer, the algorithm is called a strict descent algorithm. The gradient descent algorithm in Example 8.1 is a strict descent algorithm. However, even for a strict descent algorithm, \\(H\\) may just descent in smaller and smaller steps without converging toward a local minimum – even if \\(H\\) is bounded below. Suppose now that \\(H\\) is level bounded, meaning that the closed set \\[\\mathrm{lev}(\\theta_0) = \\{\\theta \\in \\Theta \\mid H(\\theta) \\leq H(\\theta_0)\\}\\] is bounded (and thereby compact). Then \\(H\\) is bounded from below and \\(H(\\theta_n)\\) is convergent for any descent algorithm. Restricting attention to the gradient descent algorithm, we see that \\[\\begin{align} H(\\theta_n) &amp; = (H(\\theta_n) - H(\\theta_{n-1})) + (H(\\theta_{n-1}) - H(\\theta_{n-2})) + ... + (H(\\theta_1) - H(\\theta_0)) + H(\\theta_0) \\\\ &amp; \\leq H(\\theta_0) - \\frac{1}{(L + 1)^2} \\sum_{k=1}^n \\|\\nabla H(\\theta_{k-1})\\|_2^2. \\end{align}\\] Because \\(H\\) is bounded below, this implies that \\(\\sum_{k=1}^{\\infty} \\|\\nabla H(\\theta_{k-1})\\|_2^2 &lt; \\infty\\) and hence \\[\\|\\nabla H(\\theta_{n})\\|_2 \\rightarrow 0\\] for \\(n \\to \\infty\\). By compactness of \\(\\mathrm{lev}(\\theta_0)\\), \\(\\theta_n\\) has a convergent subsequence with limit \\(\\theta_{\\infty}\\), and we conclude by continuity of \\(\\nabla H\\) that \\[\\nabla H(\\theta_{\\infty}) = 0,\\] and \\(\\theta_{\\infty}\\) is a stationary point. In fact, this holds for any limit point of the sequence, and this implies that if \\(H\\) has a unique stationary point in \\(\\mathrm{lev}(\\theta_0)\\), \\(\\theta_{\\infty}\\) is a minimizer, and \\[\\theta_n \\rightarrow \\theta_{\\infty}\\] for \\(n \\to \\infty\\). To summarize, if \\(D^2H(\\theta)\\) has uniformly bounded numerical radius, and if \\(H\\) is level bounded with a unique stationary point in \\(\\mathrm{lev}(\\theta_0)\\), then the gradient descent algorithm of Example 8.1 is a strict descent algorithm that converges toward that minimum. A sufficient condition on \\(H\\) for this to hold is that the eigenvalues of (the symmetric) matrix \\(D^2H(\\theta)\\) for all \\(\\theta\\) are contained in an interval \\([l, L]\\) with \\(0 &lt; l \\leq L\\). In this case, \\(H\\) is a strongly convex function with a unique global minimizer. 8.1.2 Maps and fixed points Most algorithms take the form of an update scheme, which from a mathematical viewpoint is a map \\(\\Phi : \\Theta \\to \\Theta\\) such that \\[\\theta_n = \\Phi(\\theta_{n-1}) = \\Phi \\circ \\Phi (\\theta_{n-2}) = \\Phi^{\\circ n}(\\theta_0).\\] The gradient descent algorithm from Example 8.1 is given by the map \\[\\Phi_{\\nabla}(\\theta) = \\theta - \\frac{1}{L + 1} \\nabla H(\\theta).\\] When the map \\(\\Phi\\) is continuous and \\(\\theta_n \\rightarrow \\theta_{\\infty}\\) it follows that \\[\\Phi(\\theta_n) \\rightarrow \\Phi(\\theta_{\\infty}).\\] Since \\(\\Phi(\\theta_n) = \\theta_{n+1} \\rightarrow \\theta_{\\infty}\\) we see that \\[\\Phi(\\theta_{\\infty}) = \\theta_{\\infty}.\\] That is, \\(\\theta_{\\infty}\\) is a fixed point of \\(\\Phi\\). The gradient descent map, \\(\\Phi_{\\nabla}\\), has \\(\\theta\\) as fixed point if and only if \\[\\nabla H(\\theta) = 0,\\] that is, if and only if \\(\\theta\\) is a stationary point. We can use the observation above to flip the perspective around. Instead of asking if \\(\\theta_n\\) converges to a local minimizer for a given algorithm, we can ask if we can find a map \\(\\Phi: \\Theta \\to \\Theta\\) whose fixed points are local minimizers. If so, we can ask if the iterates \\(\\Phi^{\\circ n}(\\theta_0)\\) converge. Mathematics is full of fixed point theorems that: i) give conditions under which a map has a fixed point; and ii) in some cases guarantee that the iterates \\(\\Phi^{\\circ n}(\\theta_0)\\) converge. The most prominent such fixed point theorem is Banach’s fixed point theorem. It states that if \\(\\Phi\\) is a contraction, that is, \\[\\| \\Phi(\\theta) - \\Phi(\\theta&#39;)\\| \\leq c \\|\\theta - \\theta&#39;\\|\\] for a constant \\(c \\in [0,1)\\) (using any norm), then \\(\\Phi\\) has a unique fixed point and \\(\\Phi^{\\circ n}(\\theta_0)\\) converges to that fixed point for any starting point \\(\\theta_0 \\in \\Theta\\). We will show that \\(\\Phi_{\\nabla}\\) is a contraction under the assumption that the eigenvalues of \\(D^2H(\\theta)\\) for all \\(\\theta\\) are contained in an interval \\([l, L]\\) with \\(0 &lt; l \\leq L\\). If \\(\\theta, \\theta&#39; \\in \\Theta\\) we find by Taylor’s theorem that \\[\\nabla H(\\theta) = \\nabla H(\\theta&#39;) + D^2H(\\tilde{\\theta})(\\theta - \\theta&#39;)\\] for some \\(\\tilde{\\theta}\\) on the line between \\(\\theta\\) and \\(\\theta&#39;\\). For the gradient descent map this gives that \\[\\begin{align*} \\|\\Phi_{\\nabla}(\\theta) - \\Phi_{\\nabla}(\\theta&#39;)\\|_2 &amp; = \\left\\|\\theta - \\theta&#39; - \\frac{1}{L+1}\\left(\\nabla H(\\theta) - \\nabla H(\\theta&#39;)\\right)\\right\\|_2 \\\\ &amp; = \\left\\|\\theta - \\theta&#39; - \\frac{1}{L+1}\\left( D^2H(\\tilde{\\theta})(\\theta - \\theta&#39;)\\right)\\right\\|_2 \\\\ &amp; = \\left\\|\\left(I - \\frac{1}{L+1} D^2H(\\tilde{\\theta}) \\right) (\\theta - \\theta&#39;)\\right\\|_2 \\\\ &amp; \\leq \\left(1 - \\frac{l}{L + 1}\\right) \\|\\theta - \\theta&#39;\\|_2, \\end{align*}\\] since the eigenvalues of \\(I - \\frac{1}{L+1} D^2H(\\tilde{\\theta})\\) are all between \\(1 - L/(L + 1)\\) and \\(1 - l/(L+1)\\). This shows that \\(\\Phi_{\\nabla}\\) is a contraction with \\(c = 1 - l/(L + 1) &lt; 1\\), and it provides an alternative proof, via Banach’s fixed point theorem, of convergence of the gradient descent algorithm in Example 8.1 for a strongly convex \\(H\\) with uniformly bounded Hessian. 8.1.3 Convergence rate Banach’s fixed point theorem tells us more. It actually tells us that \\[\\|\\theta_n - \\theta_{\\infty}\\| = \\|\\Phi(\\theta_{n-1}) - \\theta_{\\infty}\\| \\leq c \\|\\theta_{n-1} - \\theta_{\\infty}\\| \\leq c^n \\|\\theta_0 - \\theta_{\\infty}\\|.\\] That is, \\(\\|\\theta_n - \\theta_{\\infty}\\| \\to 0\\) with at least geometric rate \\(c &lt; 1\\). To discuss how fast numerical optimization algorithms converge in general, there is a refined notion of asymptotic convergence order as well as rate. We say that the algorithm has asymptotic convergence order \\(q\\) with asymptotic rate \\(r\\) if \\[\\lim_{n \\to \\infty} \\frac{\\|\\theta_{n} - \\theta_{\\infty}\\|}{\\|\\theta_{n-1} - \\theta_{\\infty}\\|^q} = r.\\] If the order is \\(q = 1\\) we say that the convergence is linear, if \\(q = 2\\) we say that the convergence is quadratic and so on. If \\[\\limsup_{n \\to \\infty} \\frac{\\|\\theta_{n} - \\theta_{\\infty}\\|}{\\|\\theta_{n-1} - \\theta_{\\infty}\\|} = 1\\] we say that convergence is sublinear. Clearly, Banach’s fixed point theorem implies a convergence that is at least as fast as linear convergence with asymptotic rate \\(c\\). If the smallest possible \\(c\\) is close to 1, the convergence may be relatively slow, but it is still linear and thus (asymptotically) faster than sublinear convergence. It is, of course, also possible to investigate how \\(H(\\theta_n)\\) converges toward a local minimum, or how the gradient, \\(\\nabla H(\\theta_n)\\), converges toward zero. We will use the same terminology of order and rate for these sequences. For applications it is of interest to estimate order and rate from running the algorithm. One way to do it is by running the algorithm for a large number, \\(N\\), say, of iterations – ideally so that \\(\\theta_N = \\theta_{\\infty}\\) up to computer precision. If the order is \\(q\\) and the rate is \\(r\\) then \\[\\log \\|\\theta_{n} - \\theta_{N}\\| \\simeq q \\log \\|\\theta_{n-1} - \\theta_{N}\\| + \\log(r)\\] for \\(n = N_0, \\ldots, N\\) for some \\(N_0\\). We can use that to estimate \\(q\\) and \\(\\log(r)\\) by fitting a linear function by least squares to these log-log transformed norms of errors. Alternatively, if \\(\\Phi\\) is a contraction for \\(n \\geq N_0\\) for some \\(N_0\\), then for \\(n \\geq N_0\\) \\[\\|\\theta_{n + 1} - \\theta_{n}\\| \\leq r^n \\| \\theta_1 - \\theta_0\\|.\\] The convergence may be superlinear, but if it linear, the rate is bounded by \\(r\\). If the inequality is approximately an equality, the convergence is linear and the asymptotic rate is \\(r\\). Moreover, \\[R_n = \\frac{\\|\\theta_{n + 1} - \\theta_{n}\\|}{\\|\\theta_{n} - \\theta_{n- 1}\\|} \\rightarrow r.\\] We can monitor and plot the ratio \\(R_n\\) as the algorithm is running, and we can use \\(R_n\\) as an estimate of \\(r\\) for large \\(n\\). If \\(R_n \\to 1\\) the algorithm is called logarithmically convergent (by definition) and it has sublinear convergence. Observing \\(R_n \\rightarrow 0\\) is an indication of superlinear convergence, while observing \\(R_n \\rightarrow r \\in (0,1)\\) is an indication of linear convergence with rate \\(r\\). We can also consider \\[\\log \\|\\theta_{n + 1} - \\theta_{n}\\| \\simeq n \\log(r) + d,\\] and we can plot and monitor \\(\\log \\|\\theta_{n + 1} - \\theta_{n}\\|\\) as the algorithm is running. It should decay approximately linearly as a function of \\(n\\) with slope \\(\\log(r) &lt; 0\\) that can be estimated by least squares. If the algorithm has sublinear convergence we will see this as a slower-than-linear decay. As mentioned above, we can monitor the convergence of the sequences \\(H(\\theta_n)\\) or \\(\\nabla H(\\theta_n)\\), instead of \\(\\theta_n\\), using the same techniques as described for \\(\\theta_n\\). Here \\(\\nabla H(\\theta_n)\\) is particularly appealing as we know that the limit should be \\(0\\). Thus we can directly monitor \\(\\log \\| \\nabla H(\\theta_n) \\|\\) as a function of \\(n\\), plot it against \\(\\log \\| \\nabla H(\\theta_{n-1}) \\|\\) and estimate asymptotic order and rate. 8.1.4 Stopping criteria All the stopping criteria considered here depend on choosing a tolerance parameter \\(\\varepsilon &gt; 0\\). Small relative descent: Stop when \\[H(\\theta_{n-1}) - H(\\theta_n) \\leq \\varepsilon (H(\\theta_n) + \\varepsilon).\\] The reason for this formulation, and in particular the added \\(\\varepsilon\\) on the right hand side, is for the criterion to be well behaved even if \\(H(\\theta_n)\\) comes close to zero. Small gradient: Stop when \\[\\|\\nabla H(\\theta_n)\\| \\leq \\varepsilon.\\] Note that many different norms, \\(\\|\\cdot\\|\\), may be used. If the coordinates of the gradient generally are of different orders of magnitude a norm that rescales the coordinates can be chosen. Small relative change: Stop when \\[\\|\\theta_n - \\theta_{n-1}\\| \\leq \\varepsilon(\\|\\theta_n\\| + \\varepsilon).\\] "],
["descent-direction-algorithms.html", "8.2 Descent direction algorithms", " 8.2 Descent direction algorithms The negative gradient of \\(H\\) in \\(\\theta\\) is the direction of steepest descent. Starting from \\(\\theta_0\\) and with the goal of minimizing \\(H\\), it is natural to move away from \\(\\theta_0\\) in the direction of \\(-\\nabla H(\\theta_0)\\). Thus we could define \\[\\theta_1 = \\theta_0 - \\gamma \\nabla H(\\theta_0)\\] for a suitably chosen \\(\\gamma &gt; 0\\). By Taylor’s theorem \\[H(\\theta_1) = H(\\theta_0) - \\gamma \\|\\nabla H(\\theta_0)\\|^2_2 + o(\\gamma),\\] which means that if \\(\\theta_0\\) is not a stationary point (\\(\\nabla H(\\theta_0) \\neq 0\\)) then \\[H(\\theta_1) &lt; H(\\theta_0)\\] for \\(\\gamma\\) small enough. More generally, we define a descent direction in \\(\\theta_0\\) as a vector \\(\\rho_0 \\in \\mathbb{R}^p\\) such that \\[\\nabla H(\\theta_0)^T \\rho_0 &lt; 0.\\] By the same kind of Taylor argument as above, \\(H\\) will descent for a sufficiently small step size in the direction of any descent direction. And if \\(\\theta_0\\) is not a stationary point, \\(-\\nabla H(\\theta_0)^T\\) is a descent direction. One strategy for choosing \\(\\gamma\\) is to minimize the univariate function \\[\\gamma \\mapsto H(\\theta_0 + \\gamma \\rho_0),\\] which is an example of a line search method. Such a minimization would give the maximal possible descent in the direction \\(\\rho_0\\), and as we have argued, if \\(\\rho_0\\) is a descent direction, a minimizer \\(\\gamma &gt; 0\\) guarantees descent of \\(H\\). However, unless the minimization can be done analytically it is often computationally too expensive. Less will also do, and as shown in Example 8.1, if the Hessian has uniformly bounded numerical radius it is possible to fix one (sufficiently small) step length that will guarantee descent. 8.2.1 Line search We consider algorithms of the form \\[\\theta_{n+1} = \\theta_n + \\gamma_{n} \\rho_n\\] for descent directions \\(\\rho_n\\) and starting in \\(\\theta_0\\). The step lengths, \\(\\gamma_n\\), are chosen so as to give sufficient descent in each iteration. We let \\(h(\\gamma) = H(\\theta_{n} + \\gamma \\rho_{n})\\) denote the univariate and differentiable function of \\(\\gamma\\), \\[h : [0,\\infty) \\to \\mathbb{R},\\] that gives the value of \\(H\\) in the direction of the descent direction \\(\\rho_n\\). We can observe that \\[h&#39;(\\gamma) = \\nabla H(\\theta_{n} + \\gamma \\rho_{n})^T \\rho_{n},\\] and maximal descent in direction \\(\\rho_n\\) can be found by solving \\(h&#39;(\\gamma) = 0\\) for \\(\\gamma\\). As mentioned above, less will do. First note that \\[h&#39;(0) = \\nabla H(\\theta_{n})^T \\rho_{n} &lt; 0,\\] so \\(h\\) has a negative slope in \\(0\\). It descents in a sufficiently small interval \\([0, \\varepsilon)\\), and it is even true that for any \\(c \\in (0, 1)\\) there is an \\(\\varepsilon &gt; 0\\) such that \\[h(\\gamma) \\leq h(0) + c \\gamma h&#39;(0)\\] for \\(\\gamma \\in [0, \\varepsilon)\\). We note that this inequality can be checked easily for any given \\(\\gamma &gt; 0\\), and is known as the sufficient descent condition. Sufficient descent is not enough in itself as the step length could be arbitrarily small, and the algorithm could effectively get stuck. To prevent too small steps we can enforce another condition. Very close to \\(0\\), \\(h\\) will have almost the same slope, \\(h&#39;(0)\\), as it has in \\(0\\). If we therefore require that the slope in \\(\\gamma\\) should be larger than \\(\\tilde{c} h&#39;(0)\\) for some \\(\\tilde{c} \\in (0, 1)\\), \\(\\gamma\\) is forced away from \\(0\\). This is known as the curvature condition. The combined conditions on \\(\\gamma\\), \\[h(\\gamma) \\leq h(0) + c \\gamma h&#39;(0)\\] for a \\(c \\in (0, 1)\\) and \\[h&#39;(\\gamma) \\geq \\tilde{c} h&#39;(0)\\] for a \\(\\tilde{c} \\in (c, 1)\\) are known collectively as the Wolfe conditions. It can be shown that if \\(h\\) is bounded below there exists a step length satisfying the Wolfe conditions (Lemma 3.1 in Nocedal and Wright (2006)). Even when choosing \\(\\gamma_{n}\\) to fulfill the Wolfe conditions there is no guarantee that \\(\\theta_n\\) will converge let alone converge toward a global minimizer. The best we can hope for in general is that \\[\\|\\nabla H(\\theta_n)\\|_2 \\rightarrow 0\\] for \\(n \\to \\infty\\), and this will happen under some relatively weak conditions on \\(H\\) (Theorem 3.2 Nocedal and Wright (2006)) under the assumption that \\[\\frac{\\nabla H(\\theta_n)^T \\rho_n}{\\|\\nabla H(\\theta_n)\\|_2 \\| \\rho_n\\|_2} \\leq - \\delta &lt; 0.\\] That is, the angle between the descent direction and the gradient should be uniformly bounded away from \\(90^{\\circ}\\). A practical way of searching for a step length is via backtracking. Choosing a \\(\\gamma_0\\) and a constant \\(d \\in (0, 1)\\) we can search through the sequence of step lengths \\[\\gamma_0, d \\gamma_0, d^2 \\gamma_0, d^3 \\gamma_0, \\ldots\\] and stop the first time we find a step length satisfying the Wolfe conditions. Using backtracking, we can actually dispense of the curvature condition and simply check the sufficient descent condition \\[H(\\theta_{n} + d^k \\gamma_0 \\rho_{n}) \\leq H(\\theta_n) + cd^k \\gamma_0 \\nabla H(\\theta_{n})^T \\rho_{n}\\] for \\(c \\in (0, 1)\\). The implementation of backtracking requires the choice of the three parameters: \\(\\gamma_0 &gt; 0\\), \\(d \\in (0, 1)\\) and \\(c \\in (0, 1)\\). A good choice depends quite a lot on the algorithm used for choosing the descent direction, but choosing \\(c\\) too close to 1 can make the algorithm take too small steps, and taking \\(d\\) too small can likewise generate small step lengths. Thus \\(d = 0.8\\) or \\(d = 0.9\\) and \\(c = 0.1\\) or even smaller are sensible choices. For some algorithms, like the Newton algorithm to be dealt with below, there is a natural choice of \\(\\gamma_0 = 1\\). But for other algorithms a good choice depends crucially on the scale of the parameters, and there is then no general advice on choosing \\(\\gamma_0\\) that can be justified theoretically. 8.2.2 Gradient descent We return to the Poisson regression example and implement functions in R for computing the negative log-likelihood and its gradient. We exploit the model.matrix function to construct the model matrix from the data via a formula. The sufficient statistic is computed upfront, and the implementations use this vector and relies on linear algebra and vectorized computations. We choose to normalize by the number of observations \\(n\\) (the number of rows in the model matrix). This does have a small computational cost, but the resulting numerical values become less dependent upon \\(n\\), which makes it easier to choose sensible default values of various parameters for the numerical optimization algorithms. X &lt;- model.matrix(sale ~ log(normalSale), data = vegetables) y &lt;- vegetables$sale ## The function `drop` drops the dimensions attribute t_map &lt;- drop(crossprod(X, y)) ## More efficient than drop(t(X) %*% y) H &lt;- function(beta) (drop(sum(exp(X %*% beta)) - beta %*% t_map)) / nrow(X) grad_H &lt;- function(beta) (colSums(drop(exp(X %*% beta)) * X) - t_map) / nrow(X) We implement a gradient descent algorithm with backtracking that uses the squared norm of the gradient as a stopping criterion. For gradient descent, the sufficient descent condition amounts to choosing the smallest \\(k \\geq 0\\) such that \\[H(\\theta_{n} + d^k \\gamma_0 \\nabla H(\\theta_{n})) \\leq H(\\theta_n) - cd^k \\gamma_0 \\|\\nabla H(\\theta_{n})\\|_2^2.\\] We insert a call to a trace function in each iteration given that it has been supplied as an argument. This gives us the possibility of extracting or printing values of variables during evaluation, which can be highly useful for understanding the inner workings of the algorithm. The actual implementation of the a trace function is given below, and can be adapted as we like to provide the information we want. GD &lt;- function(par, d = 0.8, c = 0.1, gamma0 = 0.01, epsilon = 1e-4, trace = NULL) { repeat { value &lt;- H(par) grad &lt;- grad_H(par) h_prime &lt;- sum(grad^2) if(!is.null(trace)) trace() ## Convergence criterion based on gradient norm if(h_prime &lt;= epsilon) break gamma &lt;- gamma0 ## First proposed descent step par1 &lt;- par - gamma * grad ## Backtracking while descent is insufficient while(H(par1) &gt; value - c * gamma * h_prime) { gamma &lt;- d * gamma par1 &lt;- par - gamma * grad } par &lt;- par1 } par } Gradient descent is very slow for the large Poisson model with individual store effects, so we consider only the simple model with two parameters. pois_GD &lt;- GD(rep(0, ncol(X))) The gradient descent implementation is tested by comparing the minimizer to the estimated parameters as computed by glm. as.numeric(coefficients(pois_model_null)) ## as.numeric just to strip names as.numeric(pois_GD) ## [1] 1.4614403396 0.9215698864 ## [1] 1.4603520741 0.9219357553 We get the same result up to the first two decimals. The convergence criterion on our gradient descent algorithm was quite loose (\\(\\varepsilon = 10^{-4}\\), which means that the norm of the gradient is smaller than \\(10^{-2}\\) when the algorithm stops). This choice of \\(\\varepsilon\\) in combination with \\(\\gamma_0 = 0.01\\) implies that the algorithm stops when the gradient is so small that the changes are at most of norm \\(10^{-4}\\). Comparing the resulting values of the negative log-likelihood shows agreement up to the first five decimals, but we notice that the value for the parameters fitted using glm is just slightly smaller. H(coefficients(pois_model_null)) H(pois_GD) ## [1] -124.406827879897 ## [1] -124.406825325047 To investigate what actually went on inside the gradient descent algorithm we implement a trace function. In fact, we implement a function for constructing a tracer object, which has a way of saving and printing trace information during the evaluation of the gradient descent algorithm – or any other algorithm that implements a similar trace functionality. The tracer object does this by storing information in the enclosing environment of the trace function that is passed to the GD function. This trace function looks up variables in the evaluation environment of GD, stores them and prints them if requested, and store run time information as well. After the algorithm has converged the trace information can be accessed via the summary method for the tracer object. This implementation of tracer objects should not be confused with the trace function from the R base package. It has a related functionality that can be used with any function and is used for debugging. The tracer object as implemented here can be used with functions such as GD above that explicitly support calling a trace function, and it monitors the internal state during evaluation of the function without interrupting evaluation. ## Function `tracer` constructs a tracer object containing a `trace` function ## that can be passed as an argument to other functions that support a trace ## function. Arguments are ## ## object: a character vector of names of the objects in the evaluation ## environment that are traced. ## N: a numeric specifying if and how often trace information is printed. ## N = 1 (default) means every iteration. N = 0 means never. ## save: a logical value. Should the trace information be saved or just ## printed. ## time: a logical value. Should run time information be save. ## ...: other arguments passed to `format` for printing. tracer &lt;- function(objects = NULL, N = 1, save = TRUE, time = TRUE, ...) { n &lt;- 1 values_save &lt;- list() last_time &lt;- proc.time() trace &lt;- function() { if(is.null(objects)) objects &lt;- ls(parent.frame()) values &lt;- mget(objects, envir = parent.frame(), ifnotfound = list(NA)) if(N &amp;&amp; (n == 1 || n %% N == 0)) cat(&quot;n = &quot;, n, &quot;: &quot;, paste(names(values), &quot; = &quot;, format(values, ...), &quot;; &quot;, sep = &quot;&quot;), &quot;\\n&quot;, sep = &quot;&quot;) if(save) if(time) { time_diff &lt;- as.numeric(proc.time() - last_time)[c(1, 2)] values[[&quot;.time&quot;]] &lt;- time_diff[1] + time_diff[2] last_time &lt;&lt;- proc.time() } values_save[[n]] &lt;&lt;- values n &lt;&lt;- n + 1 } get &lt;- function(simplify = FALSE) { if(simplify) { col_names &lt;- unique(unlist(lapply(values_save, names))) values_save &lt;- lapply( col_names, function(x) do.call(rbind, unlist(lapply(values_save, function(y) y[x]), recursive = FALSE)) ) names(values_save) &lt;- col_names values_save &lt;- lapply(col_names, function(x) { x_val &lt;- values_save[[x]] if(!is.null(ncol(x_val)) &amp;&amp; ncol(x_val) == 1) { colnames(x_val) &lt;- x } else { if(is.null(colnames(x_val))) colnames(x_val) &lt;- 1:ncol(x_val) colnames(x_val) &lt;- paste(x, &quot;.&quot;, colnames(x_val), sep = &quot;&quot;) } x_val }) values_save &lt;- do.call(cbind, values_save) row.names(values_save) &lt;- 1:nrow(values_save) } values_save } structure(list(trace = trace, get = get), class = &quot;tracer&quot;) } ## Methods for subsetting, printing and summarizing tracer objects &#39;[.tracer&#39; &lt;- function(x, i, j, ..., drop = TRUE) { values &lt;- x$get(...)[i] if (drop &amp;&amp; length(i) == 1) values &lt;- values[[1]] values } print.tracer &lt;- function(x, ...) print(x$get(...)) summary.tracer &lt;- function(x, ...) { x &lt;- suppressWarnings(x$get(simplify = TRUE)) x[, &quot;.time&quot;] &lt;- c(0, cumsum(x[-1, &quot;.time&quot;])) as.data.frame(x) } We use the tracer object with the our gradient descent implementation and print trace information every 50th iteration. GD_tracer &lt;- tracer(c(&quot;value&quot;, &quot;h_prime&quot;, &quot;gamma&quot;), N = 50) system.time(pois_GD &lt;- GD(rep(0, ncol(X)), trace = GD_tracer$trace)) ## n = 1: value = 1; h_prime = 14268.59; gamma = NA; ## n = 50: value = -123.9395; h_prime = 15.45722; gamma = 0.004096; ## n = 100: value = -124.3243; h_prime = 3.133931; gamma = 0.00512; ## n = 150: value = -124.3935; h_prime = 0.601431; gamma = 0.00512; ## n = 200: value = -124.4048; h_prime = 0.09805907; gamma = 0.00512; ## n = 250: value = -124.4065; h_prime = 0.0109742; gamma = 0.00512; ## n = 300: value = -124.4068; h_prime = 0.002375827; gamma = 0.00512; ## n = 350: value = -124.4068; h_prime = 0.000216502; gamma = 0.004096; ## user system elapsed ## 0.067 0.005 0.073 We see that the gradient descent algorithm runs for a little more than 350 iterations, and we can observe how the value of the negative log-likelihood is descending. We can also see that the step length \\(\\gamma\\) bounces between \\(0.004096 = 0.8^4 \\times 0.01\\) and \\(0.00512 = 0.8^3 \\times 0.01\\), thus the backtracking takes 3 to 4 iterations to find a step length with sufficient descent. The printed trace does not reveal the run time information. The run time information is computed and stored as differences between process timings at each iteration of the algorithm, and the precision is at best of the order of one millisecond (see ?proc.time). Hence the run time associated to one single iteration may be fairly inaccurate for fast iterations, but the cumulative run time can still give a reasonable indication of time usage over many iterations. This information is, however, best inspected and computed after the algorithm has converged, and it is computed and returned by the summary method for tracer objects. tail(summary(GD_tracer)) ## value h_prime gamma .time ## 372 -124.4068 1.125779e-04 0.005120 0.064 ## 373 -124.4068 1.218925e-04 0.005120 0.065 ## 374 -124.4068 1.323878e-04 0.005120 0.065 ## 375 -124.4068 1.441965e-04 0.005120 0.065 ## 376 -124.4068 1.574572e-04 0.005120 0.065 ## 377 -124.4068 7.600796e-05 0.004096 0.065 The trace information is stored in a list. The summary method transforms the trace information into a data frame with one row per iteration. We can also access individual entries of the list of trace information via subsetting. GD_tracer[377] ## $value ## [1] -124.4068 ## ## $h_prime ## [1] 7.600796e-05 ## ## $gamma ## [1] 0.004096 ## ## $.time ## [1] 0 Figure 8.1: Gradient norm (top) and value of the negative log-likelihood (bottom) above the limit value \\(H(\\theta_{\\infty})\\). The straight line is fitted to the data points except the first ten using least squares, and the rate is computed from this estimate and reported per ms. 8.2.3 Conjugate gradients The gradient direction is typically not the best descent direction. It is too local, and convergence can be quite slow. One of the better algorithms that is still a “first order algorithm” (using only gradient information) is the nonlinear conjugate gradient algorithm. In the Fletcher–Reeves version of the algorithm the descent direction is initialized as the negative gradient \\(\\rho_0 = - \\nabla H(\\theta_{0})\\) and then updated as \\[\\rho_{n} = - \\nabla H(\\theta_{n}) + \\frac{\\|\\nabla H(\\theta_n)\\|_2^2}{\\|\\nabla H(\\theta_{n-1})\\|_2^2} \\rho_{n-1}.\\] That is, the descent direction, \\(\\rho_{n}\\), is the negative gradient but modified according to the previous descent direction. There is plenty of opportunity to vary the the prefactor of \\(\\rho_{n-1}\\), and the one presented here is what makes it the Fletcher–Reeves version. Other versions go by the names of their inventors such as Polak–Ribière or Hestenes–Stiefel. In fact, \\(\\rho_{n}\\) need not be a descent direction unless we put some restrictions on the step lengths. One possibility is to require that the step length \\(\\gamma_{n}\\) satisfies the strong curvature condition \\[|h&#39;(\\gamma)| = |\\nabla H(\\theta_n + \\gamma \\rho_n)^T \\rho_n | \\leq \\tilde{c} |\\nabla H(\\theta_n)^T \\rho_n| = \\tilde{c} |h&#39;(0)|\\] for a \\(\\tilde{c} &lt; \\frac{1}{2}\\). Then \\(\\rho_{n + 1}\\) can be shown to be a descent direction if \\(\\rho_{n}\\) is. We implement the conjugate gradient method in a slightly different way. Instead of introducing the more advanced curvature condition, we simply reset the algorithm to use the gradient direction in any case where a non-descent direction has been chosen. Resets of descent direction every \\(p\\)th iteration is recommended anyway for the nonlinear conjugate gradient algorithm. CG &lt;- function(par, d = 0.8, c = 0.1, gamma0 = 1, epsilon = 1e-6, trace = NULL) { p &lt;- length(par) m &lt;- 1 rho0 &lt;- numeric(p) repeat { value &lt;- H(par) grad &lt;- grad_H(par) grad_norm_sq &lt;- sum(grad^2) if(!is.null(trace)) trace() if(grad_norm_sq &lt;= epsilon) break gamma &lt;- gamma0 ## Descent direction rho &lt;- - grad + grad_norm_sq * rho0 h_prime &lt;- drop(t(grad) %*% rho) ## Reset to gradient descent if m &gt; p or rho is not a descent direction if(m &gt; p || h_prime &gt;= 0) { rho &lt;- - grad h_prime &lt;- - grad_norm_sq m &lt;- 1 } par1 &lt;- par + gamma * rho ## Backtracking while(H(par1) &gt; value + c * gamma * h_prime) { gamma &lt;- d * gamma par1 &lt;- par + gamma * rho } rho0 &lt;- rho / grad_norm_sq par &lt;- par1 m &lt;- m + 1 } par } CG_tracer &lt;- tracer(c(&quot;value&quot;, &quot;gamma&quot;, &quot;grad_norm_sq&quot;), N = 10) pois_CG &lt;- CG(rep(0, ncol(X)), trace = CG_tracer$trace) ## n = 1: value = 1; gamma = NA; grad_norm_sq = 14269; ## n = 10: value = -123.15; gamma = 0.018014; grad_norm_sq = 129.78; ## n = 20: value = -123.92; gamma = 0.022518; grad_norm_sq = 77.339; ## n = 30: value = -124.23; gamma = 0.018014; grad_norm_sq = 22.227; ## n = 40: value = -124.41; gamma = 0.0092234; grad_norm_sq = 0.179; ## n = 50: value = -124.41; gamma = 0.10737; grad_norm_sq = 0.028232; ## n = 60: value = -124.41; gamma = 0.0092234; grad_norm_sq = 0.00021747; ## n = 70: value = -124.41; gamma = 0.0092234; grad_norm_sq = 1.7488e-06; Figure 8.2: Gradient norms (top) and negative log-likelihoods (bottom) for gradient descent (left) and conjugate gradient (right). This algorithm is fast enough to fit the large Poisson regression model. X &lt;- model.matrix(sale ~ store + log(normalSale) - 1, data = vegetables) t_map &lt;- drop(crossprod(X, y)) CG_tracer &lt;- tracer(c(&quot;value&quot;, &quot;gamma&quot;, &quot;grad_norm_sq&quot;), N = 100) pois_CG &lt;- CG(rep(0, ncol(X)), trace = CG_tracer$trace) ## n = 1: value = 1; gamma = NA; grad_norm_sq = 12737; ## n = 100: value = -127.7; gamma = 0.014412; grad_norm_sq = 4.6263; ## n = 200: value = -128.02; gamma = 0.014412; grad_norm_sq = 0.41723; ## n = 300: value = -128.35; gamma = 0.0024179; grad_norm_sq = 0.063094; ## n = 400: value = -128.56; gamma = 0.04398; grad_norm_sq = 0.64184; ## n = 500: value = -128.58; gamma = 0.028147; grad_norm_sq = 0.012616; ## n = 600: value = -128.59; gamma = 0.0030223; grad_norm_sq = 0.00028513; ## n = 700: value = -128.59; gamma = 0.0030223; grad_norm_sq = 1.0811e-05; ## n = 800: value = -128.59; gamma = 0.0019343; grad_norm_sq = 4.2685e-06; tail(summary(CG_tracer)) ## value gamma grad_norm_sq .time ## 890 -128.5894 0.134217728 2.968376e-04 10.820 ## 891 -128.5894 0.003777893 2.511863e-06 10.832 ## 892 -128.5894 0.004722366 1.510951e-06 10.844 ## 893 -128.5894 1.000000000 1.835397e-04 10.846 ## 894 -128.5894 0.005902958 3.238532e-05 10.858 ## 895 -128.5894 0.003777893 6.586532e-07 10.871 Using optim with the conjugate gradient method. system.time(pois_optim_CG &lt;- optim(rep(0, length = ncol(X)), H, grad_H, method = &quot;CG&quot;, control = list(maxit = 10000))) ## user system elapsed ## 14.163 1.133 15.315 pois_optim_CG[c(&quot;value&quot;, &quot;counts&quot;)] ## $value ## [1] -128.5895 ## ## $counts ## function gradient ## 12008 5118 8.2.4 Peppered Moths Returning to the peppered moth from Section 7.2.1 we implemented in that section the log-likelihood for general multinomial cell collapsing and applied the implementation to compute the maximum-likelihood estimate. In this section we implement the gradient as well. From the expression for the log-likelihood in (7.2) it follows that the gradient equals \\[\\nabla \\ell(\\theta) = \\sum_{j = 1}^{K_0} \\frac{ x_j }{ M(p(\\theta))_j}\\nabla M(p(\\theta))_j = \\sum_{j = 1}^{K_0} \\sum_{k \\in A_j} \\frac{ x_j}{ M(p(\\theta))_j} \\nabla p_k(\\theta).\\] Letting \\(j(k)\\) be defined by \\(k \\in A_{j(k)}\\) we see that the gradient can also be written as \\[\\nabla \\ell(\\theta) = \\sum_{k=1}^K \\frac{x_{j(k)}}{ M(p(\\theta))_{j(k)}} \\nabla p_k(\\theta) = \\mathbf{\\tilde{x}}(\\theta) \\mathrm{D}p(\\theta),\\] where \\(\\mathrm{D}p(\\theta)\\) is the Jacobian of the parametrization \\(\\theta \\mapsto p(\\theta)\\), and \\(\\mathbf{\\tilde{x}}(\\theta)\\) is the vector with \\[\\mathbf{\\tilde{x}}(\\theta)_k = \\frac{ x_{j(k)}}{M(p(\\theta))_{j(k)}}.\\] grad_loglik &lt;- function(par, x, prob, Dprob, group) { p &lt;- prob(par) if(is.null(p)) return(rep(NA, length(par))) - (x[group] / M(p, group)[group]) %*% Dprob(par) } The Jacobian needs to be implemented for the specific example of peppered moths. Dprob &lt;- function(p) { p[3] &lt;- 1 - p[1] - p[2] matrix( c(2 * p[1], 0, 2 * p[2], 2 * p[1], 2* p[3] - 2 * p[1], -2 * p[1], 0, 2 * p[2], -2 * p[2], 2 * p[3] - 2 * p[2], -2 * p[3], -2 * p[3]), ncol = 2, nrow = 6, byrow = TRUE) } We can then use the conjugate gradient algorithm to compute the maximum-likelihood estimate. optim(c(0.3, 0.3), loglik, grad_loglik, x = c(85, 196, 341), prob = prob, Dprob = Dprob, group = c(1, 1, 1, 2, 2, 3), method = &quot;CG&quot;) ## $par ## [1] 0.07083691 0.18873652 ## ## $value ## [1] 600.481 ## ## $counts ## function gradient ## 92 19 ## ## $convergence ## [1] 0 ## ## $message ## NULL The peppered Moth example is very simple. The log-likelihood can easily be computed, and we used this problem to illustrate ways of implementing a likelihood in R and how to use optim to maximize it. One of the likelihood implementations was very problem specific while the other more abstract and general, and we used the same general and abstract approach to implement the gradient above. The gradient could then be used for other optimization algorithms, still using optim, such as conjugate gradient. In fact, you can use conjugate gradient without computing and implementing the gradient. optim(c(0.3, 0.3), loglik, x = c(85, 196, 341), prob = prob, group = c(1, 1, 1, 2, 2, 3), method = &quot;CG&quot;) ## $par ## [1] 0.07084109 0.18873718 ## ## $value ## [1] 600.481 ## ## $counts ## function gradient ## 107 15 ## ## $convergence ## [1] 0 ## ## $message ## NULL If we don’t implement a gradient, a numerical gradient is used by optim. This can very well result in a slower algorithm than if the gradient is implemented, but more seriously, in can result in convergence problems. This is because there is a subtle tradeoff between numerical accuracy and accuracy of the finite difference approximation used to approximate the gradient. We didn’t experience convergence problems in the example above, but one way to remedy such problems is to set the parscale or fnscale entries in the control list argument to optim. In the following chapter the peppered moth example is used to illustrate the EM algorithm. It is important to understand that the EM algorithm doesn’t rely on the ability to compute the likelihood or the gradient of the likelihood for that matter. In many real applications of the EM algorithm the computation of the likelihood is challenging or even impossible, thus most standard optimization algorithms will not be directly applicable. References "],
["newton-type-algorithms.html", "8.3 Newton-type algorithms", " 8.3 Newton-type algorithms The Newton algorithm is very similar to gradient descent except that the gradient descent direction is replaced by \\[\\rho_n = - D^2 H(\\theta_n)^{-1} \\nabla H(\\theta_n).\\] The Newton algorithm is typically much more efficient than gradient descent and will converge in few iterations. However, the storage of the \\(p \\times p\\) Hessian, its computation, and the solution of the equation to compute \\(\\rho_n\\) all scale like \\(p^2\\) and this can make the algorithm useless for very large \\(p\\). A variety of alternatives to the Newton algorithm exist that replace the Hessian by another matrix that can be easier to compute and update. It should be noted that if we choose a matrix \\(B_n\\) in the \\(n\\)th iteration, then \\(- B_n \\nabla H(\\theta_n)\\) is a descent direction whenever \\(B_n\\) is a positive definite matrix. Newton implementation (with trace). Newton &lt;- function(par, d = 0.8, c = 0.1, gamma0 = 1, epsilon = 1e-10, trace = NULL) { repeat { value &lt;- H(par) grad &lt;- grad_H(par) if(!is.null(trace)) trace() if(sum(grad^2) &lt;= epsilon) break Hessian &lt;- Hessian_H(par) rho &lt;- - drop(solve(Hessian, grad)) gamma &lt;- gamma0 par1 &lt;- par + gamma * rho h_prime &lt;- t(grad) %*% rho while(H(par1) &gt; value + c * gamma * h_prime) { gamma &lt;- d * gamma par1 &lt;- par + gamma * rho } par &lt;- par1 } par } 8.3.1 Poisson regression It requires the implementation of the Hessian matrix. Hessian_H &lt;- function(beta) (crossprod(X, drop(exp(X %*% beta)) * X)) / nrow(X) Newton_tracer &lt;- tracer(c(&quot;value&quot;, &quot;h_prime&quot;, &quot;gamma&quot;), N = 1) pois_Newton &lt;- Newton(rep(0, ncol(X)), trace = Newton_tracer$trace) ## n = 1: value = 1; h_prime = NA; gamma = NA; ## n = 2: value = -14.8327; h_prime = -4140.563; gamma = 0.022518; ## n = 3: value = -64.81635; h_prime = -402.9847; gamma = 0.262144; ## n = 4: value = -111.3365; h_prime = -76.36275; gamma = 1; ## n = 5: value = -124.2494; h_prime = -21.0416; gamma = 1; ## n = 6: value = -127.7112; h_prime = -5.652483; gamma = 1; ## n = 7: value = -128.4973; h_prime = -1.3326; gamma = 1; ## n = 8: value = -128.5873; h_prime = -0.1647034; gamma = 1; ## n = 9: value = -128.5894; h_prime = -0.004159696; gamma = 1; ## n = 10: value = -128.5895; h_prime = -3.288913e-06; gamma = 1; range(pois_Newton - pois_model$coefficients) ## [1] -4.979404e-10 1.298776e-06 H(pois_Newton) ## [1] -128.58945047446988497 H(coefficients(pois_model)) ## [1] -128.58945047447093657 summary(Newton_tracer) ## value h_prime gamma .time ## 1 1.00000 NA NA 0.000 ## 2 -14.83270 -4.140563e+03 0.022518 0.191 ## 3 -64.81635 -4.029847e+02 0.262144 0.373 ## 4 -111.33647 -7.636275e+01 1.000000 0.556 ## 5 -124.24937 -2.104160e+01 1.000000 0.731 ## 6 -127.71116 -5.652483e+00 1.000000 0.908 ## 7 -128.49729 -1.332600e+00 1.000000 1.082 ## 8 -128.58733 -1.647034e-01 1.000000 1.265 ## 9 -128.58945 -4.159696e-03 1.000000 1.447 ## 10 -128.58945 -3.288913e-06 1.000000 1.623 The R function glm.fit uses a Newton algorithm (without backtracking) and is about a factor five faster on this example. system.time(glm.fit(X, y, family = poisson())) ## user system elapsed ## 0.425 0.009 0.437 One should be careful when comparing run times for different optimization algorithms, but in this case they have achieved about the same precision with glm.fit even obtaining the smallest negative log-likelihood value. 8.3.2 Quasi-Newton algorithms We turn to other descent direction algorithms that are more efficient than gradient descent by choosing the descent direction in a more clever way but less computationally demanding than the Newton algorithm that requires the computation of the full Hessian in each iteration. We will only consider the application of the BFGS algorithm via the implementation in the R function optim. system.time( pois_BFGS &lt;- optim(rep(0, length = ncol(X)), H, grad_H, method = &quot;BFGS&quot;, control = list(maxit = 10000))) ## user system elapsed ## 0.517 0.040 0.559 range(pois_BFGS$par - coefficients(pois_model)) ## [1] -0.00954968 0.08481093 pois_BFGS[c(&quot;value&quot;, &quot;counts&quot;)] ## $value ## [1] -128.5894 ## ## $counts ## function gradient ## 158 148 8.3.3 Sparsity One of the benefits of the implementations of \\(H\\) and its derivatives as well as of the descent algorithms is that they can exploit sparsity of \\(\\mathbf{X}\\) almost for free. The implementations have not done that in previous computations, because \\(\\mathbf{X}\\) has been stored as a dense matrix. In reality, \\(\\mathbf{X}\\) is a very sparse matrix (the vast majority of the matrix entries are zero), and if we convert it into a sparse matrix, all the matrix-vector products will be more run time efficient. Sparse matrices are implemented in the R package Matrix. library(Matrix) X &lt;- Matrix(X) Without changing any other code, we get an immediate run time improvement using e.g. optim and the BFGS algorithm. system.time( pois_BFGS_sparse &lt;- optim(rep(0, length = ncol(X)), H, grad_H, method = &quot;BFGS&quot;, control = list(maxit = 10000)) ) ## user system elapsed ## 0.150 0.009 0.160 We should in real applications avoid constructing a dense intermediate model matrix as a step toward constructing a sparse model matrix. This is possible by constructing the sparse model matrix directly using a function from the R package MatrixModels. library(MatrixModels) X &lt;- model.Matrix(sale ~ store + log(normalSale) - 1, data = vegetables, sparse = TRUE) class(X) ## [1] &quot;dsparseModelMatrix&quot; ## attr(,&quot;package&quot;) ## [1] &quot;MatrixModels&quot; The Newton implementation benefits enormously from using sparse matrices because the bottleneck is the computation of the Hessian. Newton_tracer &lt;- tracer(c(&quot;value&quot;, &quot;h_prime&quot;, &quot;gamma&quot;), N = 0) pois_Newton &lt;- Newton(rep(0, ncol(X)), trace = Newton_tracer$trace) summary(Newton_tracer) ## value h_prime gamma .time ## 1 1.00000 NA NA 0.000 ## 2 -14.83270 -4.140563e+03 0.022518 0.005 ## 3 -64.81635 -4.029847e+02 0.262144 0.009 ## 4 -111.33647 -7.636275e+01 1.000000 0.012 ## 5 -124.24937 -2.104160e+01 1.000000 0.014 ## 6 -127.71116 -5.652483e+00 1.000000 0.017 ## 7 -128.49729 -1.332600e+00 1.000000 0.019 ## 8 -128.58733 -1.647034e-01 1.000000 0.022 ## 9 -128.58945 -4.159696e-03 1.000000 0.025 ## 10 -128.58945 -3.288913e-06 1.000000 0.027 Run time efficiency is not the only argument for using sparse matrices as they are also more memory efficient. It is memory (and time) inefficient to use dense intermediates, and for truly large scale problems impossible. Using sparse model matrices for regression models allows us to work with larger models that have more variables, more factor levels and more observations than if we use dense model matrices. For the Poisson regression model the memory used by either representation can be found. object.size(X) object.size(as.matrix(X)) ## Sparse matrix memory usage: ## 123440 bytes ## Dense matrix memory usage: ## 3103728 bytes We see that the dense matrix uses around a factor 30 more memory than the sparse representation. In this case it means using around 3 MB for storing the dense matrix instead of around 100 kB, which won’t be a problem on a contemporary computer. However, going from using 3 GB for storing a matrix to using 100 Mb could be the difference between not being able to work with the matrix on a standard laptop to running the computations with no problems. Using model.Matrix makes it possible to construct sparse model matrices directly and avoid all dense intermediates. This is exploited in the glm4 function from the MatrixModels package for fitting regression models, which can thus be useful in cases where your model matrix becomes very large but sparse. There are two main cases where the model matrix becomes sparse. When you model the response using one or more factors, and possibly their interactions, the model matrix will become particularly sparse if the factors have many levels. Another case is when you model the response via basis expansions of quantitative predictors and use basis functions with local support. The B-splines form an important example of such a basis with local support that results in a sparse model matrix. "],
["misc-.html", "8.4 Misc.", " 8.4 Misc. If \\(\\Phi\\) is just nonexpansive (the constant \\(c\\) above is one), this is no longer true, but replacing \\(\\Phi\\) by \\(\\alpha \\Phi + (1 - \\alpha) I\\) for \\(\\alpha \\in (0,1)\\) we get Krasnoselskii-Mann iterates of the form \\[\\theta_n = \\alpha \\Phi(\\theta_{n-1}) + (1 - \\alpha) \\theta_{n-1}\\] that will converge to a fixed point of \\(\\Phi\\) provided it has one. Iteration, fixed points, convergence criteria. Ref to Nonlinear Parameter Optimization Using R Tools. "],
["em.html", "Chapter 9 Expectation maximization algorithms", " Chapter 9 Expectation maximization algorithms Somewhat surprisingly, it is possible to develop an algorithm, known as the expectation-maximization algorithm, for computing the maximum of a likelihood function in situations where computing the likelihood itself is quite difficult. This is possible in situations where the model is defined in terms of certain unobserved components, and where likelihood computations and optimization is relatively easy had we had the complete observation. The EM algorithm exploits this special structure, and is thus not a general optimization algorithm, but the situation where it applies is common enough in statistics that it is one of the core optimization algorithms used for computing maximum-likelihood estimates. In this chapter it is shown that the algorithm is generally an descent algorithm of the negative log-likelihood, and examples of its implementation are given to multinomial cell collapsing and Gaussian mixtures. The theoretical results needed for the EM algorithm for a special case of mixed models are given as well. Finally, some theoretical results as well as practical implementations for computing estimates of the Fisher information are presented. "],
["basic-properties.html", "9.1 Basic properties", " 9.1 Basic properties In this section the EM algorithm is formulated and shown to be a descent algorithm for the negative log-likelihood. Allele frequency estimation for the peppered moth is considered as a simple example showing how the algorithm can be implemented. 9.1.1 Incomplete data likelihood Suppose that \\(Y\\) is a random variable and \\(X = M(Y)\\). Suppose that \\(Y\\) has density \\(f(\\cdot \\mid \\theta)\\) and that \\(X\\) has marginal density \\(g(x \\mid \\theta)\\). The marginal density is typically of the form \\[g(x \\mid \\theta) = \\int_{\\{y: M(y) = x\\}} f(y \\mid \\theta) \\ \\mu_x(\\mathrm{d} y)\\] for a suitable measure \\(\\mu_x\\) depending on \\(M\\) and \\(x\\) but not \\(\\theta\\). The general argument for the marginal density relies on the coarea formula. The log-likelihood for observing \\(X = x\\) is \\[\\ell(\\theta) = \\log g(x \\mid \\theta).\\] The log-likelihood is often impossible to compute analytically and difficult and expensive to compute numerically. The complete log-likelihood, \\(\\log f(y \\mid \\theta)\\), is often easy to compute, but we don’t know \\(Y\\), only that \\(M(Y) = x\\). In some cases it is possible to compute \\[Q(\\theta \\mid \\theta&#39;) := E_{\\theta&#39;}(\\log f(Y \\mid \\theta) \\mid X = x),\\] which is the conditional expectation of the complete log-likelihood given the observed data and computed using the probability measure given by \\(\\theta&#39;\\). Thus for fixed \\(\\theta&#39;\\) this is a computable function of \\(\\theta\\) depending only on the observed data \\(x\\). One could get the following idea: with an initial guess of \\(\\theta&#39; = \\theta_0\\) compute iteratively \\[\\theta_{n + 1} = \\textrm{arg max} \\ Q(\\theta \\mid \\theta_n)\\] for \\(n = 0, 1, 2, \\ldots\\). This idea is the EM algorithm: E-step: Compute the conditional expectation \\(Q(\\theta \\mid \\theta_n )\\). M-step: Maximize \\(\\theta \\mapsto Q(\\theta \\mid \\theta_n )\\). It is a bit weird to present the algorithm as a two-step algorithm in its abstract formulation. Even though we can regard \\(Q(\\theta \\mid \\theta_n)\\) as something we can compute abstractly for each \\(\\theta\\) for a given \\(\\theta_n\\), the maximization is in practice not really done using all these evaluations. It is computed either by an analytic formula involving \\(x\\) and \\(\\theta_n\\), or by a numerical algorithm that computes certain evaluations of \\(Q( \\cdot \\mid \\theta_n)\\) and perhaps its gradient and Hessian. In computing these specific evaluations there is, of course, a need for the computation of conditional expectations, but we would compute these as they are needed and not upfront. However, in some of the most important applications of the EM algorithm, particularly for exponential families covered in Section 9.2, it makes a lot of sense to regard the algorithm as a two-step algorithm. This is the case whenever \\(Q(\\theta \\mid \\theta_n) = q(\\theta, t(x, \\theta_n))\\) is given in terms of \\(\\theta\\) and a function \\(t(x, \\theta_n )\\) of \\(x\\) and \\(\\theta_n\\) that doesn’t depend on \\(\\theta\\). Then the E-step becomes the computation of \\(t(x, \\theta_n )\\), and in the M-step, \\(Q(\\cdot \\mid \\theta_n )\\) is maximized by maximizing \\(q(\\cdot, t(x, \\theta_n ))\\), and the maximum is a function of \\(t(x, \\theta_n )\\). 9.1.2 Monotonicity of the EM algorithm We prove below that the algorithm (weakly) increases the log-likelihood in every step, and thus is a descent algorithm for the negative log-likelihood \\(H = - \\ell\\). It holds in great generality that the conditional distribution of \\(Y\\) given \\(X = x\\) has density \\[\\begin{equation} h(y \\mid x, \\theta) = \\frac{f(y \\mid \\theta)}{g(x \\mid \\theta)} \\tag{9.1} \\end{equation}\\] w.r.t. the measure \\(\\mu_x\\) as above (that does not depend upon \\(\\theta\\)), and where \\(g\\) is the density for the marginal distribution. This can be verified quite easily for discrete distributions and when \\(Y = (Z, X)\\) with joint density w.r.t. a product measure \\(\\mu \\otimes \\nu\\) that does not depend upon \\(\\theta\\). In the latter case, \\(f(y \\mid \\theta) = f(z, x \\mid \\theta)\\) and \\[g(x \\mid \\theta) = \\int f(z, x \\mid \\theta) \\ \\mu(\\mathrm{d} z)\\] is the marginal density w.r.t. \\(\\nu\\). Whenever (9.1) holds it follows that \\[\\ell(\\theta) = \\log g(x \\mid \\theta) = \\log f(y \\mid \\theta) - \\log h(y \\mid x, \\theta),\\] where \\(\\ell(\\theta)\\) is the log-likelihood. Theorem 9.1 If \\(\\log f(Y \\mid \\theta)\\) as well as \\(\\log h(Y \\mid x, \\theta)\\) have finite \\(\\theta&#39;\\)-conditional expectation given \\(M(Y) = x\\) then \\[Q(\\theta \\mid \\theta&#39;) &gt; Q(\\theta&#39; \\mid \\theta&#39;) \\quad \\Rightarrow \\quad \\ell(\\theta) &gt; \\ell(\\theta&#39;).\\] Proof. Since \\(\\ell(\\theta)\\) depends on \\(y\\) only through \\(M(y) = x\\), \\[\\begin{align*} \\ell(\\theta) &amp; = E_{\\theta&#39;} ( \\ell(\\theta) \\mid X = x) \\\\ &amp; = \\underbrace{E_{\\theta&#39;} ( \\log f(Y \\mid \\theta) \\mid X = x)}_{Q(\\theta \\mid \\theta&#39;)} + \\underbrace{ E_{\\theta&#39;} ( - \\log h(Y \\mid x, \\theta) \\mid X = x)}_{H(\\theta \\mid \\theta&#39;)} \\\\ &amp; = Q(\\theta \\mid \\theta&#39;) + H(\\theta \\mid \\theta&#39;). \\end{align*}\\] Now for the second term we find, using Jensen’s inequality for the convex function \\(-\\log\\), that \\[\\begin{align*} H(\\theta \\mid \\theta&#39;) &amp; = \\int - \\log(h(y \\mid x, \\theta)) h(y \\mid x, \\theta&#39;) \\mu_x(\\mathrm{d}y) \\\\ &amp; = \\int - \\log\\left(\\frac{h(y \\mid x, \\theta)}{ h(y \\mid x, \\theta&#39;)}\\right) h(y \\mid x, \\theta&#39;) \\mu_x(\\mathrm{d}y) + \\int - \\log(h(y \\mid x, \\theta&#39;)) h(y \\mid x, \\theta&#39;) \\mu_x(\\mathrm{d}y) \\\\ &amp; \\geq -\\log \\left( \\int \\frac{h(y \\mid x, \\theta)}{ h(y \\mid x, \\theta&#39;)} h(y \\mid x, \\theta&#39;) \\mu_x(\\mathrm{d}y) \\right) + H(\\theta&#39; \\mid \\theta&#39;) \\\\ &amp; = -\\log\\underbrace{\\left( \\int h(y \\mid x, \\theta) \\mu_x(\\mathrm{d}y)\\right) }_{=1} + H(\\theta&#39; \\mid \\theta&#39;) \\\\ &amp; = H(\\theta&#39; \\mid \\theta&#39;). \\end{align*}\\] From this we see that \\[\\ell(\\theta) \\geq Q(\\theta \\mid \\theta&#39;) + H(\\theta&#39; \\mid \\theta&#39;)\\] for all \\(\\theta\\) and the right hand side is a so-called minorant for the log-likelihood. Observing that \\[\\ell(\\theta&#39;) = Q(\\theta&#39; \\mid \\theta&#39;) + H(\\theta&#39; \\mid \\theta&#39;).\\] completes the proof of the theorem. Note that the proof above can also be given by referring to Gibbs’ inequality in information theory stating that the Kullback-Leibler divergence is positive, or equivalently that the cross-entropy \\(H(\\theta \\mid \\theta&#39;)\\) is smaller than the entropy \\(H(\\theta&#39; \\mid \\theta&#39;)\\), but the proof of this is, in itself, a consequence of Jensen’s inequality just as above. It follows from Theorem 9.1 that if \\(\\theta_n\\) is computed iteratively starting from \\(\\theta_0\\) such that \\[Q(\\theta_{n+1} \\mid \\theta_{n}) &gt; Q(\\theta_{n} \\mid \\theta_{n}),\\] then \\[H(\\theta_0) &gt; H(\\theta_1) &gt; H(\\theta_2) &gt; \\ldots.\\] This proves that the EM algorithm is a strict descent algorithm for the negative log-likelihood as long as it is possible in each iteration to strictly increase \\(\\theta \\mapsto Q(\\theta \\mid \\theta_{n})\\) above \\(Q(\\theta_{n} \\mid \\theta_{n}).\\) The term EM algorithm is usually reserved for the specific algorithm that maximizes \\(Q(\\cdot \\mid \\theta_n)\\) in the M-step, but as we have seen, there is no reason to insist on the M-step being a maximization. A choice of descent direction of \\(Q(\\cdot \\mid \\theta_n)\\) and a step-length guaranteeing sufficient descent of \\(H\\) (sufficient ascent of \\(Q(\\cdot \\mid \\theta_n)\\)) will be enough to give a descent algorithm. Any such variation is usually termed a generalized EM algorithm. One may imagine that the minorant could actually be a useful lower bound on the difficult-to-compute log-likelihood. The additive constant \\(H(\\theta&#39; \\mid \\theta&#39;)\\) in the minorant is, however, not going to be computable in general either, and it is not clear that there is any way to use the bound quantitatively. 9.1.3 Peppered moths We return in this section to the peppered moths and the implementation of the EM algorithm for multinomial cell collapsing. The EM algorithm can be implemented by two simple functions that compute the conditional expectations above (the E-step) and then maximization of the complete observation log-likelihood. EStep0 &lt;- function(p, x, group) { x[group] * p / M(p, group)[group] } The MLE of the complete log-likelihood is a linear estimator, as is the case in many examples with explicit MLEs. MStep0 &lt;- function(n, X) as.vector(X %*% n / (sum(n))) The EStep0 and MStep0 functions are abstract implementations. They require specification of the arguments group and X, respectively, to become concrete. The M-step is only implemented in the case where the complete-data MLE is a linear estimator, that is, a linear map of the complete data vector \\(y\\) that can be expressed in terms of a matrix \\(\\mathbf{X}\\). EStep &lt;- function(par, x) EStep0(prob(par), x, c(1, 1, 1, 2, 2, 3)) MStep &lt;- function(n) { X &lt;- matrix( c(2, 1, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0) / 2, 2, 6, byrow = TRUE) MStep0(n, X) } The EM algorithm is finally implemented as an iterative, alternating call of EStep and MStep until convergence as measured in terms of the relative change from iteration to iteration being sufficiently small. EM &lt;- function(par, x, epsilon = 1e-6, trace = NULL) { repeat{ par0 &lt;- par par &lt;- MStep(EStep(par, x)) if(!is.null(trace)) trace() if(sum((par - par0)^2) &lt;= epsilon * (sum(par^2) + epsilon)) break } par ## Remember to return the parameter estimate } phat &lt;- EM(c(0.3, 0.3), c(85, 196, 341)) phat ## [1] 0.07083693 0.18877365 We check what is going on in each step of the EM algorithm. EM_tracer &lt;- tracer(&quot;par&quot;) EM(c(0.3, 0.3), c(85, 196, 341), trace = EM_tracer$trace) ## n = 1: par = 0.08038585, 0.22464192; ## n = 2: par = 0.07118928, 0.19546961; ## n = 3: par = 0.07084985, 0.18993393; ## n = 4: par = 0.07083738, 0.18894757; ## n = 5: par = 0.07083693, 0.18877365; ## [1] 0.07083693 0.18877365 EM_tracer &lt;- tracer(c(&quot;par0&quot;, &quot;par&quot;), N = 0) phat &lt;- EM(c(0.3, 0.3), c(85, 196, 341), epsilon = 1e-20, trace = EM_tracer$trace) EM_trace &lt;- summary(EM_tracer) EM_trace &lt;- transform( EM_trace, n = 1:nrow(EM_trace), par_norm_diff = sqrt((par0.1 - par.1)^2 + (par0.2 - par.2)^2) ) qplot(n, log(par_norm_diff), data = EM_trace) Note the log-axis. The EM-algorithm converges linearly (this is the terminology, see Algorithms and Convergence). The log-rate of the convergence can be estimated by least-squares. log_rate_fit &lt;- lm(log(par_norm_diff) ~ n, data = EM_trace) exp(coefficients(log_rate_fit)[&quot;n&quot;]) ## n ## 0.1750251 The rate is very small in this case implying fast convergence. This is not always the case. If the log-likelihood is flat, the EM-algorithm can become quite slow with a rate close to 1. "],
["EM-exp.html", "9.2 Exponential families", " 9.2 Exponential families We consider in this section the special case where the model of \\(\\mathbf{y}\\) is given as an exponential family Bayesian network as in Section 7.1.2 and \\(x = M(\\mathbf{y})\\) is the observed transformation. The complete data log-likelihood is \\[\\theta \\mapsto \\theta^T t(\\mathbf{y}) - \\kappa(\\theta) = \\theta^T \\sum_{j=1}^m t_j(y_j) - \\kappa(\\theta),\\] and we find that \\[Q(\\theta \\mid \\theta&#39;) = \\theta^T \\sum_{j=1}^m E_{\\theta&#39;}(t_j(Y_j) \\mid X = x) - E_{\\theta&#39;}( \\kappa(\\theta) \\mid X = x).\\] To maximize \\(Q\\) we differentiate \\(Q\\) and equate the derivative equal to zero. We find that the resulting equation is \\[\\sum_{j=1}^m E_{\\theta&#39;}(t_j(Y_j) \\mid X = x) = E_{\\theta&#39;}( \\nabla \\kappa(\\theta) \\mid X = x).\\] Alternatively, one may also note the following general equation for finding the maximum of \\(Q(\\cdot \\mid \\theta&#39;)\\) \\[\\sum_{j=1}^m E_{\\theta&#39;}(t_j(Y_j) \\mid X = x) = \\sum_{j=1}^m E_{\\theta&#39;}(E_{\\theta}(t_j(Y_j) \\mid y_1, \\ldots, y_{j-1}) \\mid X = x),\\] since \\[E_{\\theta&#39;}(\\nabla \\kappa(\\theta)\\mid X = x) = \\sum_{j=1}^m E_{\\theta&#39;}(\\nabla \\log \\varphi_j(\\theta) \\mid X = x) = \\sum_{j=1}^m E_{\\theta&#39;}(E_{\\theta}(t_j(Y_j) \\mid y_1, \\ldots, y_{j-1}) \\mid X = x) \\] Example 9.1 Continuing Example 7.4 with \\(M\\) the projection map \\[(\\mathbf{y}, \\mathbf{z}) \\mapsto \\mathbf{y}\\] we see that \\(Q\\) is maximized in \\(\\theta\\) by solving \\[\\sum_{i,j} E_{\\theta&#39;}(t(Y_{ij} \\mid Z_i) \\mid \\mathbf{Y} = \\mathbf{y}) = \\sum_{i} m_i E_{\\theta&#39;}(\\nabla \\kappa(\\theta \\mid Z_i) \\mid \\mathbf{Y} = \\mathbf{y}).\\] By using Example 7.2 we see that \\[\\kappa(\\theta \\mid Z_i) = \\frac{(\\theta_1 + \\theta_3 Z_i)^2}{4\\theta_2} - \\frac{1}{2}\\log \\theta_2,\\] hence \\[\\nabla \\kappa(\\theta \\mid Z_i) = \\frac{1}{2\\theta_2} \\left(\\begin{array}{cc} \\theta_1 + \\theta_3 Z_i \\\\ - \\frac{(\\theta_1 + \\theta_3 Z_i)^2}{2\\theta_2} - 1 \\\\ \\theta_1 Z_i + \\theta_3 Z_i^2 \\end{array}\\right) = \\left(\\begin{array}{cc} \\beta_0 + \\nu Z_i \\\\ - (\\beta_0 + \\nu Z_i)^2 - \\sigma^2 \\\\ \\beta_0 Z_i + \\nu Z_i^2 \\end{array}\\right).\\] Therefore, \\(Q\\) is maximized by solving the equation \\[\\sum_{i,j} \\left(\\begin{array}{cc} y_{ij} \\\\ - y_{ij}^2 \\\\ E_{\\theta&#39;}(Z_i \\mid \\mathbf{Y} = \\mathbf{y}) y_{ij} \\end{array}\\right) = \\sum_{i} m_i \\left(\\begin{array}{cc} \\beta_0 + \\nu E_{\\theta&#39;}(Z_i \\mid \\mathbf{Y}_i = \\mathbf{y}_i) \\\\ - E_{\\theta&#39;}((\\beta_0 + \\nu Z_i)^2 \\mid \\mathbf{Y} = \\mathbf{y}) - \\sigma^2 \\\\ \\beta_0 E_{\\theta&#39;}(Z_i \\mid \\mathbf{Y} = \\mathbf{y}) + \\nu E_{\\theta&#39;}(Z_i^2 \\mid \\mathbf{Y} = \\mathbf{y}) \\end{array}\\right).\\] Introducing first \\(\\xi_i = E_{\\theta&#39;}(Z_i \\mid \\mathbf{Y} = \\mathbf{y})\\) and \\(\\zeta_i = E_{\\theta&#39;}(Z_i^2 \\mid \\mathbf{Y} = \\mathbf{y})\\) we can rewrite the first and last of the three equations as the linear equation \\[ \\left(\\begin{array}{cc} \\sum_{i} m_i&amp; \\sum_{i} m_i\\xi_i \\\\ \\sum_{i} m_i\\xi_i &amp; \\sum_{i} m_i\\zeta_i \\end{array}\\right) \\left(\\begin{array}{c} \\beta_0 \\\\ \\nu \\end{array}\\right) = \\left(\\begin{array}{cc} \\sum_{i,j} y_{ij} \\\\ \\sum_{i,j} \\xi_i y_{ij} \\end{array}\\right).\\] Plugging the solution for \\(\\beta_0\\) and \\(\\nu\\) into the second equation we find \\[\\sigma^2 = \\frac{1}{\\sum_{i} m_i}\\left(\\sum_{ij} y_{ij}^2 - \\sum_{i} m_i(\\beta_0^2 + \\nu^2 \\zeta_i + 2 \\beta_0 \\nu \\xi_i)\\right).\\] This solves the M-step of the EM algorithm for the mixed effects model. What remains is the E-step that amounts to the computation of \\(\\xi_i\\) and \\(\\zeta_i\\). We know that the joint distribution of \\(\\mathbf{Y}\\) and \\(\\mathbf{Z}\\) is Gaussian, and we can easily compute the variances and covariances: \\[\\mathrm{cov}(Z_i, Z_j) = \\delta_{ij}\\] \\[\\mathrm{cov}(Y_{ij}, Y_{kl}) = \\left\\{ \\begin{array}{ll} \\nu^2 + \\sigma^2 &amp; \\quad \\text{if } i = k, j = l \\\\ \\nu^2 &amp; \\quad \\text{if } i = k, j \\neq l \\\\ 0 &amp; \\quad \\text{otherwise } \\end{array} \\right.\\] \\[\\mathrm{cov}(Z_i, Y_{kl}) = \\left\\{ \\begin{array}{ll} \\nu &amp; \\quad \\text{if } i = k \\\\ 0 &amp; \\quad \\text{otherwise } \\end{array} \\right.\\] This gives a joint Gaussian distribution \\[\\left( \\begin{array}{c} \\mathbf{Z} \\\\ \\mathbf{Y} \\end{array} \\right) \\sim \\mathcal{N}\\left( \\left(\\begin{array}{c} \\mathbf{0} \\\\ \\beta_0 \\mathbf{1}\\end{array} \\right), \\left(\\begin{array}{cc} \\Sigma_{11} &amp; \\Sigma_{12} \\\\ \\Sigma_{21} &amp; \\Sigma_{22} \\end{array}\\right)\\right).\\] From this and the general formulas for computing conditional distributions in the multivariate Gaussian distribution: \\[\\mathbf{Z} \\mid \\mathbf{Y} \\sim \\mathcal{N}\\left( \\Sigma_{12} \\Sigma_{22}^{-1}(\\mathbf{Y} - \\beta_0 \\mathbf{1}), \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21} \\right).\\] The conditional means, \\(\\xi_i\\), are thus the coordinates of \\(\\Sigma_{12} \\Sigma_{22}^{-1}(\\mathbf{Y} - \\beta_0 \\mathbf{1})\\). The conditional second moments, \\(\\zeta_i\\), can be found as the diagonal elements of the conditional covariance matrix plus \\(\\xi_i^2\\). "],
["fisher-information.html", "9.3 Fisher information", " 9.3 Fisher information For statistics relying on classical asymptotic theory we need an estimate of the Fisher information, e.g. the observed Fisher information (Hessian of the negative log-likelihood for the observed data). For numerical optimization of \\(Q\\) or variants of the EM algorithm (like EM gradient or acceleration methods) the gradient and Hessian of \\(Q\\) can be useful. However, these do not directly inform us on the Fisher information. In this section we show some interesting and useful relations between the derivatives of the log-likelihood for the observed data and derivatives of \\(Q\\) with the primary purpose of estimating the Fisher information. First we look at the peppered moth example, where we note that with \\(p = p(\\theta)\\) being some parametrization of the cell probabilities, \\[Q(\\theta \\mid \\theta&#39;) = \\sum_{k=1}^K \\frac{x_{j(k)} p_k(\\theta&#39;)}{M(p(\\theta&#39;))_{j(k)}} \\log p_k(\\theta),\\] where \\(j(k)\\) is defined by \\(k \\in A_{j(k)}\\). The gradient of \\(Q\\) w.r.t. \\(\\theta\\) is therefore \\[\\nabla_{\\theta} Q(\\theta \\mid \\theta&#39;) = \\sum_{k = 1}^K \\frac{x_{j(k)} p_k(\\theta&#39;)}{M(p(\\theta&#39;))_{j(k)} p_k(\\theta)} \\nabla_{\\theta} p_k(\\theta&#39;).\\] We recognize from previous computations in Section 8.2.4 that when we evaluate \\(\\nabla_{\\theta} Q(\\theta \\mid \\theta&#39;)\\) in \\(\\theta = \\theta&#39;\\) we get \\[\\nabla_{\\theta} Q(\\theta&#39; \\mid \\theta&#39;) = \\sum_{i = 1}^K \\frac{x_{j(i)} }{M(p(\\theta&#39;))_{j(i)}} \\nabla_{\\theta} p_i(\\theta&#39;) = \\nabla_{\\theta} \\ell(\\theta&#39;),\\] thus the gradient of \\(\\ell\\) in \\(\\theta&#39;\\) is actually identical to the gradient of \\(Q(\\cdot \\mid \\theta&#39;)\\) in \\(\\theta&#39;\\). This is not a coincidence, and it holds generally that \\[\\nabla_{\\theta} Q(\\theta&#39; \\mid \\theta&#39;) = \\nabla_{\\theta} \\ell(\\theta&#39;).\\] This follows from the fact we derived in the proof of Theorem 9.1 that \\(\\theta&#39;\\) minimizes \\[\\theta \\mapsto \\ell(\\theta) - Q(\\theta \\mid \\theta&#39;).\\] Another way to phrase this is that the minorant of \\(\\ell(\\theta)\\) touches \\(\\ell\\) tangentially in \\(\\theta&#39;\\). In the case where the observation \\(\\mathbf{y}\\) consists of \\(n\\) i.i.d. observations from the model with parameter \\(\\theta_0\\), \\(\\ell\\) as well as \\(Q(\\cdot \\mid \\theta&#39;)\\) are sums of terms for which the gradient identity above holds for each term. In particular, \\[\\nabla_{\\theta} \\ell(\\theta_0) = \\sum_{i=1}^n \\nabla_{\\theta} \\ell_i(\\theta_0) = \\sum_{i=1}^n \\nabla_{\\theta} Q_i(\\theta_0 \\mid \\theta_0),\\] and using the second Bartlett identity \\[\\mathcal{I}(\\theta_0) = V_{\\theta_0}(\\nabla_{\\theta} \\ell(\\theta_0))\\] we see that \\[\\hat{\\mathcal{I}}(\\theta_0) = \\sum_{i=1}^n \\big(\\nabla_{\\theta} Q_i(\\theta_0 \\mid \\theta_0) - n^{-1} \\nabla_{\\theta} \\ell(\\theta_0)\\big)\\big(\\nabla_{\\theta} Q_i(\\theta_0 \\mid \\theta_0) - n^{-1} \\nabla_{\\theta} \\ell(\\theta_0)\\big)^T\\] is almost an unbiased estimator of the Fisher information. It does have mean \\(\\mathcal{I}(\\theta_0)\\), but it is not an estimator as \\(\\theta_0\\) is not known. Using a plug-in-estimator, \\(\\hat{\\theta}\\), of \\(\\theta_0\\) we get a real estimator \\[\\hat{\\mathcal{I}} = \\hat{\\mathcal{I}}(\\hat{\\theta}) = \\sum_{i=1}^n \\big(\\nabla_{\\theta} Q_i(\\hat{\\theta} \\mid \\hat{\\theta}) - n^{-1} \\nabla_{\\theta} \\ell(\\hat{\\theta})\\big)\\big(\\nabla_{\\theta} Q_i(\\hat{\\theta} \\mid \\hat{\\theta}) - n^{-1} \\nabla_{\\theta} \\ell(\\hat{\\theta})\\big)^T,\\] though \\(\\hat{\\mathcal{I}}\\) will no longer necessarily be unbiased. We refer to \\(\\hat{\\mathcal{I}}\\) as the empirical Fisher information given by the estimator \\(\\hat{\\theta}\\). In most cases, \\(\\hat{\\theta}\\) is the maximum-likelihood estimator, in which case \\(\\nabla_{\\theta} \\ell(\\hat{\\theta}) = 0\\) and the empirical Fisher information simplifies to \\[\\hat{\\mathcal{I}} = \\sum_{i=1}^n \\nabla_{\\theta} Q_i(\\hat{\\theta} \\mid \\hat{\\theta}) \\nabla_{\\theta} Q_i(\\hat{\\theta} \\mid \\hat{\\theta})^T.\\] However, \\(\\nabla_{\\theta} \\ell(\\hat{\\theta})\\) is in practice only approximately equal to zero, and it is unclear if it should be dropped. For the peppered moths, where data is collected as i.i.d. samples of \\(n\\) individual specimens and tabulated according to phenotype, we implement the empirical Fisher information with the optional possibility of centering the gradients before computing the information estimate. We note that only three different observations of phenotype are possible, giving rise to three different possible terms in the sum. The implementation works directly on the tabulated data by computing all the three possible terms and then forming a weighted sum according to the number of times each term is present. empFisher &lt;- function(par, x, grad, center = FALSE) { grad_MLE &lt;- 0 ## is supposed to be 0 in the MLE if (center) grad_MLE &lt;- grad(par, x) / sum(x) grad1 &lt;- grad(par, c(1, 0, 0)) - grad_MLE grad2 &lt;- grad(par, c(0, 1, 0)) - grad_MLE grad3 &lt;- grad(par, c(0, 0, 1)) - grad_MLE x[1] * t(grad1) %*% grad1 + x[2] * t(grad2) %*% grad2 + x[3] * t(grad3) %*% grad3 } We test the implementation with and without centering and compare the result to a numerically computed hessian using optimHess (it is possible to get optim to compute the Hessian numerically in the minimizer as a final step, but optimHess does this computation separately). ## The gradient of Q (equivalently the log-likelihood) was ## implemented earlier as &#39;grad_loglik&#39;. grad &lt;- function(par, x) grad_loglik(par, x, prob, Dprob, c(1, 1, 1, 2, 2, 3)) empFisher(phat, c(85, 196, 341), grad) ## [,1] [,2] ## [1,] 18487.558 1384.626 ## [2,] 1384.626 6816.612 empFisher(phat, c(85, 196, 341), grad, center = TRUE) ## [,1] [,2] ## [1,] 18487.558 1384.626 ## [2,] 1384.626 6816.612 optimHess(phat, loglik, grad_loglik, x = c(85, 196, 341), prob = prob, Dprob = Dprob, group = c(1, 1, 1, 2, 2, 3)) ## [,1] [,2] ## [1,] 18490.938 1384.629 ## [2,] 1384.629 6816.769 Note that the numerically computed Hessian (the observed Fisher information) and the empirical Fisher information are different estimates of the same quantity. Thus they are not supposed to be identical on a given data set, but they are supposed to be estimates of the same thing and thus to be similar. An alternative to the empirical Fisher information or a direct computation of the observed Fisher information is supplemented EM (SEM). This is a general method for computing the observed Fisher information that relies only on EM steps and a numerical differentiation scheme. Define the EM map \\(\\Phi : \\Theta \\mapsto \\Theta\\) by \\[\\Phi(\\theta&#39;) = \\textrm{arg max}_{\\theta} \\ Q(\\theta \\mid \\theta&#39;).\\] A global maximum of the likelihood is a fixed point of \\(\\Phi\\), and the EM algorithm searches for a fixed point for \\(\\Phi\\), that is, a solution to \\[\\Phi(\\theta) = \\theta.\\] Variations of the EM-algorithm can often be seen as other ways to find a fixed point for \\(\\Phi\\). From \\[\\ell(\\theta) = Q(\\theta \\mid \\theta&#39;) + H(\\theta \\mid \\theta&#39;)\\] it follows that the observed Fisher information equals \\[\\hat{i}_X := - D^2_{\\theta} \\ell(\\hat{\\theta}) = \\underbrace{-D^2_{\\theta} Q(\\hat{\\theta} \\mid \\theta&#39;)}_{= \\hat{i}_Y(\\theta&#39;)} - D \\underbrace{^2_{\\theta} H(\\hat{\\theta} \\mid \\theta&#39;)}_{= \\hat{i}_{Y \\mid X}(\\theta&#39;)}.\\] It is possible to compute \\(\\hat{i}_Y := \\hat{i}_Y(\\hat{\\theta})\\). For peppered moths (and exponential families) it is as difficult as computing the Fisher information for complete observations. We want to compute \\(\\hat{i}_X\\) but \\(\\hat{i}_{Y \\mid X} := \\hat{i}_{Y \\mid X}(\\hat{\\theta})\\) is not computable either. It can, however, be shown that \\[D_{\\theta} \\Phi(\\hat{\\theta})^T = \\hat{i}_{Y\\mid X} \\left(\\hat{i}_Y\\right)^{-1}.\\] Hence \\[\\begin{align} \\hat{i}_X &amp; = \\left(I - \\hat{i}_{Y\\mid X} \\left(\\hat{i}_Y\\right)^{-1}\\right) \\hat{i}_Y \\\\ &amp; = \\left(I - D_{\\theta} \\Phi(\\hat{\\theta})^T\\right) \\hat{i}_Y. \\end{align}\\] Though the EM map \\(\\Phi\\) might not have a simple analytic expression, its Jacobian, \\(D_{\\theta} \\Phi(\\hat{\\theta})\\), can be computed via numerical differentiation once we have implemented \\(\\Phi\\). We also need the hessian of the map \\(Q\\), which we implement as an R function as well. Q &lt;- function(p, pp, x = c(85, 196, 341), group) { p[3] &lt;- 1 - p[1] - p[2] pp[3] &lt;- 1 - pp[1] - pp[2] - (x[group] * prob(pp) / M(prob(pp), group)[group]) %*% log(prob(p)) } The R package numDeriv contains functions that compute numerical derivatives. library(numDeriv) The Hessian of \\(Q\\) can be computed using this package. iY &lt;- hessian(Q, phat, pp = phat, group = c(1, 1, 1, 2, 2, 3)) Supplemented EM can then be implemented by computing the Jacobian of \\(\\Phi\\) using numDeriv as well. Phi &lt;- function(pp) MStep(EStep(pp, x = c(85, 196, 341))) DPhi &lt;- jacobian(Phi, phat) ## Using numDeriv function &#39;jacobian&#39; iX &lt;- (diag(1, 2) - t(DPhi)) %*% iY iX ## [,1] [,2] ## [1,] 18487.558 1384.626 ## [2,] 1384.626 6816.612 For statistics, we actually need the inverse Fisher information, which can be computed by inverting \\(\\hat{i}_X\\), but we also have the following interesting identity \\[\\begin{align} \\hat{i}_X^{-1} &amp; = \\hat{i}_Y^{-1} \\left(I - D_{\\theta} \\Phi(\\hat{\\theta})^T\\right)^{-1} \\\\ &amp; = \\hat{i}_Y^{-1} \\left(I + \\sum_{n=1}^{\\infty} \\left(D_{\\theta} \\Phi(\\hat{\\theta})^T\\right)^n \\right) \\\\ &amp; = \\hat{i}_Y^{-1} + \\hat{i}_Y^{-1} D_{\\theta} \\Phi(\\hat{\\theta})^T \\left(I - D_{\\theta} \\Phi(\\hat{\\theta})^T\\right)^{-1} \\end{align}\\] where the second identity follows by the Neumann series. The last formula above explicitly gives the asymptotic variance for the incomplete observation \\(X\\) as the asymptotic variance for the complete observation \\(Y\\) plus a correction term. iYinv &lt;- solve(iY) iYinv + iYinv %*% t(solve(diag(1, 2) - DPhi, DPhi)) ## [,1] [,2] ## [1,] 5.492602e-05 -1.115686e-05 ## [2,] -1.115686e-05 1.489667e-04 solve(iX) ## SEM-based, but different use of inversion ## [,1] [,2] ## [1,] 5.492602e-05 -1.115686e-05 ## [2,] -1.115686e-05 1.489667e-04 The SEM implementation above relies on the hessian and jacobian functions from the numDeriv package for numerical differentiation. It is possible to implement the computation of the hessian of \\(Q\\) analytically for the peppered moths, but to illustrate functionality of the numDeriv package we implemented the computation numerically above. Variants on the strategy for computing \\(D_{\\theta} \\Phi(\\hat{\\theta})\\) via numerical differentiation have been suggested in the literature, specifically using difference quotient approximations along the sequence of EM steps. This is not going to work as well as standard numerical differentiation since this method ignores numerical errors, and when the algorithm gets sufficiently close to the MLE, the numerical errors will dominate in the difference quotients. "],
["two-examples-revisited.html", "9.4 Two examples revisited", " 9.4 Two examples revisited 9.4.1 Gaussian mixtures In a two-component Gaussian mixture model the marginal density of the distribution of \\(Y\\) is \\[ f(y) = p \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} e^{-\\frac{(y - \\mu_1)^2}{2 \\sigma_1^2}} + (1 - p)\\frac{1}{\\sqrt{2 \\pi \\sigma_2^2}}e^{-\\frac{(y - \\mu_2)^2}{2 \\sigma_2^2}}.\\] The following is a simulation of data from such a mixture model. sigma1 &lt;- 1 sigma2 &lt;- 2 mu1 &lt;- -0.5 mu2 &lt;- 4 p &lt;- 0.5 n &lt;- 1000 z &lt;- sample(c(TRUE, FALSE), n, replace = TRUE, prob = c(p, 1 - p)) y &lt;- numeric(n) n1 &lt;- sum(z) y[z] &lt;- rnorm(n1, mu1, sigma1) y[!z] &lt;- rnorm(n - n1, mu2, sigma2) We implement the log-likelihood assuming that the variances are known. Note that the implementation takes just one single parameter argument, which is then supposed to be a vector of all parameters in the model. Internally to the function one has to decide for each entry in the parameter vector what parameter in the model it corresponds to. loglik &lt;- function(par, y) { p &lt;- par[1] if(p &lt; 0 || p &gt; 1) return(Inf) mu1 &lt;- par[2] mu2 &lt;- par[3] -sum(log(p * exp(-(y - mu1)^2 / (2 * sigma1^2)) / sigma1 + (1 - p) * exp(-(y - mu2)^2 / (2 * sigma2^2)) / sigma2)) } Without further implementations, optim can find the maximum-likelihood estimate if we have a sensible initial parameter guess. In this case we use the true parameters, which can be used when algorithms are tested, but they are, of course, not available for real applications. optim(c(0.5, -0.5, 4), loglik, y = y)[c(1, 2)] ## $par ## [1] 0.4808268 -0.5514402 3.9953178 ## ## $value ## [1] 1391.647 However, if we initialize the optimization badly, it does not find the maximum but a local maximum instead. optim(c(0.9, 3, 1), loglik, y = y)[c(1, 2)] ## $par ## [1] 0.2271762 5.6993719 0.6651568 ## ## $value ## [1] 1489.408 We will implement the EM algorithm for the Gaussian mixture model by implementing and E-step and an M-step function. We know from Section 7.4.1 how the complete log-likelihood looks, and the E-step becomes a matter of computing \\[p_i(\\mathbf{y}) = E(1(Z_i = 1) \\mid \\mathbf{Y} = \\mathbf{y}) = P(Z_i = 1 \\mid \\mathbf{Y} = \\mathbf{y}).\\] The M-step becomes identical to the MLE, which can be found explicitly, but where the indicators \\(1(Z_i = 1)\\) and \\(1(Z_i = 2) = 1 - 1(Z_i = 1)\\) are replaced by the conditional probabilities \\(p_i(\\mathbf{y})\\) and \\(1 - p_i(\\mathbf{y})\\), respectively. EStep &lt;- function(par, y) { p &lt;- par[1] mu1 &lt;- par[2] mu2 &lt;- par[3] a &lt;- p * exp(- (y - mu1)^2 / (2 * sigma1^2)) / sigma1 b &lt;- (1 - p) * exp(- (y - mu2)^2 / (2 * sigma2^2)) / sigma2 b / (a + b) } MStep &lt;- function(y, pz) { n &lt;- length(y) N2 &lt;- sum(pz) N1 &lt;- n - N2 c(N1 / n, sum((1 - pz) * y) / N1, sum(pz * y) / N2) } EM &lt;- function(par, y, epsilon = 1e-12) { repeat{ par0 &lt;- par par &lt;- MStep(y, EStep(par, y)) if(sum((par - par0)^2) &lt;= epsilon * (sum(par^2) + epsilon)) break } par ## Remember to return the parameter estimate } EM(c(0.5, -0.5, 4), y) ## [1] 0.4809303 -0.5512653 3.9952526 The EM algorithm may, just as any other optimization algorithm, end up in a local maximum, if it is started wrongly. EM(c(0.9, 3, 1), y) ## [1] 0.2271013 5.6996677 0.6654074 9.4.2 Gaussian state space "],
["stochopt.html", "Chapter 10 Stochastic Optimization", " Chapter 10 Stochastic Optimization Something "],
["stochastic-gradient.html", "10.1 Stochastic gradient", " 10.1 Stochastic gradient "],
["stochastic-em.html", "10.2 Stochastic EM", " 10.2 Stochastic EM "],
["references.html", "References", " References "]
]

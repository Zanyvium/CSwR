<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.2 Kernel methods | Computational Statistics with R</title>
  <meta name="description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2.2 Kernel methods | Computational Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.2 Kernel methods | Computational Statistics with R" />
  
  <meta name="twitter:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

<meta name="author" content="Niels Richard Hansen" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2-1-unidens.html"/>
<link rel="next" href="2-3-bandwidth.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-intro.html"><a href="1-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html"><i class="fa fa-check"></i><b>1.1</b> Smoothing</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#intro-angles"><i class="fa fa-check"></i><b>1.1.1</b> Angle distributions in proteins</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#using-ggplot2"><i class="fa fa-check"></i><b>1.1.2</b> Using ggplot2</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#changing-the-defaults"><i class="fa fa-check"></i><b>1.1.3</b> Changing the defaults</a></li>
<li class="chapter" data-level="1.1.4" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#multivariate-smoothing"><i class="fa fa-check"></i><b>1.1.4</b> Multivariate methods</a></li>
<li class="chapter" data-level="1.1.5" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#large-scale-smoothing"><i class="fa fa-check"></i><b>1.1.5</b> Large scale smoothing</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#vM"><i class="fa fa-check"></i><b>1.2.1</b> Univariate von Mises distributions</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#mixtures-of-von-mises-distributions"><i class="fa fa-check"></i><b>1.2.2</b> Mixtures of von Mises distributions</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#large-scale-monte-carlo-methods"><i class="fa fa-check"></i><b>1.2.3</b> Large scale Monte Carlo methods</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-optimization.html"><a href="1-3-optimization.html"><i class="fa fa-check"></i><b>1.3</b> Optimization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-optimization.html"><a href="1-3-optimization.html#the-em-algorithm"><i class="fa fa-check"></i><b>1.3.1</b> The EM-algorithm</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-3-optimization.html"><a href="1-3-optimization.html#large-scale-optimization"><i class="fa fa-check"></i><b>1.3.2</b> Large scale optimization</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Smoothing</b></span></li>
<li class="chapter" data-level="2" data-path="2-density.html"><a href="2-density.html"><i class="fa fa-check"></i><b>2</b> Density estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-unidens.html"><a href="2-1-unidens.html"><i class="fa fa-check"></i><b>2.1</b> Univariate density estimation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-1-unidens.html"><a href="2-1-unidens.html#likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Likelihood considerations</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-1-unidens.html"><a href="2-1-unidens.html#sieves"><i class="fa fa-check"></i><b>2.1.2</b> Method of sieves</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-1-unidens.html"><a href="2-1-unidens.html#basis-density"><i class="fa fa-check"></i><b>2.1.3</b> Basis expansions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html"><i class="fa fa-check"></i><b>2.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#benchmarking"><i class="fa fa-check"></i><b>2.2.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html"><i class="fa fa-check"></i><b>2.3</b> Bandwidth selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#rectangular"><i class="fa fa-check"></i><b>2.3.1</b> Revisiting the rectangular kernel</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#ise-mise-and-mse-for-kernel-estimators"><i class="fa fa-check"></i><b>2.3.2</b> ISE, MISE and MSE for kernel estimators</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#plug-in-estimation-of-the-oracle-bandwidth"><i class="fa fa-check"></i><b>2.3.3</b> Plug-in estimation of the oracle bandwidth</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#cross-validation"><i class="fa fa-check"></i><b>2.3.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-exercises.html"><a href="2-4-exercises.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="2-4-exercises.html"><a href="2-4-exercises.html#kernel-density-estimation"><i class="fa fa-check"></i>Kernel density estimation</a></li>
<li class="chapter" data-level="" data-path="2-4-exercises.html"><a href="2-4-exercises.html#benchmarking-1"><i class="fa fa-check"></i>Benchmarking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-bivariate.html"><a href="3-bivariate.html"><i class="fa fa-check"></i><b>3</b> Bivariate smoothing</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html"><i class="fa fa-check"></i><b>3.1</b> Nearest neighbor smoothers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#linear-smoothers"><i class="fa fa-check"></i><b>3.1.1</b> Linear smoothers</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#implementing-the-running-mean"><i class="fa fa-check"></i><b>3.1.2</b> Implementing the running mean</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#choose-k-by-cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Choose <span class="math inline">\(k\)</span> by cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html"><i class="fa fa-check"></i><b>3.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html#nadarayawatson-kernel-smoothing"><i class="fa fa-check"></i><b>3.2.1</b> Nadaraya–Watson kernel smoothing</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html#local-regression-smoothers"><i class="fa fa-check"></i><b>3.2.2</b> Local regression smoothers</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-sparse-linear-algebra.html"><a href="3-3-sparse-linear-algebra.html"><i class="fa fa-check"></i><b>3.3</b> Sparse linear algebra</a></li>
<li class="chapter" data-level="3.4" data-path="3-4-onb.html"><a href="3-4-onb.html"><i class="fa fa-check"></i><b>3.4</b> Orthogonal basis expansions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-4-onb.html"><a href="3-4-onb.html#polynomial-expansions"><i class="fa fa-check"></i><b>3.4.1</b> Polynomial expansions</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-4-onb.html"><a href="3-4-onb.html#fourier-expansions"><i class="fa fa-check"></i><b>3.4.2</b> Fourier expansions</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-5-splines.html"><a href="3-5-splines.html"><i class="fa fa-check"></i><b>3.5</b> Splines</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-5-splines.html"><a href="3-5-splines.html#smoothing-splines"><i class="fa fa-check"></i><b>3.5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-5-splines.html"><a href="3-5-splines.html#splines-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Splines in R</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-5-splines.html"><a href="3-5-splines.html#efficient-splines"><i class="fa fa-check"></i><b>3.5.3</b> Efficient computation with splines</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-6-gaussian-processes.html"><a href="3-6-gaussian-processes.html"><i class="fa fa-check"></i><b>3.6</b> Gaussian processes</a></li>
<li class="chapter" data-level="3.7" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html"><i class="fa fa-check"></i><b>3.7</b> The Kalman filter</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#ar1-example"><i class="fa fa-check"></i><b>3.7.1</b> AR(1)-example</a></li>
<li class="chapter" data-level="3.7.2" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#the-kalman-smoother"><i class="fa fa-check"></i><b>3.7.2</b> The Kalman smoother</a></li>
<li class="chapter" data-level="3.7.3" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#implementation"><i class="fa fa-check"></i><b>3.7.3</b> Implementation</a></li>
<li class="chapter" data-level="3.7.4" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#the-kalman-filter-1"><i class="fa fa-check"></i><b>3.7.4</b> The Kalman filter</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="2-4-exercises.html"><a href="2-4-exercises.html#exercises"><i class="fa fa-check"></i><b>3.8</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="3-8-exercises.html"><a href="3-8-exercises.html"><i class="fa fa-check"></i>Nearest neighbors</a></li>
<li class="chapter" data-level="" data-path="3-8-exercises.html"><a href="3-8-exercises.html#kernel-estimators"><i class="fa fa-check"></i>Kernel estimators</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Monte Carlo Methods</b></span></li>
<li class="chapter" data-level="4" data-path="4-univariate-random-variables.html"><a href="4-univariate-random-variables.html"><i class="fa fa-check"></i><b>4</b> Univariate random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html"><i class="fa fa-check"></i><b>4.1</b> Pseudorandom number generators</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html#implementing-a-pseudorandom-number-generator"><i class="fa fa-check"></i><b>4.1.1</b> Implementing a pseudorandom number generator</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html#pseudorandom-number-packages"><i class="fa fa-check"></i><b>4.1.2</b> Pseudorandom number packages</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-2-transformation-techniques.html"><a href="4-2-transformation-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Transformation techniques</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-2-transformation-techniques.html"><a href="4-2-transformation-techniques.html#sampling-from-a-t-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Sampling from a <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html#vMsim"><i class="fa fa-check"></i><b>4.3.1</b> von Mises distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html"><i class="fa fa-check"></i><b>4.4</b> Adaptive envelopes</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html#beta-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Beta distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html#von-mises-distribution"><i class="fa fa-check"></i><b>4.4.2</b> von Mises distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="2-4-exercises.html"><a href="2-4-exercises.html#exercises"><i class="fa fa-check"></i><b>4.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="4-5-exercises.html"><a href="4-5-exercises.html"><i class="fa fa-check"></i>Rejection sampling of Gaussian random variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-mci.html"><a href="5-mci.html"><i class="fa fa-check"></i><b>5</b> Monte Carlo integration</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-assessment.html"><a href="5-1-assessment.html"><i class="fa fa-check"></i><b>5.1</b> Assessment</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-1-assessment.html"><a href="5-1-assessment.html#using-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.1.1</b> Using the central limit theorem</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-1-assessment.html"><a href="5-1-assessment.html#concentration-inequalities"><i class="fa fa-check"></i><b>5.1.2</b> Concentration inequalities</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-1-assessment.html"><a href="5-1-assessment.html#exponential-tail-bound-for-gamma-distributed-variables"><i class="fa fa-check"></i><b>5.1.3</b> Exponential tail bound for Gamma distributed variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html"><i class="fa fa-check"></i><b>5.2</b> Importance sampling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html#unknown-normalization-constants"><i class="fa fa-check"></i><b>5.2.1</b> Unknown normalization constants</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html#hd-int"><i class="fa fa-check"></i><b>5.2.2</b> Computing a high-dimensional integral</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-3-network-failure.html"><a href="5-3-network-failure.html"><i class="fa fa-check"></i><b>5.3</b> Network failure</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-3-network-failure.html"><a href="5-3-network-failure.html#object-oriented-implementation"><i class="fa fa-check"></i><b>5.3.1</b> Object oriented implementation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part III: Optimization</b></span></li>
<li class="chapter" data-level="6" data-path="6-four-examples.html"><a href="6-four-examples.html"><i class="fa fa-check"></i><b>6</b> Four Examples</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html"><i class="fa fa-check"></i><b>6.1</b> Exponential families</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#full-exponential-families"><i class="fa fa-check"></i><b>6.1.1</b> Full exponential families</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#bayes-net"><i class="fa fa-check"></i><b>6.1.2</b> Exponential family Bayesian networks</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#exp-fam-deriv"><i class="fa fa-check"></i><b>6.1.3</b> Likelihood computations</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#curved-exponential-families"><i class="fa fa-check"></i><b>6.1.4</b> Curved exponential families</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-2-multinomial-models.html"><a href="6-2-multinomial-models.html"><i class="fa fa-check"></i><b>6.2</b> Multinomial models</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-2-multinomial-models.html"><a href="6-2-multinomial-models.html#pep-moth"><i class="fa fa-check"></i><b>6.2.1</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-3-regression.html"><a href="6-3-regression.html"><i class="fa fa-check"></i><b>6.3</b> Regression models</a></li>
<li class="chapter" data-level="6.4" data-path="6-4-finite-mixture-models.html"><a href="6-4-finite-mixture-models.html"><i class="fa fa-check"></i><b>6.4</b> Finite mixture models</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-4-finite-mixture-models.html"><a href="6-4-finite-mixture-models.html#Gaus-mix-ex"><i class="fa fa-check"></i><b>6.4.1</b> Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-5-mixed-models.html"><a href="6-5-mixed-models.html"><i class="fa fa-check"></i><b>6.5</b> Mixed models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-numopt.html"><a href="7-numopt.html"><i class="fa fa-check"></i><b>7</b> Numerical optimization</a><ul>
<li class="chapter" data-level="7.1" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html"><i class="fa fa-check"></i><b>7.1</b> Algorithms and convergence</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#descent-algorithms"><i class="fa fa-check"></i><b>7.1.1</b> Descent algorithms</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#maps-and-fixed-points"><i class="fa fa-check"></i><b>7.1.2</b> Maps and fixed points</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#convergence-rate"><i class="fa fa-check"></i><b>7.1.3</b> Convergence rate</a></li>
<li class="chapter" data-level="7.1.4" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#stopping-criteria"><i class="fa fa-check"></i><b>7.1.4</b> Stopping criteria</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html"><i class="fa fa-check"></i><b>7.2</b> Descent direction algorithms</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#line-search"><i class="fa fa-check"></i><b>7.2.1</b> Line search</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>7.2.2</b> Gradient descent</a></li>
<li class="chapter" data-level="7.2.3" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#conjugate-gradients"><i class="fa fa-check"></i><b>7.2.3</b> Conjugate gradients</a></li>
<li class="chapter" data-level="7.2.4" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#pep-moth-descent"><i class="fa fa-check"></i><b>7.2.4</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-3-newton-type-algorithms.html"><a href="7-3-newton-type-algorithms.html"><i class="fa fa-check"></i><b>7.3</b> Newton-type algorithms</a><ul>
<li class="chapter" data-level="7.3.1" data-path="7-3-newton-type-algorithms.html"><a href="7-3-newton-type-algorithms.html#poisson-regression"><i class="fa fa-check"></i><b>7.3.1</b> Poisson regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="7-3-newton-type-algorithms.html"><a href="7-3-newton-type-algorithms.html#quasi-newton-algorithms"><i class="fa fa-check"></i><b>7.3.2</b> Quasi-Newton algorithms</a></li>
<li class="chapter" data-level="7.3.3" data-path="7-3-newton-type-algorithms.html"><a href="7-3-newton-type-algorithms.html#sparsity"><i class="fa fa-check"></i><b>7.3.3</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7-4-misc-.html"><a href="7-4-misc-.html"><i class="fa fa-check"></i><b>7.4</b> Misc.</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-em.html"><a href="8-em.html"><i class="fa fa-check"></i><b>8</b> Expectation maximization algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html"><i class="fa fa-check"></i><b>8.1</b> Basic properties</a><ul>
<li class="chapter" data-level="8.1.1" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#incomplete-data-likelihood"><i class="fa fa-check"></i><b>8.1.1</b> Incomplete data likelihood</a></li>
<li class="chapter" data-level="8.1.2" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#monotonicity-of-the-em-algorithm"><i class="fa fa-check"></i><b>8.1.2</b> Monotonicity of the EM algorithm</a></li>
<li class="chapter" data-level="8.1.3" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#peppered-moths"><i class="fa fa-check"></i><b>8.1.3</b> Peppered moths</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8-2-EM-exp.html"><a href="8-2-EM-exp.html"><i class="fa fa-check"></i><b>8.2</b> Exponential families</a></li>
<li class="chapter" data-level="8.3" data-path="8-3-fisher-information.html"><a href="8-3-fisher-information.html"><i class="fa fa-check"></i><b>8.3</b> Fisher information</a></li>
<li class="chapter" data-level="8.4" data-path="8-4-revisiting-gaussian-mixtures.html"><a href="8-4-revisiting-gaussian-mixtures.html"><i class="fa fa-check"></i><b>8.4</b> Revisiting Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-StochOpt.html"><a href="9-StochOpt.html"><i class="fa fa-check"></i><b>9</b> Stochastic Optimization</a><ul>
<li class="chapter" data-level="9.1" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html"><i class="fa fa-check"></i><b>9.1</b> Stochastic gradient algorithms</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#section"><i class="fa fa-check"></i><b>9.1.1</b> </a></li>
<li class="chapter" data-level="9.1.2" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#online-stochastic-gradient-descent"><i class="fa fa-check"></i><b>9.1.2</b> Online stochastic gradient descent</a></li>
<li class="chapter" data-level="9.1.3" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>9.1.3</b> Stochastic gradient descent</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9-2-nonlinear-least-squares.html"><a href="9-2-nonlinear-least-squares.html"><i class="fa fa-check"></i><b>9.2</b> Nonlinear least squares</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-app-R.html"><a href="A-app-R.html"><i class="fa fa-check"></i><b>A</b> R programming</a><ul>
<li class="chapter" data-level="A.1" data-path="A-1-functions.html"><a href="A-1-functions.html"><i class="fa fa-check"></i><b>A.1</b> Functions</a><ul>
<li class="chapter" data-level="A.1.1" data-path="A-1-functions.html"><a href="A-1-functions.html#vectorization"><i class="fa fa-check"></i><b>A.1.1</b> Vectorization</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="A-2-objects-and-methods.html"><a href="A-2-objects-and-methods.html"><i class="fa fa-check"></i><b>A.2</b> Objects and methods</a></li>
<li class="chapter" data-level="A.3" data-path="A-3-environments.html"><a href="A-3-environments.html"><i class="fa fa-check"></i><b>A.3</b> Environments</a><ul>
<li class="chapter" data-level="A.3.1" data-path="A-3-environments.html"><a href="A-3-environments.html#function-factories"><i class="fa fa-check"></i><b>A.3.1</b> Function factories</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="A-4-performance.html"><a href="A-4-performance.html"><i class="fa fa-check"></i><b>A.4</b> Performance</a><ul>
<li class="chapter" data-level="A.4.1" data-path="A-4-performance.html"><a href="A-4-performance.html#parallel-computations"><i class="fa fa-check"></i><b>A.4.1</b> Parallel computations</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="2-4-exercises.html"><a href="2-4-exercises.html#exercises"><i class="fa fa-check"></i><b>A.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="A-5-exercises.html"><a href="A-5-exercises.html"><i class="fa fa-check"></i>R training exercises</a></li>
<li class="chapter" data-level="" data-path="A-5-exercises.html"><a href="A-5-exercises.html#histograms-with-non-equidistant-breaks"><i class="fa fa-check"></i>Histograms with non-equidistant breaks</a></li>
<li class="chapter" data-level="" data-path="A-5-exercises.html"><a href="A-5-exercises.html#functions-and-functional-programming"><i class="fa fa-check"></i>Functions and functional programming</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="kernel-density" class="section level2">
<h2><span class="header-section-number">2.2</span> Kernel methods</h2>
<p>In Section <a href="2-1-unidens.html#unidens">2.1</a> we pursued the principled but also somewhat
abstract approach to density estimation via maximum-likelihood estimation
over a suitably constrained set of distributions. In this section we
will pursue the more basic idea of smooth density estimation relying on
the approximation
<span class="math display">\[P(X \in (x-h, x+h)) = \int_{x-h}^{x+h} f_0(z) \ dz \simeq f_0(x) 2h,\]</span>
which is valid for any continuous density <span class="math inline">\(f_0\)</span>. Inverting this approximation
and using the law of large numbers,</p>
<p><span class="math display">\[\begin{align*}
f_0(x) &amp; \simeq \frac{1}{2h}P(X \in (x-h, x+h)) \\
&amp; \simeq \frac{1}{2hn} \sum_{i=1}^n 1_{(x-h, x+h)}(x_i) \\
&amp; =  \underbrace{\frac{1}{2hn} \sum_{i=1}^n 1_{(-h, h)}(x - x_i)}_{\hat{f}_h(x)}
\end{align*}\]</span></p>
<p>for i.i.d. observations <span class="math inline">\(x_1, \ldots, x_n\)</span> from the distribution <span class="math inline">\(f_0 \cdot m\)</span>.
The function <span class="math inline">\(\hat{f}_h\)</span> defined as above is an example of a kernel
density estimator with a rectangular kernel. We immediately note that <span class="math inline">\(h\)</span> has
to be chosen appropriately. If <span class="math inline">\(h\)</span> is large, <span class="math inline">\(\hat{f}_h\)</span> will be flat and close to
a constant. If <span class="math inline">\(h\)</span> is small, <span class="math inline">\(\hat{f}_h\)</span> will make large jumps close to the observations.</p>
<p>What do we then mean by an “appropriate” choice of <span class="math inline">\(h\)</span> above? Just as for the
methods of sieves we must have some prior assumptions about what we expect
<span class="math inline">\(f_0\)</span> to look like. Typically, we expect <span class="math inline">\(f_0\)</span> to have few oscillations and to be
fairly smooth, and we want <span class="math inline">\(\hat{f}_h\)</span> to reflect that. A too large <span class="math inline">\(h\)</span> will oversmooth
the data relative to <span class="math inline">\(f_0\)</span> by effectively ignoring the data, while a too small <span class="math inline">\(h\)</span> will undersmooth
the data relative to <span class="math inline">\(f_0\)</span> by allowing individual data points to have large local effects that
make the estimate wiggly. More formally, we can look at the mean and variance
of <span class="math inline">\(\hat{f}_h\)</span>. Letting <span class="math inline">\(p(x, h) = P(X \in (x-h, x+h))\)</span>, it follows that
<span class="math inline">\(f_h(x) = E(\hat{f}_h(x)) = p(x, h) / (2h)\)</span> while</p>
<p><span class="math display" id="eq:varRect">\[\begin{equation} 
V(\hat{f}_h(x)) = \frac{p(x, h) (1 - p(x, h))}{4h^2 n} \simeq f_h(x) \frac{1}{2hn}.
\tag{2.1}
\end{equation}\]</span></p>
<p>We see from these computations that for the <span class="math inline">\(\hat{f}_h(x)\)</span> to be approximately unbiased for any <span class="math inline">\(x\)</span>
we need <span class="math inline">\(h\)</span> to be small – ideally letting <span class="math inline">\(h \to 0\)</span> since then <span class="math inline">\(f_h(x) \to f_0(x)\)</span>.
However, this will make the variance blow up, and to minimize variance we should
instead choose <span class="math inline">\(h\)</span> as large as possible. One way to define “appropriate” is
then to strike a balance between the bias and the variance as a function
of <span class="math inline">\(h\)</span> so as to minimize the mean squared error of <span class="math inline">\(\hat{f}_h(x)\)</span>.</p>
<p>We will find the optimal tradeoff for the rectangular kernel in Section <a href="2-3-bandwidth.html#bandwidth">2.3</a>
on <a href="2-3-bandwidth.html#bandwidth">bandwidth selection</a>. It’s not difficult, and you are encouraged
to try finding it yourself at this point. In this section we will focus on
computational aspects of kernel density estimation, but first we will generalize
the estimator by allowing for other kernels.</p>
<p>The estimate <span class="math inline">\(\hat{f}_h(x)\)</span> will be unbiased if <span class="math inline">\(f_0\)</span> is constantly equal to <span class="math inline">\(f_0(x)\)</span>
in the entire interval <span class="math inline">\((x-h, x+h)\)</span>. This is atypical and can only happen for all <span class="math inline">\(x\)</span>
if <span class="math inline">\(f_0\)</span> is constant. We expect the typical situation to be that
<span class="math inline">\(f_0\)</span> deviates the most from <span class="math inline">\(f_0(x)\)</span> close to <span class="math inline">\(x \pm h\)</span>, and
that this causes a bias of <span class="math inline">\(\hat{f}_h(x).\)</span>
Observations falling close to <span class="math inline">\(x + h\)</span>, say, should thus count less
than observations falling close to <span class="math inline">\(x\)</span>? The rectangular kernel makes a sharp
cut; either a data point is in or it is out. If we use a smooth weighting
function instead of a sharp cut, we might be able to include more
data points and lower the variance while keeping the bias small. This is
precisely the idea of <em>kernel estimators</em>, defined generally as</p>
<p><span class="math display" id="eq:kernel-def">\[\begin{equation}
\hat{f}_h(x) = \frac{1}{hn} \sum_{i=1}^n K\left(\frac{x - x_i}{h}\right)
\tag{2.2}
\end{equation}\]</span></p>
<p>for a kernel <span class="math inline">\(K : \mathbb{R} \to \mathbb{R}\)</span>. The parameter <span class="math inline">\(h &gt; 0\)</span> is known
as the <em>bandwidth</em>. Examples of kernels include the <em>uniform</em> or <em>rectangular kernel</em>
<span class="math display">\[K(x) = \frac{1}{2} 1_{(-1,1)}(x),\]</span>
and the <em>Gaussian kernel</em>
<span class="math display">\[K(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}.\]</span></p>
<p>One direct benefit of considering other kernels than the rectangular is that
<span class="math inline">\(\hat{f}_h\)</span> inherits all smoothness properties from <span class="math inline">\(K\)</span>. Whereas the rectangular
kernel is not even continuous, the Gaussian kernel is <span class="math inline">\(C^{\infty}\)</span> and so is
the resulting kernel density estimate.</p>
<p>We may note that <span class="math inline">\(\overline{f}_h\)</span> considered in Section <a href="2-1-unidens.html#likelihood">2.1.1</a> simply is
the kernel density estimator with the Gaussian kernel and bandwidth <span class="math inline">\(h\)</span>.</p>
<div id="implementation" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Implementation</h3>
<p>What should be computed to compute a kernel density estimate? That is, in fact,
a good question, because the definition actually just specifies how to evaluate
<span class="math inline">\(\hat{f}_h\)</span> in any given point <span class="math inline">\(x\)</span>, but there is really not anything to compute
until we need to evaluate <span class="math inline">\(\hat{f}_h\)</span>. Thus when we implement kernel density
estimation we really implement algorithms for evaluating a density estimate
in a finite number of points.</p>
<p>Our first implementation is a fairly low-level implementation that returns
the evaluation of the density estimate in a given number of equidistant
points. The function mimics some of the defaults of <code>density</code> so that it
actually evaluates the estimate in the same points as <code>density</code>.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="2-2-kernel-density.html#cb33-1"></a><span class="co">## This is an implementation of the function &#39;kernDens&#39; that computes </span></span>
<span id="cb33-2"><a href="2-2-kernel-density.html#cb33-2"></a><span class="co">## evaluations of Gaussian kernel density estimates in a grid of points.</span></span>
<span id="cb33-3"><a href="2-2-kernel-density.html#cb33-3"></a><span class="co">##</span></span>
<span id="cb33-4"><a href="2-2-kernel-density.html#cb33-4"></a><span class="co">## The function has three formal arguments: &#39;x&#39; is the numeric vector of data </span></span>
<span id="cb33-5"><a href="2-2-kernel-density.html#cb33-5"></a><span class="co">## points, &#39;h&#39; is the bandwidth and &#39;m&#39; is the number of grid points. </span></span>
<span id="cb33-6"><a href="2-2-kernel-density.html#cb33-6"></a><span class="co">## The default value of 512 is chosen to match the default of &#39;density&#39;. </span></span>
<span id="cb33-7"><a href="2-2-kernel-density.html#cb33-7"></a>kernDens &lt;-<span class="st"> </span><span class="cf">function</span> (x, h, <span class="dt">m =</span> <span class="dv">512</span>) {</span>
<span id="cb33-8"><a href="2-2-kernel-density.html#cb33-8"></a>  rg &lt;-<span class="st"> </span><span class="kw">range</span>(x)</span>
<span id="cb33-9"><a href="2-2-kernel-density.html#cb33-9"></a>  <span class="co">## xx is equivalent to grid points in &#39;density&#39;</span></span>
<span id="cb33-10"><a href="2-2-kernel-density.html#cb33-10"></a>  xx &lt;-<span class="st"> </span><span class="kw">seq</span>(rg[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>h, rg[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>h, <span class="dt">length.out =</span> m)</span>
<span id="cb33-11"><a href="2-2-kernel-density.html#cb33-11"></a>  y &lt;-<span class="st"> </span><span class="kw">numeric</span>(m) <span class="co">## The evaluations, initialized as a vector of zeroes</span></span>
<span id="cb33-12"><a href="2-2-kernel-density.html#cb33-12"></a>  <span class="co">## The actual computation is done using nested for-loops. The outer loop</span></span>
<span id="cb33-13"><a href="2-2-kernel-density.html#cb33-13"></a>  <span class="co">## is over the grid points, and the inner loop is over the data points.</span></span>
<span id="cb33-14"><a href="2-2-kernel-density.html#cb33-14"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_along</span>(xx))</span>
<span id="cb33-15"><a href="2-2-kernel-density.html#cb33-15"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="kw">seq_along</span>(x))</span>
<span id="cb33-16"><a href="2-2-kernel-density.html#cb33-16"></a>      y[i] &lt;-<span class="st"> </span>y[i] <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span><span class="st"> </span>(xx[i] <span class="op">-</span><span class="st"> </span>x[j])<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>h<span class="op">^</span><span class="dv">2</span>))  </span>
<span id="cb33-17"><a href="2-2-kernel-density.html#cb33-17"></a>  y &lt;-<span class="st"> </span>y <span class="op">/</span><span class="st"> </span>(<span class="kw">sqrt</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi) <span class="op">*</span><span class="st"> </span>h <span class="op">*</span><span class="st"> </span><span class="kw">length</span>(x))</span>
<span id="cb33-18"><a href="2-2-kernel-density.html#cb33-18"></a>  <span class="kw">list</span>(<span class="dt">x =</span> xx, <span class="dt">y =</span> y)</span>
<span id="cb33-19"><a href="2-2-kernel-density.html#cb33-19"></a>}</span></code></pre></div>
<p>Note that the function returns a list containing the grid points (<code>x</code>) where
the density estimate is evaluated as well as the estimated density
evaluations (<code>y</code>). Note also that the argument <code>m</code> above sets the number of
grid points, whereas <code>density</code> uses the argument <code>n</code> for that. The
latter can be a bit confusing as <span class="math inline">\(n\)</span> is often used to denote the
number of data points.</p>
<p>We will immediately test if the implementation works as expected – in this
case by comparing it to our reference implementation <code>density</code>.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="2-2-kernel-density.html#cb34-1"></a>f_hat &lt;-<span class="st"> </span><span class="kw">kernDens</span>(psi, <span class="fl">0.2</span>)</span>
<span id="cb34-2"><a href="2-2-kernel-density.html#cb34-2"></a>f_hat_dens &lt;-<span class="st"> </span><span class="kw">density</span>(psi, <span class="fl">0.2</span>)</span>
<span id="cb34-3"><a href="2-2-kernel-density.html#cb34-3"></a><span class="kw">plot</span>(f_hat, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">lwd =</span> <span class="dv">4</span>, <span class="dt">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>)</span>
<span id="cb34-4"><a href="2-2-kernel-density.html#cb34-4"></a><span class="kw">lines</span>(f_hat_dens, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb34-5"><a href="2-2-kernel-density.html#cb34-5"></a><span class="kw">plot</span>(f_hat<span class="op">$</span>x, f_hat<span class="op">$</span>y <span class="op">-</span><span class="st"> </span>f_hat_dens<span class="op">$</span>y, </span>
<span id="cb34-6"><a href="2-2-kernel-density.html#cb34-6"></a>     <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Difference&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:kernDens-fig"></span>
<img src="CSwR_files/figure-html/kernDens-fig-1.png" alt="Kernel density estimates with the Gaussian kernel (left) using R's implementation (black) and our implementation (red) together with differences of the estimates (right)." width="49%" /><img src="CSwR_files/figure-html/kernDens-fig-2.png" alt="Kernel density estimates with the Gaussian kernel (left) using R's implementation (black) and our implementation (red) together with differences of the estimates (right)." width="49%" />
<p class="caption">
Figure 2.5: Kernel density estimates with the Gaussian kernel (left) using R’s implementation (black) and our implementation (red) together with differences of the estimates (right).
</p>
</div>
<p>Figure <a href="2-2-kernel-density.html#fig:kernDens-fig">2.5</a> suggests that the estimates computed by our
implementation and by <code>density</code> are the same when we just visually compare the
plotted densities. However, if we look at the differences instead,
we see that they are as large as <span class="math inline">\(4 \times 10^{-4}\)</span> in absolute value. This is
way above what we should expect from rounding errors alone when using
double precision arithmetic. Thus the two implementations only compute
<em>approximately</em> the same, which is, in fact, because <code>density</code> relies
on certain approximations for run time efficiency.</p>
<p>In R we can often beneficially implement computations in a vectorized way
instead of using an explicit loop. It is fairly easy to change the implementation
to be more vectorized by computing each evaluation in one single line using
the <code>sum</code> function and the fact that <code>exp</code> and squaring are vectorized.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="2-2-kernel-density.html#cb35-1"></a>kernDens_vec &lt;-<span class="st"> </span><span class="cf">function</span> (x, h, <span class="dt">m =</span> <span class="dv">512</span>) {</span>
<span id="cb35-2"><a href="2-2-kernel-density.html#cb35-2"></a>  rg &lt;-<span class="st"> </span><span class="kw">range</span>(x)</span>
<span id="cb35-3"><a href="2-2-kernel-density.html#cb35-3"></a>  xx &lt;-<span class="st"> </span><span class="kw">seq</span>(rg[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>h, rg[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>h, <span class="dt">length.out =</span> m)</span>
<span id="cb35-4"><a href="2-2-kernel-density.html#cb35-4"></a>  y &lt;-<span class="st"> </span><span class="kw">numeric</span>(m) </span>
<span id="cb35-5"><a href="2-2-kernel-density.html#cb35-5"></a>  <span class="co">## The inner loop from &#39;kernDens&#39; has been vectorized, and only the </span></span>
<span id="cb35-6"><a href="2-2-kernel-density.html#cb35-6"></a>  <span class="co">## outer loop over the grid points remains. </span></span>
<span id="cb35-7"><a href="2-2-kernel-density.html#cb35-7"></a>  const &lt;-<span class="st"> </span>(<span class="kw">sqrt</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi) <span class="op">*</span><span class="st"> </span>h <span class="op">*</span><span class="st"> </span><span class="kw">length</span>(x))</span>
<span id="cb35-8"><a href="2-2-kernel-density.html#cb35-8"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_along</span>(xx))</span>
<span id="cb35-9"><a href="2-2-kernel-density.html#cb35-9"></a>      y[i] &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">exp</span>(<span class="op">-</span>(xx[i] <span class="op">-</span><span class="st"> </span>x)<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>h<span class="op">^</span><span class="dv">2</span>))) <span class="op">/</span><span class="st"> </span>const</span>
<span id="cb35-10"><a href="2-2-kernel-density.html#cb35-10"></a>  <span class="kw">list</span>(<span class="dt">x =</span> xx, <span class="dt">y =</span> y)</span>
<span id="cb35-11"><a href="2-2-kernel-density.html#cb35-11"></a>}</span></code></pre></div>
<p>We test this new implementation by comparing it to our previous
implementation.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="2-2-kernel-density.html#cb36-1"></a><span class="kw">range</span>(<span class="kw">kernDens</span>(psi, <span class="fl">0.2</span>)<span class="op">$</span>y <span class="op">-</span><span class="st"> </span><span class="kw">kernDens_vec</span>(psi, <span class="fl">0.2</span>)<span class="op">$</span>y)</span></code></pre></div>
<pre><code>## [1] -5.551115e-16  3.885781e-16</code></pre>
<p>The magnitude of the differences are of order at most <span class="math inline">\(10^{-16}\)</span>, which is what
can be expected due to rounding errors. Thus we conclude that up to rounding
errors, <code>kernDens</code> and <code>kernDens_vec</code> return the same on this data set. This is,
of course, not a comprehensive test, but it is an example of one among a
number of tests that should be considered.</p>
<p>There are several ways to get completely rid of the explicit loops and write
an entirely vectorized implementation in R. One of the solutions will use the
<code>sapply</code> function, which belongs to the <a href="http://adv-r.had.co.nz/Functionals.html#functionals-loop">family of <code>*apply</code> functions</a>
that apply a function to each element in a vector or a list. In the
parlance of functional programming the <code>*apply</code> functions are variations
of the functional, or higher-order-function, known as <a href="https://en.wikipedia.org/wiki/Map_(higher-order_function)">map</a>.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="2-2-kernel-density.html#cb38-1"></a>kernDens_apply &lt;-<span class="st"> </span><span class="cf">function</span> (x, h, <span class="dt">m =</span> <span class="dv">512</span>) {</span>
<span id="cb38-2"><a href="2-2-kernel-density.html#cb38-2"></a>  rg &lt;-<span class="st"> </span><span class="kw">range</span>(x)</span>
<span id="cb38-3"><a href="2-2-kernel-density.html#cb38-3"></a>  xx &lt;-<span class="st"> </span><span class="kw">seq</span>(rg[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>h, rg[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>h, <span class="dt">length.out =</span> m)</span>
<span id="cb38-4"><a href="2-2-kernel-density.html#cb38-4"></a>  const &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi) <span class="op">*</span><span class="st"> </span>h <span class="op">*</span><span class="st"> </span><span class="kw">length</span>(x)</span>
<span id="cb38-5"><a href="2-2-kernel-density.html#cb38-5"></a>  y &lt;-<span class="st"> </span><span class="kw">sapply</span>(xx, <span class="cf">function</span>(z) <span class="kw">sum</span>(<span class="kw">exp</span>(<span class="op">-</span>(z <span class="op">-</span><span class="st"> </span>x)<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>h<span class="op">^</span><span class="dv">2</span>))) <span class="op">/</span><span class="st"> </span>const)</span>
<span id="cb38-6"><a href="2-2-kernel-density.html#cb38-6"></a>  <span class="kw">list</span>(<span class="dt">x =</span> xx, <span class="dt">y =</span> y)</span>
<span id="cb38-7"><a href="2-2-kernel-density.html#cb38-7"></a>}</span></code></pre></div>
<p>The <code>sapply</code> call above will apply the function <code>function(z) sum(dnorm(...</code>
to every element in the vector <code>xx</code> and return the result as a vector. The
function is an example of an <em>anonymous function</em> that doesn’t get a name and
exists only during the <code>sapply</code> evaluation. Instead of <code>sapply</code> it is possible
to use <code>lapply</code> that returns a list. In fact, <code>sapply</code> is a
simple wrapper around <code>lapply</code> that attempts to “simplify” the result from
a list to an array (and in this case to a vector).</p>
<p>An alternative, and also completely vectorized, solution can be based on
the functions <code>outer</code> and <code>rowMeans</code>.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="2-2-kernel-density.html#cb39-1"></a>kernDens_outer &lt;-<span class="st"> </span><span class="cf">function</span> (x, h, <span class="dt">m =</span> <span class="dv">512</span>) {</span>
<span id="cb39-2"><a href="2-2-kernel-density.html#cb39-2"></a>  rg &lt;-<span class="st"> </span><span class="kw">range</span>(x)</span>
<span id="cb39-3"><a href="2-2-kernel-density.html#cb39-3"></a>  xx &lt;-<span class="st"> </span><span class="kw">seq</span>(rg[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>h, rg[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>h, <span class="dt">length.out =</span> m)</span>
<span id="cb39-4"><a href="2-2-kernel-density.html#cb39-4"></a>  y &lt;-<span class="st"> </span><span class="kw">outer</span>(xx, x, <span class="cf">function</span>(zz, z) <span class="kw">exp</span>(<span class="op">-</span>(zz <span class="op">-</span><span class="st"> </span>z)<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>h<span class="op">^</span><span class="dv">2</span>)))</span>
<span id="cb39-5"><a href="2-2-kernel-density.html#cb39-5"></a>  y &lt;-<span class="st"> </span><span class="kw">rowMeans</span>(y) <span class="op">/</span><span class="st"> </span>(<span class="kw">sqrt</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi) <span class="op">*</span><span class="st"> </span>h)</span>
<span id="cb39-6"><a href="2-2-kernel-density.html#cb39-6"></a>  <span class="kw">list</span>(<span class="dt">x =</span> xx, <span class="dt">y =</span> y)</span>
<span id="cb39-7"><a href="2-2-kernel-density.html#cb39-7"></a>}</span></code></pre></div>
<p>The <code>outer</code> function evaluates the kernel in all combinations
of the grid and data points and returns a matrix of
dimensions <span class="math inline">\(m \times n\)</span>. The function <code>rowMeans</code> computes
the means of each row and returns a vector of length <span class="math inline">\(m\)</span>.</p>
<p>We should, of course, also remember to test these two last implementations.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="2-2-kernel-density.html#cb40-1"></a><span class="kw">range</span>(<span class="kw">kernDens</span>(psi, <span class="fl">0.2</span>)<span class="op">$</span>y <span class="op">-</span><span class="st"> </span><span class="kw">kernDens_apply</span>(psi, <span class="fl">0.2</span>)<span class="op">$</span>y)</span></code></pre></div>
<pre><code>## [1] -5.551115e-16  3.885781e-16</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="2-2-kernel-density.html#cb42-1"></a><span class="kw">range</span>(<span class="kw">kernDens</span>(psi, <span class="fl">0.2</span>)<span class="op">$</span>y <span class="op">-</span><span class="st"> </span><span class="kw">kernDens_outer</span>(psi, <span class="fl">0.2</span>)<span class="op">$</span>y)</span></code></pre></div>
<pre><code>## [1] -4.996004e-16  3.885781e-16</code></pre>
<p>The natural question is then how to choose between the different implementations?
Besides being correct it is important that the code is easy to read
and understand. Which of the four implementations above that is best in
this respect may depend a lot on the background of the reader. If you strip
the implementations for comments, all four are arguably quite readable, but
<code>kernDens</code> with the double loop might appeal a bit more to people
used to imperative programming, while <code>kernDens_apply</code> might
appeal more to people with a preference for functional programming.
This functional and vectorized solution is also
a bit closer to the mathematical notation with
e.g. the sum sign <span class="math inline">\(\Sigma\)</span> being mapped directly to the <code>sum</code> function
instead of the incremental addition in the for-loop. For
these specific implementations these differences are mostly
aesthetic nuances and preferences may be more subjective than substantial.</p>
<p>To make a qualified choice between the implementations we should
investigate if they differ in terms of run time and memory consumption, and
this is precisely the topic of the next section.</p>
</div>
<div id="benchmarking" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Benchmarking</h3>
<p>Benchmarking is about measuring and comparing performance. For software
this often means measuring run time and memory usage, though there are clearly
many other aspects of software that should be benchmarked in general. This
includes user experience, energy consumption and implementation and maintenance
time. In this section we focus on benchmarking run time.</p>
<p>The function <code>system.time</code> in R provides a simple way of benchmarking run time
measured in seconds.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="2-2-kernel-density.html#cb44-1"></a><span class="kw">system.time</span>(<span class="kw">kernDens</span>(psi, <span class="fl">0.2</span>))</span>
<span id="cb44-2"><a href="2-2-kernel-density.html#cb44-2"></a><span class="kw">system.time</span>(<span class="kw">kernDens_vec</span>(psi, <span class="fl">0.2</span>))</span>
<span id="cb44-3"><a href="2-2-kernel-density.html#cb44-3"></a><span class="kw">system.time</span>(<span class="kw">kernDens_apply</span>(psi, <span class="fl">0.2</span>))</span>
<span id="cb44-4"><a href="2-2-kernel-density.html#cb44-4"></a><span class="kw">system.time</span>(<span class="kw">kernDens_outer</span>(psi, <span class="fl">0.2</span>))</span></code></pre></div>
<pre><code>## kernDens:
##    user  system elapsed 
##   0.283   0.391   0.614 
## kernDens_vec:
##    user  system elapsed 
##   0.004   0.000   0.005 
## kernDens_apply:
##    user  system elapsed 
##   0.004   0.001   0.005 
## kernDens_outer:
##    user  system elapsed 
##   0.006   0.001   0.006</code></pre>
<p>The “elapsed” time is the total run time as experienced, while the “user” and
“system” times are how long the CPU spent on executing your code and
operating system code on behalf of your code, respectively.</p>
<p>From this simple benchmark, <code>kernDens</code> is clearly substantially slower
than the three other implementations. For more systematic benchmarking of run
time, the R package microbenchmark is useful.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="2-2-kernel-density.html#cb46-1"></a><span class="kw">library</span>(microbenchmark)</span></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="2-2-kernel-density.html#cb47-1"></a>kern_bench &lt;-<span class="st"> </span><span class="kw">microbenchmark</span>(</span>
<span id="cb47-2"><a href="2-2-kernel-density.html#cb47-2"></a>  <span class="kw">kernDens</span>(psi, <span class="fl">0.2</span>),</span>
<span id="cb47-3"><a href="2-2-kernel-density.html#cb47-3"></a>  <span class="kw">kernDens_vec</span>(psi, <span class="fl">0.2</span>),</span>
<span id="cb47-4"><a href="2-2-kernel-density.html#cb47-4"></a>  <span class="kw">kernDens_apply</span>(psi, <span class="fl">0.2</span>),</span>
<span id="cb47-5"><a href="2-2-kernel-density.html#cb47-5"></a>  <span class="kw">kernDens_outer</span>(psi, <span class="fl">0.2</span>)</span>
<span id="cb47-6"><a href="2-2-kernel-density.html#cb47-6"></a>)</span></code></pre></div>
<p>The result stored in <code>kern_bench</code> is a data frame with two columns. The first
contains the R expressions evaluated, and the second is the evaluation
time measured in nanoseconds. Each of the four expressions were evaluated 100 times,
and the data frame thus has 400 rows. The <code>times</code> argument to <code>microbenchmark</code>
can be used to change the number of evaluations per expression if needed.</p>
<p>It may not be immediately obvious that <code>kern_bench</code>
is a data frame, because printing will automatically summarize the
data, but the actual data structure is revealed by the R function <code>str</code>.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="2-2-kernel-density.html#cb48-1"></a><span class="kw">str</span>(kern_bench)</span></code></pre></div>
<pre><code>## Classes &#39;microbenchmark&#39; and &#39;data.frame&#39;:	400 obs. of  2 variables:
##  $ expr: Factor w/ 4 levels &quot;kernDens(psi, 0.2)&quot;,..: 3 3 3 4 3 2 1 1 2 2 ...
##  $ time: num  4475928 4120693 4104450 4111522 4106339 ...</code></pre>
<p>A total of 400 evaluations were done for the above benchmark, and <code>microbenchmark</code><br />
does the evaluations in a random order by default. Measuring evaluation time on a
complex system like a modern computer is an empirical science, and
the order of evaluation can potentially affect the results as the
conditions for the evaluation change over time. The purpose of the
randomization is to avoid that the ordering causes systematically
misleading results.</p>
<p>The microbenchmark package implements some methods for summarizing and
printing the results such as the following summary table with times in
milliseconds.</p>
<pre><code>## Unit: milliseconds
##                      expr   min    lq  mean median    uq   max neval
##        kernDens(psi, 0.2) 28.36 29.14 32.83  29.64 30.44 236.1   100
##    kernDens_vec(psi, 0.2)  2.44  2.57  3.05   2.68  3.02  12.5   100
##  kernDens_apply(psi, 0.2)  2.81  2.96  3.45   3.10  3.43  10.5   100
##  kernDens_outer(psi, 0.2)  3.06  3.28  5.82   3.49  5.00 113.7   100</code></pre>
<p>The summary table shows some key statistics like median and mean evaluation
times but also extremes and upper and lower quartiles. The distributions
of run times can be investigated further using the <code>autoplot</code> function, which is
based on ggplot2 and thus easy to modify.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="2-2-kernel-density.html#cb51-1"></a><span class="kw">autoplot</span>(kern_bench) <span class="op">+</span><span class="st"> </span></span>
<span id="cb51-2"><a href="2-2-kernel-density.html#cb51-2"></a><span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">position =</span> <span class="kw">position_jitter</span>(<span class="fl">0.2</span>, <span class="dv">0</span>), </span>
<span id="cb51-3"><a href="2-2-kernel-density.html#cb51-3"></a>              <span class="kw">aes</span>(<span class="dt">color =</span> expr), <span class="dt">alpha =</span> <span class="fl">0.4</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb51-4"><a href="2-2-kernel-density.html#cb51-4"></a><span class="st">  </span><span class="kw">aes</span>(<span class="dt">fill =</span> <span class="kw">I</span>(<span class="st">&quot;gray&quot;</span>)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb51-5"><a href="2-2-kernel-density.html#cb51-5"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span></code></pre></div>
<p><img src="CSwR_files/figure-html/kern-bench-autoplot-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>This more refined benchmark study doesn’t change our initial impression from
using <code>system.time</code> substantially. The function <code>kernDens</code> is notably
slower than the three vectorized implementations, but we are now able to
more clearly see the minor differences among them. For instance, <code>kernDens_vec</code>
and <code>kernDens_apply</code> have very similar run time distributions, while
<code>kernDens_outer</code> clearly has a larger median run time and also a
run time distribution that is more spread out to the right.</p>
<p>In many cases when we benchmark run time it is of interest
to investigate how run time depends on various parameters. This is so
for kernel density estimation, where we want to understand how changes in
the number of data points, <span class="math inline">\(n\)</span>, and the number of grid points, <span class="math inline">\(m\)</span>, affect
run time. We can still use <code>microbenchmark</code> for running the benchmark
experiment, but we will typically process and plot the benchmark data afterwards
in a customized way.</p>
<div class="figure" style="text-align: center"><span id="fig:kern-bench-fig"></span>
<img src="CSwR_files/figure-html/kern-bench-fig-1.png" alt="Median run times for the four different implementations of kernel density estimation. The dashed gray line is a reference line with slope 1." width="100%" />
<p class="caption">
Figure 2.6: Median run times for the four different implementations of kernel density estimation. The dashed gray line is a reference line with slope 1.
</p>
</div>
<p>Figure <a href="2-2-kernel-density.html#fig:kern-bench-fig">2.6</a> shows median run times for an experiment with
28 combinations of parameters for each of the four different implementations
yielding a total of 112 different R expressions being benchmarked. The number of replications
for each expression was set to 40. The results confirm that <code>kernDens</code>
is substantially slower than the vectorized implementations for all combinations
of <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span>. However, Figure <a href="2-2-kernel-density.html#fig:kern-bench-fig">2.6</a> also reveals a new
pattern; <code>kernDens_outer</code> appears to scale with <span class="math inline">\(n\)</span> in a slightly different way
than the two other vectorized implementations for small <span class="math inline">\(n\)</span>. It is comparable to
or even a bit faster than
<code>kernDens_vec</code> and <code>kernDens_apply</code> for very small data sets, while it becomes
slower for the larger data sets.</p>
<p>Run time for many algorithms have to a good approximation a dominating
power law behavior as a function of typical size parameters, that is, the
run time will scale approximately like <span class="math inline">\(n \mapsto C n^a\)</span> for constants <span class="math inline">\(C\)</span> and <span class="math inline">\(a\)</span>
and with <span class="math inline">\(n\)</span> denoting a generic size parameter. Therefore it is
beneficial to plot run time using log-log scales and to design benchmark studies
with size parameters being equidistant on a log-scale. With approximate power
law scaling, the log run time behaves like
<span class="math display">\[\log(C) + a \log(n),\]</span>
that is, on a log-log scale we see approximate straight lines. The slope
reveals the exponent <span class="math inline">\(a\)</span>, and two different algorithms for solving the
same problem might have different exponents and thus different slopes
on the log-log-scale. Two different implementations of the same
algorithm should have approximately the same slope
but may differ in the constant <span class="math inline">\(C\)</span> depending upon how efficient the
particular implementation is in the particular programming language used.
Differences in <span class="math inline">\(C\)</span> correspond to vertical translations on the log-log scale.</p>
<p>In practice, we will see some deviations from straight lines on the log-log plot
for a number of reasons. Writing the run time as <span class="math inline">\(C n^a + R(n)\)</span>,
the residual term <span class="math inline">\(R(n)\)</span> will often be noticeable or even dominating and positive
for small <span class="math inline">\(n\)</span>. It is only for large enough <span class="math inline">\(n\)</span>
that the power law term, <span class="math inline">\(C n^a\)</span>, will dominate. In addition, run time
can be affected by hardware constraints such as cache and memory sizes, which
can cause abrupt jumps in run time.</p>
<p>Using <code>microbenchmark</code> over <code>system.time</code> has two main benefits. First,
it handles the replication and randomization automatically, which is
convenient. Second, it attempts to provide more accurate timings. The latter
is mostly important when we benchmark very fast computations.</p>
<p>It can be <a href="https://radfordneal.wordpress.com/2014/02/02/inaccurate-results-from-microbenchmark/">debated if a median summary of randomly ordered evaluations</a>
is the best way to summarize run time. This is due to the way R does memory
management. R allocates and deallocates memory automatically
and uses <a href="https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)">garbage collection</a>
for the deallocation. This means that computations occasionally, and in a somewhat
unpredictable manner, trigger the garbage collector, and as a result a small
fraction of the evaluations may take substantially longer time than the
rest. The median will typically be almost unaffected, and memory deallocation
is thus effectively (and wrongly) disregarded from run time when the median
summary is used. This is an argument for using the mean instead of the median,
but due to the randomization the computation that triggered
the garbage collector might not be the one that caused the memory allocation
in the first place. Using the mean instead of the median will therefore smear
out the garbage collection run time on all benchmarked expressions. Setting
the argument <code>control = list(order = "block")</code> for <code>microbenchmark</code> will
evaluate the expressions in blocks, which in combination with a mean
summary more correctly accounts for memory allocation and deallocation in the
run time. The downside is that without the randomization the results might
suffer from other artefacts. This book will use randomization
and median summaries throughout, but we keep in mind that this
could underestimate actual average run time depending upon how much
memory a given computation requires. Memory usage and how it affects
run time by triggering garbage collection will be dealt with via
code profiling tools instead.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-1-unidens.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2-3-bandwidth.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

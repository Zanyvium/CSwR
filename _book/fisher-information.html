<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8.3 Fisher information | Computational Statistics with R</title>
  <meta name="description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="8.3 Fisher information | Computational Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8.3 Fisher information | Computational Statistics with R" />
  
  <meta name="twitter:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

<meta name="author" content="Niels Richard Hansen" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="EM-exp.html"/>
<link rel="next" href="revisiting-gaussian-mixtures.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro-smooth.html"><a href="intro-smooth.html"><i class="fa fa-check"></i><b>1.1</b> Smoothing</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro-smooth.html"><a href="intro-smooth.html#intro-angles"><i class="fa fa-check"></i><b>1.1.1</b> Angle distributions in proteins</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-smooth.html"><a href="intro-smooth.html#using-ggplot2"><i class="fa fa-check"></i><b>1.1.2</b> Using ggplot2</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-smooth.html"><a href="intro-smooth.html#changing-the-defaults"><i class="fa fa-check"></i><b>1.1.3</b> Changing the defaults</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-smooth.html"><a href="intro-smooth.html#multivariate-smoothing"><i class="fa fa-check"></i><b>1.1.4</b> Multivariate methods</a></li>
<li class="chapter" data-level="1.1.5" data-path="intro-smooth.html"><a href="intro-smooth.html#large-scale-smoothing"><i class="fa fa-check"></i><b>1.1.5</b> Large scale smoothing</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo Methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#vM"><i class="fa fa-check"></i><b>1.2.1</b> Univariate von Mises distributions</a></li>
<li class="chapter" data-level="1.2.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#mixtures-of-von-mises-distributions"><i class="fa fa-check"></i><b>1.2.2</b> Mixtures of von Mises distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="optimization.html"><a href="optimization.html"><i class="fa fa-check"></i><b>1.3</b> Optimization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="optimization.html"><a href="optimization.html#the-em-algorithm"><i class="fa fa-check"></i><b>1.3.1</b> The EM-algorithm</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.4</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#r-training-exercises"><i class="fa fa-check"></i>R training exercises</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#histograms-with-non-equidistant-breaks"><i class="fa fa-check"></i>Histograms with non-equidistant breaks</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#functions-and-functional-programming"><i class="fa fa-check"></i>Functions and functional programming</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Smoothing</b></span></li>
<li class="chapter" data-level="2" data-path="density.html"><a href="density.html"><i class="fa fa-check"></i><b>2</b> Density estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="unidens.html"><a href="unidens.html"><i class="fa fa-check"></i><b>2.1</b> Univariate density estimation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="unidens.html"><a href="unidens.html#likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Likelihood considerations</a></li>
<li class="chapter" data-level="2.1.2" data-path="unidens.html"><a href="unidens.html#sieves"><i class="fa fa-check"></i><b>2.1.2</b> Method of sieves</a></li>
<li class="chapter" data-level="2.1.3" data-path="unidens.html"><a href="unidens.html#basis-density"><i class="fa fa-check"></i><b>2.1.3</b> Basis expansions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="kernel-density.html"><a href="kernel-density.html"><i class="fa fa-check"></i><b>2.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="kernel-density.html"><a href="kernel-density.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="kernel-density.html"><a href="kernel-density.html#benchmarking"><i class="fa fa-check"></i><b>2.2.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bandwidth.html"><a href="bandwidth.html"><i class="fa fa-check"></i><b>2.3</b> Bandwidth selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bandwidth.html"><a href="bandwidth.html#rectangular"><i class="fa fa-check"></i><b>2.3.1</b> Revisiting the rectangular kernel</a></li>
<li class="chapter" data-level="2.3.2" data-path="bandwidth.html"><a href="bandwidth.html#ise-mise-and-mse-for-kernel-estimators"><i class="fa fa-check"></i><b>2.3.2</b> ISE, MISE and MSE for kernel estimators</a></li>
<li class="chapter" data-level="2.3.3" data-path="bandwidth.html"><a href="bandwidth.html#plug-in-estimation-of-the-oracle-bandwidth"><i class="fa fa-check"></i><b>2.3.3</b> Plug-in estimation of the oracle bandwidth</a></li>
<li class="chapter" data-level="2.3.4" data-path="bandwidth.html"><a href="bandwidth.html#cross-validation"><i class="fa fa-check"></i><b>2.3.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#kernel-density-estimation"><i class="fa fa-check"></i>Kernel density estimation</a></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#benchmarking-1"><i class="fa fa-check"></i>Benchmarking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i><b>3</b> Bivariate smoothing</a><ul>
<li class="chapter" data-level="3.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html"><i class="fa fa-check"></i><b>3.1</b> Nearest neighbor smoothers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#linear-smoothers"><i class="fa fa-check"></i><b>3.1.1</b> Linear smoothers</a></li>
<li class="chapter" data-level="3.1.2" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#implementing-the-running-mean"><i class="fa fa-check"></i><b>3.1.2</b> Implementing the running mean</a></li>
<li class="chapter" data-level="3.1.3" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#choose-k-by-cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Choose <span class="math inline">\(k\)</span> by cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="kernel-methods.html"><a href="kernel-methods.html"><i class="fa fa-check"></i><b>3.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="kernel-methods.html"><a href="kernel-methods.html#nadarayawatson-kernel-smoothing"><i class="fa fa-check"></i><b>3.2.1</b> Nadaraya–Watson kernel smoothing</a></li>
<li class="chapter" data-level="3.2.2" data-path="kernel-methods.html"><a href="kernel-methods.html#local-regression-smoothers"><i class="fa fa-check"></i><b>3.2.2</b> Local regression smoothers</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sparse-linear-algebra.html"><a href="sparse-linear-algebra.html"><i class="fa fa-check"></i><b>3.3</b> Sparse linear algebra</a></li>
<li class="chapter" data-level="3.4" data-path="onb.html"><a href="onb.html"><i class="fa fa-check"></i><b>3.4</b> Orthogonal basis expansions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="onb.html"><a href="onb.html#polynomial-expansions"><i class="fa fa-check"></i><b>3.4.1</b> Polynomial expansions</a></li>
<li class="chapter" data-level="3.4.2" data-path="onb.html"><a href="onb.html#fourier-expansions"><i class="fa fa-check"></i><b>3.4.2</b> Fourier expansions</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>3.5</b> Splines</a><ul>
<li class="chapter" data-level="3.5.1" data-path="splines.html"><a href="splines.html#smoothing-splines"><i class="fa fa-check"></i><b>3.5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="3.5.2" data-path="splines.html"><a href="splines.html#splines-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Splines in R</a></li>
<li class="chapter" data-level="3.5.3" data-path="splines.html"><a href="splines.html#efficient-splines"><i class="fa fa-check"></i><b>3.5.3</b> Efficient computation with splines</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="gaussian-processes.html"><a href="gaussian-processes.html"><i class="fa fa-check"></i><b>3.6</b> Gaussian processes</a></li>
<li class="chapter" data-level="3.7" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html"><i class="fa fa-check"></i><b>3.7</b> The Kalman filter</a><ul>
<li class="chapter" data-level="3.7.1" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#ar1-example"><i class="fa fa-check"></i><b>3.7.1</b> AR(1)-example</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-smoother"><i class="fa fa-check"></i><b>3.7.2</b> The Kalman smoother</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#implementation-1"><i class="fa fa-check"></i><b>3.7.3</b> Implementation</a></li>
<li class="chapter" data-level="3.7.4" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-filter-1"><i class="fa fa-check"></i><b>3.7.4</b> The Kalman filter</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#nearest-neighbors"><i class="fa fa-check"></i>Nearest neighbors</a></li>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#kernel-estimators"><i class="fa fa-check"></i>Kernel estimators</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Monte Carlo Methods</b></span></li>
<li class="chapter" data-level="4" data-path="univariate-random-variables.html"><a href="univariate-random-variables.html"><i class="fa fa-check"></i><b>4</b> Univariate random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="pseudo-random-numbers.html"><a href="pseudo-random-numbers.html"><i class="fa fa-check"></i><b>4.1</b> Pseudo random numbers</a></li>
<li class="chapter" data-level="4.2" data-path="transformation-techniques.html"><a href="transformation-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Transformation techniques</a><ul>
<li class="chapter" data-level="4.2.1" data-path="transformation-techniques.html"><a href="transformation-techniques.html#sampling-from-a-t-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Sampling from a <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="reject-samp.html"><a href="reject-samp.html"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a><ul>
<li class="chapter" data-level="4.3.1" data-path="reject-samp.html"><a href="reject-samp.html#vMsim"><i class="fa fa-check"></i><b>4.3.1</b> von Mises distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="reject-samp.html"><a href="reject-samp.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="adaptive.html"><a href="adaptive.html"><i class="fa fa-check"></i><b>4.4</b> Adaptive envelopes</a><ul>
<li class="chapter" data-level="4.4.1" data-path="adaptive.html"><a href="adaptive.html#beta-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Beta distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="adaptive.html"><a href="adaptive.html#von-mises-distribution"><i class="fa fa-check"></i><b>4.4.2</b> von Mises distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-3.html"><a href="exercises-3.html#rejection-sampling-of-gaussian-random-variables"><i class="fa fa-check"></i>Rejection sampling of Gaussian random variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mci.html"><a href="mci.html"><i class="fa fa-check"></i><b>5</b> Monte Carlo integration</a><ul>
<li class="chapter" data-level="5.1" data-path="assessment.html"><a href="assessment.html"><i class="fa fa-check"></i><b>5.1</b> Assessment</a><ul>
<li class="chapter" data-level="5.1.1" data-path="assessment.html"><a href="assessment.html#using-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.1.1</b> Using the central limit theorem</a></li>
<li class="chapter" data-level="5.1.2" data-path="assessment.html"><a href="assessment.html#concentration-inequalities"><i class="fa fa-check"></i><b>5.1.2</b> Concentration inequalities</a></li>
<li class="chapter" data-level="5.1.3" data-path="assessment.html"><a href="assessment.html#exponential-tail-bound-for-gamma-distributed-variables"><i class="fa fa-check"></i><b>5.1.3</b> Exponential tail bound for Gamma distributed variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>5.2</b> Importance sampling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importance-sampling.html"><a href="importance-sampling.html#unknown-normalization-constants"><i class="fa fa-check"></i><b>5.2.1</b> Unknown normalization constants</a></li>
<li class="chapter" data-level="5.2.2" data-path="importance-sampling.html"><a href="importance-sampling.html#hd-int"><i class="fa fa-check"></i><b>5.2.2</b> Computing a high-dimensional integral</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="network-failure.html"><a href="network-failure.html"><i class="fa fa-check"></i><b>5.3</b> Network failure</a><ul>
<li class="chapter" data-level="5.3.1" data-path="network-failure.html"><a href="network-failure.html#object-oriented-implementation"><i class="fa fa-check"></i><b>5.3.1</b> Object oriented implementation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part III: Optimization</b></span></li>
<li class="chapter" data-level="6" data-path="four-examples.html"><a href="four-examples.html"><i class="fa fa-check"></i><b>6</b> Four Examples</a><ul>
<li class="chapter" data-level="6.1" data-path="exp-fam.html"><a href="exp-fam.html"><i class="fa fa-check"></i><b>6.1</b> Exponential families</a><ul>
<li class="chapter" data-level="6.1.1" data-path="exp-fam.html"><a href="exp-fam.html#full-exponential-families"><i class="fa fa-check"></i><b>6.1.1</b> Full exponential families</a></li>
<li class="chapter" data-level="6.1.2" data-path="exp-fam.html"><a href="exp-fam.html#bayes-net"><i class="fa fa-check"></i><b>6.1.2</b> Exponential family Bayesian networks</a></li>
<li class="chapter" data-level="6.1.3" data-path="exp-fam.html"><a href="exp-fam.html#exp-fam-deriv"><i class="fa fa-check"></i><b>6.1.3</b> Likelihood computations</a></li>
<li class="chapter" data-level="6.1.4" data-path="exp-fam.html"><a href="exp-fam.html#curved-exponential-families"><i class="fa fa-check"></i><b>6.1.4</b> Curved exponential families</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multinomial-models.html"><a href="multinomial-models.html"><i class="fa fa-check"></i><b>6.2</b> Multinomial models</a><ul>
<li class="chapter" data-level="6.2.1" data-path="multinomial-models.html"><a href="multinomial-models.html#pep-moth"><i class="fa fa-check"></i><b>6.2.1</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>6.3</b> Regression models</a></li>
<li class="chapter" data-level="6.4" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html"><i class="fa fa-check"></i><b>6.4</b> Finite mixture models</a><ul>
<li class="chapter" data-level="6.4.1" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#Gaus-mix-ex"><i class="fa fa-check"></i><b>6.4.1</b> Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="mixed-models.html"><a href="mixed-models.html"><i class="fa fa-check"></i><b>6.5</b> Mixed models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="numopt.html"><a href="numopt.html"><i class="fa fa-check"></i><b>7</b> Numerical optimization</a><ul>
<li class="chapter" data-level="7.1" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html"><i class="fa fa-check"></i><b>7.1</b> Algorithms and convergence</a><ul>
<li class="chapter" data-level="7.1.1" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#descent-algorithms"><i class="fa fa-check"></i><b>7.1.1</b> Descent algorithms</a></li>
<li class="chapter" data-level="7.1.2" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#maps-and-fixed-points"><i class="fa fa-check"></i><b>7.1.2</b> Maps and fixed points</a></li>
<li class="chapter" data-level="7.1.3" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#convergence-rate"><i class="fa fa-check"></i><b>7.1.3</b> Convergence rate</a></li>
<li class="chapter" data-level="7.1.4" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#stopping-criteria"><i class="fa fa-check"></i><b>7.1.4</b> Stopping criteria</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html"><i class="fa fa-check"></i><b>7.2</b> Descent direction algorithms</a><ul>
<li class="chapter" data-level="7.2.1" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#line-search"><i class="fa fa-check"></i><b>7.2.1</b> Line search</a></li>
<li class="chapter" data-level="7.2.2" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>7.2.2</b> Gradient descent</a></li>
<li class="chapter" data-level="7.2.3" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#conjugate-gradients"><i class="fa fa-check"></i><b>7.2.3</b> Conjugate gradients</a></li>
<li class="chapter" data-level="7.2.4" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#pep-moth-descent"><i class="fa fa-check"></i><b>7.2.4</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html"><i class="fa fa-check"></i><b>7.3</b> Newton-type algorithms</a><ul>
<li class="chapter" data-level="7.3.1" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#poisson-regression"><i class="fa fa-check"></i><b>7.3.1</b> Poisson regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#quasi-newton-algorithms"><i class="fa fa-check"></i><b>7.3.2</b> Quasi-Newton algorithms</a></li>
<li class="chapter" data-level="7.3.3" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#sparsity"><i class="fa fa-check"></i><b>7.3.3</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="misc-.html"><a href="misc-.html"><i class="fa fa-check"></i><b>7.4</b> Misc.</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="em.html"><a href="em.html"><i class="fa fa-check"></i><b>8</b> Expectation maximization algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="basic-properties.html"><a href="basic-properties.html"><i class="fa fa-check"></i><b>8.1</b> Basic properties</a><ul>
<li class="chapter" data-level="8.1.1" data-path="basic-properties.html"><a href="basic-properties.html#incomplete-data-likelihood"><i class="fa fa-check"></i><b>8.1.1</b> Incomplete data likelihood</a></li>
<li class="chapter" data-level="8.1.2" data-path="basic-properties.html"><a href="basic-properties.html#monotonicity-of-the-em-algorithm"><i class="fa fa-check"></i><b>8.1.2</b> Monotonicity of the EM algorithm</a></li>
<li class="chapter" data-level="8.1.3" data-path="basic-properties.html"><a href="basic-properties.html#peppered-moths"><i class="fa fa-check"></i><b>8.1.3</b> Peppered moths</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="EM-exp.html"><a href="EM-exp.html"><i class="fa fa-check"></i><b>8.2</b> Exponential families</a></li>
<li class="chapter" data-level="8.3" data-path="fisher-information.html"><a href="fisher-information.html"><i class="fa fa-check"></i><b>8.3</b> Fisher information</a></li>
<li class="chapter" data-level="8.4" data-path="revisiting-gaussian-mixtures.html"><a href="revisiting-gaussian-mixtures.html"><i class="fa fa-check"></i><b>8.4</b> Revisiting Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="fisher-information" class="section level2">
<h2><span class="header-section-number">8.3</span> Fisher information</h2>
<p>For statistics relying on classical asymptotic theory
we need an estimate of the Fisher information, e.g. the observed Fisher information
(Hessian of the negative log-likelihood for the observed data). For numerical
optimization of <span class="math inline">\(Q\)</span> or variants
of the EM algorithm (like EM gradient or acceleration methods) the gradient and Hessian of <span class="math inline">\(Q\)</span>
can be useful. However, these do not directly inform us on the Fisher information.
In this section we show some interesting and useful relations between
the derivatives of the log-likelihood for the observed data and derivatives of
<span class="math inline">\(Q\)</span> with the primary purpose of estimating the Fisher information.</p>
<p>First we look at the peppered moth example, where we note that with <span class="math inline">\(p = p(\theta)\)</span>
being some parametrization of the cell probabilities,
<span class="math display">\[Q(\theta \mid \theta&#39;) = \sum_{k=1}^K \frac{x_{j(k)} p_k(\theta&#39;)}{M(p(\theta&#39;))_{j(k)}} \log p_k(\theta),\]</span>
where <span class="math inline">\(j(k)\)</span> is defined by <span class="math inline">\(k \in A_{j(k)}\)</span>. The gradient of <span class="math inline">\(Q\)</span> w.r.t.
<span class="math inline">\(\theta\)</span> is therefore</p>
<p><span class="math display">\[\nabla_{\theta} Q(\theta \mid \theta&#39;) = 
\sum_{k = 1}^K \frac{x_{j(k)} p_k(\theta&#39;)}{M(p(\theta&#39;))_{j(k)} p_k(\theta)} \nabla_{\theta} p_k(\theta&#39;).\]</span></p>
<p>We recognize from previous computations in Section <a href="descent-direction-algorithms.html#pep-moth-descent">7.2.4</a>
that when we evaluate <span class="math inline">\(\nabla_{\theta} Q(\theta \mid \theta&#39;)\)</span> in <span class="math inline">\(\theta = \theta&#39;\)</span>
we get</p>
<p><span class="math display">\[\nabla_{\theta} Q(\theta&#39; \mid \theta&#39;) = \sum_{i = 1}^K \frac{x_{j(i)} }{M(p(\theta&#39;))_{j(i)}} \nabla_{\theta} p_i(\theta&#39;) = \nabla_{\theta} \ell(\theta&#39;),\]</span></p>
<p>thus the gradient of <span class="math inline">\(\ell\)</span> in <span class="math inline">\(\theta&#39;\)</span> is actually
identical to the gradient of <span class="math inline">\(Q(\cdot \mid \theta&#39;)\)</span> in <span class="math inline">\(\theta&#39;\)</span>. This
is not a coincidence, and it holds generally that
<span class="math display">\[\nabla_{\theta} Q(\theta&#39; \mid \theta&#39;) = \nabla_{\theta} \ell(\theta&#39;).\]</span>
This follows from the fact we derived in the proof of Theorem <a href="basic-properties.html#thm:EM-inequality">8.1</a>
that <span class="math inline">\(\theta&#39;\)</span> minimizes</p>
<p><span class="math display">\[\theta \mapsto \ell(\theta) - Q(\theta \mid \theta&#39;).\]</span></p>
<p>Another way to phrase this is that the minorant of <span class="math inline">\(\ell(\theta)\)</span> touches
<span class="math inline">\(\ell\)</span> tangentially in <span class="math inline">\(\theta&#39;\)</span>.</p>
<p>In the case where the observation <span class="math inline">\(\mathbf{y}\)</span> consists of <span class="math inline">\(n\)</span> i.i.d. observations
from the model with parameter <span class="math inline">\(\theta_0\)</span>, <span class="math inline">\(\ell\)</span> as well as <span class="math inline">\(Q(\cdot \mid \theta&#39;)\)</span> are sums of terms for which
the gradient identity above holds for each term. In particular,
<span class="math display">\[\nabla_{\theta} \ell(\theta_0) = \sum_{i=1}^n \nabla_{\theta} \ell_i(\theta_0) = \sum_{i=1}^n \nabla_{\theta} Q_i(\theta_0 \mid \theta_0),\]</span>
and using the second Bartlett identity</p>
<p><span class="math display">\[\mathcal{I}(\theta_0) = V_{\theta_0}(\nabla_{\theta} \ell(\theta_0))\]</span></p>
<p>we see that</p>
<p><span class="math display">\[\hat{\mathcal{I}}(\theta_0) =  \sum_{i=1}^n \big(\nabla_{\theta} Q_i(\theta_0 \mid \theta_0) - n^{-1} \nabla_{\theta} \ell(\theta_0)\big)\big(\nabla_{\theta} Q_i(\theta_0 \mid \theta_0) - n^{-1} \nabla_{\theta} \ell(\theta_0)\big)^T\]</span></p>
<p>is almost an unbiased estimator of the
Fisher information. It does have mean <span class="math inline">\(\mathcal{I}(\theta_0)\)</span>, but it is not an
estimator as <span class="math inline">\(\theta_0\)</span> is not known. Using a plug-in-estimator,
<span class="math inline">\(\hat{\theta}\)</span>, of <span class="math inline">\(\theta_0\)</span> we get a real estimator</p>
<p><span class="math display">\[\hat{\mathcal{I}} = \hat{\mathcal{I}}(\hat{\theta}) =  \sum_{i=1}^n \big(\nabla_{\theta} Q_i(\hat{\theta} \mid \hat{\theta}) - n^{-1} \nabla_{\theta} \ell(\hat{\theta})\big)\big(\nabla_{\theta} Q_i(\hat{\theta} \mid \hat{\theta}) - n^{-1} \nabla_{\theta} \ell(\hat{\theta})\big)^T,\]</span></p>
<p>though <span class="math inline">\(\hat{\mathcal{I}}\)</span> will no longer necessarily be unbiased.</p>
<p>We refer to <span class="math inline">\(\hat{\mathcal{I}}\)</span> as the <em>empirical Fisher information</em> given by
the estimator <span class="math inline">\(\hat{\theta}\)</span>. In most cases, <span class="math inline">\(\hat{\theta}\)</span> is the maximum-likelihood
estimator, in which case <span class="math inline">\(\nabla_{\theta} \ell(\hat{\theta}) = 0\)</span> and the empirical
Fisher information simplifies to
<span class="math display">\[\hat{\mathcal{I}} = \sum_{i=1}^n \nabla_{\theta} Q_i(\hat{\theta} \mid \hat{\theta}) \nabla_{\theta} Q_i(\hat{\theta} \mid \hat{\theta})^T.\]</span>
However, <span class="math inline">\(\nabla_{\theta} \ell(\hat{\theta})\)</span> is in practice only approximately
equal to zero, and it is unclear if it should be dropped.</p>
<p>For the peppered moths, where data is collected as i.i.d. samples of <span class="math inline">\(n\)</span>
individual specimens and tabulated according to phenotype, we implement
the empirical Fisher information with the optional possibility of centering
the gradients before computing the information estimate. We note that only
three different observations of phenotype are possible, giving rise to
three different possible terms in the sum. The implementation
works directly on the tabulated data by computing all the three possible
terms and then forming a weighted sum according to the number of times each
term is present.</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb318-1" data-line-number="1">empFisher &lt;-<span class="st"> </span><span class="cf">function</span>(par, x, grad, <span class="dt">center =</span> <span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb318-2" data-line-number="2">  grad_MLE &lt;-<span class="st"> </span><span class="dv">0</span> <span class="co">## is supposed to be 0 in the MLE</span></a>
<a class="sourceLine" id="cb318-3" data-line-number="3">  <span class="cf">if</span> (center) </a>
<a class="sourceLine" id="cb318-4" data-line-number="4">     grad_MLE &lt;-<span class="st">  </span><span class="kw">grad</span>(par, x) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(x)</a>
<a class="sourceLine" id="cb318-5" data-line-number="5">   grad1 &lt;-<span class="st"> </span><span class="kw">grad</span>(par, <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>)) <span class="op">-</span><span class="st"> </span>grad_MLE</a>
<a class="sourceLine" id="cb318-6" data-line-number="6">   grad2 &lt;-<span class="st"> </span><span class="kw">grad</span>(par, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="op">-</span><span class="st"> </span>grad_MLE</a>
<a class="sourceLine" id="cb318-7" data-line-number="7">   grad3 &lt;-<span class="st"> </span><span class="kw">grad</span>(par, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">-</span><span class="st"> </span>grad_MLE</a>
<a class="sourceLine" id="cb318-8" data-line-number="8">   x[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span><span class="kw">t</span>(grad1) <span class="op">%*%</span><span class="st"> </span>grad1 <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb318-9" data-line-number="9"><span class="st">     </span>x[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span><span class="kw">t</span>(grad2) <span class="op">%*%</span><span class="st"> </span>grad2 <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb318-10" data-line-number="10"><span class="st">     </span>x[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span><span class="kw">t</span>(grad3) <span class="op">%*%</span><span class="st"> </span>grad3 </a>
<a class="sourceLine" id="cb318-11" data-line-number="11">}</a></code></pre></div>
<p>We test the implementation with and without centering and compare
the result to a numerically computed hessian using <code>optimHess</code> (it is
possible to get <code>optim</code> to compute the Hessian numerically in the minimizer
as a final step, but <code>optimHess</code> does this computation separately).</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb319-1" data-line-number="1"><span class="co">## The gradient of Q (equivalently the log-likelihood) was </span></a>
<a class="sourceLine" id="cb319-2" data-line-number="2"><span class="co">## implemented earlier as &#39;grad_loglik&#39;.</span></a>
<a class="sourceLine" id="cb319-3" data-line-number="3">grad &lt;-<span class="st"> </span><span class="cf">function</span>(par, x) <span class="kw">grad_loglik</span>(par, x, prob, Dprob, <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb319-4" data-line-number="4"><span class="kw">empFisher</span>(phat, <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), grad)</a></code></pre></div>
<pre><code>##           [,1]     [,2]
## [1,] 18487.558 1384.626
## [2,]  1384.626 6816.612</code></pre>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb321-1" data-line-number="1"><span class="kw">empFisher</span>(phat, <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), grad, <span class="dt">center =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>##           [,1]     [,2]
## [1,] 18487.558 1384.626
## [2,]  1384.626 6816.612</code></pre>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb323-1" data-line-number="1"><span class="kw">optimHess</span>(phat, loglik, grad_loglik, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), </a>
<a class="sourceLine" id="cb323-2" data-line-number="2">          <span class="dt">prob =</span> prob, <span class="dt">Dprob =</span> Dprob, <span class="dt">group =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>))</a></code></pre></div>
<pre><code>##           [,1]     [,2]
## [1,] 18490.938 1384.629
## [2,]  1384.629 6816.769</code></pre>
<p>Note that the numerically computed Hessian (the <em>observed</em> Fisher information)
and the empirical Fisher information are different
estimates of the same quantity. Thus they are <em>not</em> supposed to be identical on
a given data set, but they are supposed to be estimates of the same thing
and thus to be similar.</p>
<p>An alternative to the empirical Fisher information or a direct computation of
the observed Fisher information is supplemented EM (SEM). This is a general method
for computing the observed Fisher
information that relies only on EM steps and a numerical differentiation scheme.
Define the EM map <span class="math inline">\(\Phi : \Theta \mapsto \Theta\)</span> by</p>
<p><span class="math display">\[\Phi(\theta&#39;) = \textrm{arg max}_{\theta} \ Q(\theta \mid \theta&#39;).\]</span></p>
<p>A global maximum of the likelihood is a fixed point of <span class="math inline">\(\Phi\)</span>, and the
EM algorithm searches for a fixed point for <span class="math inline">\(\Phi\)</span>, that is, a solution to</p>
<p><span class="math display">\[\Phi(\theta) = \theta.\]</span></p>
<p>Variations of the EM-algorithm can often be seen as other ways to
find a fixed point for <span class="math inline">\(\Phi\)</span>. From
<span class="math display">\[\ell(\theta) = Q(\theta \mid \theta&#39;) + H(\theta \mid \theta&#39;)\]</span>
it follows that the observed Fisher information equals</p>
<p><span class="math display">\[\hat{i}_X := - D^2_{\theta} \ell(\hat{\theta}) = 
\underbrace{-D^2_{\theta} Q(\hat{\theta} \mid \theta&#39;)}_{= \hat{i}_Y(\theta&#39;)} - D
\underbrace{^2_{\theta} H(\hat{\theta} \mid \theta&#39;)}_{= \hat{i}_{Y \mid X}(\theta&#39;)}.\]</span></p>
<p>It is possible to compute <span class="math inline">\(\hat{i}_Y := \hat{i}_Y(\hat{\theta})\)</span>.
For peppered moths (and exponential families)
it is as difficult as computing the Fisher information for complete observations.</p>
<p>We want to compute <span class="math inline">\(\hat{i}_X\)</span> but <span class="math inline">\(\hat{i}_{Y \mid X} := \hat{i}_{Y \mid X}(\hat{\theta})\)</span>
is not computable either. It can, however, be shown that</p>
<p><span class="math display">\[D_{\theta} \Phi(\hat{\theta})^T = \hat{i}_{Y\mid X} \left(\hat{i}_Y\right)^{-1}.\]</span></p>
<p>Hence
<span class="math display">\[\begin{align}
\hat{i}_X &amp; = \left(I - \hat{i}_{Y\mid X} \left(\hat{i}_Y\right)^{-1}\right) \hat{i}_Y \\
&amp; = \left(I - D_{\theta} \Phi(\hat{\theta})^T\right) \hat{i}_Y.
\end{align}\]</span></p>
<p>Though the EM map <span class="math inline">\(\Phi\)</span> might not have a simple analytic expression,
its Jacobian, <span class="math inline">\(D_{\theta} \Phi(\hat{\theta})\)</span>, can be computed via numerical
differentiation once we have implemented <span class="math inline">\(\Phi\)</span>. We also need the
hessian of the map <span class="math inline">\(Q\)</span>, which we implement as an R function as well.</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb325-1" data-line-number="1">Q &lt;-<span class="st"> </span><span class="cf">function</span>(p, pp, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), group) {</a>
<a class="sourceLine" id="cb325-2" data-line-number="2">  p[<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>p[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb325-3" data-line-number="3">  pp[<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>pp[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>pp[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb325-4" data-line-number="4">  <span class="op">-</span><span class="st"> </span>(x[group] <span class="op">*</span><span class="st"> </span><span class="kw">prob</span>(pp) <span class="op">/</span><span class="st"> </span><span class="kw">M</span>(<span class="kw">prob</span>(pp), group)[group]) <span class="op">%*%</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">prob</span>(p))</a>
<a class="sourceLine" id="cb325-5" data-line-number="5">}</a></code></pre></div>
<p>The R package numDeriv contains functions that compute numerical derivatives.</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb326-1" data-line-number="1"><span class="kw">library</span>(numDeriv)</a></code></pre></div>
<p>The Hessian of <span class="math inline">\(Q\)</span> can be computed using this package.</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb327-1" data-line-number="1">iY &lt;-<span class="st"> </span><span class="kw">hessian</span>(Q, phat, <span class="dt">pp =</span> phat, <span class="dt">group =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>))</a></code></pre></div>
<p>Supplemented EM can then be implemented by computing the Jacobian of
<span class="math inline">\(\Phi\)</span> using numDeriv as well.</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb328-1" data-line-number="1">Phi &lt;-<span class="st"> </span><span class="cf">function</span>(pp) <span class="kw">MStep</span>(<span class="kw">EStep</span>(pp, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>)))</a>
<a class="sourceLine" id="cb328-2" data-line-number="2">DPhi &lt;-<span class="st"> </span><span class="kw">jacobian</span>(Phi, phat)  <span class="co">## Using numDeriv function &#39;jacobian&#39;</span></a>
<a class="sourceLine" id="cb328-3" data-line-number="3">iX &lt;-<span class="st"> </span>(<span class="kw">diag</span>(<span class="dv">1</span>, <span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="kw">t</span>(DPhi)) <span class="op">%*%</span><span class="st"> </span>iY</a>
<a class="sourceLine" id="cb328-4" data-line-number="4">iX</a></code></pre></div>
<pre><code>##           [,1]     [,2]
## [1,] 18487.558 1384.626
## [2,]  1384.626 6816.612</code></pre>
<p>For statistics, we actually need the inverse Fisher information, which can
be computed by inverting <span class="math inline">\(\hat{i}_X\)</span>, but we also have the following
interesting identity</p>
<p><span class="math display">\[\begin{align}
\hat{i}_X^{-1} &amp; = \hat{i}_Y^{-1} \left(I - D_{\theta} \Phi(\hat{\theta})^T\right)^{-1} \\
 &amp; = \hat{i}_Y^{-1} \left(I + \sum_{n=1}^{\infty} \left(D_{\theta} \Phi(\hat{\theta})^T\right)^n \right) \\
 &amp; = \hat{i}_Y^{-1} + \hat{i}_Y^{-1} D_{\theta} \Phi(\hat{\theta})^T \left(I - D_{\theta} \Phi(\hat{\theta})^T\right)^{-1}
\end{align}\]</span></p>
<p>where the second identity follows by the
<a href="https://en.wikipedia.org/wiki/Neumann_series">Neumann series</a>.</p>
<p>The last formula above explicitly gives the asymptotic variance for the incomplete
observation <span class="math inline">\(X\)</span> as the asymptotic variance for the complete observation <span class="math inline">\(Y\)</span> plus
a correction term.</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb330-1" data-line-number="1">iYinv &lt;-<span class="st"> </span><span class="kw">solve</span>(iY)</a>
<a class="sourceLine" id="cb330-2" data-line-number="2">iYinv <span class="op">+</span><span class="st"> </span>iYinv <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(<span class="kw">solve</span>(<span class="kw">diag</span>(<span class="dv">1</span>, <span class="dv">2</span>) <span class="op">-</span><span class="st"> </span>DPhi, DPhi))</a></code></pre></div>
<pre><code>##               [,1]          [,2]
## [1,]  5.492602e-05 -1.115686e-05
## [2,] -1.115686e-05  1.489667e-04</code></pre>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb332-1" data-line-number="1"><span class="kw">solve</span>(iX) <span class="co">## SEM-based, but different use of inversion</span></a></code></pre></div>
<pre><code>##               [,1]          [,2]
## [1,]  5.492602e-05 -1.115686e-05
## [2,] -1.115686e-05  1.489667e-04</code></pre>
<p>The SEM implementation above relies on the <code>hessian</code> and <code>jacobian</code> functions from the
numDeriv package for numerical differentiation.</p>
<p>It is possible to implement the computation of the hessian of <span class="math inline">\(Q\)</span> analytically
for the peppered moths, but to illustrate functionality of the numDeriv package
we implemented the computation numerically above.</p>
<p>Variants on the strategy for computing <span class="math inline">\(D_{\theta} \Phi(\hat{\theta})\)</span> via
numerical differentiation have been suggested in the literature, specifically
using difference quotient approximations along the
sequence of EM steps. This is not going to work as well as standard numerical
differentiation since this method ignores numerical errors, and when the algorithm
gets sufficiently close to the MLE, the numerical errors will dominate in
the difference quotients.</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="EM-exp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="revisiting-gaussian-mixtures.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Statistics with R</title>
  <meta name="description" content="Lecture notes providing an introduction to computational statistics using the R programming language.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Statistics with R" />
  
  <meta name="twitter:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

<meta name="author" content="Niels Richard Hansen">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="em.html">
<link rel="next" href="EM-exp.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro-smooth.html"><a href="intro-smooth.html"><i class="fa fa-check"></i><b>1.1</b> Smoothing</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro-smooth.html"><a href="intro-smooth.html#intro-angles"><i class="fa fa-check"></i><b>1.1.1</b> Angle distributions in proteins</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-smooth.html"><a href="intro-smooth.html#using-ggplot2"><i class="fa fa-check"></i><b>1.1.2</b> Using ggplot2</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-smooth.html"><a href="intro-smooth.html#changing-the-defaults"><i class="fa fa-check"></i><b>1.1.3</b> Changing the defaults</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-smooth.html"><a href="intro-smooth.html#multivariate-smoothing"><i class="fa fa-check"></i><b>1.1.4</b> Multivariate methods</a></li>
<li class="chapter" data-level="1.1.5" data-path="intro-smooth.html"><a href="intro-smooth.html#large-scale-smoothing"><i class="fa fa-check"></i><b>1.1.5</b> Large scale smoothing</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo Methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#vM"><i class="fa fa-check"></i><b>1.2.1</b> Univariate von Mises distributions</a></li>
<li class="chapter" data-level="1.2.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#mixtures-of-von-mises-distributions"><i class="fa fa-check"></i><b>1.2.2</b> Mixtures of von Mises distributions</a></li>
<li class="chapter" data-level="1.2.3" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#large-scale-simulation"><i class="fa fa-check"></i><b>1.2.3</b> Large scale simulation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="optimization.html"><a href="optimization.html"><i class="fa fa-check"></i><b>1.3</b> Optimization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="optimization.html"><a href="optimization.html#the-em-algorithm"><i class="fa fa-check"></i><b>1.3.1</b> The EM-algorithm</a></li>
<li class="chapter" data-level="1.3.2" data-path="optimization.html"><a href="optimization.html#large-scale-optimization"><i class="fa fa-check"></i><b>1.3.2</b> Large scale optimization</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.4</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#r-training-exercises"><i class="fa fa-check"></i>R training exercises</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#histograms-with-non-equidistant-breaks"><i class="fa fa-check"></i>Histograms with non-equidistant breaks</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#functions-and-functional-programming"><i class="fa fa-check"></i>Functions and functional programming</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Smoothing</b></span></li>
<li class="chapter" data-level="2" data-path="density.html"><a href="density.html"><i class="fa fa-check"></i><b>2</b> Density estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="unidens.html"><a href="unidens.html"><i class="fa fa-check"></i><b>2.1</b> Univariate density estimation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="unidens.html"><a href="unidens.html#likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Likelihood considerations</a></li>
<li class="chapter" data-level="2.1.2" data-path="unidens.html"><a href="unidens.html#sieves"><i class="fa fa-check"></i><b>2.1.2</b> Method of sieves</a></li>
<li class="chapter" data-level="2.1.3" data-path="unidens.html"><a href="unidens.html#basis-density"><i class="fa fa-check"></i><b>2.1.3</b> Basis expansions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="kernel-density.html"><a href="kernel-density.html"><i class="fa fa-check"></i><b>2.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="kernel-density.html"><a href="kernel-density.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="kernel-density.html"><a href="kernel-density.html#benchmarking"><i class="fa fa-check"></i><b>2.2.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bandwidth.html"><a href="bandwidth.html"><i class="fa fa-check"></i><b>2.3</b> Bandwidth selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bandwidth.html"><a href="bandwidth.html#rectangular"><i class="fa fa-check"></i><b>2.3.1</b> Revisiting the rectangular kernel</a></li>
<li class="chapter" data-level="2.3.2" data-path="bandwidth.html"><a href="bandwidth.html#ise-mise-and-mse-for-kernel-estimators"><i class="fa fa-check"></i><b>2.3.2</b> ISE, MISE and MSE for kernel estimators</a></li>
<li class="chapter" data-level="2.3.3" data-path="bandwidth.html"><a href="bandwidth.html#plug-in-estimation-of-the-oracle-bandwidth"><i class="fa fa-check"></i><b>2.3.3</b> Plug-in estimation of the oracle bandwidth</a></li>
<li class="chapter" data-level="2.3.4" data-path="bandwidth.html"><a href="bandwidth.html#cross-validation"><i class="fa fa-check"></i><b>2.3.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#kernel-density-estimation"><i class="fa fa-check"></i>Kernel density estimation</a></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#benchmarking-1"><i class="fa fa-check"></i>Benchmarking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i><b>3</b> Bivariate smoothing</a><ul>
<li class="chapter" data-level="3.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html"><i class="fa fa-check"></i><b>3.1</b> Nearest neighbor smoothers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#linear-smoothers"><i class="fa fa-check"></i><b>3.1.1</b> Linear smoothers</a></li>
<li class="chapter" data-level="3.1.2" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#implementing-the-running-mean"><i class="fa fa-check"></i><b>3.1.2</b> Implementing the running mean</a></li>
<li class="chapter" data-level="3.1.3" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#choose-k-by-cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Choose <span class="math inline">\(k\)</span> by cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="kernel-methods.html"><a href="kernel-methods.html"><i class="fa fa-check"></i><b>3.2</b> Kernel methods</a></li>
<li class="chapter" data-level="3.3" data-path="sparse-linear-algebra.html"><a href="sparse-linear-algebra.html"><i class="fa fa-check"></i><b>3.3</b> Sparse linear algebra</a></li>
<li class="chapter" data-level="3.4" data-path="onb.html"><a href="onb.html"><i class="fa fa-check"></i><b>3.4</b> Orthogonal basis expansions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="onb.html"><a href="onb.html#polynomial-expansions"><i class="fa fa-check"></i><b>3.4.1</b> Polynomial expansions</a></li>
<li class="chapter" data-level="3.4.2" data-path="onb.html"><a href="onb.html#fourier-expansions"><i class="fa fa-check"></i><b>3.4.2</b> Fourier expansions</a></li>
<li class="chapter" data-level="3.4.3" data-path="onb.html"><a href="onb.html#wavelets"><i class="fa fa-check"></i><b>3.4.3</b> Wavelets</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>3.5</b> Splines</a><ul>
<li class="chapter" data-level="3.5.1" data-path="splines.html"><a href="splines.html#smoothing-splines"><i class="fa fa-check"></i><b>3.5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="3.5.2" data-path="splines.html"><a href="splines.html#splines-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Splines in R</a></li>
<li class="chapter" data-level="3.5.3" data-path="splines.html"><a href="splines.html#efficient-computation-with-splines"><i class="fa fa-check"></i><b>3.5.3</b> Efficient computation with splines</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="gaussian-processes.html"><a href="gaussian-processes.html"><i class="fa fa-check"></i><b>3.6</b> Gaussian processes</a></li>
<li class="chapter" data-level="3.7" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html"><i class="fa fa-check"></i><b>3.7</b> The Kalman filter</a><ul>
<li class="chapter" data-level="3.7.1" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#ar1-example"><i class="fa fa-check"></i><b>3.7.1</b> AR(1)-example</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-smoother"><i class="fa fa-check"></i><b>3.7.2</b> The Kalman smoother</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#implementation-1"><i class="fa fa-check"></i><b>3.7.3</b> Implementation</a></li>
<li class="chapter" data-level="3.7.4" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-filter-1"><i class="fa fa-check"></i><b>3.7.4</b> The Kalman filter</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Monte Carlo Methods</b></span></li>
<li class="chapter" data-level="4" data-path="univariate-random-variables.html"><a href="univariate-random-variables.html"><i class="fa fa-check"></i><b>4</b> Univariate random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="pseudo-random-numbers.html"><a href="pseudo-random-numbers.html"><i class="fa fa-check"></i><b>4.1</b> Pseudo random numbers</a></li>
<li class="chapter" data-level="4.2" data-path="transformation-techniques.html"><a href="transformation-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Transformation techniques</a><ul>
<li class="chapter" data-level="4.2.1" data-path="transformation-techniques.html"><a href="transformation-techniques.html#sampling-from-a-t-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Sampling from a <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="reject-samp.html"><a href="reject-samp.html"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a><ul>
<li class="chapter" data-level="4.3.1" data-path="reject-samp.html"><a href="reject-samp.html#vMsim"><i class="fa fa-check"></i><b>4.3.1</b> von Mises distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="reject-samp.html"><a href="reject-samp.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="adaptive.html"><a href="adaptive.html"><i class="fa fa-check"></i><b>4.4</b> Adaptive envelopes</a><ul>
<li class="chapter" data-level="4.4.1" data-path="adaptive.html"><a href="adaptive.html#beta-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Beta distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="adaptive.html"><a href="adaptive.html#von-mises-distribution"><i class="fa fa-check"></i><b>4.4.2</b> von Mises distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#rejection-sampling-of-gaussian-random-variables"><i class="fa fa-check"></i>Rejection sampling of Gaussian random variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mci.html"><a href="mci.html"><i class="fa fa-check"></i><b>5</b> Monte Carlo integration</a><ul>
<li class="chapter" data-level="5.1" data-path="assessment.html"><a href="assessment.html"><i class="fa fa-check"></i><b>5.1</b> Assessment</a><ul>
<li class="chapter" data-level="5.1.1" data-path="assessment.html"><a href="assessment.html#using-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.1.1</b> Using the central limit theorem</a></li>
<li class="chapter" data-level="5.1.2" data-path="assessment.html"><a href="assessment.html#concentration-inequalities"><i class="fa fa-check"></i><b>5.1.2</b> Concentration inequalities</a></li>
<li class="chapter" data-level="5.1.3" data-path="assessment.html"><a href="assessment.html#exponential-tail-bound-for-gamma-distributed-variables"><i class="fa fa-check"></i><b>5.1.3</b> Exponential tail bound for Gamma distributed variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>5.2</b> Importance sampling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importance-sampling.html"><a href="importance-sampling.html#unknown-normalization-constants"><i class="fa fa-check"></i><b>5.2.1</b> Unknown normalization constants</a></li>
<li class="chapter" data-level="5.2.2" data-path="importance-sampling.html"><a href="importance-sampling.html#hd-int"><i class="fa fa-check"></i><b>5.2.2</b> Computing a high-dimensional integral</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="network-failure.html"><a href="network-failure.html"><i class="fa fa-check"></i><b>5.3</b> Network failure</a><ul>
<li class="chapter" data-level="5.3.1" data-path="network-failure.html"><a href="network-failure.html#object-oriented-implementation"><i class="fa fa-check"></i><b>5.3.1</b> Object oriented implementation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="design-of-experiments.html"><a href="design-of-experiments.html"><i class="fa fa-check"></i><b>5.4</b> Design of experiments</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-random-variables.html"><a href="multivariate-random-variables.html"><i class="fa fa-check"></i><b>6</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="6.1" data-path="sequential-simulation.html"><a href="sequential-simulation.html"><i class="fa fa-check"></i><b>6.1</b> Sequential simulation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="sequential-simulation.html"><a href="sequential-simulation.html#sequential-mc-for-the-ar1-process"><i class="fa fa-check"></i><b>6.1.1</b> Sequential MC for the AR(1)-process</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="gaussian-random-variables.html"><a href="gaussian-random-variables.html"><i class="fa fa-check"></i><b>6.2</b> Gaussian random variables</a></li>
</ul></li>
<li class="part"><span><b>Part III: Optimization</b></span></li>
<li class="chapter" data-level="7" data-path="five-examples.html"><a href="five-examples.html"><i class="fa fa-check"></i><b>7</b> Five Examples</a><ul>
<li class="chapter" data-level="7.1" data-path="exp-fam.html"><a href="exp-fam.html"><i class="fa fa-check"></i><b>7.1</b> Exponential families</a><ul>
<li class="chapter" data-level="7.1.1" data-path="exp-fam.html"><a href="exp-fam.html#full-exponential-families"><i class="fa fa-check"></i><b>7.1.1</b> Full exponential families</a></li>
<li class="chapter" data-level="7.1.2" data-path="exp-fam.html"><a href="exp-fam.html#bayes-net"><i class="fa fa-check"></i><b>7.1.2</b> Exponential family Bayesian networks</a></li>
<li class="chapter" data-level="7.1.3" data-path="exp-fam.html"><a href="exp-fam.html#exp-fam-deriv"><i class="fa fa-check"></i><b>7.1.3</b> Likelihood computations</a></li>
<li class="chapter" data-level="7.1.4" data-path="exp-fam.html"><a href="exp-fam.html#curved-exponential-families"><i class="fa fa-check"></i><b>7.1.4</b> Curved exponential families</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="multinomial-models.html"><a href="multinomial-models.html"><i class="fa fa-check"></i><b>7.2</b> Multinomial models</a><ul>
<li class="chapter" data-level="7.2.1" data-path="multinomial-models.html"><a href="multinomial-models.html#pep-moth"><i class="fa fa-check"></i><b>7.2.1</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>7.3</b> Regression models</a></li>
<li class="chapter" data-level="7.4" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html"><i class="fa fa-check"></i><b>7.4</b> Finite mixture models</a><ul>
<li class="chapter" data-level="7.4.1" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#Gaus-mix-ex"><i class="fa fa-check"></i><b>7.4.1</b> Gaussian mixtures</a></li>
<li class="chapter" data-level="7.4.2" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#von-mises-mixtures"><i class="fa fa-check"></i><b>7.4.2</b> von Mises mixtures</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="mixed-models.html"><a href="mixed-models.html"><i class="fa fa-check"></i><b>7.5</b> Mixed models</a></li>
<li class="chapter" data-level="7.6" data-path="state-space-models.html"><a href="state-space-models.html"><i class="fa fa-check"></i><b>7.6</b> State space models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="numopt.html"><a href="numopt.html"><i class="fa fa-check"></i><b>8</b> Numerical optimization</a><ul>
<li class="chapter" data-level="8.1" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html"><i class="fa fa-check"></i><b>8.1</b> Algorithms and convergence</a><ul>
<li class="chapter" data-level="8.1.1" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#descent-algorithms"><i class="fa fa-check"></i><b>8.1.1</b> Descent algorithms</a></li>
<li class="chapter" data-level="8.1.2" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#maps-and-fixed-points"><i class="fa fa-check"></i><b>8.1.2</b> Maps and fixed points</a></li>
<li class="chapter" data-level="8.1.3" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#convergence-rate"><i class="fa fa-check"></i><b>8.1.3</b> Convergence rate</a></li>
<li class="chapter" data-level="8.1.4" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#stopping-criteria"><i class="fa fa-check"></i><b>8.1.4</b> Stopping criteria</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html"><i class="fa fa-check"></i><b>8.2</b> Descent direction algorithms</a><ul>
<li class="chapter" data-level="8.2.1" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#line-search"><i class="fa fa-check"></i><b>8.2.1</b> Line search</a></li>
<li class="chapter" data-level="8.2.2" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>8.2.2</b> Gradient descent</a></li>
<li class="chapter" data-level="8.2.3" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#conjugate-gradients"><i class="fa fa-check"></i><b>8.2.3</b> Conjugate gradients</a></li>
<li class="chapter" data-level="8.2.4" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#pep-moth-descent"><i class="fa fa-check"></i><b>8.2.4</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html"><i class="fa fa-check"></i><b>8.3</b> Newton-type algorithms</a><ul>
<li class="chapter" data-level="8.3.1" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#poisson-regression"><i class="fa fa-check"></i><b>8.3.1</b> Poisson regression</a></li>
<li class="chapter" data-level="8.3.2" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#quasi-newton-algorithms"><i class="fa fa-check"></i><b>8.3.2</b> Quasi-Newton algorithms</a></li>
<li class="chapter" data-level="8.3.3" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#sparsity"><i class="fa fa-check"></i><b>8.3.3</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="misc-.html"><a href="misc-.html"><i class="fa fa-check"></i><b>8.4</b> Misc.</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="em.html"><a href="em.html"><i class="fa fa-check"></i><b>9</b> Expectation maximization algorithms</a><ul>
<li class="chapter" data-level="9.1" data-path="basic-properties.html"><a href="basic-properties.html"><i class="fa fa-check"></i><b>9.1</b> Basic properties</a><ul>
<li class="chapter" data-level="9.1.1" data-path="basic-properties.html"><a href="basic-properties.html#incomplete-data-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Incomplete data likelihood</a></li>
<li class="chapter" data-level="9.1.2" data-path="basic-properties.html"><a href="basic-properties.html#monotonicity-of-the-em-algorithm"><i class="fa fa-check"></i><b>9.1.2</b> Monotonicity of the EM algorithm</a></li>
<li class="chapter" data-level="9.1.3" data-path="basic-properties.html"><a href="basic-properties.html#peppered-moths"><i class="fa fa-check"></i><b>9.1.3</b> Peppered moths</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="EM-exp.html"><a href="EM-exp.html"><i class="fa fa-check"></i><b>9.2</b> Exponential families</a></li>
<li class="chapter" data-level="9.3" data-path="fisher-information.html"><a href="fisher-information.html"><i class="fa fa-check"></i><b>9.3</b> Fisher information</a></li>
<li class="chapter" data-level="9.4" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html"><i class="fa fa-check"></i><b>9.4</b> Two examples revisited</a><ul>
<li class="chapter" data-level="9.4.1" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html#gaussian-mixtures"><i class="fa fa-check"></i><b>9.4.1</b> Gaussian mixtures</a></li>
<li class="chapter" data-level="9.4.2" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html#gaussian-state-space"><i class="fa fa-check"></i><b>9.4.2</b> Gaussian state space</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="stochopt.html"><a href="stochopt.html"><i class="fa fa-check"></i><b>10</b> Stochastic Optimization</a><ul>
<li class="chapter" data-level="10.1" data-path="stochastic-gradient.html"><a href="stochastic-gradient.html"><i class="fa fa-check"></i><b>10.1</b> Stochastic gradient</a></li>
<li class="chapter" data-level="10.2" data-path="stochastic-em.html"><a href="stochastic-em.html"><i class="fa fa-check"></i><b>10.2</b> Stochastic EM</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="basic-properties" class="section level2">
<h2><span class="header-section-number">9.1</span> Basic properties</h2>
<p>In this section the EM algorithm is formulated and shown to be a descent algorithm for the negative log-likelihood. Allele frequency estimation for the peppered moth is considered as a simple example showing how the algorithm can be implemented.</p>
<div id="incomplete-data-likelihood" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Incomplete data likelihood</h3>
<p>Suppose that <span class="math inline">\(Y\)</span> is a random variable and <span class="math inline">\(X = M(Y)\)</span>. Suppose that <span class="math inline">\(Y\)</span> has density <span class="math inline">\(f(\cdot \mid \theta)\)</span> and that <span class="math inline">\(X\)</span> has marginal density <span class="math inline">\(g(x \mid \theta)\)</span>.</p>
<p>The marginal density is typically of the form <span class="math display">\[g(x \mid \theta) = \int_{\{y: M(y) = x\}} f(y \mid \theta) \ \mu_x(\mathrm{d} y)\]</span> for a suitable measure <span class="math inline">\(\mu_x\)</span> depending on <span class="math inline">\(M\)</span> and <span class="math inline">\(x\)</span> but not <span class="math inline">\(\theta\)</span>. The general argument for the marginal density relies on the coarea formula.</p>
<p>The log-likelihood for observing <span class="math inline">\(X = x\)</span> is <span class="math display">\[\ell(\theta) = \log g(x \mid \theta).\]</span> The log-likelihood is often impossible to compute analytically and difficult and expensive to compute numerically. The complete log-likelihood, <span class="math inline">\(\log f(y \mid \theta)\)</span>, is often easy to compute, but we don’t know <span class="math inline">\(Y\)</span>, only that <span class="math inline">\(M(Y) = x\)</span>.</p>
<p>In some cases it is possible to compute <span class="math display">\[Q(\theta \mid \theta&#39;) := E_{\theta&#39;}(\log f(Y \mid \theta) \mid X = x),\]</span> which is the conditional expectation of the complete log-likelihood given the observed data and computed using the probability measure given by <span class="math inline">\(\theta&#39;\)</span>. Thus for fixed <span class="math inline">\(\theta&#39;\)</span> this is a computable function of <span class="math inline">\(\theta\)</span> depending only on the observed data <span class="math inline">\(x\)</span>.</p>
<p>One could get the following idea: with an initial guess of <span class="math inline">\(\theta&#39; = \theta_0\)</span> compute iteratively <span class="math display">\[\theta_{n + 1} = \textrm{arg max} \ Q(\theta \mid \theta_n)\]</span> for <span class="math inline">\(n = 0, 1, 2, \ldots\)</span>. This idea is the EM algorithm:</p>
<ul>
<li><strong>E-step</strong>: Compute the conditional expectation <span class="math inline">\(Q(\theta \mid \theta_n )\)</span>.</li>
<li><strong>M-step</strong>: Maximize <span class="math inline">\(\theta \mapsto Q(\theta \mid \theta_n )\)</span>.</li>
</ul>
<p>It is a bit weird to present the algorithm as a two-step algorithm in its abstract formulation. Even though we can regard <span class="math inline">\(Q(\theta \mid \theta_n)\)</span> as something we can compute abstractly for each <span class="math inline">\(\theta\)</span> for a given <span class="math inline">\(\theta_n\)</span>, the maximization is in practice not really done using all these evaluations. It is computed either by an analytic formula involving <span class="math inline">\(x\)</span> and <span class="math inline">\(\theta_n\)</span>, or by a numerical algorithm that computes certain evaluations of <span class="math inline">\(Q( \cdot \mid \theta_n)\)</span> and perhaps its gradient and Hessian. In computing these specific evaluations there is, of course, a need for the computation of conditional expectations, but we would compute these as they are needed and not upfront.</p>
<p>However, in some of the most important applications of the EM algorithm, particularly for exponential families covered in Section <a href="EM-exp.html#EM-exp">9.2</a>, it makes a lot of sense to regard the algorithm as a two-step algorithm. This is the case whenever <span class="math inline">\(Q(\theta \mid \theta_n) = q(\theta, t(x, \theta_n))\)</span> is given in terms of <span class="math inline">\(\theta\)</span> and a function <span class="math inline">\(t(x, \theta_n )\)</span> of <span class="math inline">\(x\)</span> and <span class="math inline">\(\theta_n\)</span> that doesn’t depend on <span class="math inline">\(\theta\)</span>. Then the E-step becomes the computation of <span class="math inline">\(t(x, \theta_n )\)</span>, and in the M-step, <span class="math inline">\(Q(\cdot \mid \theta_n )\)</span> is maximized by maximizing <span class="math inline">\(q(\cdot, t(x, \theta_n ))\)</span>, and the maximum is a function of <span class="math inline">\(t(x, \theta_n )\)</span>.</p>
</div>
<div id="monotonicity-of-the-em-algorithm" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Monotonicity of the EM algorithm</h3>
<p>We prove below that the algorithm (weakly) increases the log-likelihood in every step, and thus is a descent algorithm for the negative log-likelihood <span class="math inline">\(H = - \ell\)</span>.</p>
<p>It holds in great generality that the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span> has density</p>
<span class="math display" id="eq:cond-dist">\[\begin{equation}
h(y \mid x, \theta) = \frac{f(y \mid \theta)}{g(x \mid \theta)}
\tag{9.1}
\end{equation}\]</span>
<p>w.r.t. the measure <span class="math inline">\(\mu_x\)</span> as above (that does not depend upon <span class="math inline">\(\theta\)</span>), and where <span class="math inline">\(g\)</span> is the density for the marginal distribution.</p>
<p>This can be verified quite easily for discrete distributions and when <span class="math inline">\(Y = (Z, X)\)</span> with joint density w.r.t. a product measure <span class="math inline">\(\mu \otimes \nu\)</span> that does not depend upon <span class="math inline">\(\theta\)</span>. In the latter case, <span class="math inline">\(f(y \mid \theta) = f(z, x \mid \theta)\)</span> and <span class="math display">\[g(x \mid \theta) = \int f(z, x \mid \theta) \ \mu(\mathrm{d} z)\]</span> is the marginal density w.r.t. <span class="math inline">\(\nu\)</span>.</p>
<p>Whenever <a href="basic-properties.html#eq:cond-dist">(9.1)</a> holds it follows that<br />
<span class="math display">\[\ell(\theta) = \log g(x \mid \theta) = \log f(y \mid \theta) - \log h(y \mid x, \theta),\]</span> where <span class="math inline">\(\ell(\theta)\)</span> is the log-likelihood.</p>

<div class="theorem">
<span id="thm:EM-inequality" class="theorem"><strong>Theorem 9.1  </strong></span>If <span class="math inline">\(\log f(Y \mid \theta)\)</span> as well as <span class="math inline">\(\log h(Y \mid x, \theta)\)</span> have finite <span class="math inline">\(\theta&#39;\)</span>-conditional expectation given <span class="math inline">\(M(Y) = x\)</span> then <span class="math display">\[Q(\theta \mid \theta&#39;) &gt; Q(\theta&#39; \mid \theta&#39;) \quad \Rightarrow \quad  \ell(\theta) &gt; \ell(\theta&#39;).\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Since <span class="math inline">\(\ell(\theta)\)</span> depends on <span class="math inline">\(y\)</span> only through <span class="math inline">\(M(y) = x\)</span>,</p>
<span class="math display">\[\begin{align}
\ell(\theta) &amp; = E_{\theta&#39;} ( \ell(\theta) \mid X = x) \\
&amp; =  \underbrace{E_{\theta&#39;} ( \log f(Y \mid \theta) \mid X = x)}_{Q(\theta \mid \theta&#39;)} +  \underbrace{ E_{\theta&#39;} ( - \log h(Y \mid x, \theta) \mid X = x)}_{H(\theta \mid\theta&#39;)} \\
&amp; = Q(\theta \mid \theta&#39;) + H(\theta \mid \theta&#39;). 
\end{align}\]</span>
<p>Now for the second term we find, using <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality#Measure-theoretic_and_probabilistic_form">Jensen’s inequality</a> for the convex function <span class="math inline">\(-\log\)</span>, that</p>
<span class="math display">\[\begin{align}
H(\theta \mid \theta&#39;) &amp; = \int - \log(h(y \mid x, \theta)) h(y \mid x, \theta&#39;) \mu_x(\mathrm{d}y) \\
&amp; = \int - \log\left(\frac{h(y \mid x, \theta)}{ h(y \mid x, \theta&#39;)}\right) h(y \mid x, \theta&#39;) \mu_x(\mathrm{d}y) + \int - \log(h(y \mid x, \theta&#39;)) h(y \mid x, \theta&#39;) \mu_x(\mathrm{d}y) \\
&amp; \geq  -\log \left( \int \frac{h(y \mid x, \theta)}{ h(y \mid x, \theta&#39;)} h(y \mid x, \theta&#39;) \mu_x(\mathrm{d}y) \right) + H(\theta&#39; \mid \theta&#39;) \\
&amp; = -\log\underbrace{\left( \int h(y \mid x, \theta) \mu_x(\mathrm{d}y)\right) }_{=1} + H(\theta&#39; \mid \theta&#39;) \\
&amp; =  H(\theta&#39; \mid \theta&#39;).
\end{align}\]</span>
<p>From this we see that</p>
<p><span class="math display">\[\ell(\theta) \geq  Q(\theta \mid \theta&#39;) + H(\theta&#39; \mid \theta&#39;)\]</span></p>
<p>for all <span class="math inline">\(\theta\)</span> and the right hand side is a so-called minorant for the log-likelihood. Observing that</p>
<p><span class="math display">\[\ell(\theta&#39;) = Q(\theta&#39; \mid \theta&#39;) + H(\theta&#39; \mid \theta&#39;).\]</span></p>
completes the proof of the theorem.
</div>

<p>Note that the proof above can also be given by referring to <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality#Information_theory">Gibbs’ inequality in information theory</a> stating that the Kullback-Leibler divergence is positive, or equivalently that the cross-entropy <span class="math inline">\(H(\theta \mid \theta&#39;)\)</span> is smaller than the entropy <span class="math inline">\(H(\theta&#39; \mid \theta&#39;)\)</span>, but the proof of this is, in itself, a consequence of Jensen’s inequality just as above.</p>
<p>It follows from Theorem <a href="basic-properties.html#thm:EM-inequality">9.1</a> that if <span class="math inline">\(\theta_n\)</span> is computed iteratively starting from <span class="math inline">\(\theta_0\)</span> such that <span class="math display">\[Q(\theta_{n+1} \mid \theta_{n}) &gt; Q(\theta_{n} \mid \theta_{n}),\]</span> then <span class="math display">\[H(\theta_0) &gt; H(\theta_1) &gt; H(\theta_2) &gt; \ldots.\]</span> This proves that the EM algorithm is a strict descent algorithm for the negative log-likelihood as long as it is possible in each iteration to strictly increase <span class="math inline">\(\theta \mapsto Q(\theta \mid \theta_{n})\)</span> above <span class="math inline">\(Q(\theta_{n} \mid \theta_{n}).\)</span></p>
<p>The term <em>EM algorithm</em> is usually reserved for the specific algorithm that maximizes <span class="math inline">\(Q(\cdot \mid \theta_n)\)</span> in the M-step, but as we have seen, there is no reason to insist on the M-step being a maximization. A choice of descent direction of <span class="math inline">\(Q(\cdot \mid \theta_n)\)</span> and a step-length guaranteeing sufficient descent will be enough to give a descent algorithm. Any such variation is usually termed a generalized EM algorithm.</p>
<p>One may imagine that the minorant could actually be a useful lower bound on the difficult-to-compute log-likelihood. The additive constant <span class="math inline">\(H(\theta&#39; \mid \theta&#39;)\)</span> in the minorant is, however, not going to be computable in general either, and it is not clear that there is any way to use the bound quantitatively.</p>
</div>
<div id="peppered-moths" class="section level3">
<h3><span class="header-section-number">9.1.3</span> Peppered moths</h3>
<p>We return in this section to the peppered moths and the implementation of the EM algorithm for multinomial cell collapsing.</p>
<p>The EM algorithm can be implemented by two simple functions that compute the conditional expectations above (the E-step) and then maximization of the complete observation log-likelihood.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">EStep0 &lt;-<span class="st"> </span><span class="cf">function</span>(p, x, group) {
  x[group] <span class="op">*</span><span class="st"> </span>p <span class="op">/</span><span class="st"> </span><span class="kw">M</span>(p, group)[group]
}</code></pre></div>
<p>The MLE of the complete log-likelihood is a linear estimator, as is the case in many examples with explicit MLEs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MStep0 &lt;-<span class="st"> </span><span class="cf">function</span>(n, X)
  <span class="kw">as.vector</span>(X <span class="op">%*%</span><span class="st"> </span>n <span class="op">/</span><span class="st"> </span>(<span class="kw">sum</span>(n)))</code></pre></div>
<p>The <code>EStep0</code> and <code>MStep0</code> functions are abstract implementations. They require specification of the arguments <code>group</code> and <code>X</code>, respectively, to become concrete.</p>
<p>The M-step is only implemented in the case where the complete-data MLE is a <em>linear estimator</em>, that is, a linear map of the complete data vector <span class="math inline">\(y\)</span> that can be expressed in terms of a matrix <span class="math inline">\(\mathbf{X}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">EStep &lt;-<span class="st"> </span><span class="cf">function</span>(par, x)
  <span class="kw">EStep0</span>(<span class="kw">prob</span>(par), x, <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>))

MStep &lt;-<span class="st"> </span><span class="cf">function</span>(n) {
  X &lt;-<span class="st"> </span><span class="kw">matrix</span>(
  <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,
    <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>,
  <span class="dv">2</span>, <span class="dv">6</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
  
  <span class="kw">MStep0</span>(n, X)
}</code></pre></div>
<p>The EM algorithm is finally implemented as an iterative, alternating call of <code>EStep</code> and <code>MStep</code> until convergence as measured in terms of the relative change from iteration to iteration being sufficiently small.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">EM &lt;-<span class="st"> </span><span class="cf">function</span>(par, x, <span class="dt">epsilon =</span> <span class="fl">1e-6</span>, <span class="dt">trace =</span> <span class="ot">NULL</span>) {
  <span class="cf">repeat</span>{
    par0 &lt;-<span class="st"> </span>par
    par &lt;-<span class="st"> </span><span class="kw">MStep</span>(<span class="kw">EStep</span>(par, x))
    <span class="cf">if</span>(<span class="op">!</span><span class="kw">is.null</span>(trace)) <span class="kw">trace</span>()
    <span class="cf">if</span>(<span class="kw">sum</span>((par <span class="op">-</span><span class="st"> </span>par0)<span class="op">^</span><span class="dv">2</span>) <span class="op">&lt;=</span><span class="st"> </span>epsilon <span class="op">*</span><span class="st"> </span>(<span class="kw">sum</span>(par<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>epsilon))
      <span class="cf">break</span>
  } 
  par  ## Remember to return the parameter estimate
}
  
phat &lt;-<span class="st"> </span><span class="kw">EM</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>))
phat</code></pre></div>
<pre><code>## [1] 0.07083693 0.18877365</code></pre>
<p>We check what is going on in each step of the EM algorithm.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">EM_tracer &lt;-<span class="st"> </span><span class="kw">tracer</span>(<span class="st">&quot;par&quot;</span>)
<span class="kw">EM</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), <span class="dt">trace =</span> EM_tracer<span class="op">$</span>trace)</code></pre></div>
<pre><code>## n = 1: par = 0.08038585, 0.22464192; 
## n = 2: par = 0.07118928, 0.19546961; 
## n = 3: par = 0.07084985, 0.18993393; 
## n = 4: par = 0.07083738, 0.18894757; 
## n = 5: par = 0.07083693, 0.18877365;</code></pre>
<pre><code>## [1] 0.07083693 0.18877365</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">EM_tracer &lt;-<span class="st"> </span><span class="kw">tracer</span>(<span class="kw">c</span>(<span class="st">&quot;par0&quot;</span>, <span class="st">&quot;par&quot;</span>), <span class="dt">N =</span> <span class="dv">0</span>)
phat &lt;-<span class="st"> </span><span class="kw">EM</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), <span class="dt">epsilon =</span> <span class="fl">1e-20</span>, 
           <span class="dt">trace =</span> EM_tracer<span class="op">$</span>trace)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">EM_trace &lt;-<span class="st"> </span><span class="kw">summary</span>(EM_tracer)
  EM_trace &lt;-<span class="st"> </span><span class="kw">transform</span>(
  EM_trace, 
  <span class="dt">n =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(EM_trace),
  <span class="dt">par_norm_diff =</span> <span class="kw">sqrt</span>((par0.<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>par.<span class="dv">1</span>)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>(par0.<span class="dv">2</span> <span class="op">-</span><span class="st"> </span>par.<span class="dv">2</span>)<span class="op">^</span><span class="dv">2</span>)
)
<span class="kw">qplot</span>(n, <span class="kw">log</span>(par_norm_diff), <span class="dt">data =</span> EM_trace)</code></pre></div>
<p><img src="CSwR_files/figure-html/moth_EM_figure-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Note the log-axis. The EM-algorithm converges linearly (this is the terminology, see <a href="algorithms-and-convergence.html#algorithms-and-convergence">Algorithms and Convergence</a>). The log-rate of the convergence can be estimated by least-squares.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_rate_fit &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(par_norm_diff) <span class="op">~</span><span class="st"> </span>n,  <span class="dt">data =</span> EM_trace)
<span class="kw">exp</span>(<span class="kw">coefficients</span>(log_rate_fit)[<span class="st">&quot;n&quot;</span>])</code></pre></div>
<pre><code>##         n 
## 0.1750251</code></pre>
<p>The rate is very small in this case implying fast convergence. This is not always the case. If the log-likelihood is flat, the EM-algorithm can become quite slow with a rate close to 1.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="em.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="EM-exp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

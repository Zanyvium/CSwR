<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Statistics with R</title>
  <meta name="description" content="Lecture notes providing an introduction to computational statistics using the R programming language.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Statistics with R" />
  
  <meta name="twitter:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

<meta name="author" content="Niels Richard Hansen">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="EM.html">
<link rel="next" href="EM-exp.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro-smooth.html"><a href="intro-smooth.html"><i class="fa fa-check"></i><b>1.1</b> Smoothing</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro-smooth.html"><a href="intro-smooth.html#intro-angles"><i class="fa fa-check"></i><b>1.1.1</b> Angle distributions in proteins</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-smooth.html"><a href="intro-smooth.html#using-ggplot2"><i class="fa fa-check"></i><b>1.1.2</b> Using ggplot2</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-smooth.html"><a href="intro-smooth.html#changing-the-defaults"><i class="fa fa-check"></i><b>1.1.3</b> Changing the defaults</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-smooth.html"><a href="intro-smooth.html#large-scale-smoothing"><i class="fa fa-check"></i><b>1.1.4</b> Large scale smoothing</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo Methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#vM"><i class="fa fa-check"></i><b>1.2.1</b> Univariate von Mises distributions</a></li>
<li class="chapter" data-level="1.2.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#mixtures-of-von-mises-distributions"><i class="fa fa-check"></i><b>1.2.2</b> Mixtures of von Mises distributions</a></li>
<li class="chapter" data-level="1.2.3" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#large-scale-simulation"><i class="fa fa-check"></i><b>1.2.3</b> Large scale simulation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="optimization.html"><a href="optimization.html"><i class="fa fa-check"></i><b>1.3</b> Optimization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="optimization.html"><a href="optimization.html#the-em-algorithm"><i class="fa fa-check"></i><b>1.3.1</b> The EM-algorithm</a></li>
<li class="chapter" data-level="1.3.2" data-path="optimization.html"><a href="optimization.html#large-scale-optimization"><i class="fa fa-check"></i><b>1.3.2</b> Large scale optimization</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.4</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#r-training-exercises"><i class="fa fa-check"></i>R training exercises</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#histograms-with-non-equidistant-breaks"><i class="fa fa-check"></i>Histograms with non-equidistant breaks</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#functions-and-functional-programming"><i class="fa fa-check"></i>Functions and functional programming</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Smoothing</b></span></li>
<li class="chapter" data-level="2" data-path="density.html"><a href="density.html"><i class="fa fa-check"></i><b>2</b> Density estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="unidens.html"><a href="unidens.html"><i class="fa fa-check"></i><b>2.1</b> Univariate density estimation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="unidens.html"><a href="unidens.html#likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Likelihood considerations</a></li>
<li class="chapter" data-level="2.1.2" data-path="unidens.html"><a href="unidens.html#sieves"><i class="fa fa-check"></i><b>2.1.2</b> Method of sieves</a></li>
<li class="chapter" data-level="2.1.3" data-path="unidens.html"><a href="unidens.html#basis-density"><i class="fa fa-check"></i><b>2.1.3</b> Basis expansions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="kernel-density.html"><a href="kernel-density.html"><i class="fa fa-check"></i><b>2.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="kernel-density.html"><a href="kernel-density.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="kernel-density.html"><a href="kernel-density.html#benchmarking"><i class="fa fa-check"></i><b>2.2.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bandwidth.html"><a href="bandwidth.html"><i class="fa fa-check"></i><b>2.3</b> Bandwidth selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bandwidth.html"><a href="bandwidth.html#rectangular"><i class="fa fa-check"></i><b>2.3.1</b> Revisiting the rectangular kernel</a></li>
<li class="chapter" data-level="2.3.2" data-path="bandwidth.html"><a href="bandwidth.html#ise-mise-and-mse-for-kernel-estimators"><i class="fa fa-check"></i><b>2.3.2</b> ISE, MISE and MSE for kernel estimators</a></li>
<li class="chapter" data-level="2.3.3" data-path="bandwidth.html"><a href="bandwidth.html#plug-in-estimation-of-the-oracle-bandwidth"><i class="fa fa-check"></i><b>2.3.3</b> Plug-in estimation of the oracle bandwidth</a></li>
<li class="chapter" data-level="2.3.4" data-path="bandwidth.html"><a href="bandwidth.html#cross-validation"><i class="fa fa-check"></i><b>2.3.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multivariate-smoothing.html"><a href="multivariate-smoothing.html"><i class="fa fa-check"></i><b>2.4</b> Multivariate methods</a></li>
<li class="chapter" data-level="2.5" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#kernel-density-estimation"><i class="fa fa-check"></i>Kernel density estimation</a></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#benchmarking-1"><i class="fa fa-check"></i>Benchmarking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i><b>3</b> Bivariate smoothing</a><ul>
<li class="chapter" data-level="3.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html"><i class="fa fa-check"></i><b>3.1</b> Nearest neighbor smoothers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#linear-smoothers"><i class="fa fa-check"></i><b>3.1.1</b> Linear smoothers</a></li>
<li class="chapter" data-level="3.1.2" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#implementing-the-running-mean"><i class="fa fa-check"></i><b>3.1.2</b> Implementing the running mean</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="kernel-methods.html"><a href="kernel-methods.html"><i class="fa fa-check"></i><b>3.2</b> Kernel methods</a></li>
<li class="chapter" data-level="3.3" data-path="sparse-linear-algebra.html"><a href="sparse-linear-algebra.html"><i class="fa fa-check"></i><b>3.3</b> Sparse linear algebra</a></li>
<li class="chapter" data-level="3.4" data-path="orthogonal-basis-expansions.html"><a href="orthogonal-basis-expansions.html"><i class="fa fa-check"></i><b>3.4</b> Orthogonal basis expansions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="orthogonal-basis-expansions.html"><a href="orthogonal-basis-expansions.html#polynomial-expansions"><i class="fa fa-check"></i><b>3.4.1</b> Polynomial expansions</a></li>
<li class="chapter" data-level="3.4.2" data-path="orthogonal-basis-expansions.html"><a href="orthogonal-basis-expansions.html#fourier-expansions"><i class="fa fa-check"></i><b>3.4.2</b> Fourier expansions</a></li>
<li class="chapter" data-level="3.4.3" data-path="orthogonal-basis-expansions.html"><a href="orthogonal-basis-expansions.html#wavelets"><i class="fa fa-check"></i><b>3.4.3</b> Wavelets</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>3.5</b> Splines</a></li>
<li class="chapter" data-level="3.6" data-path="gaussian-processes.html"><a href="gaussian-processes.html"><i class="fa fa-check"></i><b>3.6</b> Gaussian processes</a></li>
<li class="chapter" data-level="3.7" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html"><i class="fa fa-check"></i><b>3.7</b> The Kalman filter</a><ul>
<li class="chapter" data-level="3.7.1" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#ar1-example"><i class="fa fa-check"></i><b>3.7.1</b> AR(1)-example</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-smoother"><i class="fa fa-check"></i><b>3.7.2</b> The Kalman smoother</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#implementation-1"><i class="fa fa-check"></i><b>3.7.3</b> Implementation</a></li>
<li class="chapter" data-level="3.7.4" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-filter-1"><i class="fa fa-check"></i><b>3.7.4</b> The Kalman filter</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Monte Carlo Methods</b></span></li>
<li class="chapter" data-level="4" data-path="univariate-random-variables.html"><a href="univariate-random-variables.html"><i class="fa fa-check"></i><b>4</b> Univariate random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="pseudo-random-numbers.html"><a href="pseudo-random-numbers.html"><i class="fa fa-check"></i><b>4.1</b> Pseudo random numbers</a></li>
<li class="chapter" data-level="4.2" data-path="transformation-techniques.html"><a href="transformation-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Transformation techniques</a><ul>
<li class="chapter" data-level="4.2.1" data-path="transformation-techniques.html"><a href="transformation-techniques.html#sampling-from-a-t-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Sampling from a <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="reject-samp.html"><a href="reject-samp.html"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a><ul>
<li class="chapter" data-level="4.3.1" data-path="reject-samp.html"><a href="reject-samp.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.1</b> Gamma distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="MCI.html"><a href="MCI.html"><i class="fa fa-check"></i><b>5</b> Monte Carlo integration</a><ul>
<li class="chapter" data-level="5.1" data-path="assessment.html"><a href="assessment.html"><i class="fa fa-check"></i><b>5.1</b> Assessment</a><ul>
<li class="chapter" data-level="5.1.1" data-path="assessment.html"><a href="assessment.html#using-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.1.1</b> Using the central limit theorem</a></li>
<li class="chapter" data-level="5.1.2" data-path="assessment.html"><a href="assessment.html#concentration-inequalities"><i class="fa fa-check"></i><b>5.1.2</b> Concentration inequalities</a></li>
<li class="chapter" data-level="5.1.3" data-path="assessment.html"><a href="assessment.html#exponential-tail-bound-for-gamma-distributed-variables"><i class="fa fa-check"></i><b>5.1.3</b> Exponential tail bound for Gamma distributed variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>5.2</b> Importance sampling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importance-sampling.html"><a href="importance-sampling.html#unknown-normalization-constants"><i class="fa fa-check"></i><b>5.2.1</b> Unknown normalization constants</a></li>
<li class="chapter" data-level="5.2.2" data-path="importance-sampling.html"><a href="importance-sampling.html#computing-a-high-dimensional-integral"><i class="fa fa-check"></i><b>5.2.2</b> Computing a high-dimensional integral</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="network-failure.html"><a href="network-failure.html"><i class="fa fa-check"></i><b>5.3</b> Network failure</a><ul>
<li class="chapter" data-level="5.3.1" data-path="network-failure.html"><a href="network-failure.html#object-oriented-implementation"><i class="fa fa-check"></i><b>5.3.1</b> Object oriented implementation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="design-of-experiments.html"><a href="design-of-experiments.html"><i class="fa fa-check"></i><b>5.4</b> Design of experiments</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-random-variables.html"><a href="multivariate-random-variables.html"><i class="fa fa-check"></i><b>6</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="6.1" data-path="sequential-simulation.html"><a href="sequential-simulation.html"><i class="fa fa-check"></i><b>6.1</b> Sequential simulation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="sequential-simulation.html"><a href="sequential-simulation.html#sequential-mc-for-the-ar1-process"><i class="fa fa-check"></i><b>6.1.1</b> Sequential MC for the AR(1)-process</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="gaussian-random-variables.html"><a href="gaussian-random-variables.html"><i class="fa fa-check"></i><b>6.2</b> Gaussian random variables</a></li>
</ul></li>
<li class="part"><span><b>Part III: Optimization</b></span></li>
<li class="chapter" data-level="7" data-path="four-examples.html"><a href="four-examples.html"><i class="fa fa-check"></i><b>7</b> Four Examples</a><ul>
<li class="chapter" data-level="7.1" data-path="multinomial-models.html"><a href="multinomial-models.html"><i class="fa fa-check"></i><b>7.1</b> Multinomial models</a><ul>
<li class="chapter" data-level="7.1.1" data-path="multinomial-models.html"><a href="multinomial-models.html#peppered-moths"><i class="fa fa-check"></i><b>7.1.1</b> Peppered Moths</a></li>
<li class="chapter" data-level="7.1.2" data-path="multinomial-models.html"><a href="multinomial-models.html#multinomial-cell-collapsing"><i class="fa fa-check"></i><b>7.1.2</b> Multinomial cell collapsing</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="mixtures.html"><a href="mixtures.html"><i class="fa fa-check"></i><b>7.2</b> Mixtures</a><ul>
<li class="chapter" data-level="7.2.1" data-path="mixtures.html"><a href="mixtures.html#gaussian-mixtures"><i class="fa fa-check"></i><b>7.2.1</b> Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="mixed-effects-models.html"><a href="mixed-effects-models.html"><i class="fa fa-check"></i><b>7.3</b> Mixed effects models</a></li>
<li class="chapter" data-level="7.4" data-path="gaussian-state-space-models.html"><a href="gaussian-state-space-models.html"><i class="fa fa-check"></i><b>7.4</b> Gaussian state space models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="numerical-optimization.html"><a href="numerical-optimization.html"><i class="fa fa-check"></i><b>8</b> Numerical optimization</a><ul>
<li class="chapter" data-level="8.1" data-path="gradient-based-algorithms.html"><a href="gradient-based-algorithms.html"><i class="fa fa-check"></i><b>8.1</b> Gradient based algorithms</a><ul>
<li class="chapter" data-level="8.1.1" data-path="gradient-based-algorithms.html"><a href="gradient-based-algorithms.html#gradient-descent-and-conjugate-gradient"><i class="fa fa-check"></i><b>8.1.1</b> Gradient descent and conjugate gradient</a></li>
<li class="chapter" data-level="8.1.2" data-path="gradient-based-algorithms.html"><a href="gradient-based-algorithms.html#peppered-moths-1"><i class="fa fa-check"></i><b>8.1.2</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html"><i class="fa fa-check"></i><b>8.2</b> Newton-type algorithms</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="EM.html"><a href="EM.html"><i class="fa fa-check"></i><b>9</b> Expectation Maximization algorithms</a><ul>
<li class="chapter" data-level="9.1" data-path="basic-properties.html"><a href="basic-properties.html"><i class="fa fa-check"></i><b>9.1</b> Basic properties</a><ul>
<li class="chapter" data-level="9.1.1" data-path="basic-properties.html"><a href="basic-properties.html#incomplete-data-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Incomplete data likelihood</a></li>
<li class="chapter" data-level="9.1.2" data-path="basic-properties.html"><a href="basic-properties.html#the-em-algorithm-is-ascending"><i class="fa fa-check"></i><b>9.1.2</b> The EM-algorithm is ascending</a></li>
<li class="chapter" data-level="9.1.3" data-path="basic-properties.html"><a href="basic-properties.html#multinomial-cell-collapsing-1"><i class="fa fa-check"></i><b>9.1.3</b> Multinomial cell collapsing</a></li>
<li class="chapter" data-level="9.1.4" data-path="basic-properties.html"><a href="basic-properties.html#peppered-moths-e--and-m-steps"><i class="fa fa-check"></i><b>9.1.4</b> Peppered Moths E- and M-steps</a></li>
<li class="chapter" data-level="9.1.5" data-path="basic-properties.html"><a href="basic-properties.html#inside-the-em"><i class="fa fa-check"></i><b>9.1.5</b> Inside the EM</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="EM-exp.html"><a href="EM-exp.html"><i class="fa fa-check"></i><b>9.2</b> Exponential families</a></li>
<li class="chapter" data-level="9.3" data-path="fisher-information.html"><a href="fisher-information.html"><i class="fa fa-check"></i><b>9.3</b> Fisher information</a></li>
<li class="chapter" data-level="9.4" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html"><i class="fa fa-check"></i><b>9.4</b> Two examples revisited</a><ul>
<li class="chapter" data-level="9.4.1" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html#gaussian-mixtures-1"><i class="fa fa-check"></i><b>9.4.1</b> Gaussian mixtures</a></li>
<li class="chapter" data-level="9.4.2" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html#gaussian-state-space"><i class="fa fa-check"></i><b>9.4.2</b> Gaussian state space</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="stochastic-optimization.html"><a href="stochastic-optimization.html"><i class="fa fa-check"></i><b>10</b> Stochastic Optimization</a><ul>
<li class="chapter" data-level="10.1" data-path="stochastic-gradient.html"><a href="stochastic-gradient.html"><i class="fa fa-check"></i><b>10.1</b> Stochastic gradient</a></li>
<li class="chapter" data-level="10.2" data-path="stochastic-em.html"><a href="stochastic-em.html"><i class="fa fa-check"></i><b>10.2</b> Stochastic EM</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="basic-properties" class="section level2">
<h2><span class="header-section-number">9.1</span> Basic properties</h2>
<div id="incomplete-data-likelihood" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Incomplete data likelihood</h3>
<p>Suppose that <span class="math inline">\(Y\)</span> is a random variable and <span class="math inline">\(X = M(Y)\)</span>. Suppose that <span class="math inline">\(Y\)</span> has density <span class="math inline">\(f(\cdot \mid \theta)\)</span> and that <span class="math inline">\(X\)</span> has marginal density <span class="math inline">\(g(x \mid \theta)\)</span>.</p>
<p>The marginal density is typically of the form <span class="math display">\[g(x \mid \theta) = \int_{\{y: M(y) = x\}} f(y \mid \theta) \ \mu_x(\mathrm{d} y)\]</span> for a suitable measure <span class="math inline">\(\mu_x\)</span> depending on <span class="math inline">\(M\)</span> and <span class="math inline">\(x\)</span> but not <span class="math inline">\(\theta\)</span>. The general argument for the marginal density relies on the coarea formula.</p>
<p>The log-likelihood for observing <span class="math inline">\(X = x\)</span> is <span class="math display">\[\ell(\theta) = \log g(x \mid \theta).\]</span> The marginal likelihood is often impossible to compute analytically and difficult and expensive to compute numerically. The complete log-likelihood, <span class="math inline">\(\log f(y \mid \theta)\)</span>, is often easy to compute, but we don’t know <span class="math inline">\(Y\)</span>, only that <span class="math inline">\(M(Y) = x\)</span>.</p>
<p>In some cases it is possible to compute <span class="math display">\[Q(\theta \mid \theta&#39;) := E_{\theta&#39;}(\log f(Y \mid \theta) \mid X = x),\]</span> which is the conditional expectation of the complete log-likelihood given the observed data and computed using the probability measure given by <span class="math inline">\(\theta&#39;\)</span>. Thus for fixed <span class="math inline">\(\theta&#39;\)</span> this is a computable function of <span class="math inline">\(\theta\)</span> depending only on the observed data <span class="math inline">\(x\)</span>.</p>
<p>One could get the following idea: with an initial guess of <span class="math inline">\(\theta&#39; = \theta^{(0)}\)</span> compute iteratively <span class="math display">\[\theta^{(t + 1)} = \textrm{arg max} \ Q(\theta \mid \theta^{(t)})\]</span> for <span class="math inline">\(t = 0, 1, 2, \ldots\)</span>. This idea is the EM-algorithm:</p>
<ul>
<li><strong>E-step</strong>: Compute the conditional expectation <span class="math inline">\(Q(\theta \mid \theta^{(t)})\)</span>.</li>
<li><strong>M-step</strong>: Maximize <span class="math inline">\(\theta \mapsto Q(\theta \mid \theta^{(t)})\)</span>.</li>
</ul>
<p>It is a bit weird to present the algorithm as a two-step algorithm in its abstract formulation. Even though we can regard <span class="math inline">\(Q(\theta \mid \theta^{(t)})\)</span> as something we can compute abstractly for each <span class="math inline">\(\theta\)</span> for a given <span class="math inline">\(\theta^{(t)}\)</span>, the maximization is in practice not really done using all these evaluations. It is computed either by an analytic formula involving <span class="math inline">\(x\)</span> and <span class="math inline">\(\theta^{(t)}\)</span>, or by a numerical algorithm that computes certain evaluations of <span class="math inline">\(Q( \cdot \mid \theta^{(t)})\)</span> and perhaps its gradient and Hessian. In computing these specific evaluations there is, of course, a need for the computation of conditional expectations, but we would compute these as they are needed and not upfront.</p>
<p>However, in some of the most important applications of the EM-algorithm, particularly for exponential families covered in Section <a href="EM-exp.html#EM-exp">9.2</a>, it makes a lot of sense to regard the algorithm as a two-step algorithm. This is the case whenever <span class="math inline">\(Q(\theta \mid \theta^{(t)}) = q(\theta, t(x, \theta^{(t)}))\)</span> is given in terms of <span class="math inline">\(\theta\)</span> and a function <span class="math inline">\(t(x, \theta^{(t)})\)</span> of <span class="math inline">\(x\)</span> and <span class="math inline">\(\theta^{(t)}\)</span> that doesn’t depend on <span class="math inline">\(\theta\)</span>. Then the E-step becomes the computation of <span class="math inline">\(t(x, \theta^{(t)})\)</span>, and in the M-step, <span class="math inline">\(Q(\cdot \mid \theta^{(t)})\)</span> is maximized by maximizing <span class="math inline">\(q(\cdot, t(x, \theta^{(t)}))\)</span>, and the maximum is a function of <span class="math inline">\(t(x, \theta^{(t)})\)</span>.</p>
</div>
<div id="the-em-algorithm-is-ascending" class="section level3">
<h3><span class="header-section-number">9.1.2</span> The EM-algorithm is ascending</h3>
<p>We prove below that the algorithm is an ascent algorithm; it (weakly) increases the marginal likelihood in every step.</p>
<p>THEOREM AND PROOF</p>
<p>It holds in great generality that the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span> has density <span class="math display">\[h(y \mid x, \theta) = \frac{f(y \mid \theta)}{g(x \mid \theta)}\]</span> w.r.t. a suitable measure <span class="math inline">\(\mu_x\)</span> that does not depend upon <span class="math inline">\(\theta\)</span>.</p>
<p>You can verify this for discrete distributions and when <span class="math inline">\(Y = (Z, X)\)</span> with joint density w.r.t. a product measure <span class="math inline">\(\mu \otimes \nu\)</span> that does not depend upon <span class="math inline">\(\theta\)</span>.</p>
<p>In the latter case, <span class="math inline">\(f(y \mid \theta) = f(z, x \mid \theta)\)</span> and <span class="math display">\[g(x \mid \theta) = \int f(z, x \mid \theta) \ \mu(\mathrm{d} z)\]</span> is the marginal density w.r.t. <span class="math inline">\(\nu\)</span>.</p>
<p>In general <span class="math display">\[\log g(x \mid \theta) = \log f(y \mid \theta) - \log h(y \mid x, \theta),\]</span> and under some integrability conditions, this decomposition is used to show that the EM-algorithm increases the likelihood, <span class="math inline">\(\ell(\theta)\)</span>, in each iteration.</p>
</div>
<div id="multinomial-cell-collapsing-1" class="section level3">
<h3><span class="header-section-number">9.1.3</span> Multinomial cell collapsing</h3>
<p>The EM-algorithm can be implemented by two simple functions that compute the conditional expectations above (the E-step) and then maximization of the complete observation log-likelihood.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">EStep0 &lt;-<span class="st"> </span><span class="cf">function</span>(p, x, group) {
  pp &lt;-<span class="st"> </span><span class="kw">prob</span>(p)
  x[group] <span class="op">*</span><span class="st"> </span>pp <span class="op">/</span><span class="st"> </span><span class="kw">M</span>(pp, group)[group]
}</code></pre></div>
<p>The MLE of the complete log-likelihood is a linear estimator, as is the case in many examples with explicit MLEs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MStep0 &lt;-<span class="st"> </span><span class="cf">function</span>(n, X)
  <span class="kw">as.vector</span>(X <span class="op">%*%</span><span class="st"> </span>n <span class="op">/</span><span class="st"> </span>(<span class="kw">sum</span>(n)))</code></pre></div>
<p>The <code>EStep0</code> and <code>MStep0</code> functions are abstract implementations. They require specification of the arguments <code>group</code> and <code>X</code>, respectively, to become concrete.</p>
</div>
<div id="peppered-moths-e--and-m-steps" class="section level3">
<h3><span class="header-section-number">9.1.4</span> Peppered Moths E- and M-steps</h3>
<p>Concrete functions for the E- and M-steps are implemented for the particular example.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">EStep &lt;-<span class="st"> </span><span class="cf">function</span>(p, x)
  <span class="kw">EStep0</span>(p, x, <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>))

MStep &lt;-<span class="st"> </span><span class="cf">function</span>(n) {
  X &lt;-<span class="st"> </span><span class="kw">matrix</span>(
  <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,
    <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>,
  <span class="dv">2</span>, <span class="dv">6</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
  <span class="kw">MStep0</span>(n, X)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">EM &lt;-<span class="st"> </span><span class="cf">function</span>(par, x, <span class="dt">tmax =</span> <span class="dv">10</span>) {
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>tmax) 
    par &lt;-<span class="st"> </span><span class="kw">MStep</span>(<span class="kw">EStep</span>(par, x))
  par  ## Remember to return the parameter estimate
}
  
phat &lt;-<span class="st"> </span><span class="kw">EM</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>))
phat</code></pre></div>
<pre><code>## [1] 0.07083691 0.18873652</code></pre>
</div>
<div id="inside-the-em" class="section level3">
<h3><span class="header-section-number">9.1.5</span> Inside the EM</h3>
<p>Check what is going on in each step of the EM-algorithm.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">conv_diag &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> <span class="dv">10</span>, <span class="dt">ncol =</span> <span class="dv">5</span>)
<span class="kw">colnames</span>(conv_diag) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;t&quot;</span>, <span class="st">&quot;pC&quot;</span>, <span class="st">&quot;pI&quot;</span>, <span class="st">&quot;R&quot;</span>, <span class="st">&quot;loglik&quot;</span>)
par &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>)
<span class="cf">for</span>(t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) {
    par0 &lt;-<span class="st"> </span>par
    par &lt;-<span class="st"> </span><span class="kw">EM</span>(par0, <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), <span class="dt">tmax =</span> <span class="dv">1</span>)
    R &lt;-<span class="st"> </span>par <span class="op">-</span><span class="st"> </span>par0
    R &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">t</span>(R) <span class="op">%*%</span><span class="st"> </span>R <span class="op">/</span><span class="st"> </span>(<span class="kw">t</span>(par0) <span class="op">%*%</span><span class="st"> </span>par0))
    conv_diag[t, ] &lt;-<span class="st"> </span><span class="kw">c</span>(t, par[<span class="dv">1</span>], par[<span class="dv">2</span>], R, 
                        <span class="kw">loglik</span>(par[<span class="op">-</span><span class="dv">3</span>], <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>)))
}</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">t</th>
<th align="right">pC</th>
<th align="right">pI</th>
<th align="right">R</th>
<th align="right">loglik</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0.0803859</td>
<td align="right">0.2246419</td>
<td align="right">0.5472619</td>
<td align="right">605.7930</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0.0711893</td>
<td align="right">0.1954696</td>
<td align="right">0.1282007</td>
<td align="right">600.6372</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">0.0708498</td>
<td align="right">0.1899339</td>
<td align="right">0.0266600</td>
<td align="right">600.4859</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">0.0708374</td>
<td align="right">0.1889476</td>
<td align="right">0.0048661</td>
<td align="right">600.4811</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0.0708369</td>
<td align="right">0.1887737</td>
<td align="right">0.0008619</td>
<td align="right">600.4810</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0.0708369</td>
<td align="right">0.1887430</td>
<td align="right">0.0001518</td>
<td align="right">600.4810</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">0.0708369</td>
<td align="right">0.1887377</td>
<td align="right">0.0000267</td>
<td align="right">600.4810</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">0.0708369</td>
<td align="right">0.1887367</td>
<td align="right">0.0000047</td>
<td align="right">600.4810</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">0.0708369</td>
<td align="right">0.1887366</td>
<td align="right">0.0000008</td>
<td align="right">600.4810</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">0.0708369</td>
<td align="right">0.1887365</td>
<td align="right">0.0000001</td>
<td align="right">600.4810</td>
</tr>
</tbody>
</table>
<p><img src="CSwR_files/figure-html/moth_EM_figure-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Note the log-axis. The EM-algorithm converges rapidly in this example. This is not always the case. If the log-likelihood is flat, the EM-algorithm can become quite slow.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="EM.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="EM-exp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

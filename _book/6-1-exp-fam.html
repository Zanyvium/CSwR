<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.1 Exponential families | Computational Statistics with R</title>
  <meta name="description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="6.1 Exponential families | Computational Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.1 Exponential families | Computational Statistics with R" />
  
  <meta name="twitter:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

<meta name="author" content="Niels Richard Hansen" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="6-four-examples.html"/>
<link rel="next" href="6-2-multinomial-models.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-intro.html"><a href="1-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html"><i class="fa fa-check"></i><b>1.1</b> Smoothing</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#intro-angles"><i class="fa fa-check"></i><b>1.1.1</b> Angle distributions in proteins</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#using-ggplot2"><i class="fa fa-check"></i><b>1.1.2</b> Using ggplot2</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#changing-the-defaults"><i class="fa fa-check"></i><b>1.1.3</b> Changing the defaults</a></li>
<li class="chapter" data-level="1.1.4" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#multivariate-smoothing"><i class="fa fa-check"></i><b>1.1.4</b> Multivariate methods</a></li>
<li class="chapter" data-level="1.1.5" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#large-scale-smoothing"><i class="fa fa-check"></i><b>1.1.5</b> Large scale smoothing</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#vM"><i class="fa fa-check"></i><b>1.2.1</b> Univariate von Mises distributions</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#mixtures-of-von-mises-distributions"><i class="fa fa-check"></i><b>1.2.2</b> Mixtures of von Mises distributions</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#large-scale-monte-carlo-methods"><i class="fa fa-check"></i><b>1.2.3</b> Large scale Monte Carlo methods</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-optimization.html"><a href="1-3-optimization.html"><i class="fa fa-check"></i><b>1.3</b> Optimization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-optimization.html"><a href="1-3-optimization.html#the-em-algorithm"><i class="fa fa-check"></i><b>1.3.1</b> The EM-algorithm</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-3-optimization.html"><a href="1-3-optimization.html#large-scale-optimization"><i class="fa fa-check"></i><b>1.3.2</b> Large scale optimization</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Smoothing</b></span></li>
<li class="chapter" data-level="2" data-path="2-density.html"><a href="2-density.html"><i class="fa fa-check"></i><b>2</b> Density estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-unidens.html"><a href="2-1-unidens.html"><i class="fa fa-check"></i><b>2.1</b> Univariate density estimation</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html"><i class="fa fa-check"></i><b>2.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#benchmarking"><i class="fa fa-check"></i><b>2.2.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html"><i class="fa fa-check"></i><b>2.3</b> Bandwidth selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#rectangular"><i class="fa fa-check"></i><b>2.3.1</b> Revisiting the rectangular kernel</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#ise-mise-and-mse-for-kernel-estimators"><i class="fa fa-check"></i><b>2.3.2</b> ISE, MISE and MSE for kernel estimators</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#plug-in-estimation-of-the-oracle-bandwidth"><i class="fa fa-check"></i><b>2.3.3</b> Plug-in estimation of the oracle bandwidth</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#cv"><i class="fa fa-check"></i><b>2.3.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-likelihood.html"><a href="2-4-likelihood.html"><i class="fa fa-check"></i><b>2.4</b> Likelihood considerations</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-4-likelihood.html"><a href="2-4-likelihood.html#sieves"><i class="fa fa-check"></i><b>2.4.1</b> Method of sieves</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-4-likelihood.html"><a href="2-4-likelihood.html#basis-density"><i class="fa fa-check"></i><b>2.4.2</b> Basis expansions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-5-density-ex.html"><a href="2-5-density-ex.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="2-5-density-ex.html"><a href="2-5-density-ex.html#kernel-density-estimation"><i class="fa fa-check"></i>Kernel density estimation</a></li>
<li class="chapter" data-level="" data-path="2-5-density-ex.html"><a href="2-5-density-ex.html#benchmarking-1"><i class="fa fa-check"></i>Benchmarking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-bivariate.html"><a href="3-bivariate.html"><i class="fa fa-check"></i><b>3</b> Bivariate smoothing</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html"><i class="fa fa-check"></i><b>3.1</b> Nearest neighbor smoothers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#linear-smoothers"><i class="fa fa-check"></i><b>3.1.1</b> Linear smoothers</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#implementing-the-running-mean"><i class="fa fa-check"></i><b>3.1.2</b> Implementing the running mean</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#choose-k-by-cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Choose <span class="math inline">\(k\)</span> by cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html"><i class="fa fa-check"></i><b>3.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html#nadarayawatson-kernel-smoothing"><i class="fa fa-check"></i><b>3.2.1</b> Nadaraya–Watson kernel smoothing</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html#local-regression-smoothers"><i class="fa fa-check"></i><b>3.2.2</b> Local regression smoothers</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-sparse-linear-algebra.html"><a href="3-3-sparse-linear-algebra.html"><i class="fa fa-check"></i><b>3.3</b> Sparse linear algebra</a></li>
<li class="chapter" data-level="3.4" data-path="3-4-onb.html"><a href="3-4-onb.html"><i class="fa fa-check"></i><b>3.4</b> Orthogonal basis expansions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-4-onb.html"><a href="3-4-onb.html#polynomial-expansions"><i class="fa fa-check"></i><b>3.4.1</b> Polynomial expansions</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-4-onb.html"><a href="3-4-onb.html#fourier-expansions"><i class="fa fa-check"></i><b>3.4.2</b> Fourier expansions</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-5-splines.html"><a href="3-5-splines.html"><i class="fa fa-check"></i><b>3.5</b> Splines</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-5-splines.html"><a href="3-5-splines.html#smoothing-splines"><i class="fa fa-check"></i><b>3.5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-5-splines.html"><a href="3-5-splines.html#splines-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Splines in R</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-5-splines.html"><a href="3-5-splines.html#efficient-splines"><i class="fa fa-check"></i><b>3.5.3</b> Efficient computation with splines</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-6-gaussian-processes.html"><a href="3-6-gaussian-processes.html"><i class="fa fa-check"></i><b>3.6</b> Gaussian processes</a></li>
<li class="chapter" data-level="3.7" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html"><i class="fa fa-check"></i><b>3.7</b> The Kalman filter</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#ar1-example"><i class="fa fa-check"></i><b>3.7.1</b> AR(1)-example</a></li>
<li class="chapter" data-level="3.7.2" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#the-kalman-smoother"><i class="fa fa-check"></i><b>3.7.2</b> The Kalman smoother</a></li>
<li class="chapter" data-level="3.7.3" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#implementation"><i class="fa fa-check"></i><b>3.7.3</b> Implementation</a></li>
<li class="chapter" data-level="3.7.4" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#the-kalman-filter-1"><i class="fa fa-check"></i><b>3.7.4</b> The Kalman filter</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3-8-bivariate-ex.html"><a href="3-8-bivariate-ex.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="3-8-bivariate-ex.html"><a href="3-8-bivariate-ex.html#nearest-neighbors"><i class="fa fa-check"></i>Nearest neighbors</a></li>
<li class="chapter" data-level="" data-path="3-8-bivariate-ex.html"><a href="3-8-bivariate-ex.html#kernel-estimators"><i class="fa fa-check"></i>Kernel estimators</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Monte Carlo Methods</b></span></li>
<li class="chapter" data-level="4" data-path="4-univariate-random-variables.html"><a href="4-univariate-random-variables.html"><i class="fa fa-check"></i><b>4</b> Univariate random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html"><i class="fa fa-check"></i><b>4.1</b> Pseudorandom number generators</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html#implementing-a-pseudorandom-number-generator"><i class="fa fa-check"></i><b>4.1.1</b> Implementing a pseudorandom number generator</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html#pseudorandom-number-packages"><i class="fa fa-check"></i><b>4.1.2</b> Pseudorandom number packages</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-2-transformation-techniques.html"><a href="4-2-transformation-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Transformation techniques</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-2-transformation-techniques.html"><a href="4-2-transformation-techniques.html#sampling-from-a-t-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Sampling from a <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html#vMsim"><i class="fa fa-check"></i><b>4.3.1</b> von Mises distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html"><i class="fa fa-check"></i><b>4.4</b> Adaptive envelopes</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html#beta-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Beta distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html#von-mises-distribution"><i class="fa fa-check"></i><b>4.4.2</b> von Mises distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-5-univariate-ex.html"><a href="4-5-univariate-ex.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a><ul>
<li class="chapter" data-level="4.5.1" data-path="4-5-univariate-ex.html"><a href="4-5-univariate-ex.html#rejection-sampling-of-gaussian-random-variables"><i class="fa fa-check"></i><b>4.5.1</b> Rejection sampling of Gaussian random variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-mci.html"><a href="5-mci.html"><i class="fa fa-check"></i><b>5</b> Monte Carlo integration</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-assessment.html"><a href="5-1-assessment.html"><i class="fa fa-check"></i><b>5.1</b> Assessment</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-1-assessment.html"><a href="5-1-assessment.html#CLT-gamma"><i class="fa fa-check"></i><b>5.1.1</b> Using the central limit theorem</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-1-assessment.html"><a href="5-1-assessment.html#concentration-inequalities"><i class="fa fa-check"></i><b>5.1.2</b> Concentration inequalities</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-1-assessment.html"><a href="5-1-assessment.html#exponential-tail-bound-for-gamma-distributed-variables"><i class="fa fa-check"></i><b>5.1.3</b> Exponential tail bound for Gamma distributed variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html"><i class="fa fa-check"></i><b>5.2</b> Importance sampling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html#unknown-normalization-constants"><i class="fa fa-check"></i><b>5.2.1</b> Unknown normalization constants</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html#hd-int"><i class="fa fa-check"></i><b>5.2.2</b> Computing a high-dimensional integral</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-3-network.html"><a href="5-3-network.html"><i class="fa fa-check"></i><b>5.3</b> Network failure</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-3-network.html"><a href="5-3-network.html#object-oriented-implementations"><i class="fa fa-check"></i><b>5.3.1</b> Object oriented implementations</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part III: Optimization</b></span></li>
<li class="chapter" data-level="6" data-path="6-four-examples.html"><a href="6-four-examples.html"><i class="fa fa-check"></i><b>6</b> Four Examples</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html"><i class="fa fa-check"></i><b>6.1</b> Exponential families</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#full-exponential-families"><i class="fa fa-check"></i><b>6.1.1</b> Full exponential families</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#bayes-net"><i class="fa fa-check"></i><b>6.1.2</b> Exponential family Bayesian networks</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#exp-fam-deriv"><i class="fa fa-check"></i><b>6.1.3</b> Likelihood computations</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#curved-exponential-families"><i class="fa fa-check"></i><b>6.1.4</b> Curved exponential families</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-2-multinomial-models.html"><a href="6-2-multinomial-models.html"><i class="fa fa-check"></i><b>6.2</b> Multinomial models</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-2-multinomial-models.html"><a href="6-2-multinomial-models.html#pep-moth"><i class="fa fa-check"></i><b>6.2.1</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-3-regression.html"><a href="6-3-regression.html"><i class="fa fa-check"></i><b>6.3</b> Regression models</a></li>
<li class="chapter" data-level="6.4" data-path="6-4-finite-mixture-models.html"><a href="6-4-finite-mixture-models.html"><i class="fa fa-check"></i><b>6.4</b> Finite mixture models</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-4-finite-mixture-models.html"><a href="6-4-finite-mixture-models.html#Gaus-mix-ex"><i class="fa fa-check"></i><b>6.4.1</b> Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-5-mixed-models.html"><a href="6-5-mixed-models.html"><i class="fa fa-check"></i><b>6.5</b> Mixed models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-numopt.html"><a href="7-numopt.html"><i class="fa fa-check"></i><b>7</b> Numerical optimization</a><ul>
<li class="chapter" data-level="7.1" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html"><i class="fa fa-check"></i><b>7.1</b> Algorithms and convergence</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#descent-algorithms"><i class="fa fa-check"></i><b>7.1.1</b> Descent algorithms</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#maps-and-fixed-points"><i class="fa fa-check"></i><b>7.1.2</b> Maps and fixed points</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#convergence-rate"><i class="fa fa-check"></i><b>7.1.3</b> Convergence rate</a></li>
<li class="chapter" data-level="7.1.4" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#stopping-criteria"><i class="fa fa-check"></i><b>7.1.4</b> Stopping criteria</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html"><i class="fa fa-check"></i><b>7.2</b> Descent direction algorithms</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#line-search"><i class="fa fa-check"></i><b>7.2.1</b> Line search</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>7.2.2</b> Gradient descent</a></li>
<li class="chapter" data-level="7.2.3" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#conjugate-gradients"><i class="fa fa-check"></i><b>7.2.3</b> Conjugate gradients</a></li>
<li class="chapter" data-level="7.2.4" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#pep-moth-descent"><i class="fa fa-check"></i><b>7.2.4</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-3-Newton.html"><a href="7-3-Newton.html"><i class="fa fa-check"></i><b>7.3</b> Newton-type algorithms</a><ul>
<li class="chapter" data-level="7.3.1" data-path="7-3-Newton.html"><a href="7-3-Newton.html#poisson-regression"><i class="fa fa-check"></i><b>7.3.1</b> Poisson regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="7-3-Newton.html"><a href="7-3-Newton.html#quasi-newton-algorithms"><i class="fa fa-check"></i><b>7.3.2</b> Quasi-Newton algorithms</a></li>
<li class="chapter" data-level="7.3.3" data-path="7-3-Newton.html"><a href="7-3-Newton.html#sparsity"><i class="fa fa-check"></i><b>7.3.3</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7-4-misc-.html"><a href="7-4-misc-.html"><i class="fa fa-check"></i><b>7.4</b> Misc.</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-em.html"><a href="8-em.html"><i class="fa fa-check"></i><b>8</b> Expectation maximization algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html"><i class="fa fa-check"></i><b>8.1</b> Basic properties</a><ul>
<li class="chapter" data-level="8.1.1" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#incomplete-data-likelihood"><i class="fa fa-check"></i><b>8.1.1</b> Incomplete data likelihood</a></li>
<li class="chapter" data-level="8.1.2" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#monotonicity-of-the-em-algorithm"><i class="fa fa-check"></i><b>8.1.2</b> Monotonicity of the EM algorithm</a></li>
<li class="chapter" data-level="8.1.3" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#peppered-moths"><i class="fa fa-check"></i><b>8.1.3</b> Peppered moths</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8-2-EM-exp.html"><a href="8-2-EM-exp.html"><i class="fa fa-check"></i><b>8.2</b> Exponential families</a></li>
<li class="chapter" data-level="8.3" data-path="8-3-fisher-information.html"><a href="8-3-fisher-information.html"><i class="fa fa-check"></i><b>8.3</b> Fisher information</a></li>
<li class="chapter" data-level="8.4" data-path="8-4-revisiting-gaussian-mixtures.html"><a href="8-4-revisiting-gaussian-mixtures.html"><i class="fa fa-check"></i><b>8.4</b> Revisiting Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-StochOpt.html"><a href="9-StochOpt.html"><i class="fa fa-check"></i><b>9</b> Stochastic Optimization</a><ul>
<li class="chapter" data-level="9.1" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html"><i class="fa fa-check"></i><b>9.1</b> Stochastic gradient algorithms</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#section"><i class="fa fa-check"></i><b>9.1.1</b> </a></li>
<li class="chapter" data-level="9.1.2" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#online-stochastic-gradient-descent"><i class="fa fa-check"></i><b>9.1.2</b> Online stochastic gradient descent</a></li>
<li class="chapter" data-level="9.1.3" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>9.1.3</b> Stochastic gradient descent</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9-2-nonlinear-least-squares.html"><a href="9-2-nonlinear-least-squares.html"><i class="fa fa-check"></i><b>9.2</b> Nonlinear least squares</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-app-R.html"><a href="A-app-R.html"><i class="fa fa-check"></i><b>A</b> R programming</a><ul>
<li class="chapter" data-level="A.1" data-path="A-1-functions.html"><a href="A-1-functions.html"><i class="fa fa-check"></i><b>A.1</b> Functions</a><ul>
<li class="chapter" data-level="A.1.1" data-path="A-1-functions.html"><a href="A-1-functions.html#vectorization"><i class="fa fa-check"></i><b>A.1.1</b> Vectorization</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="A-2-objects-and-methods.html"><a href="A-2-objects-and-methods.html"><i class="fa fa-check"></i><b>A.2</b> Objects and methods</a></li>
<li class="chapter" data-level="A.3" data-path="A-3-environments.html"><a href="A-3-environments.html"><i class="fa fa-check"></i><b>A.3</b> Environments</a><ul>
<li class="chapter" data-level="A.3.1" data-path="A-3-environments.html"><a href="A-3-environments.html#function-factories"><i class="fa fa-check"></i><b>A.3.1</b> Function factories</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="A-4-performance.html"><a href="A-4-performance.html"><i class="fa fa-check"></i><b>A.4</b> Performance</a><ul>
<li class="chapter" data-level="A.4.1" data-path="A-4-performance.html"><a href="A-4-performance.html#parallel-computations"><i class="fa fa-check"></i><b>A.4.1</b> Parallel computations</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html"><i class="fa fa-check"></i><b>A.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#functions-1"><i class="fa fa-check"></i>Functions</a></li>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#histograms-with-non-equidistant-breaks"><i class="fa fa-check"></i>Histograms with non-equidistant breaks</a></li>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#functions-and-objects"><i class="fa fa-check"></i>Functions and objects</a></li>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#functions-and-environments"><i class="fa fa-check"></i>Functions and environments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="exp-fam" class="section level2">
<h2><span class="header-section-number">6.1</span> Exponential families</h2>
<p>This section introduces exponential families in a concise way. The
crucial observation is that the log-likelihood is concave, and that
we can derive general formulas for derivatives. This
will be important for the optimization algorithms developed
later for computing maximum-likelihood estimates and
for answering standard asymptotic inference questions.</p>
<p>The exponential families are extremely well behaved from
a mathematical as well as a computational viewpoint, but they may
be inadequate for modeling data in some cases. A
typical practical problem is that there is heterogeneous variation in data
beyond what can be captured by any single exponential family.
A fairly common technique is then to build an exponential family
model of the observed variables <em>as well as some latent variables</em>.
The latent variables then serve the purpose of modeling the
heterogeneity. The resulting model of the observed variables is
consequently the marginalization of an exponential family, which is
generally not an exponential family and in many ways
less well behaved. It is nevertheless possible to exploit the exponential
family structure underlying the marginalized model for many
computations of statistical importance. The EM-algorithm as
treated in Chapter <a href="8-em.html#em">8</a> is one particularly good example, but
Bayesian computations can in similar ways exploit the structure.</p>
<div id="full-exponential-families" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Full exponential families</h3>
<p>In this section we consider statistical models on an abstract
product sample space
<span class="math display">\[\mathcal{Y} = \mathcal{Y}_1 \times \ldots \times \mathcal{Y}_m.\]</span>
We will be interested in models of
observations <span class="math inline">\(y_1 \in \mathcal{Y}_1, \ldots, y_m \in \mathcal{Y}_m\)</span>
that are independent but not necessarily identically distributed.</p>
<p>An exponential family is defined in terms of two ingredients:</p>
<ul>
<li>maps <span class="math inline">\(t_j : \mathcal{Y}_j \to \mathbb{R}^p\)</span> for <span class="math inline">\(j = 1, \ldots, m\)</span>,</li>
<li>and non-trivial <span class="math inline">\(\sigma\)</span>-finite measures <span class="math inline">\(\nu_j\)</span> on <span class="math inline">\(\mathcal{Y}_j\)</span> for <span class="math inline">\(j = 1, \ldots, m\)</span>.</li>
</ul>
<p>The maps <span class="math inline">\(t_j\)</span> are called <em>sufficient statistics</em>, and in terms
of these and the <em>base measures</em> <span class="math inline">\(\nu_j\)</span> we define
<span class="math display">\[\varphi_j(\theta) = \int e^{\theta^T t_j(u)} \nu_j(\mathrm{d}u).\]</span>
These functions are well defined as functions
<span class="math display">\[\varphi_j : \mathbb{R}^p \to (0,\infty].\]</span>
We define
<span class="math display">\[\Theta_j = \mathrm{int}(\{ \theta \in \mathbb{R}^p \mid \varphi_j(\theta) &lt; \infty \}),\]</span>
which by definition is an open set as. It can be shown
that <span class="math inline">\(\Theta_j\)</span> is convex and that <span class="math inline">\(\varphi_j\)</span> is a log-convex function.
Defining
<span class="math display">\[\Theta = \bigcap_{j=1}^m \Theta_j,\]</span>
then <span class="math inline">\(\Theta\)</span> is likewise open and convex, and we define the <em>exponential family</em>
as the distributions parametrized by <span class="math inline">\(\theta \in \Theta\)</span> that have densities</p>
<p><span class="math display" id="eq:exp-dens">\[\begin{equation}
f(\mathbf{y} \mid \theta) = \prod_{j=1}^m \frac{1}{\varphi_j(\theta)} e^{\theta^T t_j(y_j)} = e^{\theta^T \sum_{j=1}^m t_j(y_j) - \sum_{j=1}^m \log \varphi_j(\theta)}, \quad \mathbf{y} \in \mathcal{Y},
\tag{6.1}
\end{equation}\]</span></p>
<p>w.r.t. <span class="math inline">\(\otimes_{j=1}^m \nu_j\)</span>. The
case where <span class="math inline">\(\Theta = \emptyset\)</span> is of no interest, and we will thus assume
that the parameter set <span class="math inline">\(\Theta\)</span> is non-empty. The parameter <span class="math inline">\(\theta\)</span> is called
the <em>canonical parameter</em> and <span class="math inline">\(\Theta\)</span> is the canonical parameter space.
We may also say that the exponential family is canonically parametrized by
<span class="math inline">\(\theta\)</span>. It is important to realize that an exponential family may come
with a non-canonical parametrization that doesn’t reveal right away that
it is an exponential family. Thus a bit of work is then needed to show
that the parametrized family of distributions can, indeed, be reparametrized
as an exponential family. In the non-canonical parametrization, the
family is then an example of a <em>curved exponential family</em> as defined below.</p>

<div class="example">
<p><span id="exm:von-Mises-exponential" class="example"><strong>Example 6.1  </strong></span>The von Mises distributions on <span class="math inline">\(\mathcal{Y} = (-\pi, \pi]\)</span> form an exponential family
with <span class="math inline">\(m = 1\)</span>. The sufficient statistic <span class="math inline">\(t_1 : (-\pi, \pi] \mapsto \mathbb{R}^2\)</span>
is
<span class="math display">\[t_1(y) = \left(\begin{array}{c} \cos(y) \\ \sin(y) \end{array}\right),\]</span>
and
<span class="math display">\[\varphi(\theta) = \int_{-\pi}^{\pi} e^{\theta_1 \cos(u) + \theta_2 \sin(u)} \mathrm{d}u &lt; \infty\]</span>
for all <span class="math inline">\(\theta = (\theta_1, \theta_2)^T \in \mathbb{R}^2\)</span>. Thus
the canonical parameter space is <span class="math inline">\(\Theta = \mathbb{R}^2\)</span>.</p>
<p>As mentioned in Section <a href="1-2-monte-carlo-methods.html#vM">1.2.1</a>, the function <span class="math inline">\(\varphi(\theta)\)</span> can be
expressed in terms of a modified Bessel function, but it doesn’t have
an expression in terms of elementary functions. Likewise in Section <a href="1-2-monte-carlo-methods.html#vM">1.2.1</a>,
an alternative parametrization (polar coordinates) was given;
<span class="math display">\[(\kappa, \mu) \mapsto \theta = \kappa \left(\begin{array}{c} \cos(\mu) \\ \sin(\mu) \end{array}\right)\]</span>
that maps <span class="math inline">\([0,\infty) \times (-\pi, \pi]\)</span> onto <span class="math inline">\(\Theta\)</span>. The von Mises
distributions form a curved exponential family in the
<span class="math inline">\((\kappa, \mu)\)</span>-parametrization, but this parametrization has several problems.
First, the <span class="math inline">\(\mu\)</span> parameter is not identifiable if <span class="math inline">\(\kappa = 0\)</span>, which is
reflected by the fact that the reparametrization is not a one-to-one map.
Second, the parameter space is not open, which can be quite a nuisance
for e.g. maxmimum-likelihood estimation. We could circumvent these problems by restricting
attention to <span class="math inline">\((\kappa, \mu) \in (0,\infty) \times (-\pi, \pi)\)</span>, but
we would then miss some of the distributions in the exponential family –
notably the uniform distribution corresponding to <span class="math inline">\(\theta = 0\)</span>. In conclusion,
the canonical parametrization of the family of distributions as an
exponential family is preferable for mathematical and computational reasons.</p>
</div>


<div class="example">
<p><span id="exm:gaussian-exponential" class="example"><strong>Example 6.2  </strong></span>The family of Gaussian distributions on <span class="math inline">\(\mathbb{R}\)</span> is an example of an exponential
family as defined above with <span class="math inline">\(m = 1\)</span> and <span class="math inline">\(\mathcal{Y} = \mathbb{R}\)</span>.
The density of the <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span> distribution is
<span class="math display">\[\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y - \mu)^2}{2\sigma^2}\right) = 
  \frac{1}{\sqrt{\pi}} \exp\left(\frac{\mu}{\sigma^2} y - \frac{1}{2\sigma^2} y^2 - \frac{\mu^2}{2\sigma^2} - 
                                     \frac{1}{2}\log (2\sigma^2) \right).\]</span>
Letting the base measure <span class="math inline">\(\nu_1\)</span> be Lebesgue measure scaled by <span class="math inline">\(1/\sqrt{\pi}\)</span>,
and <span class="math inline">\(t_1 : \mathbb{R} \mapsto \mathbb{R}^2\)</span> be
<span class="math display">\[t_1(y) = \left(\begin{array}{c} y \\ - y^2 \end{array}\right)\]</span>
we identify this family of distributions as an exponential family with canonical
parameter
<span class="math display">\[\theta = \left(\begin{array}{c} \frac{\mu}{\sigma^2} \\ \frac{1}{2 \sigma^2} \end{array}\right).\]</span></p>
<p>We can express the mean and variance in terms of <span class="math inline">\(\theta\)</span> as
<span class="math display">\[\sigma^2 = \frac{1}{2\theta_2} \quad \text{and} \quad \mu = \frac{\theta_1}{2\theta_2},\]</span>
and we find that
<span class="math display">\[\log \varphi_1(\theta) = \frac{\mu^2}{2\sigma^2} + \frac{1}{2} \log(2 \sigma^2) = 
  \frac{\theta_1^2}{4\theta_2} - \frac{1}{2} \log \theta_2.\]</span></p>
<p>We note that the reparametrization <span class="math inline">\((\mu, \sigma^2) \mapsto \theta\)</span> maps
<span class="math inline">\(\mathbb{R} \times (0,\infty)\)</span> bijectively onto the open set <span class="math inline">\(\mathbb{R} \times (0,\infty)\)</span>,
and that the formula above for <span class="math inline">\(\log \varphi_1(\theta)\)</span> only holds on this set.
It is natural to ask if the canonical parameter space is actually larger for this
exponential family. That is, is <span class="math inline">\(\varphi_1(\theta) &lt; \infty\)</span> for <span class="math inline">\(\theta_2 \leq 0\)</span>?
To this end observe that if <span class="math inline">\(\theta_2 \leq 0\)</span>
<span class="math display">\[\varphi_1(\theta) = \frac{1}{\sqrt{2\pi}} \int e^{\theta_1 u - \theta_2 \frac{u^2}{2}} \mathrm{d}u
\geq \frac{1}{\sqrt{2\pi}} \int e^{\theta_1 u} \mathrm{d} u = \infty,\]</span>
and we conclude that
<span class="math display">\[\Theta = \mathbb{R} \times (0,\infty).\]</span></p>
<p>The family of Gaussian distributions is an example of a family of distributions
whose commonly used parametrization in terms of mean and variance differs from
the canonical parametrization as an exponential family. The mean and
variance are easy to interpret, but in terms of mean
and variance, the Gaussian distributions form a curved exponential family. For
mathematical and computational purposes the canonical parametrization is preferable.</p>
</div>

<p>For general exponential families it may seem restrictive that all
the sufficient statistics, <span class="math inline">\(t_j\)</span>, take values in the
same <span class="math inline">\(p\)</span>-dimensional space, and that all marginal distributions share a common
parameter vector <span class="math inline">\(\theta\)</span>. This is, however, not a restriction. Say we have
two distributions with sufficient statistics <span class="math inline">\(\tilde{t}_1 : \mathcal{Y}_1 \to \mathbb{R}^{p_1}\)</span>
and <span class="math inline">\(\tilde{t}_2 : \mathcal{Y}_2 \to \mathbb{R}^{p_2}\)</span> and corresponding
parameters <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>, then we construct
<span class="math display">\[t_1(y_1) = \left(\begin{array}{c} \tilde{t}_1(y_1) \\ 0 \end{array}\right) \quad 
\text{and} \quad t_2(y_2) = \left(\begin{array}{c} 0 \\ \tilde{t}_2(y_2) \end{array}\right).\]</span>
Now <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span> both map into <span class="math inline">\(\mathbb{R}^{p}\)</span> with <span class="math inline">\(p = p_1 + p_2\)</span>, and
we can bundle the parameters together into the vector
<span class="math display">\[\theta = \left(\begin{array}{c} \theta_1 \\ \theta_2 \end{array}\right) \in \mathbb{R}^p.\]</span>
Clearly, this construction can be generalized to any number of distributions
and allows us to always assume a common parameter space. The sufficient
statistics then take care of selecting
which of the coordinates in the parameter vector that is used for any
particular marginal distribution.</p>
</div>
<div id="bayes-net" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Exponential family Bayesian networks</h3>
<p>An exponential family is defined above as a parametrized family of distributions
on <span class="math inline">\(\mathcal{Y}\)</span> of <em>independent</em> variables. That is, the joint density
in <a href="6-1-exp-fam.html#eq:exp-dens">(6.1)</a> factorizes w.r.t. a product measure. Without really
changing the notation this assumption can be relaxed considerably.</p>
<p>If the sufficient statistic <span class="math inline">\(t_j\)</span>, instead of being a fixed map, is
allowed to depend on <span class="math inline">\(y_1, \ldots, y_{j-1}\)</span>, <span class="math inline">\(f(\mathbf{y} \mid \theta)\)</span> as
defined in <a href="6-1-exp-fam.html#eq:exp-dens">(6.1)</a>
is still a joint density. The only difference is that the factor
<span class="math display">\[f_j(y_j \mid y_1, \ldots, y_{j-1}, \theta) = e^{\theta^T t_j(y_j) - \log \varphi_j(\theta)}\]</span>
is now the <em>conditional</em> density of <span class="math inline">\(y_j\)</span> given <span class="math inline">\(y_1, \ldots, y_{j-1}\)</span>.
In the notation we let the dependence of <span class="math inline">\(t_j\)</span> on <span class="math inline">\(y_1, \ldots, y_{j-1}\)</span>
be implicit as this will not affect the abstract theory. In any concrete
example though, it will be clear how <span class="math inline">\(t_j\)</span> actually depends upon all, some
or none of these variables. Note that <span class="math inline">\(\varphi_j\)</span> may now also depend
upon the data through <span class="math inline">\(y_1, \ldots, y_{j-1}\)</span>.</p>
<p>The observation above is powerful. It allows us to develop a unified
approach based on exponential families for a majority of all statistical
models that are applied in practice. The models we consider make two essential
assumptions</p>
<ol style="list-style-type: decimal">
<li>the variables that we model can be ordered such that <span class="math inline">\(y_j\)</span> only
depends on <span class="math inline">\(y_1, \ldots, y_{j-1}\)</span> for <span class="math inline">\(j = 1, \ldots, m\)</span>,</li>
<li>all the conditional distributions form exponential families themselves,
with the conditioning variables entering through the <span class="math inline">\(t_j\)</span>-maps.</li>
</ol>
<p>The first of these assumptions is equivalent to the joint distribution
being a Bayesian network, that is, a distribution whose density factorizes
according to a directed acyclic graph. This includes time series models
and hierarchical models. The second assumption is more restrictive, but
is a common practice in applied work. Moreover, as many standard statistical
models of univariate discrete and
continuous variables are, in fact, exponential families, building
up a joint distribution as a Bayesian network via conditional
binomial, Poisson, beta, Gamma and Gaussian distributions among others
is a rather flexible framework, and yet it will
result in a model with a density that factorizes as in <a href="6-1-exp-fam.html#eq:exp-dens">(6.1)</a>.</p>
<p>Bayesian networks is a large and interesting subject in itself, and it is unfair
to gloss over all the details. One of the main points is that for many
computations it is possible to develop efficient algorithms by exploiting the
graph structure. The seminal paper by <span class="citation">Lauritzen and Spiegelhalter (<a href="#ref-Lauritzen:1988" role="doc-biblioref">1988</a>)</span> demonstrated this
for the computation of conditional distributions. The mere fact that
the variables can be ordered in a way that aligns with how
the variables depend on each other is useful, but there can be
many ways to do this, and just specifying an ordering ignores
important details of the graph. It is, however, beyond the scope of
this book to get into the graph algorithms required for a thorough
general treatment of Bayesian networks.</p>
</div>
<div id="exp-fam-deriv" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Likelihood computations</h3>
<p>To simplify the notation we introduce
<span class="math display">\[t(\mathbf{y}) = \sum_{j=1}^m t_j(y_j),\]</span>
which we refer to as the sufficient statistic, and
<span class="math display">\[\kappa(\theta) = \sum_{j=1}^m \log \varphi_j(\theta),\]</span>
which is a convex <span class="math inline">\(C^{\infty}\)</span>-function on <span class="math inline">\(\Theta\)</span>.</p>
<p>Having observed <span class="math inline">\(\mathbf{y} \in \mathcal{Y}\)</span> it is evident that the log-likelihood
for the exponential family specified by <a href="6-1-exp-fam.html#eq:exp-dens">(6.1)</a> is
<span class="math display">\[\ell(\theta) = \log f(\mathbf{y} \mid \theta) = \theta^T t(\mathbf{y}) - \kappa(\theta).\]</span>
From this it follows that the gradient of the log-likelihood is
<span class="math display">\[\nabla \ell(\theta) = t(\mathbf{y}) - \nabla \kappa(\theta)\]</span>
and the Hessian is
<span class="math display">\[D^2 \ell(\theta) = - D^2 \kappa(\theta),\]</span>
which is always negative semidefinite. The maximum-likelihood estimator exists
if and only if there is a solution to the <em>score equation</em>
<span class="math display">\[t(\mathbf{y}) = \nabla \kappa(\theta),\]</span>
and it is unique if there is such a solution, <span class="math inline">\(\hat{\theta}\)</span>, for which
<span class="math inline">\(I(\hat{\theta}) = D^2 \kappa(\hat{\theta})\)</span> is positive definite. We
call <span class="math inline">\(I(\hat{\theta})\)</span> the <em>observed Fisher information</em>.</p>
<p>Note that under the independence assumption,
<span class="math display">\[\nabla \log \varphi_j(\theta) = \frac{1}{\varphi_j(\theta)} \int t_j(u) e^{\theta^T t_j(u)} \nu_i(\mathrm{d} u) = E_{\theta}(t_j(Y)), \]</span>
which means that the score equation can be expressed as
<span class="math display">\[t(\mathbf{y}) = \sum_{j=1}^m E_{\theta}(t_j(Y)).\]</span>
In the Bayesian network setup <span class="math inline">\(\nabla \log \varphi_j(\theta) = E_{\theta}(t_j(Y) \mid Y_1, \ldots, Y_{j-1}),\)</span>
and the score equation is
<span class="math display">\[t(\mathbf{y}) = \sum_{j=1}^m E_{\theta}(t_j(Y) \mid y_1, \ldots, y_{j-1}),\]</span>
which is a bit more complicated as the right-hand-side depends on the
observations.</p>
<p>The definition of an exponential family in Section <a href="6-1-exp-fam.html#exp-fam">6.1</a> encompasses
the situation where <span class="math inline">\(y_1, \ldots, y_m\)</span> are i.i.d. by taking <span class="math inline">\(t_j\)</span> to be
independent of <span class="math inline">\(j\)</span>. In that case, <span class="math inline">\(\kappa(\theta) = m \kappa_1(\theta)\)</span>,
the score equation can be rewritten as
<span class="math display">\[\frac{1}{m} \sum_{j=1}^m t_1(y_j) = \kappa_1(\theta),\]</span>
and the Fisher information becomes
<span class="math display">\[I(\hat{\theta}) = m D^2 \kappa_1(\hat{\theta}).\]</span></p>
<p>However, the point of the general formulation is
that it includes regression models, and, via the Bayesian networks extension
above, models with dependence structures. We could, of course, then
have i.i.d. replications <span class="math inline">\(\mathbf{y}_1, \ldots, \mathbf{y}_n\)</span> of
the entire <span class="math inline">\(m\)</span>-dimensional vector <span class="math inline">\(\mathbf{y}\)</span>, and we would get
the score equation
<span class="math display">\[\frac{1}{n} \sum_{i=1}^n t(\mathbf{y}_i) = \kappa(\theta),\]</span>
and corresponding Fisher information
<span class="math display">\[I(\hat{\theta}) = n D^2 \kappa(\hat{\theta}).\]</span></p>
</div>
<div id="curved-exponential-families" class="section level3">
<h3><span class="header-section-number">6.1.4</span> Curved exponential families</h3>
<p>A <em>curved exponential family</em> consists of an exponential family together with a
<span class="math inline">\(C^2\)</span>-map <span class="math inline">\(\theta : \Psi \to \Theta\)</span> from a set <span class="math inline">\(\Psi \subseteq \mathbb{R}^q\)</span>.
The set <span class="math inline">\(\Psi\)</span> provides a parametrization
of the subset <span class="math inline">\(\theta(\Psi) \subseteq \Theta\)</span> of the full exponential family,
and the log-likelihood in the <span class="math inline">\(\psi\)</span>-parameter is
<span class="math display">\[\ell(\psi) = \theta(\psi)^T t(\mathbf{y}) - \kappa(\theta(\psi)).\]</span>
It has gradient
<span class="math display">\[\nabla \ell(\psi) = D \theta(\psi)^T t(\mathbf{y}) - \nabla \kappa(\theta(\psi)) D \theta(\psi)\]</span>
and Hessian
<span class="math display">\[D^2 \ell(\psi) =  \sum_{k=1}^p D^2\theta_k(\psi) (t(\mathbf{y})_k - \partial_k \kappa(\theta(\psi))) -   
D \theta(\psi)^T D^2 \kappa(\theta(\psi)) D \theta(\psi).\]</span></p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Lauritzen:1988">
<p>Lauritzen, S. L., and D. J. Spiegelhalter. 1988. “Local Computations with Probabilities on Graphical Structures and Their Application to Expert Systems.” <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 50 (2): 157–224.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="6-four-examples.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="6-2-multinomial-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

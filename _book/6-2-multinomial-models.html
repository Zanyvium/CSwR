<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.2 Multinomial models | Computational Statistics with R</title>
  <meta name="description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="6.2 Multinomial models | Computational Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.2 Multinomial models | Computational Statistics with R" />
  
  <meta name="twitter:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

<meta name="author" content="Niels Richard Hansen" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="6-1-exp-fam.html"/>
<link rel="next" href="6-3-regression.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-intro.html"><a href="1-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html"><i class="fa fa-check"></i><b>1.1</b> Smoothing</a><ul>
<li class="chapter" data-level="1.1.1" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#intro-angles"><i class="fa fa-check"></i><b>1.1.1</b> Angle distributions in proteins</a></li>
<li class="chapter" data-level="1.1.2" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#using-ggplot2"><i class="fa fa-check"></i><b>1.1.2</b> Using ggplot2</a></li>
<li class="chapter" data-level="1.1.3" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#changing-the-defaults"><i class="fa fa-check"></i><b>1.1.3</b> Changing the defaults</a></li>
<li class="chapter" data-level="1.1.4" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#multivariate-smoothing"><i class="fa fa-check"></i><b>1.1.4</b> Multivariate methods</a></li>
<li class="chapter" data-level="1.1.5" data-path="1-1-intro-smooth.html"><a href="1-1-intro-smooth.html#large-scale-smoothing"><i class="fa fa-check"></i><b>1.1.5</b> Large scale smoothing</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#vM"><i class="fa fa-check"></i><b>1.2.1</b> Univariate von Mises distributions</a></li>
<li class="chapter" data-level="1.2.2" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#mixtures-of-von-mises-distributions"><i class="fa fa-check"></i><b>1.2.2</b> Mixtures of von Mises distributions</a></li>
<li class="chapter" data-level="1.2.3" data-path="1-2-monte-carlo-methods.html"><a href="1-2-monte-carlo-methods.html#large-scale-monte-carlo-methods"><i class="fa fa-check"></i><b>1.2.3</b> Large scale Monte Carlo methods</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1-3-optimization.html"><a href="1-3-optimization.html"><i class="fa fa-check"></i><b>1.3</b> Optimization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="1-3-optimization.html"><a href="1-3-optimization.html#the-em-algorithm"><i class="fa fa-check"></i><b>1.3.1</b> The EM-algorithm</a></li>
<li class="chapter" data-level="1.3.2" data-path="1-3-optimization.html"><a href="1-3-optimization.html#large-scale-optimization"><i class="fa fa-check"></i><b>1.3.2</b> Large scale optimization</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Smoothing</b></span></li>
<li class="chapter" data-level="2" data-path="2-density.html"><a href="2-density.html"><i class="fa fa-check"></i><b>2</b> Density estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-unidens.html"><a href="2-1-unidens.html"><i class="fa fa-check"></i><b>2.1</b> Univariate density estimation</a></li>
<li class="chapter" data-level="2.2" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html"><i class="fa fa-check"></i><b>2.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#benchmarking"><i class="fa fa-check"></i><b>2.2.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html"><i class="fa fa-check"></i><b>2.3</b> Bandwidth selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#rectangular"><i class="fa fa-check"></i><b>2.3.1</b> Revisiting the rectangular kernel</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#ise-mise-and-mse-for-kernel-estimators"><i class="fa fa-check"></i><b>2.3.2</b> ISE, MISE and MSE for kernel estimators</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#plug-in-estimation-of-the-oracle-bandwidth"><i class="fa fa-check"></i><b>2.3.3</b> Plug-in estimation of the oracle bandwidth</a></li>
<li class="chapter" data-level="2.3.4" data-path="2-3-bandwidth.html"><a href="2-3-bandwidth.html#cv"><i class="fa fa-check"></i><b>2.3.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-likelihood.html"><a href="2-4-likelihood.html"><i class="fa fa-check"></i><b>2.4</b> Likelihood considerations</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-4-likelihood.html"><a href="2-4-likelihood.html#sieves"><i class="fa fa-check"></i><b>2.4.1</b> Method of sieves</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-4-likelihood.html"><a href="2-4-likelihood.html#basis-density"><i class="fa fa-check"></i><b>2.4.2</b> Basis expansions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-5-density-ex.html"><a href="2-5-density-ex.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="2-5-density-ex.html"><a href="2-5-density-ex.html#kernel-density-estimation"><i class="fa fa-check"></i>Kernel density estimation</a></li>
<li class="chapter" data-level="" data-path="2-5-density-ex.html"><a href="2-5-density-ex.html#benchmarking-1"><i class="fa fa-check"></i>Benchmarking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-bivariate.html"><a href="3-bivariate.html"><i class="fa fa-check"></i><b>3</b> Bivariate smoothing</a><ul>
<li class="chapter" data-level="3.1" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html"><i class="fa fa-check"></i><b>3.1</b> Nearest neighbor smoothers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#linear-smoothers"><i class="fa fa-check"></i><b>3.1.1</b> Linear smoothers</a></li>
<li class="chapter" data-level="3.1.2" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#implementing-the-running-mean"><i class="fa fa-check"></i><b>3.1.2</b> Implementing the running mean</a></li>
<li class="chapter" data-level="3.1.3" data-path="3-1-nearest-neighbor-smoothers.html"><a href="3-1-nearest-neighbor-smoothers.html#choose-k-by-cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Choose <span class="math inline">\(k\)</span> by cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html"><i class="fa fa-check"></i><b>3.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html#nadarayawatson-kernel-smoothing"><i class="fa fa-check"></i><b>3.2.1</b> Nadaraya–Watson kernel smoothing</a></li>
<li class="chapter" data-level="3.2.2" data-path="3-2-kernel-methods.html"><a href="3-2-kernel-methods.html#local-regression-smoothers"><i class="fa fa-check"></i><b>3.2.2</b> Local regression smoothers</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-3-sparse-linear-algebra.html"><a href="3-3-sparse-linear-algebra.html"><i class="fa fa-check"></i><b>3.3</b> Sparse linear algebra</a></li>
<li class="chapter" data-level="3.4" data-path="3-4-onb.html"><a href="3-4-onb.html"><i class="fa fa-check"></i><b>3.4</b> Orthogonal basis expansions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-4-onb.html"><a href="3-4-onb.html#polynomial-expansions"><i class="fa fa-check"></i><b>3.4.1</b> Polynomial expansions</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-4-onb.html"><a href="3-4-onb.html#fourier-expansions"><i class="fa fa-check"></i><b>3.4.2</b> Fourier expansions</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-5-splines.html"><a href="3-5-splines.html"><i class="fa fa-check"></i><b>3.5</b> Splines</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-5-splines.html"><a href="3-5-splines.html#smoothing-splines"><i class="fa fa-check"></i><b>3.5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-5-splines.html"><a href="3-5-splines.html#splines-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Splines in R</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-5-splines.html"><a href="3-5-splines.html#efficient-splines"><i class="fa fa-check"></i><b>3.5.3</b> Efficient computation with splines</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-6-gaussian-processes.html"><a href="3-6-gaussian-processes.html"><i class="fa fa-check"></i><b>3.6</b> Gaussian processes</a></li>
<li class="chapter" data-level="3.7" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html"><i class="fa fa-check"></i><b>3.7</b> The Kalman filter</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#ar1-example"><i class="fa fa-check"></i><b>3.7.1</b> AR(1)-example</a></li>
<li class="chapter" data-level="3.7.2" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#the-kalman-smoother"><i class="fa fa-check"></i><b>3.7.2</b> The Kalman smoother</a></li>
<li class="chapter" data-level="3.7.3" data-path="2-2-kernel-density.html"><a href="2-2-kernel-density.html#implementation"><i class="fa fa-check"></i><b>3.7.3</b> Implementation</a></li>
<li class="chapter" data-level="3.7.4" data-path="3-7-the-kalman-filter.html"><a href="3-7-the-kalman-filter.html#the-kalman-filter-1"><i class="fa fa-check"></i><b>3.7.4</b> The Kalman filter</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3-8-bivariate-ex.html"><a href="3-8-bivariate-ex.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="3-8-bivariate-ex.html"><a href="3-8-bivariate-ex.html#nearest-neighbors"><i class="fa fa-check"></i>Nearest neighbors</a></li>
<li class="chapter" data-level="" data-path="3-8-bivariate-ex.html"><a href="3-8-bivariate-ex.html#kernel-estimators"><i class="fa fa-check"></i>Kernel estimators</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Monte Carlo Methods</b></span></li>
<li class="chapter" data-level="4" data-path="4-univariate-random-variables.html"><a href="4-univariate-random-variables.html"><i class="fa fa-check"></i><b>4</b> Univariate random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html"><i class="fa fa-check"></i><b>4.1</b> Pseudorandom number generators</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html#implementing-a-pseudorandom-number-generator"><i class="fa fa-check"></i><b>4.1.1</b> Implementing a pseudorandom number generator</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-1-pseudorandom-number-generators.html"><a href="4-1-pseudorandom-number-generators.html#pseudorandom-number-packages"><i class="fa fa-check"></i><b>4.1.2</b> Pseudorandom number packages</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-2-transformation-techniques.html"><a href="4-2-transformation-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Transformation techniques</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-2-transformation-techniques.html"><a href="4-2-transformation-techniques.html#sampling-from-a-t-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Sampling from a <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a><ul>
<li class="chapter" data-level="4.3.1" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html#vMsim"><i class="fa fa-check"></i><b>4.3.1</b> von Mises distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="4-3-reject-samp.html"><a href="4-3-reject-samp.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html"><i class="fa fa-check"></i><b>4.4</b> Adaptive envelopes</a><ul>
<li class="chapter" data-level="4.4.1" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html#beta-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Beta distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="4-4-adaptive.html"><a href="4-4-adaptive.html#von-mises-distribution"><i class="fa fa-check"></i><b>4.4.2</b> von Mises distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4-5-univariate-ex.html"><a href="4-5-univariate-ex.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a><ul>
<li class="chapter" data-level="4.5.1" data-path="4-5-univariate-ex.html"><a href="4-5-univariate-ex.html#rejection-sampling-of-gaussian-random-variables"><i class="fa fa-check"></i><b>4.5.1</b> Rejection sampling of Gaussian random variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-mci.html"><a href="5-mci.html"><i class="fa fa-check"></i><b>5</b> Monte Carlo integration</a><ul>
<li class="chapter" data-level="5.1" data-path="5-1-assessment.html"><a href="5-1-assessment.html"><i class="fa fa-check"></i><b>5.1</b> Assessment</a><ul>
<li class="chapter" data-level="5.1.1" data-path="5-1-assessment.html"><a href="5-1-assessment.html#CLT-gamma"><i class="fa fa-check"></i><b>5.1.1</b> Using the central limit theorem</a></li>
<li class="chapter" data-level="5.1.2" data-path="5-1-assessment.html"><a href="5-1-assessment.html#concentration-inequalities"><i class="fa fa-check"></i><b>5.1.2</b> Concentration inequalities</a></li>
<li class="chapter" data-level="5.1.3" data-path="5-1-assessment.html"><a href="5-1-assessment.html#exponential-tail-bound-for-gamma-distributed-variables"><i class="fa fa-check"></i><b>5.1.3</b> Exponential tail bound for Gamma distributed variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html"><i class="fa fa-check"></i><b>5.2</b> Importance sampling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html#unknown-normalization-constants"><i class="fa fa-check"></i><b>5.2.1</b> Unknown normalization constants</a></li>
<li class="chapter" data-level="5.2.2" data-path="5-2-importance-sampling.html"><a href="5-2-importance-sampling.html#hd-int"><i class="fa fa-check"></i><b>5.2.2</b> Computing a high-dimensional integral</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5-3-network.html"><a href="5-3-network.html"><i class="fa fa-check"></i><b>5.3</b> Network failure</a><ul>
<li class="chapter" data-level="5.3.1" data-path="5-3-network.html"><a href="5-3-network.html#object-oriented-implementations"><i class="fa fa-check"></i><b>5.3.1</b> Object oriented implementations</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part III: Optimization</b></span></li>
<li class="chapter" data-level="6" data-path="6-four-examples.html"><a href="6-four-examples.html"><i class="fa fa-check"></i><b>6</b> Four Examples</a><ul>
<li class="chapter" data-level="6.1" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html"><i class="fa fa-check"></i><b>6.1</b> Exponential families</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#full-exponential-families"><i class="fa fa-check"></i><b>6.1.1</b> Full exponential families</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#bayes-net"><i class="fa fa-check"></i><b>6.1.2</b> Exponential family Bayesian networks</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#exp-fam-deriv"><i class="fa fa-check"></i><b>6.1.3</b> Likelihood computations</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-1-exp-fam.html"><a href="6-1-exp-fam.html#curved-exponential-families"><i class="fa fa-check"></i><b>6.1.4</b> Curved exponential families</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-2-multinomial-models.html"><a href="6-2-multinomial-models.html"><i class="fa fa-check"></i><b>6.2</b> Multinomial models</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-2-multinomial-models.html"><a href="6-2-multinomial-models.html#pep-moth"><i class="fa fa-check"></i><b>6.2.1</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-3-regression.html"><a href="6-3-regression.html"><i class="fa fa-check"></i><b>6.3</b> Regression models</a></li>
<li class="chapter" data-level="6.4" data-path="6-4-finite-mixture-models.html"><a href="6-4-finite-mixture-models.html"><i class="fa fa-check"></i><b>6.4</b> Finite mixture models</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-4-finite-mixture-models.html"><a href="6-4-finite-mixture-models.html#Gaus-mix-ex"><i class="fa fa-check"></i><b>6.4.1</b> Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-5-mixed-models.html"><a href="6-5-mixed-models.html"><i class="fa fa-check"></i><b>6.5</b> Mixed models</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-numopt.html"><a href="7-numopt.html"><i class="fa fa-check"></i><b>7</b> Numerical optimization</a><ul>
<li class="chapter" data-level="7.1" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html"><i class="fa fa-check"></i><b>7.1</b> Algorithms and convergence</a><ul>
<li class="chapter" data-level="7.1.1" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#descent-algorithms"><i class="fa fa-check"></i><b>7.1.1</b> Descent algorithms</a></li>
<li class="chapter" data-level="7.1.2" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#maps-and-fixed-points"><i class="fa fa-check"></i><b>7.1.2</b> Maps and fixed points</a></li>
<li class="chapter" data-level="7.1.3" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#convergence-rate"><i class="fa fa-check"></i><b>7.1.3</b> Convergence rate</a></li>
<li class="chapter" data-level="7.1.4" data-path="7-1-algorithms-and-convergence.html"><a href="7-1-algorithms-and-convergence.html#stopping-criteria"><i class="fa fa-check"></i><b>7.1.4</b> Stopping criteria</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html"><i class="fa fa-check"></i><b>7.2</b> Descent direction algorithms</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#line-search"><i class="fa fa-check"></i><b>7.2.1</b> Line search</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>7.2.2</b> Gradient descent</a></li>
<li class="chapter" data-level="7.2.3" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#conjugate-gradients"><i class="fa fa-check"></i><b>7.2.3</b> Conjugate gradients</a></li>
<li class="chapter" data-level="7.2.4" data-path="7-2-descent-direction-algorithms.html"><a href="7-2-descent-direction-algorithms.html#pep-moth-descent"><i class="fa fa-check"></i><b>7.2.4</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-3-newton-type-algorithms.html"><a href="7-3-newton-type-algorithms.html"><i class="fa fa-check"></i><b>7.3</b> Newton-type algorithms</a><ul>
<li class="chapter" data-level="7.3.1" data-path="7-3-newton-type-algorithms.html"><a href="7-3-newton-type-algorithms.html#poisson-regression"><i class="fa fa-check"></i><b>7.3.1</b> Poisson regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="7-3-newton-type-algorithms.html"><a href="7-3-newton-type-algorithms.html#quasi-newton-algorithms"><i class="fa fa-check"></i><b>7.3.2</b> Quasi-Newton algorithms</a></li>
<li class="chapter" data-level="7.3.3" data-path="7-3-newton-type-algorithms.html"><a href="7-3-newton-type-algorithms.html#sparsity"><i class="fa fa-check"></i><b>7.3.3</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7-4-misc-.html"><a href="7-4-misc-.html"><i class="fa fa-check"></i><b>7.4</b> Misc.</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-em.html"><a href="8-em.html"><i class="fa fa-check"></i><b>8</b> Expectation maximization algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html"><i class="fa fa-check"></i><b>8.1</b> Basic properties</a><ul>
<li class="chapter" data-level="8.1.1" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#incomplete-data-likelihood"><i class="fa fa-check"></i><b>8.1.1</b> Incomplete data likelihood</a></li>
<li class="chapter" data-level="8.1.2" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#monotonicity-of-the-em-algorithm"><i class="fa fa-check"></i><b>8.1.2</b> Monotonicity of the EM algorithm</a></li>
<li class="chapter" data-level="8.1.3" data-path="8-1-basic-properties.html"><a href="8-1-basic-properties.html#peppered-moths"><i class="fa fa-check"></i><b>8.1.3</b> Peppered moths</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8-2-EM-exp.html"><a href="8-2-EM-exp.html"><i class="fa fa-check"></i><b>8.2</b> Exponential families</a></li>
<li class="chapter" data-level="8.3" data-path="8-3-fisher-information.html"><a href="8-3-fisher-information.html"><i class="fa fa-check"></i><b>8.3</b> Fisher information</a></li>
<li class="chapter" data-level="8.4" data-path="8-4-revisiting-gaussian-mixtures.html"><a href="8-4-revisiting-gaussian-mixtures.html"><i class="fa fa-check"></i><b>8.4</b> Revisiting Gaussian mixtures</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-StochOpt.html"><a href="9-StochOpt.html"><i class="fa fa-check"></i><b>9</b> Stochastic Optimization</a><ul>
<li class="chapter" data-level="9.1" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html"><i class="fa fa-check"></i><b>9.1</b> Stochastic gradient algorithms</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#section"><i class="fa fa-check"></i><b>9.1.1</b> </a></li>
<li class="chapter" data-level="9.1.2" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#online-stochastic-gradient-descent"><i class="fa fa-check"></i><b>9.1.2</b> Online stochastic gradient descent</a></li>
<li class="chapter" data-level="9.1.3" data-path="9-1-stochastic-gradient-algorithms.html"><a href="9-1-stochastic-gradient-algorithms.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>9.1.3</b> Stochastic gradient descent</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9-2-nonlinear-least-squares.html"><a href="9-2-nonlinear-least-squares.html"><i class="fa fa-check"></i><b>9.2</b> Nonlinear least squares</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-app-R.html"><a href="A-app-R.html"><i class="fa fa-check"></i><b>A</b> R programming</a><ul>
<li class="chapter" data-level="A.1" data-path="A-1-functions.html"><a href="A-1-functions.html"><i class="fa fa-check"></i><b>A.1</b> Functions</a><ul>
<li class="chapter" data-level="A.1.1" data-path="A-1-functions.html"><a href="A-1-functions.html#vectorization"><i class="fa fa-check"></i><b>A.1.1</b> Vectorization</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="A-2-objects-and-methods.html"><a href="A-2-objects-and-methods.html"><i class="fa fa-check"></i><b>A.2</b> Objects and methods</a></li>
<li class="chapter" data-level="A.3" data-path="A-3-environments.html"><a href="A-3-environments.html"><i class="fa fa-check"></i><b>A.3</b> Environments</a><ul>
<li class="chapter" data-level="A.3.1" data-path="A-3-environments.html"><a href="A-3-environments.html#function-factories"><i class="fa fa-check"></i><b>A.3.1</b> Function factories</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="A-4-performance.html"><a href="A-4-performance.html"><i class="fa fa-check"></i><b>A.4</b> Performance</a><ul>
<li class="chapter" data-level="A.4.1" data-path="A-4-performance.html"><a href="A-4-performance.html#parallel-computations"><i class="fa fa-check"></i><b>A.4.1</b> Parallel computations</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html"><i class="fa fa-check"></i><b>A.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#functions-1"><i class="fa fa-check"></i>Functions</a></li>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#histograms-with-non-equidistant-breaks"><i class="fa fa-check"></i>Histograms with non-equidistant breaks</a></li>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#functions-and-objects"><i class="fa fa-check"></i>Functions and objects</a></li>
<li class="chapter" data-level="" data-path="A-5-app-ex.html"><a href="A-5-app-ex.html#functions-and-environments"><i class="fa fa-check"></i>Functions and environments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multinomial-models" class="section level2">
<h2><span class="header-section-number">6.2</span> Multinomial models</h2>
<p>The multinomial model is the model of all probability distributions on
a finite set <span class="math inline">\(\mathcal{Y} = \{1, \ldots, K\}\)</span>. The model is parametrized
by the simplex
<span class="math display">\[\Delta_K = \left\{(p_1, \ldots, p_K)^T \in \mathbb{R}^K \mid p_k \geq 0, \sum_{k=1}^K p_k = 1\right\}.\]</span>
The distributions parametrized by the relative interior of <span class="math inline">\(\Delta_K\)</span> form
an exponential family by the parametrization
<span class="math display">\[p_k = \frac{e^{\theta_k}}{\sum_{l=1}^K e^{\theta_l}} \in (0,1)\]</span>
for <span class="math inline">\((\theta_1, \ldots, \theta_K)^T \in \mathbb{R}^K.\)</span> That is,
the sufficient statistic is <span class="math inline">\(k \mapsto (\delta_{1k}, \ldots, \delta_{Kk})^T \in \mathbb{R}^{K-1}\)</span>
(where <span class="math inline">\(\delta_{lk} \in \{0, 1\}\)</span> is the Kronecker delta being 1 if and only
if <span class="math inline">\(l = k\)</span>), and
<span class="math display">\[\varphi(\theta_1, \ldots, \theta_K) = \sum_{l=1}^K e^{\theta_l}.\]</span>
We call this exponential family parametrization the <em>symmetric</em> parametrization.
The canonical parameter in the symmetric parametrization is not identifiable,
and to resolve the lack of identifiability
there is a tradition of fixing the last parameter as <span class="math inline">\(\theta_K = 0\)</span>. This
results in a canonical parameter <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_{K-1})^T \in \mathbb{R}^{K-1},\)</span>
a sufficient statistic <span class="math inline">\(t_1(k) = (\delta_{1k}, \ldots, \delta_{(K-1)k})^T \in \mathbb{R}^p\)</span>,</p>
<p><span class="math display">\[\varphi(\theta) = 1 + \sum_{l=1}^{K-1} e^{\theta_l}\]</span></p>
<p>and probabilities</p>
<p><span class="math display">\[p_k = \left\{\begin{array}{ll} \frac{e^{\theta_k}}{1 + \sum_{l=1}^{K-1} e^{\theta_l}}  &amp; \quad \text{if } k = 1, \ldots, K-1 \\  \frac{1}{1 + \sum_{l=1}^{K-1} e^{\theta_l}} &amp; \quad \text{if } k = K. \end{array}\right.\]</span></p>
<p>We see that in this parametrization <span class="math inline">\(p_k = e^{\theta_k}p_K\)</span> for <span class="math inline">\(k = 1, \ldots, K-1\)</span>,
where the probability of the last element <span class="math inline">\(K\)</span> acts as a baseline or reference probability,
and the factors <span class="math inline">\(e^{\theta_k}\)</span> act as multiplicative modifications of this
baseline. Moreover,
<span class="math display">\[\frac{p_k}{p_l} = e^{\theta_k - \theta_l},\]</span>
which is independent of the chosen baseline.</p>
<p>In the special case of <span class="math inline">\(K = 2\)</span> the two elements <span class="math inline">\(\mathcal{Y}\)</span> are often given
other labels than <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>. The most common labels are <span class="math inline">\(\{0, 1\}\)</span> and <span class="math inline">\(\{-1, 1\}\)</span>.
If we use the <span class="math inline">\(0\)</span>-<span class="math inline">\(1\)</span>-labels the convention is to use <span class="math inline">\(p_0\)</span> as the baseline and
thus
<span class="math display">\[p_1 = \frac{e^{\theta}}{1 + e^{\theta}} = e^{\theta} p_0 = e^{\theta} (1 - p_1).\]</span>
parametrized by <span class="math inline">\(\theta \in \mathbb{R}\)</span>. As this function of <span class="math inline">\(\theta\)</span> is
known as the <em>logistic function</em>, this parametrization of the probability
distributions on <span class="math inline">\(\{0,1\}\)</span> is often referred to as the logistic model. From the
above we see directly that
<span class="math display">\[\theta = \log \frac{p_1}{1 - p_1}\]</span>
is the log-odds.</p>
<p>If we use the <span class="math inline">\(\pm 1\)</span>-labels, an alternative exponential family parametrization is
<span class="math display">\[p_k = \frac{e^{k\theta}}{e^\theta + e^{-\theta}}\]</span>
for <span class="math inline">\(\theta \in \mathbb{R}\)</span> and <span class="math inline">\(k \in \{-1, 1\}\)</span>. This gives a symmetric treatment of the two
labels while retaining the identifiability.</p>
<p>With i.i.d. observations from a multinomial distribution we find
that the log-likelihood is</p>
<p><span class="math display">\[\begin{align*}
\ell(\theta) &amp; = \sum_{i=1}^n \sum_{k=1}^K \delta_{k y_i} \log(p_k(\theta)) \\
&amp; = \sum_{k=1}^K \underbrace{\sum_{i=1}^n \delta_{k y_i}}_{= n_k} \log(p_k(\theta)) \\ 
&amp; = \theta^T  \mathbf{n} - n \log \left(1 + \sum_{l=1}^{K-1} e^{\theta_l} \right).
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\mathbf{n} = (n_1, \ldots, n_K)^T = \sum_{i=1}^n t(y_i)\)</span> is the sufficient
statistic, which is simply a tabulation of the times the different elements in
<span class="math inline">\(\{1, \ldots, K\}\)</span> were observed.</p>
<div id="pep-moth" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Peppered Moths</h3>
<p>This example is on the color of the peppered Moth (<em>Birkemåler</em> in Danish).
The color of the moth is
primarily determined by one gene that occur in three different alleles denoted C,
I and T. The alleles are ordered in terms of dominance as C &gt; I &gt; T. Moths with genotype
including C are dark. Moths
with genotype TT are light colored. Moths with genotypes II and IT are mottled.
Thus there a total of six different genotypes (CC, CI, CT, II, IT and TT) and
three different phenotypes (black, mottled, light colored).</p>
<p><img src="figures/peppered-moth.jpg" width="70%" style="display: block; margin: auto;" /></p>
<p>The peppered moth provided an <a href="https://en.wikipedia.org/wiki/Peppered_moth_evolution">early demonstration of evolution</a>
in the 19th century England, where the light colored moth was outnumbered by the dark
colored variety. The dark color became dominant due to the increased
pollution, where trees were darkened by soot, and had for that reason a selective
advantage. There is a nice collection
of moth in different colors at the Danish Zoological Museum, and further
explanation of the role it played in understanding evolution.</p>
<p>We denote the allele frequencies <span class="math inline">\(p_C\)</span>, <span class="math inline">\(p_I\)</span>, <span class="math inline">\(p_T\)</span> with <span class="math inline">\(p_C + p_I + p_T = 1.\)</span>
According to the <a href="https://en.wikipedia.org/wiki/Hardy–Weinberg_principle">Hardy-Weinberg principle</a>,
the genotype frequencies are then</p>
<p><span class="math display">\[p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2,  2p_Ip_T, p_T^2.\]</span></p>
<p>If we could observe the genotypes, the complete multinomial log-likelihood would be</p>
<p><span class="math display">\[\begin{align*}
 &amp; 2n_{CC} \log(p_C) + n_{CI} \log (2 p_C p_I) + n_{CT} \log(2 p_C p_I) \\
&amp; \ \ + 2 n_{II} \log(p_I) + n_{IT} \log(2p_I p_T) + 2 n_{TT} \log(p_T) \\
&amp; = 2n_{CC} \log(p_C) + n_{CI} \log (2 p_C p_I) + n_{CT} \log(2 p_C p_I) \\
&amp; \ \ + 2 n_{II} \log(p_I) + n_{IT} \log(2p_I (1 - p_C - p_I)) + 2 n_{TT} \log(1 - p_C - p_I). 
\end{align*}\]</span></p>
<p>The log-likelihood is given in terms of the genotype counts and the two probability parameters
<span class="math inline">\(p_C, p_I \geq 0\)</span> with <span class="math inline">\(p_C + p_I \leq 1\)</span>, and on the interior of this
parameter set the model is a curved exponential family.</p>
<p>Collecting moths and determining their color will, however, only identify their phenotype,
not their genotype. Thus we observe <span class="math inline">\((n_C, n_T, n_I)\)</span>, where
<span class="math display">\[n = \underbrace{n_{CC} + n_{CI} + n_{CT}}_{= n_C} + 
\underbrace{n_{IT} + n_{II}}_{=n_I} + \underbrace{n_{TT}}_{=n_T}.\]</span></p>
<p>For maximum-likelihood estimation of the parameters in this model from
the observation <span class="math inline">\((n_C, n_T, n_I)\)</span>, we need the likelihood,
that is, the likelihood in the marginal distribution of the observed
variables.</p>
<p>The peppered Moth example is an example of <em>cell collapsing</em> in a multinomial model.
In general, let <span class="math inline">\(A_1 \cup \ldots \cup A_{K_0} = \{1, \ldots, K\}\)</span> be a partition and let
<span class="math display">\[M : \mathbb{N}_0^K \to \mathbb{N}_0^{K_0}\]</span>
be the map given by
<span class="math display">\[M((n_1, \ldots, n_K))_j = \sum_{k \in A_j} n_k.\]</span></p>
<p>If <span class="math inline">\(Y \sim \textrm{Mult}(p, n)\)</span> with <span class="math inline">\(p = (p_1, \ldots, p_K)\)</span> then
<span class="math display">\[X = M(Y) \sim \textrm{Mult}(M(p), n).\]</span>
If <span class="math inline">\(\theta \mapsto p(\theta)\)</span> is a parametrization of the cell probabilities
the log-likelihood under the collapsed multinomial model is generally</p>
<p><span class="math display" id="eq:mult-col-loglik">\[\begin{equation}
\ell(\theta) = \sum_{j = 1}^{K_0} x_j \log (M(p(\theta))_j) = \sum_{j = 1}^{K_0} x_j \log \left(\sum_{k \in A_j} p_k(\theta)\right).
\tag{6.2} 
\end{equation}\]</span></p>
<p>For the peppered Moths, <span class="math inline">\(K = 6\)</span> corresponding to the six genotypes, <span class="math inline">\(K_0 = 3\)</span> and
the partition corresponding to the phenotypes is
<span class="math display">\[\{1, 2, 3\} \cup \{4, 5\} \cup \{6\} = \{1, \ldots, 6\},\]</span>
and
<span class="math display">\[M(n_1, \ldots, n_6) = (n_1 + n_2 + n_3, n_4 + n_5, n_6).\]</span></p>
<p>In terms of the <span class="math inline">\((p_C, p_I)\)</span> parametrization, <span class="math inline">\(p_T = 1 - p_C - p_I\)</span> and
<span class="math display">\[p = (p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2,  2p_Ip_T, p_T^2).\]</span></p>
<p>Hence
<span class="math display">\[M(p) = (p_C^2 + 2p_Cp_I + 2p_Cp_T, p_I^2 +2p_Ip_T, p_T^2).\]</span></p>
<p>The log-likelihood is</p>
<p><span class="math display">\[\begin{align}
\ell(p_C, p_I) &amp; = n_C \log(p_C^2 + 2p_Cp_I + 2p_Cp_T) + n_I \log(p_I^2 +2p_Ip_T) + n_T \log (p_T^2).
\end{align}\]</span></p>
<p>We can implement the log-likelihood in a very problem specific way.
Note how the parameter constraints are encoded via the return value <span class="math inline">\(\infty\)</span>.</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="6-2-multinomial-models.html#cb262-1"></a><span class="co">## par = c(pC, pI), pT = 1 - pC - pI</span></span>
<span id="cb262-2"><a href="6-2-multinomial-models.html#cb262-2"></a><span class="co">## x is the data vector of length 3 of counts </span></span>
<span id="cb262-3"><a href="6-2-multinomial-models.html#cb262-3"></a>loglik &lt;-<span class="st"> </span><span class="cf">function</span>(par, x) {</span>
<span id="cb262-4"><a href="6-2-multinomial-models.html#cb262-4"></a>  pT &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>par[<span class="dv">2</span>]</span>
<span id="cb262-5"><a href="6-2-multinomial-models.html#cb262-5"></a>  </span>
<span id="cb262-6"><a href="6-2-multinomial-models.html#cb262-6"></a>  <span class="cf">if</span> (par[<span class="dv">1</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> </span>
<span id="cb262-7"><a href="6-2-multinomial-models.html#cb262-7"></a>      <span class="op">||</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>pT <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>)</span>
<span id="cb262-8"><a href="6-2-multinomial-models.html#cb262-8"></a>    <span class="kw">return</span>(<span class="ot">Inf</span>)</span>
<span id="cb262-9"><a href="6-2-multinomial-models.html#cb262-9"></a>  </span>
<span id="cb262-10"><a href="6-2-multinomial-models.html#cb262-10"></a>  PC &lt;-<span class="st"> </span>par[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>pT</span>
<span id="cb262-11"><a href="6-2-multinomial-models.html#cb262-11"></a>  PI &lt;-<span class="st"> </span>par[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>pT</span>
<span id="cb262-12"><a href="6-2-multinomial-models.html#cb262-12"></a>  PT &lt;-<span class="st"> </span>pT<span class="op">^</span><span class="dv">2</span></span>
<span id="cb262-13"><a href="6-2-multinomial-models.html#cb262-13"></a>  <span class="co">## The function returns the negative log-likelihood </span></span>
<span id="cb262-14"><a href="6-2-multinomial-models.html#cb262-14"></a>  <span class="op">-</span><span class="st"> </span>(x[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(PC) <span class="op">+</span><span class="st"> </span>x[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(PI) <span class="op">+</span><span class="st"> </span>x[<span class="dv">3</span>]<span class="op">*</span><span class="st"> </span><span class="kw">log</span>(PT))</span>
<span id="cb262-15"><a href="6-2-multinomial-models.html#cb262-15"></a>}</span></code></pre></div>
<p>It is possible to use <code>optim</code> in R with just this implementation to
compute the maximum-likelihood estimate of the allele parameters.</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="6-2-multinomial-models.html#cb263-1"></a><span class="kw">optim</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), loglik, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>))</span></code></pre></div>
<pre><code>## $par
## [1] 0.07084643 0.18871900
## 
## $value
## [1] 600.481
## 
## $counts
## function gradient 
##       71       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>The <code>optim</code> function uses an algorithm called <a href="https://en.wikipedia.org/wiki/Nelder–Mead_method">Nelder-Mead</a>
as the default algorithm that relies on log-likelihood evaluations only. It
is slow but fairly robust, though a bit of thought has to go into the initial
parameter choice.</p>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="6-2-multinomial-models.html#cb265-1"></a><span class="kw">optim</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), loglik, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>))</span></code></pre></div>
<pre><code>## Error in optim(c(0, 0), loglik, x = c(85, 196, 341)): function
cannot be evaluated at initial parameters```

Starting the algorithm in a boundary value where the negative log-likelihood attains
the value $\infty$ does not work. 

The computations can beneficially be implemented in greater 
generality. The function `M` sums the cells that are collapsed, 
which has to be specified by the `group` argument.


```r
M &lt;- function(x, group)
  as.vector(tapply(x, group, sum))</code></pre>
<p>The log-likelihood is then implemented for multinomial cell
collapsing via <code>M</code> and two problem specific arguments to
the <code>loglik</code> function. One of these is a vector specifying
the grouping structure of the collapsing. The other is a function
that determines the
parametrization that maps the parameter that we are optimizing over to the
cell probabilities. In the implementation it is assumed
that this <code>prob</code> function in addition encodes parameter constraints
by returning <code>NULL</code> if parameter constraints are violated.</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb267-1"><a href="6-2-multinomial-models.html#cb267-1"></a>loglik &lt;-<span class="st"> </span><span class="cf">function</span>(par, x, prob, group, ...) {</span>
<span id="cb267-2"><a href="6-2-multinomial-models.html#cb267-2"></a>  p &lt;-<span class="st"> </span><span class="kw">prob</span>(par)</span>
<span id="cb267-3"><a href="6-2-multinomial-models.html#cb267-3"></a>  <span class="cf">if</span>(<span class="kw">is.null</span>(p)) <span class="kw">return</span>(<span class="ot">Inf</span>)</span>
<span id="cb267-4"><a href="6-2-multinomial-models.html#cb267-4"></a>  <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(x <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">M</span>(<span class="kw">prob</span>(par), group)))</span>
<span id="cb267-5"><a href="6-2-multinomial-models.html#cb267-5"></a>}</span></code></pre></div>
<p>The function <code>prob</code> is implemented specifically for the peppered moths
as follows.</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="6-2-multinomial-models.html#cb268-1"></a>prob &lt;-<span class="st"> </span><span class="cf">function</span>(p) {</span>
<span id="cb268-2"><a href="6-2-multinomial-models.html#cb268-2"></a>  p[<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>p[<span class="dv">2</span>]</span>
<span id="cb268-3"><a href="6-2-multinomial-models.html#cb268-3"></a>  <span class="cf">if</span> (p[<span class="dv">1</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>p[<span class="dv">2</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>p[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>p[<span class="dv">3</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>)</span>
<span id="cb268-4"><a href="6-2-multinomial-models.html#cb268-4"></a>    <span class="kw">return</span>(<span class="ot">NULL</span>)</span>
<span id="cb268-5"><a href="6-2-multinomial-models.html#cb268-5"></a>  <span class="kw">c</span>(p[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>], <span class="dv">2</span><span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>], </span>
<span id="cb268-6"><a href="6-2-multinomial-models.html#cb268-6"></a>    p[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>], p[<span class="dv">3</span>]<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb268-7"><a href="6-2-multinomial-models.html#cb268-7"></a>}</span></code></pre></div>
<p>We test that the new implementation gives the same result when
optimized as using the more problem specific implementation.</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="6-2-multinomial-models.html#cb269-1"></a><span class="kw">optim</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), loglik, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), </span>
<span id="cb269-2"><a href="6-2-multinomial-models.html#cb269-2"></a>      <span class="dt">prob =</span> prob, <span class="dt">group =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## $par
## [1] 0.07084643 0.18871900
## 
## $value
## [1] 600.481
## 
## $counts
## function gradient 
##       71       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>Once we have found an estimate of the parameters, we can compute a prediction
of the unobserved genotype counts from the phenotype counts using
the conditional distribution of the genotypes given the phenotypes.
This is straightforward as the conditional distribution of <span class="math inline">\(Y_{A_j} = (Y_k)_{k \in A_j}\)</span>,
conditionally on <span class="math inline">\(X\)</span> is also a multinomial distribution;
<span class="math display">\[Y_{A_j} \mid X = x \sim \textrm{Mult}\left( \frac{p_{A_j}}{M(p)_j}, x_j \right).\]</span>
The probability parameters are simply <span class="math inline">\(p\)</span> restricted to <span class="math inline">\(A_j\)</span> and renormalized
to a probability vector. Observe that this gives
<span class="math display">\[E (Y_k \mid X = x) = \frac{x_j p_k}{M(p)_j}\]</span>
for <span class="math inline">\(k \in A_j\)</span>. Using the estimated parameters and the <code>M</code> function implemented
above, we can compute a prediction of the genotype counts as follows.</p>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="6-2-multinomial-models.html#cb271-1"></a>x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>)</span>
<span id="cb271-2"><a href="6-2-multinomial-models.html#cb271-2"></a>group &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb271-3"><a href="6-2-multinomial-models.html#cb271-3"></a>p &lt;-<span class="st"> </span><span class="kw">prob</span>(<span class="kw">c</span>(<span class="fl">0.07084643</span>, <span class="fl">0.18871900</span>))</span>
<span id="cb271-4"><a href="6-2-multinomial-models.html#cb271-4"></a>x[group] <span class="op">*</span><span class="st"> </span>p <span class="op">/</span><span class="st"> </span><span class="kw">M</span>(p, group)[group]</span></code></pre></div>
<pre><code>## [1]   3.121549  16.630211  65.248241  22.154520 173.845480 341.000000</code></pre>
<p>This is of interest in itself, but computing these conditional expectations
will also be central for the EM algorithm in Chapter <a href="8-em.html#em">8</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="6-1-exp-fam.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="6-3-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

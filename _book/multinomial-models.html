<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Statistics with R</title>
  <meta name="description" content="Lecture notes providing an introduction to computational statistics using the R programming language.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Statistics with R" />
  
  <meta name="twitter:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

<meta name="author" content="Niels Richard Hansen">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="exp-fam.html">
<link rel="next" href="regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro-smooth.html"><a href="intro-smooth.html"><i class="fa fa-check"></i><b>1.1</b> Smoothing</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro-smooth.html"><a href="intro-smooth.html#intro-angles"><i class="fa fa-check"></i><b>1.1.1</b> Angle distributions in proteins</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-smooth.html"><a href="intro-smooth.html#using-ggplot2"><i class="fa fa-check"></i><b>1.1.2</b> Using ggplot2</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-smooth.html"><a href="intro-smooth.html#changing-the-defaults"><i class="fa fa-check"></i><b>1.1.3</b> Changing the defaults</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-smooth.html"><a href="intro-smooth.html#large-scale-smoothing"><i class="fa fa-check"></i><b>1.1.4</b> Large scale smoothing</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo Methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#vM"><i class="fa fa-check"></i><b>1.2.1</b> Univariate von Mises distributions</a></li>
<li class="chapter" data-level="1.2.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#mixtures-of-von-mises-distributions"><i class="fa fa-check"></i><b>1.2.2</b> Mixtures of von Mises distributions</a></li>
<li class="chapter" data-level="1.2.3" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#large-scale-simulation"><i class="fa fa-check"></i><b>1.2.3</b> Large scale simulation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="optimization.html"><a href="optimization.html"><i class="fa fa-check"></i><b>1.3</b> Optimization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="optimization.html"><a href="optimization.html#the-em-algorithm"><i class="fa fa-check"></i><b>1.3.1</b> The EM-algorithm</a></li>
<li class="chapter" data-level="1.3.2" data-path="optimization.html"><a href="optimization.html#large-scale-optimization"><i class="fa fa-check"></i><b>1.3.2</b> Large scale optimization</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.4</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#r-training-exercises"><i class="fa fa-check"></i>R training exercises</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#histograms-with-non-equidistant-breaks"><i class="fa fa-check"></i>Histograms with non-equidistant breaks</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#functions-and-functional-programming"><i class="fa fa-check"></i>Functions and functional programming</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Smoothing</b></span></li>
<li class="chapter" data-level="2" data-path="density.html"><a href="density.html"><i class="fa fa-check"></i><b>2</b> Density estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="unidens.html"><a href="unidens.html"><i class="fa fa-check"></i><b>2.1</b> Univariate density estimation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="unidens.html"><a href="unidens.html#likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Likelihood considerations</a></li>
<li class="chapter" data-level="2.1.2" data-path="unidens.html"><a href="unidens.html#sieves"><i class="fa fa-check"></i><b>2.1.2</b> Method of sieves</a></li>
<li class="chapter" data-level="2.1.3" data-path="unidens.html"><a href="unidens.html#basis-density"><i class="fa fa-check"></i><b>2.1.3</b> Basis expansions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="kernel-density.html"><a href="kernel-density.html"><i class="fa fa-check"></i><b>2.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="kernel-density.html"><a href="kernel-density.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="kernel-density.html"><a href="kernel-density.html#benchmarking"><i class="fa fa-check"></i><b>2.2.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bandwidth.html"><a href="bandwidth.html"><i class="fa fa-check"></i><b>2.3</b> Bandwidth selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bandwidth.html"><a href="bandwidth.html#rectangular"><i class="fa fa-check"></i><b>2.3.1</b> Revisiting the rectangular kernel</a></li>
<li class="chapter" data-level="2.3.2" data-path="bandwidth.html"><a href="bandwidth.html#ise-mise-and-mse-for-kernel-estimators"><i class="fa fa-check"></i><b>2.3.2</b> ISE, MISE and MSE for kernel estimators</a></li>
<li class="chapter" data-level="2.3.3" data-path="bandwidth.html"><a href="bandwidth.html#plug-in-estimation-of-the-oracle-bandwidth"><i class="fa fa-check"></i><b>2.3.3</b> Plug-in estimation of the oracle bandwidth</a></li>
<li class="chapter" data-level="2.3.4" data-path="bandwidth.html"><a href="bandwidth.html#cross-validation"><i class="fa fa-check"></i><b>2.3.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multivariate-smoothing.html"><a href="multivariate-smoothing.html"><i class="fa fa-check"></i><b>2.4</b> Multivariate methods</a></li>
<li class="chapter" data-level="2.5" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#kernel-density-estimation"><i class="fa fa-check"></i>Kernel density estimation</a></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#benchmarking-1"><i class="fa fa-check"></i>Benchmarking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i><b>3</b> Bivariate smoothing</a><ul>
<li class="chapter" data-level="3.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html"><i class="fa fa-check"></i><b>3.1</b> Nearest neighbor smoothers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#linear-smoothers"><i class="fa fa-check"></i><b>3.1.1</b> Linear smoothers</a></li>
<li class="chapter" data-level="3.1.2" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#implementing-the-running-mean"><i class="fa fa-check"></i><b>3.1.2</b> Implementing the running mean</a></li>
<li class="chapter" data-level="3.1.3" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#choose-k-by-cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Choose <span class="math inline">\(k\)</span> by cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="kernel-methods.html"><a href="kernel-methods.html"><i class="fa fa-check"></i><b>3.2</b> Kernel methods</a></li>
<li class="chapter" data-level="3.3" data-path="sparse-linear-algebra.html"><a href="sparse-linear-algebra.html"><i class="fa fa-check"></i><b>3.3</b> Sparse linear algebra</a></li>
<li class="chapter" data-level="3.4" data-path="onb.html"><a href="onb.html"><i class="fa fa-check"></i><b>3.4</b> Orthogonal basis expansions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="onb.html"><a href="onb.html#polynomial-expansions"><i class="fa fa-check"></i><b>3.4.1</b> Polynomial expansions</a></li>
<li class="chapter" data-level="3.4.2" data-path="onb.html"><a href="onb.html#fourier-expansions"><i class="fa fa-check"></i><b>3.4.2</b> Fourier expansions</a></li>
<li class="chapter" data-level="3.4.3" data-path="onb.html"><a href="onb.html#wavelets"><i class="fa fa-check"></i><b>3.4.3</b> Wavelets</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>3.5</b> Splines</a><ul>
<li class="chapter" data-level="3.5.1" data-path="splines.html"><a href="splines.html#smoothing-splines"><i class="fa fa-check"></i><b>3.5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="3.5.2" data-path="splines.html"><a href="splines.html#splines-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Splines in R</a></li>
<li class="chapter" data-level="3.5.3" data-path="splines.html"><a href="splines.html#efficient-computation-with-splines"><i class="fa fa-check"></i><b>3.5.3</b> Efficient computation with splines</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="gaussian-processes.html"><a href="gaussian-processes.html"><i class="fa fa-check"></i><b>3.6</b> Gaussian processes</a></li>
<li class="chapter" data-level="3.7" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html"><i class="fa fa-check"></i><b>3.7</b> The Kalman filter</a><ul>
<li class="chapter" data-level="3.7.1" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#ar1-example"><i class="fa fa-check"></i><b>3.7.1</b> AR(1)-example</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-smoother"><i class="fa fa-check"></i><b>3.7.2</b> The Kalman smoother</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#implementation-1"><i class="fa fa-check"></i><b>3.7.3</b> Implementation</a></li>
<li class="chapter" data-level="3.7.4" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-filter-1"><i class="fa fa-check"></i><b>3.7.4</b> The Kalman filter</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Monte Carlo Methods</b></span></li>
<li class="chapter" data-level="4" data-path="univariate-random-variables.html"><a href="univariate-random-variables.html"><i class="fa fa-check"></i><b>4</b> Univariate random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="pseudo-random-numbers.html"><a href="pseudo-random-numbers.html"><i class="fa fa-check"></i><b>4.1</b> Pseudo random numbers</a></li>
<li class="chapter" data-level="4.2" data-path="transformation-techniques.html"><a href="transformation-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Transformation techniques</a><ul>
<li class="chapter" data-level="4.2.1" data-path="transformation-techniques.html"><a href="transformation-techniques.html#sampling-from-a-t-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Sampling from a <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="reject-samp.html"><a href="reject-samp.html"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a><ul>
<li class="chapter" data-level="4.3.1" data-path="reject-samp.html"><a href="reject-samp.html#vMsim"><i class="fa fa-check"></i><b>4.3.1</b> von Mises distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="reject-samp.html"><a href="reject-samp.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="adaptive.html"><a href="adaptive.html"><i class="fa fa-check"></i><b>4.4</b> Adaptive envelopes</a><ul>
<li class="chapter" data-level="4.4.1" data-path="adaptive.html"><a href="adaptive.html#beta-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Beta distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="adaptive.html"><a href="adaptive.html#von-mises-distribution"><i class="fa fa-check"></i><b>4.4.2</b> von Mises distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#rejection-sampling-of-gaussian-random-variables"><i class="fa fa-check"></i>Rejection sampling of Gaussian random variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mci.html"><a href="mci.html"><i class="fa fa-check"></i><b>5</b> Monte Carlo integration</a><ul>
<li class="chapter" data-level="5.1" data-path="assessment.html"><a href="assessment.html"><i class="fa fa-check"></i><b>5.1</b> Assessment</a><ul>
<li class="chapter" data-level="5.1.1" data-path="assessment.html"><a href="assessment.html#using-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.1.1</b> Using the central limit theorem</a></li>
<li class="chapter" data-level="5.1.2" data-path="assessment.html"><a href="assessment.html#concentration-inequalities"><i class="fa fa-check"></i><b>5.1.2</b> Concentration inequalities</a></li>
<li class="chapter" data-level="5.1.3" data-path="assessment.html"><a href="assessment.html#exponential-tail-bound-for-gamma-distributed-variables"><i class="fa fa-check"></i><b>5.1.3</b> Exponential tail bound for Gamma distributed variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>5.2</b> Importance sampling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importance-sampling.html"><a href="importance-sampling.html#unknown-normalization-constants"><i class="fa fa-check"></i><b>5.2.1</b> Unknown normalization constants</a></li>
<li class="chapter" data-level="5.2.2" data-path="importance-sampling.html"><a href="importance-sampling.html#computing-a-high-dimensional-integral"><i class="fa fa-check"></i><b>5.2.2</b> Computing a high-dimensional integral</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="network-failure.html"><a href="network-failure.html"><i class="fa fa-check"></i><b>5.3</b> Network failure</a><ul>
<li class="chapter" data-level="5.3.1" data-path="network-failure.html"><a href="network-failure.html#object-oriented-implementation"><i class="fa fa-check"></i><b>5.3.1</b> Object oriented implementation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="design-of-experiments.html"><a href="design-of-experiments.html"><i class="fa fa-check"></i><b>5.4</b> Design of experiments</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-random-variables.html"><a href="multivariate-random-variables.html"><i class="fa fa-check"></i><b>6</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="6.1" data-path="sequential-simulation.html"><a href="sequential-simulation.html"><i class="fa fa-check"></i><b>6.1</b> Sequential simulation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="sequential-simulation.html"><a href="sequential-simulation.html#sequential-mc-for-the-ar1-process"><i class="fa fa-check"></i><b>6.1.1</b> Sequential MC for the AR(1)-process</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="gaussian-random-variables.html"><a href="gaussian-random-variables.html"><i class="fa fa-check"></i><b>6.2</b> Gaussian random variables</a></li>
</ul></li>
<li class="part"><span><b>Part III: Optimization</b></span></li>
<li class="chapter" data-level="7" data-path="five-examples.html"><a href="five-examples.html"><i class="fa fa-check"></i><b>7</b> Five Examples</a><ul>
<li class="chapter" data-level="7.1" data-path="exp-fam.html"><a href="exp-fam.html"><i class="fa fa-check"></i><b>7.1</b> Exponential families</a><ul>
<li class="chapter" data-level="7.1.1" data-path="exp-fam.html"><a href="exp-fam.html#full-exponential-families"><i class="fa fa-check"></i><b>7.1.1</b> Full exponential families</a></li>
<li class="chapter" data-level="7.1.2" data-path="exp-fam.html"><a href="exp-fam.html#bayes-net"><i class="fa fa-check"></i><b>7.1.2</b> Exponential family Bayesian networks</a></li>
<li class="chapter" data-level="7.1.3" data-path="exp-fam.html"><a href="exp-fam.html#exp-fam-deriv"><i class="fa fa-check"></i><b>7.1.3</b> Likelihood computations</a></li>
<li class="chapter" data-level="7.1.4" data-path="exp-fam.html"><a href="exp-fam.html#curved-exponential-families"><i class="fa fa-check"></i><b>7.1.4</b> Curved exponential families</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="multinomial-models.html"><a href="multinomial-models.html"><i class="fa fa-check"></i><b>7.2</b> Multinomial models</a><ul>
<li class="chapter" data-level="7.2.1" data-path="multinomial-models.html"><a href="multinomial-models.html#pep-moth"><i class="fa fa-check"></i><b>7.2.1</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>7.3</b> Regression models</a></li>
<li class="chapter" data-level="7.4" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html"><i class="fa fa-check"></i><b>7.4</b> Finite mixture models</a><ul>
<li class="chapter" data-level="7.4.1" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#Gaus-mix-ex"><i class="fa fa-check"></i><b>7.4.1</b> Gaussian mixtures</a></li>
<li class="chapter" data-level="7.4.2" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#von-mises-mixtures"><i class="fa fa-check"></i><b>7.4.2</b> von Mises mixtures</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="mixed-models.html"><a href="mixed-models.html"><i class="fa fa-check"></i><b>7.5</b> Mixed models</a></li>
<li class="chapter" data-level="7.6" data-path="state-space-models.html"><a href="state-space-models.html"><i class="fa fa-check"></i><b>7.6</b> State space models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="numopt.html"><a href="numopt.html"><i class="fa fa-check"></i><b>8</b> Numerical optimization</a><ul>
<li class="chapter" data-level="8.1" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html"><i class="fa fa-check"></i><b>8.1</b> Algorithms and convergence</a><ul>
<li class="chapter" data-level="8.1.1" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#descent-algorithms"><i class="fa fa-check"></i><b>8.1.1</b> Descent algorithms</a></li>
<li class="chapter" data-level="8.1.2" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#maps-and-fixed-points"><i class="fa fa-check"></i><b>8.1.2</b> Maps and fixed points</a></li>
<li class="chapter" data-level="8.1.3" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#convergence-rate"><i class="fa fa-check"></i><b>8.1.3</b> Convergence rate</a></li>
<li class="chapter" data-level="8.1.4" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#stopping-criteria"><i class="fa fa-check"></i><b>8.1.4</b> Stopping criteria</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html"><i class="fa fa-check"></i><b>8.2</b> Descent direction algorithms</a><ul>
<li class="chapter" data-level="8.2.1" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#line-search"><i class="fa fa-check"></i><b>8.2.1</b> Line search</a></li>
<li class="chapter" data-level="8.2.2" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>8.2.2</b> Gradient descent</a></li>
<li class="chapter" data-level="8.2.3" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#conjugate-gradients"><i class="fa fa-check"></i><b>8.2.3</b> Conjugate gradients</a></li>
<li class="chapter" data-level="8.2.4" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#pep-moth-descent"><i class="fa fa-check"></i><b>8.2.4</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html"><i class="fa fa-check"></i><b>8.3</b> Newton-type algorithms</a><ul>
<li class="chapter" data-level="8.3.1" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#poisson-regression"><i class="fa fa-check"></i><b>8.3.1</b> Poisson regression</a></li>
<li class="chapter" data-level="8.3.2" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#quasi-newton-algorithms"><i class="fa fa-check"></i><b>8.3.2</b> Quasi-Newton algorithms</a></li>
<li class="chapter" data-level="8.3.3" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#sparsity"><i class="fa fa-check"></i><b>8.3.3</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="misc-.html"><a href="misc-.html"><i class="fa fa-check"></i><b>8.4</b> Misc.</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="em.html"><a href="em.html"><i class="fa fa-check"></i><b>9</b> Expectation maximization algorithms</a><ul>
<li class="chapter" data-level="9.1" data-path="basic-properties.html"><a href="basic-properties.html"><i class="fa fa-check"></i><b>9.1</b> Basic properties</a><ul>
<li class="chapter" data-level="9.1.1" data-path="basic-properties.html"><a href="basic-properties.html#incomplete-data-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Incomplete data likelihood</a></li>
<li class="chapter" data-level="9.1.2" data-path="basic-properties.html"><a href="basic-properties.html#monotonicity-of-the-em-algorithm"><i class="fa fa-check"></i><b>9.1.2</b> Monotonicity of the EM algorithm</a></li>
<li class="chapter" data-level="9.1.3" data-path="basic-properties.html"><a href="basic-properties.html#peppered-moths"><i class="fa fa-check"></i><b>9.1.3</b> Peppered moths</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="EM-exp.html"><a href="EM-exp.html"><i class="fa fa-check"></i><b>9.2</b> Exponential families</a></li>
<li class="chapter" data-level="9.3" data-path="fisher-information.html"><a href="fisher-information.html"><i class="fa fa-check"></i><b>9.3</b> Fisher information</a></li>
<li class="chapter" data-level="9.4" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html"><i class="fa fa-check"></i><b>9.4</b> Two examples revisited</a><ul>
<li class="chapter" data-level="9.4.1" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html#gaussian-mixtures"><i class="fa fa-check"></i><b>9.4.1</b> Gaussian mixtures</a></li>
<li class="chapter" data-level="9.4.2" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html#gaussian-state-space"><i class="fa fa-check"></i><b>9.4.2</b> Gaussian state space</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="stochopt.html"><a href="stochopt.html"><i class="fa fa-check"></i><b>10</b> Stochastic Optimization</a><ul>
<li class="chapter" data-level="10.1" data-path="stochastic-gradient.html"><a href="stochastic-gradient.html"><i class="fa fa-check"></i><b>10.1</b> Stochastic gradient</a></li>
<li class="chapter" data-level="10.2" data-path="stochastic-em.html"><a href="stochastic-em.html"><i class="fa fa-check"></i><b>10.2</b> Stochastic EM</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multinomial-models" class="section level2">
<h2><span class="header-section-number">7.2</span> Multinomial models</h2>
<p>The multinomial model is the model of all probability distributions on
a finite set <span class="math inline">\(\mathcal{Y} = \{1, \ldots, K\}\)</span>. The model is parametrized
by the simplex
<span class="math display">\[\Delta_K = \left\{(p_1, \ldots, p_K)^T \in \mathbb{R}^K \mid p_k \geq 0, \sum_{k=1}^K p_k = 1\right\}.\]</span>
The distributions parametrized by the relative interior of <span class="math inline">\(\Delta_K\)</span> form
an exponential family by the parametrization
<span class="math display">\[p_k = \frac{e^{\theta_k}}{\sum_{l=1}^K e^{\theta_l}} \in (0,1)\]</span>
for <span class="math inline">\((\theta_1, \ldots, \theta_K)^T \in \mathbb{R}^K.\)</span> That is,
the sufficient statistic is <span class="math inline">\(k \mapsto (\delta_{1k}, \ldots, \delta_{Kk})^T \in \mathbb{R}^{K-1}\)</span>
(where <span class="math inline">\(\delta_{lk} \in \{0, 1\}\)</span> is the Kronecker delta being 1 if and only
if <span class="math inline">\(l = k\)</span>), and
<span class="math display">\[\varphi(\theta_1, \ldots, \theta_K) = \sum_{l=1}^K e^{\theta_l}.\]</span>
We call this exponential family parametrization the <em>symmetric</em> parametrization.
The canonical parameter in the symmetric parametrization is not identifiable,
and to resolve the lack of identifiability
there is a tradition of fixing the last parameter as <span class="math inline">\(\theta_K = 0\)</span>. This
results in a canonical parameter <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_{K-1})^T \in \mathbb{R}^{K-1},\)</span>
a sufficient statistic <span class="math inline">\(t_1(k) = (\delta_{1k}, \ldots, \delta_{(K-1)k})^T \in \mathbb{R}^p\)</span>,
a
<span class="math display">\[\varphi(\theta) = 1 + \sum_{l=1}^{K-1} e^{\theta_l}\]</span>
and probabilities
<span class="math display">\[p_k = \left\{\begin{array}{ll} \frac{e^{\theta_k}}{1 + \sum_{l=1}^{K-1} e^{\theta_l}}  &amp; \quad \text{if } k = 1, \ldots, K-1 \\  \frac{1}{1 + \sum_{l=1}^{K-1} e^{\theta_l}} &amp; \quad \text{if } k = K. \end{array}\right.\]</span>
We see that in this parametrization <span class="math inline">\(p_k = e^{\theta_k}p_K\)</span> for <span class="math inline">\(k = 1, \ldots, K-1\)</span>,
where the probability of the last element <span class="math inline">\(K\)</span> acts as a baseline or reference probability,
and the factors <span class="math inline">\(e^{\theta_k}\)</span> act as multiplicative modifications of this
baseline. Moreover,
<span class="math display">\[\frac{p_k}{p_l} = e^{\theta_k - \theta_l},\]</span>
which is independent of the chosen baseline.</p>
<p>In the special case of <span class="math inline">\(K = 2\)</span> the two elements <span class="math inline">\(\mathcal{Y}\)</span> are often given
other labels than <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>. The most common labels are <span class="math inline">\(\{0, 1\}\)</span> and <span class="math inline">\(\{-1, 1\}\)</span>.
If we use the 0-1-labels the convention is to use <span class="math inline">\(p_0\)</span> as the baseline and
thus
<span class="math display">\[p_1 = \frac{e^{\theta}}{1 + e^{\theta}} = e^{\theta} p_0 = e^{\theta} (1 - p_0).\]</span>
parametrized by <span class="math inline">\(\theta \in \mathbb{R}\)</span>. As this function of <span class="math inline">\(\theta\)</span> is
known as the <em>logistic function</em>, this parametrization of the probability
distributions on <span class="math inline">\(\{0,1\}\)</span> is often referred to as the logistic model. From the
above we see directly that
<span class="math display">\[\theta = \log \frac{p_1}{1 - p_1}\]</span>
is the log-odds.</p>
<p>If we use the <span class="math inline">\(\pm 1\)</span>-labels, an alternative exponential family parametrization is
<span class="math display">\[p_z = \frac{e^{k\theta}}{e^\theta + e^{-\theta}}\]</span>
for <span class="math inline">\(\theta \in \mathbb{R}\)</span> and <span class="math inline">\(k \in \{-1, 1\}\)</span>. This gives a symmetric treatment of the two
labels while retaining the identifiability.</p>
<div id="pep-moth" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Peppered Moths</h3>
<p>This example is on the color of the peppered Moth (<em>Birkemåler</em> in Danish).
The color of the moth is
primarily determined by one gene that occur in three different alleles denoted C,
I and T. The alleles are ordered in terms of dominance as C &gt; I &gt; T. Moths with genotype
including C are dark. Moths
with genotype TT are light colored. Moths with genotypes II and IT are mottled.
Thus there a total of six different genotypes (CC, CI, CT, II, IT and TT) and
three different phenotypes (black, mottled, light colored).</p>
<p><img src="figures/peppered-moth.jpg" width="70%" style="display: block; margin: auto;" /></p>
<p>The peppered moth provided an <a href="https://en.wikipedia.org/wiki/Peppered_moth_evolution">early demonstration of evolution</a>
in the 19th century England, where the light colored moth was outnumbered by the dark
colored variety. The dark color became dominant due to the increased
pollution, where trees were darkened by soot, and had for that reason a selective
advantage. There is a nice collection
of moth in different colors at the Danish Zoological Museum, and further
explanation of the role it played in understanding evolution.</p>
<p>We denote the allele frequencies <span class="math inline">\(p_C\)</span>, <span class="math inline">\(p_I\)</span>, <span class="math inline">\(p_T\)</span> with <span class="math inline">\(p_C + p_I + p_T = 1.\)</span>
According to the <a href="https://en.wikipedia.org/wiki/Hardy–Weinberg_principle">Hardy-Weinberg principle</a>,
the genotype frequencies are then</p>
<p><span class="math display">\[p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2,  2p_Ip_T, p_T^2.\]</span></p>
<p>The complete multinomial log-likelihood is</p>
<p><span class="math display">\[\begin{align}
 &amp; 2n_{CC} \log(p_C) + n_{CI} \log (2 p_C p_I) + n_{CT} \log(2 p_C p_I) \\
&amp; \ \ + 2 n_{II} \log(p_I) + n_{IT} \log(2p_I p_T) + 2 n_{TT} \log(p_T),
\end{align}\]</span></p>
<p>In terms of the probability parameters <span class="math inline">\(p_C\)</span> and <span class="math inline">\(p_I\)</span> this is a curved
exponential family.</p>
<p>Collecting moths and determining their color will only identify their phenotype,
not their genotype. Thus we only observe <span class="math inline">\((n_C, n_T, n_I)\)</span>, where
<span class="math display">\[n = \underbrace{n_{CC} + n_{CI} + n_{CT}}_{= n_C} + 
\underbrace{n_{IT} + n_{II}}_{=n_I} + \underbrace{n_{TT}}_{=n_T}.\]</span></p>
<p>For maximum-likelihood estimation of the parameters in this model from
the observation <span class="math inline">\((n_C, n_T, n_I)\)</span>, we need the likelihood,
that is, the likelihood in the marginal distribution of the observed
variables. We will compute this in the section below by considering this
particular problem as an example of a more general problem.</p>
<p>The peppered Moth example is an example of <em>cell collapsing</em> in a multinomial model.
In general, let <span class="math inline">\(A_1 \cup \ldots \cup A_{K_0} = \{1, \ldots, K\}\)</span> be a partition and let
<span class="math display">\[M : \mathbb{N}_0^K \to \mathbb{N}_0^{K_0}\]</span>
be the map given by
<span class="math display">\[M((n_1, \ldots, n_K))_j = \sum_{k \in A_j} n_k.\]</span></p>
<p>If <span class="math inline">\(Y \sim \textrm{Mult}(p, n)\)</span> with <span class="math inline">\(p = (p_1, \ldots, p_K)\)</span> then
<span class="math display">\[X = M(Y) \sim \textrm{Mult}(M(p), n).\]</span>
If <span class="math inline">\(\theta \mapsto p(\theta)\)</span> is a parametrization of the cell probabilities
the log-likelihood under the collapsed multinomial model is generally</p>
<p><span class="math display" id="eq:mult-col-loglik">\[\begin{equation}
\ell(\theta) = \sum_{j = 1}^{K_0} x_j \log (M(p(\theta))_j) = \sum_{j = 1}^{K_0} x_j \log \left(\sum_{k \in A_j} p_k(\theta)\right).
\tag{7.2} 
\end{equation}\]</span></p>
<p>The conditional distribution of <span class="math inline">\(Y_{A_j} = (Y_k)_{k \in A_j}\)</span>, conditionally on
<span class="math inline">\(X\)</span>, can be found too. It will be used for the EM algorithm
in Chapter <a href="em.html#em">9</a>. It is easy to see that
<span class="math display">\[Y_{A_j} \mid X = x \sim \textrm{Mult}\left( \frac{p_{A_j}}{M(p)_j}, y_j \right).\]</span>
The probability parameters are simply <span class="math inline">\(p\)</span> restricted to <span class="math inline">\(A_j\)</span> and renormalized
to a probability vector. Observe that this gives
<span class="math display">\[E (Y_k \mid X = x) = \frac{x_j p_k}{M(p)_j}\]</span>
for <span class="math inline">\(k \in A_j\)</span>.</p>
<p>For the peppered Moths, <span class="math inline">\(K = 6\)</span> corresponding to the six genotypes, <span class="math inline">\(K_0 = 3\)</span> and
the partition corresponding to the phenotypes is
<span class="math display">\[\{1, 2, 3\} \cup \{4, 5\} \cup \{6\} = \{1, \ldots, 6\},\]</span>
and
<span class="math display">\[M(n_1, \ldots, n_6) = (n_1 + n_2 + n_3, n_4 + n_5, n_6).\]</span></p>
<p>In terms of the <span class="math inline">\((p_C, p_I)\)</span> parametrization, <span class="math inline">\(p_T = 1 - p_C - p_I\)</span> and
<span class="math display">\[p = (p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2,  2p_Ip_T, p_T^2).\]</span></p>
<p>Hence
<span class="math display">\[M(p) = (p_C^2 + 2p_Cp_I + 2p_Cp_T, p_I^2 +2p_Ip_T, p_T^2).\]</span></p>
<p>The log-likelihood is</p>
<p><span class="math display">\[\begin{align}
\ell(p_C, p_I) &amp; = n_C \log(p_C^2 + 2p_Cp_I + 2p_Cp_T) + n_I \log(p_I^2 +2p_Ip_T) + n_T \log (p_T^2).
\end{align}\]</span></p>
<p>We can implement the log-likelihood in a very problem specific way.
Note how the parameter constraints are encoded via the return value <span class="math inline">\(\infty\)</span>.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb214-1" data-line-number="1"><span class="co">## par = c(pC, pI), pT = 1 - pC - pI</span></a>
<a class="sourceLine" id="cb214-2" data-line-number="2"><span class="co">## x is the data vector of length 3 of counts </span></a>
<a class="sourceLine" id="cb214-3" data-line-number="3">loglik &lt;-<span class="st"> </span><span class="cf">function</span>(par, x) {</a>
<a class="sourceLine" id="cb214-4" data-line-number="4">  pT &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>par[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb214-5" data-line-number="5">  </a>
<a class="sourceLine" id="cb214-6" data-line-number="6">  <span class="cf">if</span> (par[<span class="dv">1</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> </a>
<a class="sourceLine" id="cb214-7" data-line-number="7">      <span class="op">||</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>pT <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb214-8" data-line-number="8">    <span class="kw">return</span>(<span class="ot">Inf</span>)</a>
<a class="sourceLine" id="cb214-9" data-line-number="9">  </a>
<a class="sourceLine" id="cb214-10" data-line-number="10">  PC &lt;-<span class="st"> </span>par[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>par[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>pT</a>
<a class="sourceLine" id="cb214-11" data-line-number="11">  PI &lt;-<span class="st"> </span>par[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>par[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>pT</a>
<a class="sourceLine" id="cb214-12" data-line-number="12">  PT &lt;-<span class="st"> </span>pT<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb214-13" data-line-number="13">  </a>
<a class="sourceLine" id="cb214-14" data-line-number="14">  <span class="op">-</span><span class="st"> </span>(x[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(PC) <span class="op">+</span><span class="st"> </span>x[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(PI) <span class="op">+</span><span class="st"> </span>x[<span class="dv">3</span>]<span class="op">*</span><span class="st"> </span><span class="kw">log</span>(PT))</a>
<a class="sourceLine" id="cb214-15" data-line-number="15">}</a></code></pre></div>
<p>It is possible to use <code>optim</code> in R with just this implementation to
compute the maximum-likelihood estimate of the allele parameters.</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb215-1" data-line-number="1"><span class="kw">optim</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), loglik, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>))</a></code></pre></div>
<pre><code>## $par
## [1] 0.07084643 0.18871900
## 
## $value
## [1] 600.481
## 
## $counts
## function gradient 
##       71       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p>The <code>optim</code> functions uses an algorithm called <a href="https://en.wikipedia.org/wiki/Nelder–Mead_method">Nelder-Mead</a>
as the default algorithm that relies on log-likelihood evaluations only. It
is slow but fairly robust, though a bit of thought has to go into the initial
parameter choice.</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb217-1" data-line-number="1"><span class="kw">optim</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), loglik, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>)) </a></code></pre></div>
<pre><code>## Error in optim(c(0, 0), loglik, x = c(85, 196, 341)): function cannot be evaluated at initial parameters</code></pre>
<p>Starting the algorithm in a boundary value where the negative log-likelihood attains
the value <span class="math inline">\(\infty\)</span> does not work.</p>
<p>The computations can beneficially be implemented in greater
generality. The function <code>M</code> sums the cells that are collapsed,
which has to be specified by the <code>group</code> argument.</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb219-1" data-line-number="1">M &lt;-<span class="st"> </span><span class="cf">function</span>(x, group)</a>
<a class="sourceLine" id="cb219-2" data-line-number="2">  <span class="kw">as.vector</span>(<span class="kw">tapply</span>(x, group, sum))</a></code></pre></div>
<p>The log-likelihood is then implemented for multinomial cell
collapsing via <code>M</code> and two problem specific arguments to
the <code>loglik</code> function. One of these is a vector specifying
the grouping structure of the collapsing. The other is a function
that determines the
parametrization that maps the parameter that we are optimizing over to the
cell probabilities. In the implementation it is assumed
that this <code>prob</code> function in addition encodes parameter constraints
by returning <code>NULL</code> if parameter constraints are violated.</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb220-1" data-line-number="1">loglik &lt;-<span class="st"> </span><span class="cf">function</span>(par, x, prob, group, ...) {</a>
<a class="sourceLine" id="cb220-2" data-line-number="2">  p &lt;-<span class="st"> </span><span class="kw">prob</span>(par)</a>
<a class="sourceLine" id="cb220-3" data-line-number="3">  <span class="cf">if</span>(<span class="kw">is.null</span>(p)) <span class="kw">return</span>(<span class="ot">Inf</span>)</a>
<a class="sourceLine" id="cb220-4" data-line-number="4">  <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(x <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">M</span>(<span class="kw">prob</span>(par), group)))</a>
<a class="sourceLine" id="cb220-5" data-line-number="5">}</a></code></pre></div>
<p>The function <code>prob</code> is implemented specifically for the peppered moths
as follows.</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb221-1" data-line-number="1">prob &lt;-<span class="st"> </span><span class="cf">function</span>(p) {</a>
<a class="sourceLine" id="cb221-2" data-line-number="2">  p[<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>p[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb221-3" data-line-number="3">  <span class="cf">if</span> (p[<span class="dv">1</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>p[<span class="dv">2</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>p[<span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>p[<span class="dv">3</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb221-4" data-line-number="4">    <span class="kw">return</span>(<span class="ot">NULL</span>)</a>
<a class="sourceLine" id="cb221-5" data-line-number="5">  <span class="kw">c</span>(p[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>], <span class="dv">2</span><span class="op">*</span><span class="st"> </span>p[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>], </a>
<a class="sourceLine" id="cb221-6" data-line-number="6">    p[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>p[<span class="dv">3</span>], p[<span class="dv">3</span>]<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb221-7" data-line-number="7">}</a></code></pre></div>
<p>We test that the new implementation gives the same result when
optimized as using the more problem specific implementation.</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb222-1" data-line-number="1"><span class="kw">optim</span>(<span class="kw">c</span>(<span class="fl">0.3</span>, <span class="fl">0.3</span>), loglik, <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">85</span>, <span class="dv">196</span>, <span class="dv">341</span>), </a>
<a class="sourceLine" id="cb222-2" data-line-number="2">      <span class="dt">prob =</span> prob, <span class="dt">group =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>))</a></code></pre></div>
<pre><code>## $par
## [1] 0.07084643 0.18871900
## 
## $value
## [1] 600.481
## 
## $counts
## function gradient 
##       71       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="exp-fam.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

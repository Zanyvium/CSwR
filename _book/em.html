<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 8 Expectation maximization algorithms | Computational Statistics with R</title>
<meta name="author" content="Niels Richard Hansen">
<meta name="description" content="Somewhat surprisingly, it is possible to develop an algorithm, known as the expectation-maximization algorithm, for computing the maximizer of a likelihood function in situations where computing...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 8 Expectation maximization algorithms | Computational Statistics with R">
<meta property="og:type" content="book">
<meta property="og:description" content="Somewhat surprisingly, it is possible to develop an algorithm, known as the expectation-maximization algorithm, for computing the maximizer of a likelihood function in situations where computing...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 8 Expectation maximization algorithms | Computational Statistics with R">
<meta name="twitter:description" content="Somewhat surprisingly, it is possible to develop an algorithm, known as the expectation-maximization algorithm, for computing the maximizer of a likelihood function in situations where computing...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/_Roboto%20Slab-0.4.0/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.3.0/transition.js"></script><script src="libs/bs3compat-0.3.0/tabs.js"></script><script src="libs/bs3compat-0.3.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Computational Statistics with R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li class="book-part">Part I: Smoothing</li>
<li><a class="" href="density.html"><span class="header-section-number">2</span> Density estimation</a></li>
<li><a class="" href="bivariate.html"><span class="header-section-number">3</span> Bivariate smoothing</a></li>
<li class="book-part">Part II: Monte Carlo Methods</li>
<li><a class="" href="univariate-random-variables.html"><span class="header-section-number">4</span> Univariate random variables</a></li>
<li><a class="" href="mci.html"><span class="header-section-number">5</span> Monte Carlo integration</a></li>
<li class="book-part">Part III: Optimization</li>
<li><a class="" href="four-examples.html"><span class="header-section-number">6</span> Four Examples</a></li>
<li><a class="" href="numopt.html"><span class="header-section-number">7</span> Numerical optimization</a></li>
<li><a class="active" href="em.html"><span class="header-section-number">8</span> Expectation maximization algorithms</a></li>
<li><a class="" href="StochOpt.html"><span class="header-section-number">9</span> Stochastic Optimization</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="app-R.html"><span class="header-section-number">A</span> R programming</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/nielsrhansen/CSwR">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="em" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Expectation maximization algorithms<a class="anchor" aria-label="anchor" href="#em"><i class="fas fa-link"></i></a>
</h1>
<p>Somewhat surprisingly, it is possible to develop an algorithm, known as the
<em>expectation-maximization algorithm</em>, for computing the maximizer of a likelihood
function in situations where computing the likelihood itself is quite
difficult. This is possible in situations where the model is defined in
terms of certain unobserved components, and where likelihood computations and
optimization is relatively easy had we had the complete observation. The EM algorithm
exploits this special structure, and is thus not a general optimization
algorithm, but the situation where it applies is common enough in statistics
that it is one of the core optimization algorithms used for computing
maximum-likelihood estimates.</p>
<p>In this chapter it is shown that the algorithm is generally an descent algorithm
of the negative log-likelihood, and examples of its implementation are given to
multinomial cell collapsing and Gaussian mixtures. The theoretical results needed
for the EM algorithm for a special case of mixed models are given as well. Finally,
some theoretical results as well as practical implementations for computing
estimates of the Fisher information are presented.</p>
<div id="basic-properties" class="section level2" number="8.1">
<h2>
<span class="header-section-number">8.1</span> Basic properties<a class="anchor" aria-label="anchor" href="#basic-properties"><i class="fas fa-link"></i></a>
</h2>
<p>In this section the EM algorithm is formulated and shown to be a descent algorithm
for the negative log-likelihood. Allele frequency estimation for the peppered moth
is considered as a simple example showing how the algorithm can be implemented.</p>
<div id="incomplete-data-likelihood" class="section level3" number="8.1.1">
<h3>
<span class="header-section-number">8.1.1</span> Incomplete data likelihood<a class="anchor" aria-label="anchor" href="#incomplete-data-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose that <span class="math inline">\(Y\)</span> is a random variable and <span class="math inline">\(X = M(Y)\)</span>. Suppose that <span class="math inline">\(Y\)</span> has density
<span class="math inline">\(f(\cdot \mid \theta)\)</span> and that <span class="math inline">\(X\)</span> has marginal density <span class="math inline">\(g(x \mid \theta)\)</span>.</p>
<p>The marginal density is typically of the form
<span class="math display">\[g(x \mid \theta) = \int_{\{y: M(y) = x\}} f(y \mid \theta) \ \mu_x(\mathrm{d} y)\]</span>
for a suitable measure <span class="math inline">\(\mu_x\)</span> depending on <span class="math inline">\(M\)</span> and <span class="math inline">\(x\)</span> but not <span class="math inline">\(\theta\)</span>.
The general argument for the marginal density relies on the coarea formula.</p>
<p>The log-likelihood for observing <span class="math inline">\(X = x\)</span> is
<span class="math display">\[\ell(\theta) = \log g(x \mid \theta).\]</span>
The log-likelihood is often
impossible to compute analytically and difficult and expensive to compute
numerically. The complete log-likelihood, <span class="math inline">\(\log f(y \mid \theta)\)</span>, is often easy to
compute, but we donâ€™t know <span class="math inline">\(Y\)</span>, only that <span class="math inline">\(M(Y) = x\)</span>.</p>
<p>In some cases it is possible to compute
<span class="math display">\[Q(\theta \mid \theta') := E_{\theta'}(\log f(Y \mid \theta) \mid X = x),\]</span>
which is the conditional expectation of the complete log-likelihood given
the observed data and computed using the probability measure given by <span class="math inline">\(\theta'\)</span>.
Thus for fixed <span class="math inline">\(\theta'\)</span> this is a computable function of <span class="math inline">\(\theta\)</span> depending
only on the observed data <span class="math inline">\(x\)</span>.</p>
<p>One could get the following idea: with an initial guess of
<span class="math inline">\(\theta' = \theta_0\)</span> compute iteratively
<span class="math display">\[\theta_{n + 1} = \textrm{arg max} \ Q(\theta \mid \theta_n)\]</span>
for <span class="math inline">\(n = 0, 1, 2, \ldots\)</span>. This idea is the EM algorithm:</p>
<ul>
<li>
<strong>E-step</strong>: Compute the conditional expectation <span class="math inline">\(Q(\theta \mid \theta_n )\)</span>.</li>
<li>
<strong>M-step</strong>: Maximize <span class="math inline">\(\theta \mapsto Q(\theta \mid \theta_n )\)</span>.</li>
</ul>
<p>It is a bit weird to present the algorithm as a two-step algorithm in its abstract
formulation. Even though we can regard <span class="math inline">\(Q(\theta \mid \theta_n)\)</span> as
something we can compute abstractly for each <span class="math inline">\(\theta\)</span> for a given <span class="math inline">\(\theta_n\)</span>,
the maximization is in practice not really done using all these evaluations. It is
computed either by an analytic formula involving <span class="math inline">\(x\)</span> and <span class="math inline">\(\theta_n\)</span>, or by a
numerical algorithm that computes certain evaluations of <span class="math inline">\(Q( \cdot \mid \theta_n)\)</span>
and perhaps its gradient and Hessian. In computing these specific evaluations
there is, of course, a need for the computation of conditional expectations,
but we would compute these as they are needed and not upfront.</p>
<p>However, in some of the most important applications of the EM algorithm, particularly
for exponential families covered in Section <a href="em.html#EM-exp">8.2</a>, it makes a lot of sense to regard
the algorithm as a two-step algorithm. This is the case whenever
<span class="math inline">\(Q(\theta \mid \theta_n) = q(\theta, t(x, \theta_n))\)</span> is given
in terms of <span class="math inline">\(\theta\)</span> and a function <span class="math inline">\(t(x, \theta_n )\)</span> of <span class="math inline">\(x\)</span> and <span class="math inline">\(\theta_n\)</span>
that doesnâ€™t depend on <span class="math inline">\(\theta\)</span>. Then the E-step becomes the computation of
<span class="math inline">\(t(x, \theta_n )\)</span>, and in the M-step, <span class="math inline">\(Q(\cdot \mid \theta_n )\)</span> is
maximized by maximizing <span class="math inline">\(q(\cdot, t(x, \theta_n ))\)</span>,
and the maximum is a function of <span class="math inline">\(t(x, \theta_n )\)</span>.</p>
</div>
<div id="monotonicity-of-the-em-algorithm" class="section level3" number="8.1.2">
<h3>
<span class="header-section-number">8.1.2</span> Monotonicity of the EM algorithm<a class="anchor" aria-label="anchor" href="#monotonicity-of-the-em-algorithm"><i class="fas fa-link"></i></a>
</h3>
<p>We prove below that the algorithm (weakly) increases the log-likelihood in every step,
and thus is a descent algorithm for the negative log-likelihood <span class="math inline">\(H = - \ell\)</span>.</p>
<p>It holds in great generality that the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span>
has density</p>
<p><span class="math display" id="eq:cond-dist">\[\begin{equation}
h(y \mid x, \theta) = \frac{f(y \mid \theta)}{g(x \mid \theta)}
\tag{8.1}
\end{equation}\]</span></p>
<p>w.r.t. the measure <span class="math inline">\(\mu_x\)</span> as above (that does not depend upon <span class="math inline">\(\theta\)</span>), and where
<span class="math inline">\(g\)</span> is the density for the marginal distribution.</p>
<p>This can be verified quite easily for discrete distributions and when
<span class="math inline">\(Y = (Z, X)\)</span> with joint density w.r.t. a product measure <span class="math inline">\(\mu \otimes \nu\)</span> that
does not depend upon <span class="math inline">\(\theta\)</span>. In the latter case, <span class="math inline">\(f(y \mid \theta) = f(z, x \mid \theta)\)</span> and
<span class="math display">\[g(x \mid \theta) = \int f(z, x \mid \theta) \ \mu(\mathrm{d} z)\]</span>
is the marginal density w.r.t. <span class="math inline">\(\nu\)</span>.</p>
<p>Whenever <a href="em.html#eq:cond-dist">(8.1)</a> holds it follows that<br><span class="math display">\[\ell(\theta) = \log g(x \mid \theta) = \log f(y \mid \theta) - \log h(y \mid x, \theta),\]</span>
where <span class="math inline">\(\ell(\theta)\)</span> is the log-likelihood.</p>
<div class="theorem">
<p><span id="thm:EM-inequality" class="theorem"><strong>Theorem 8.1  </strong></span>If <span class="math inline">\(\log f(Y \mid \theta)\)</span> as well as <span class="math inline">\(\log h(Y \mid x, \theta)\)</span> have
finite <span class="math inline">\(\theta'\)</span>-conditional expectation given <span class="math inline">\(M(Y) = x\)</span> then
<span class="math display">\[Q(\theta \mid \theta') &gt; Q(\theta' \mid \theta') \quad \Rightarrow \quad  \ell(\theta) &gt; \ell(\theta').\]</span></p>
</div>
<div class="proof boxed">
<p><span id="unlabeled-div-3" class="proof"><em>Proof</em>. </span>Since <span class="math inline">\(\ell(\theta)\)</span> depends on <span class="math inline">\(y\)</span> only through <span class="math inline">\(M(y) = x\)</span>,</p>
<p><span class="math display">\[\begin{align*}
\ell(\theta) &amp; = E_{\theta'} ( \ell(\theta) \mid X = x) \\
&amp; =  \underbrace{E_{\theta'} ( \log f(Y \mid \theta) \mid X = x)}_{Q(\theta \mid \theta')} +  \underbrace{ E_{\theta'} ( - \log h(Y \mid x, \theta) \mid X = x)}_{H(\theta \mid \theta')} \\
&amp; = Q(\theta \mid \theta') + H(\theta \mid \theta'). 
\end{align*}\]</span></p>
<p>Now for the second term we find, using <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality#Measure-theoretic_and_probabilistic_form">Jensenâ€™s inequality</a>
for the convex function <span class="math inline">\(-\log\)</span>, that</p>
<p><span class="math display">\[\begin{align*}
H(\theta \mid \theta') &amp; = \int - \log(h(y \mid x, \theta)) h(y \mid x, \theta') \mu_x(\mathrm{d}y) \\
&amp; = \int - \log\left(\frac{h(y \mid x, \theta)}{ h(y \mid x, \theta')}\right) h(y \mid x, \theta') \mu_x(\mathrm{d}y) \\ 
&amp; \quad + \int - \log(h(y \mid x, \theta')) h(y \mid x, \theta') \mu_x(\mathrm{d}y) \\
&amp; \geq  -\log \left( \int \frac{h(y \mid x, \theta)}{ h(y \mid x, \theta')} h(y \mid x, \theta') \mu_x(\mathrm{d}y) \right) + H(\theta' \mid \theta') \\
&amp; = -\log\Big(\underbrace{ \int h(y \mid x, \theta) \mu_x(\mathrm{d}y)}_{=1}\Big)  + H(\theta' \mid \theta') \\
&amp; =  H(\theta' \mid \theta').
\end{align*}\]</span></p>
<p>From this we see that</p>
<p><span class="math display">\[\ell(\theta) \geq  Q(\theta \mid \theta') + H(\theta' \mid \theta')\]</span></p>
<p>for all <span class="math inline">\(\theta\)</span> and the right hand side is a so-called minorant for the log-likelihood.
Observing that</p>
<p><span class="math display">\[\ell(\theta') = Q(\theta' \mid \theta') + H(\theta' \mid \theta')\]</span></p>
<p>completes the proof of the theorem.</p>
</div>
<p>Note that the proof above can also be given by referring to <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality#Information_theory">Gibbsâ€™ inequality in information theory</a>
stating that the Kullback-Leibler divergence is positive, or equivalently
that the cross-entropy <span class="math inline">\(H(\theta \mid \theta')\)</span> is smaller than the
entropy <span class="math inline">\(H(\theta' \mid \theta')\)</span>, but the proof of this is, in itself, a
consequence of Jensenâ€™s inequality just as above.</p>
<p>It follows from Theorem <a href="em.html#thm:EM-inequality">8.1</a> that if <span class="math inline">\(\theta_n\)</span> is computed
iteratively starting from <span class="math inline">\(\theta_0\)</span> such that
<span class="math display">\[Q(\theta_{n+1} \mid \theta_{n}) &gt; Q(\theta_{n} \mid \theta_{n}),\]</span>
then
<span class="math display">\[H(\theta_0) &gt; H(\theta_1) &gt; H(\theta_2) &gt; \ldots.\]</span>
This proves that the EM algorithm is a strict descent algorithm for the negative
log-likelihood as long as it is possible in each iteration to find
a <span class="math inline">\(\theta\)</span> such that <span class="math inline">\(Q(\theta \mid \theta_{n}) &gt; Q(\theta_{n} \mid \theta_{n}).\)</span></p>
<p>The term <em>EM algorithm</em> is reserved for the specific algorithm that maximizes
<span class="math inline">\(Q(\cdot \mid \theta_n)\)</span> in the M-step, but there is
no reason to insist on the M-step being a maximization. A choice
of ascent direction of <span class="math inline">\(Q(\cdot \mid \theta_n)\)</span> and a step-length
guaranteeing sufficient descent of <span class="math inline">\(H\)</span> (sufficient ascent of <span class="math inline">\(Q(\cdot \mid \theta_n)\)</span>)
will be enough to give a descent
algorithm. Any such variation is usually termed a generalized EM algorithm.</p>
<p>We could imagine that the minorant is a useful lower bound on the
difficult-to-compute log-likelihood. The additive constant <span class="math inline">\(H(\theta' \mid \theta')\)</span> in the minorant
is, however, not going to be computable in general either, and it is not clear that there is
any way to use the bound quantitatively.</p>
</div>
<div id="peppered-moths" class="section level3" number="8.1.3">
<h3>
<span class="header-section-number">8.1.3</span> Peppered moths<a class="anchor" aria-label="anchor" href="#peppered-moths"><i class="fas fa-link"></i></a>
</h3>
<p>We return in this section to the peppered moths and the implementation of
the EM algorithm for multinomial cell collapsing.</p>
<p>The EM algorithm can be implemented by two simple functions that compute
the conditional expectations (the E-step) and then maximization
of the complete observation log-likelihood.</p>
<div class="sourceCode" id="cb349"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">EStep0</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">p</span>, <span class="va">x</span>, <span class="va">group</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">x</span><span class="op">[</span><span class="va">group</span><span class="op">]</span> <span class="op">*</span> <span class="va">p</span> <span class="op">/</span> <span class="fu">M</span><span class="op">(</span><span class="va">p</span>, <span class="va">group</span><span class="op">)</span><span class="op">[</span><span class="va">group</span><span class="op">]</span>
<span class="op">}</span></code></pre></div>
<p>The MLE of the complete log-likelihood is a linear estimator,
as is the case in many examples with explicit MLEs.</p>
<div class="sourceCode" id="cb350"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">MStep0</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n</span>, <span class="va">X</span><span class="op">)</span>
  <span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="va">X</span> <span class="op">%*%</span> <span class="va">n</span> <span class="op">/</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>The <code>EStep0</code> and <code>MStep0</code> functions are abstract implementations. They require
specification of the arguments <code>group</code> and <code>X</code>, respectively, to become concrete.</p>
<p>The M-step is only implemented in the case where the complete-data MLE is a
<em>linear estimator</em>, that is, a linear map of the complete data vector <span class="math inline">\(y\)</span>
that can be expressed in terms of a matrix <span class="math inline">\(\mathbf{X}\)</span>.</p>
<div class="sourceCode" id="cb351"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">EStep</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">par</span>, <span class="va">x</span><span class="op">)</span>
  <span class="fu">EStep0</span><span class="op">(</span><span class="fu">prob</span><span class="op">(</span><span class="va">par</span><span class="op">)</span>, <span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>

<span class="va">MStep</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span>
  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">0</span>,
    <span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span> <span class="op">/</span> <span class="fl">2</span>,
  <span class="fl">2</span>, <span class="fl">6</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
  
  <span class="fu">MStep0</span><span class="op">(</span><span class="va">n</span>, <span class="va">X</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>The EM algorithm is finally implemented as an iterative, alternating call
of <code>EStep</code> and <code>MStep</code> until convergence as measured in terms of the relative
change from iteration to iteration being sufficiently small.</p>
<div class="sourceCode" id="cb352"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">EM</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">par</span>, <span class="va">x</span>, <span class="va">epsilon</span> <span class="op">=</span> <span class="fl">1e-6</span>, <span class="va">trace</span> <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span>
  <span class="kw">repeat</span><span class="op">{</span>
    <span class="va">par0</span> <span class="op">&lt;-</span> <span class="va">par</span>
    <span class="va">par</span> <span class="op">&lt;-</span> <span class="fu">MStep</span><span class="op">(</span><span class="fu">EStep</span><span class="op">(</span><span class="va">par</span>, <span class="va">x</span><span class="op">)</span><span class="op">)</span>
    <span class="kw">if</span><span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html">is.null</a></span><span class="op">(</span><span class="va">trace</span><span class="op">)</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/trace.html">trace</a></span><span class="op">(</span><span class="op">)</span>
    <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">par</span> <span class="op">-</span> <span class="va">par0</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">&lt;=</span> <span class="va">epsilon</span> <span class="op">*</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">par</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">epsilon</span><span class="op">)</span><span class="op">)</span>
      <span class="kw">break</span>
  <span class="op">}</span> 
  <span class="va">par</span>  <span class="co">## Remember to return the parameter estimate</span>
<span class="op">}</span>
  
<span class="va">phat</span> <span class="op">&lt;-</span> <span class="fu">EM</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.3</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">85</span>, <span class="fl">196</span>, <span class="fl">341</span><span class="op">)</span><span class="op">)</span>
<span class="va">phat</span></code></pre></div>
<pre><code>## [1] 0.07083693 0.18877365</code></pre>
<p>We check what is going on in each step of the EM algorithm.</p>
<div class="sourceCode" id="cb354"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">EM_tracer</span> <span class="op">&lt;-</span> <span class="fu">tracer</span><span class="op">(</span><span class="st">"par"</span><span class="op">)</span>
<span class="fu">EM</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.3</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">85</span>, <span class="fl">196</span>, <span class="fl">341</span><span class="op">)</span>, trace <span class="op">=</span> <span class="va">EM_tracer</span><span class="op">$</span><span class="va">tracer</span><span class="op">)</span></code></pre></div>
<pre><code>## n = 1: par = 0.08038585, 0.22464192; 
## n = 2: par = 0.07118928, 0.19546961; 
## n = 3: par = 0.07084985, 0.18993393; 
## n = 4: par = 0.07083738, 0.18894757; 
## n = 5: par = 0.07083693, 0.18877365;</code></pre>
<pre><code>## [1] 0.07083693 0.18877365</code></pre>
<div class="sourceCode" id="cb357"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">EM_tracer</span> <span class="op">&lt;-</span> <span class="fu">tracer</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"par0"</span>, <span class="st">"par"</span><span class="op">)</span>, N <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>
<span class="va">phat</span> <span class="op">&lt;-</span> <span class="fu">EM</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.3</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">85</span>, <span class="fl">196</span>, <span class="fl">341</span><span class="op">)</span>, epsilon <span class="op">=</span> <span class="fl">1e-20</span>, 
           trace <span class="op">=</span> <span class="va">EM_tracer</span><span class="op">$</span><span class="va">tracer</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb358"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">EM_trace</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">EM_tracer</span><span class="op">)</span>
  <span class="va">EM_trace</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/transform.html">transform</a></span><span class="op">(</span>
  <span class="va">EM_trace</span>, 
  n <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">EM_trace</span><span class="op">)</span>,
  par_norm_diff <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="op">(</span><span class="va">par0.1</span> <span class="op">-</span> <span class="va">par.1</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="op">(</span><span class="va">par0.2</span> <span class="op">-</span> <span class="va">par.2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>
<span class="op">)</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/qplot.html">qplot</a></span><span class="op">(</span><span class="va">n</span>, <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">par_norm_diff</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">EM_trace</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="CSwR_files/figure-html/moth_EM_figure-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>Note the log-axis. The EM-algorithm converges linearly (this is the terminology,
see <a href="numopt.html#algorithms-and-convergence">Algorithms and Convergence</a>). The log-rate of the convergence can be estimated
by least-squares.</p>
<div class="sourceCode" id="cb359"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">log_rate_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">par_norm_diff</span><span class="op">)</span> <span class="op">~</span> <span class="va">n</span>,  data <span class="op">=</span> <span class="va">EM_trace</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coefficients</a></span><span class="op">(</span><span class="va">log_rate_fit</span><span class="op">)</span><span class="op">[</span><span class="st">"n"</span><span class="op">]</span><span class="op">)</span></code></pre></div>
<pre><code>##         n 
## 0.1750251</code></pre>
<p>The rate is very small in this case implying fast convergence. This is not always the case.
If the log-likelihood is flat, the EM-algorithm can become quite slow with a
rate close to 1.</p>
</div>
</div>
<div id="EM-exp" class="section level2" number="8.2">
<h2>
<span class="header-section-number">8.2</span> Exponential families<a class="anchor" aria-label="anchor" href="#EM-exp"><i class="fas fa-link"></i></a>
</h2>
<p>We consider in this section the special case where the model of <span class="math inline">\(\mathbf{y}\)</span>
is given as an exponential family Bayesian network as in Section <a href="four-examples.html#bayes-net">6.1.2</a>
and <span class="math inline">\(x = M(\mathbf{y})\)</span> is the observed transformation.</p>
<p>The complete data log-likelihood is
<span class="math display">\[\theta \mapsto \theta^T t(\mathbf{y}) - \kappa(\theta)  = \theta^T \sum_{j=1}^m t_j(y_j) -  \kappa(\theta),\]</span>
and we find that
<span class="math display">\[Q(\theta \mid \theta') = \theta^T \sum_{j=1}^m E_{\theta'}(t_j(Y_j) \mid X = x)  - 
E_{\theta'}( \kappa(\theta) \mid X = x).\]</span></p>
<p>To maximize <span class="math inline">\(Q\)</span> we differentiate <span class="math inline">\(Q\)</span> and equate the derivative equal to zero. We
find that the resulting equation is
<span class="math display">\[\sum_{j=1}^m E_{\theta'}(t_j(Y_j) \mid X = x) = E_{\theta'}( \nabla \kappa(\theta) \mid X = x).\]</span></p>
<p>Alternatively, one may also note the following general equation for finding
the maximum of <span class="math inline">\(Q(\cdot \mid \theta')\)</span>
<span class="math display">\[\sum_{j=1}^m E_{\theta'}(t_j(Y_j) \mid X = x) = \sum_{j=1}^m E_{\theta'}(E_{\theta}(t_j(Y_j) \mid y_1, \ldots, y_{j-1}) \mid X = x),\]</span>
since
<span class="math display">\[E_{\theta'}(\nabla \kappa(\theta)\mid X = x) = 
\sum_{j=1}^m E_{\theta'}(\nabla \log \varphi_j(\theta) \mid X = x) = 
\sum_{j=1}^m E_{\theta'}(E_{\theta}(t_j(Y_j) \mid y_1, \ldots, y_{j-1}) \mid X = x) \]</span></p>

<div class="example">
<p><span id="exm:gaussian-mixture-em" class="example"><strong>Example 8.1  </strong></span>Continuing Example <a href="four-examples.html#exm:gaussian-mixed">6.4</a> with <span class="math inline">\(M\)</span> the projection map
<span class="math display">\[(\mathbf{y}, \mathbf{z}) \mapsto \mathbf{y}\]</span>
we see that <span class="math inline">\(Q\)</span> is maximized in <span class="math inline">\(\theta\)</span> by solving
<span class="math display">\[\sum_{i,j} E_{\theta'}(t(Y_{ij} \mid Z_i) \mid \mathbf{Y} = \mathbf{y}) = 
  \sum_{i} m_i E_{\theta'}(\nabla \kappa(\theta \mid Z_i) \mid \mathbf{Y} = \mathbf{y}).\]</span></p>
<p>By using Example <a href="four-examples.html#exm:gaussian-exponential">6.2</a> we see that
<span class="math display">\[\kappa(\theta \mid Z_i) = \frac{(\theta_1 + \theta_3 Z_i)^2}{4\theta_2} - \frac{1}{2}\log \theta_2,\]</span>
hence</p>
<p><span class="math display">\[\nabla \kappa(\theta \mid Z_i) = \frac{1}{2\theta_2} \left(\begin{array}{cc} \theta_1 + \theta_3 Z_i \\ 
- \frac{(\theta_1 + \theta_3 Z_i)^2}{2\theta_2} - 1 \\ \theta_1 Z_i + \theta_3 Z_i^2 \end{array}\right)
= \left(\begin{array}{cc} \beta_0 + \nu Z_i \\ 
- (\beta_0 + \nu Z_i)^2 - \sigma^2 \\ \beta_0 Z_i + \nu Z_i^2 \end{array}\right).\]</span></p>
<p>Therefore, <span class="math inline">\(Q\)</span> is maximized by solving the equation</p>
<p><span class="math display">\[\sum_{i,j} \left(\begin{array}{cc}  y_{ij} \\ -  y_{ij}^2 \\ E_{\theta'}(Z_i \mid \mathbf{Y} = \mathbf{y}) y_{ij} \end{array}\right) = \sum_{i}  m_i \left(\begin{array}{cc} \beta_0 + \nu E_{\theta'}(Z_i \mid \mathbf{Y}_i = \mathbf{y}_i) \\ 
- E_{\theta'}((\beta_0 + \nu Z_i)^2 \mid \mathbf{Y} = \mathbf{y}) - \sigma^2 \\ \beta_0 E_{\theta'}(Z_i \mid \mathbf{Y} = \mathbf{y}) + \nu E_{\theta'}(Z_i^2 \mid \mathbf{Y} = \mathbf{y}) \end{array}\right).\]</span>
Introducing first <span class="math inline">\(\xi_i = E_{\theta'}(Z_i \mid \mathbf{Y} = \mathbf{y})\)</span> and
<span class="math inline">\(\zeta_i = E_{\theta'}(Z_i^2 \mid \mathbf{Y} = \mathbf{y})\)</span> we can rewrite the
first and last of the three equations as the linear equation
<span class="math display">\[ \left(\begin{array}{cc} \sum_{i} m_i&amp; \sum_{i} m_i\xi_i \\ \sum_{i} m_i\xi_i &amp; \sum_{i} m_i\zeta_i \end{array}\right) 
\left(\begin{array}{c} \beta_0 \\  \nu \end{array}\right) = \left(\begin{array}{cc}  \sum_{i,j} y_{ij} \\ \sum_{i,j} \xi_i y_{ij} \end{array}\right).\]</span>
Plugging the solution for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\nu\)</span> into the second equation we
find
<span class="math display">\[\sigma^2 = \frac{1}{\sum_{i} m_i}\left(\sum_{ij} y_{ij}^2 - \sum_{i} m_i(\beta_0^2 + \nu^2 \zeta_i + 2 \beta_0 \nu \xi_i)\right).\]</span></p>
<p>This solves the M-step of the EM algorithm for the mixed effects model. What
remains is the E-step that amounts to the computation of <span class="math inline">\(\xi_i\)</span> and <span class="math inline">\(\zeta_i\)</span>.
We know that the joint distribution of <span class="math inline">\(\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{Z}\)</span> is Gaussian,
and we can easily compute the variances and covariances:
<span class="math display">\[\mathrm{cov}(Z_i, Z_j) = \delta_{ij}\]</span></p>
<p><span class="math display">\[\mathrm{cov}(Y_{ij}, Y_{kl}) = \left\{ \begin{array}{ll}  \nu^2 + \sigma^2 &amp; \quad \text{if } i = k, j = l \\
\nu^2 &amp; \quad \text{if } i = k, j \neq l  \\
0 &amp; \quad \text{otherwise } \end{array} \right.\]</span></p>
<p><span class="math display">\[\mathrm{cov}(Z_i, Y_{kl}) = \left\{ \begin{array}{ll}  \nu  &amp; \quad \text{if } i = k \\
0 &amp; \quad \text{otherwise } \end{array} \right.\]</span></p>
<p>This gives a joint Gaussian distribution
<span class="math display">\[\left( \begin{array}{c} \mathbf{Z} \\ \mathbf{Y} \end{array} \right)  \sim \mathcal{N}\left(
\left(\begin{array}{c} \mathbf{0} \\ \beta_0 \mathbf{1}\end{array} \right), 
\left(\begin{array}{cc}  \Sigma_{11} &amp; \Sigma_{12} \\ \Sigma_{21} &amp; \Sigma_{22} \end{array}\right)\right).\]</span></p>
<p>From this and the general formulas for <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions">computing conditional distributions
in the multivariate Gaussian distribution</a>:
<span class="math display">\[\mathbf{Z} \mid \mathbf{Y} \sim \mathcal{N}\left( \Sigma_{12} \Sigma_{22}^{-1}(\mathbf{Y} - \beta_0 \mathbf{1}), 
\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} \right).\]</span>
The conditional means, <span class="math inline">\(\xi_i\)</span>, are thus the coordinates of <span class="math inline">\(\Sigma_{12} \Sigma_{22}^{-1}(\mathbf{Y} - \beta_0 \mathbf{1})\)</span>. The conditional second moments, <span class="math inline">\(\zeta_i\)</span>, can be found as the diagonal
elements of the conditional covariance matrix plus <span class="math inline">\(\xi_i^2\)</span>.</p>
</div>
</div>
<div id="fisher-information" class="section level2" number="8.3">
<h2>
<span class="header-section-number">8.3</span> Fisher information<a class="anchor" aria-label="anchor" href="#fisher-information"><i class="fas fa-link"></i></a>
</h2>
<p>For statistics relying on classical asymptotic theory
we need an estimate of the Fisher information, e.g.Â the observed Fisher information
(Hessian of the negative log-likelihood for the observed data). For numerical
optimization of <span class="math inline">\(Q\)</span> or variants
of the EM algorithm (like EM gradient or acceleration methods) the gradient and Hessian of <span class="math inline">\(Q\)</span>
can be useful. However, these do not directly inform us on the Fisher information.
In this section we show some interesting and useful relations between
the derivatives of the log-likelihood for the observed data and derivatives of
<span class="math inline">\(Q\)</span> with the primary purpose of estimating the Fisher information.</p>
<p>First we look at the peppered moth example, where we note that with <span class="math inline">\(p = p(\theta)\)</span>
being some parametrization of the cell probabilities,
<span class="math display">\[Q(\theta \mid \theta') = \sum_{k=1}^K \frac{x_{j(k)} p_k(\theta')}{M(p(\theta'))_{j(k)}} \log p_k(\theta),\]</span>
where <span class="math inline">\(j(k)\)</span> is defined by <span class="math inline">\(k \in A_{j(k)}\)</span>. The gradient of <span class="math inline">\(Q\)</span> w.r.t.
<span class="math inline">\(\theta\)</span> is therefore</p>
<p><span class="math display">\[\nabla_{\theta} Q(\theta \mid \theta') = 
\sum_{k = 1}^K \frac{x_{j(k)} p_k(\theta')}{M(p(\theta'))_{j(k)} p_k(\theta)} \nabla_{\theta} p_k(\theta').\]</span></p>
<p>We recognize from previous computations in Section <a href="numopt.html#pep-moth-descent">7.2.4</a>
that when we evaluate <span class="math inline">\(\nabla_{\theta} Q(\theta \mid \theta')\)</span> in <span class="math inline">\(\theta = \theta'\)</span>
we get</p>
<p><span class="math display">\[\nabla_{\theta} Q(\theta' \mid \theta') = \sum_{i = 1}^K \frac{x_{j(i)} }{M(p(\theta'))_{j(i)}} \nabla_{\theta} p_i(\theta') = \nabla_{\theta} \ell(\theta'),\]</span></p>
<p>thus the gradient of <span class="math inline">\(\ell\)</span> in <span class="math inline">\(\theta'\)</span> is actually
identical to the gradient of <span class="math inline">\(Q(\cdot \mid \theta')\)</span> in <span class="math inline">\(\theta'\)</span>. This
is not a coincidence, and it holds generally that
<span class="math display">\[\nabla_{\theta} Q(\theta' \mid \theta') = \nabla_{\theta} \ell(\theta').\]</span>
This follows from the fact we derived in the proof of Theorem <a href="em.html#thm:EM-inequality">8.1</a>
that <span class="math inline">\(\theta'\)</span> minimizes</p>
<p><span class="math display">\[\theta \mapsto \ell(\theta) - Q(\theta \mid \theta').\]</span></p>
<p>Another way to phrase this is that the minorant of <span class="math inline">\(\ell(\theta)\)</span> touches
<span class="math inline">\(\ell\)</span> tangentially in <span class="math inline">\(\theta'\)</span>.</p>
<p>In the case where the observation <span class="math inline">\(\mathbf{y}\)</span> consists of <span class="math inline">\(n\)</span> i.i.d. observations
from the model with parameter <span class="math inline">\(\theta_0\)</span>, <span class="math inline">\(\ell\)</span> as well as <span class="math inline">\(Q(\cdot \mid \theta')\)</span> are sums of terms for which
the gradient identity above holds for each term. In particular,
<span class="math display">\[\nabla_{\theta} \ell(\theta_0) = \sum_{i=1}^n \nabla_{\theta} \ell_i(\theta_0) = \sum_{i=1}^n \nabla_{\theta} Q_i(\theta_0 \mid \theta_0),\]</span>
and using the second Bartlett identity</p>
<p><span class="math display">\[\mathcal{I}(\theta_0) = V_{\theta_0}(\nabla_{\theta} \ell(\theta_0))\]</span></p>
<p>we see that</p>
<p><span class="math display">\[\hat{\mathcal{I}}(\theta_0) =  \sum_{i=1}^n \big(\nabla_{\theta} Q_i(\theta_0 \mid \theta_0) - n^{-1} \nabla_{\theta} \ell(\theta_0)\big)\big(\nabla_{\theta} Q_i(\theta_0 \mid \theta_0) - n^{-1} \nabla_{\theta} \ell(\theta_0)\big)^T\]</span></p>
<p>is almost an unbiased estimator of the
Fisher information. It does have mean <span class="math inline">\(\mathcal{I}(\theta_0)\)</span>, but it is not an
estimator as <span class="math inline">\(\theta_0\)</span> is not known. Using a plug-in-estimator,
<span class="math inline">\(\hat{\theta}\)</span>, of <span class="math inline">\(\theta_0\)</span> we get a real estimator</p>
<p><span class="math display">\[\hat{\mathcal{I}} = \hat{\mathcal{I}}(\hat{\theta}) =  \sum_{i=1}^n \big(\nabla_{\theta} Q_i(\hat{\theta} \mid \hat{\theta}) - n^{-1} \nabla_{\theta} \ell(\hat{\theta})\big)\big(\nabla_{\theta} Q_i(\hat{\theta} \mid \hat{\theta}) - n^{-1} \nabla_{\theta} \ell(\hat{\theta})\big)^T,\]</span></p>
<p>though <span class="math inline">\(\hat{\mathcal{I}}\)</span> will no longer necessarily be unbiased.</p>
<p>We refer to <span class="math inline">\(\hat{\mathcal{I}}\)</span> as the <em>empirical Fisher information</em> given by
the estimator <span class="math inline">\(\hat{\theta}\)</span>. In most cases, <span class="math inline">\(\hat{\theta}\)</span> is the maximum-likelihood
estimator, in which case <span class="math inline">\(\nabla_{\theta} \ell(\hat{\theta}) = 0\)</span> and the empirical
Fisher information simplifies to
<span class="math display">\[\hat{\mathcal{I}} = \sum_{i=1}^n \nabla_{\theta} Q_i(\hat{\theta} \mid \hat{\theta}) \nabla_{\theta} Q_i(\hat{\theta} \mid \hat{\theta})^T.\]</span>
However, <span class="math inline">\(\nabla_{\theta} \ell(\hat{\theta})\)</span> is in practice only approximately
equal to zero, and it is unclear if it should be dropped.</p>
<p>For the peppered moths, where data is collected as i.i.d. samples of <span class="math inline">\(n\)</span>
individual specimens and tabulated according to phenotype, we implement
the empirical Fisher information with the optional possibility of centering
the gradients before computing the information estimate. We note that only
three different observations of phenotype are possible, giving rise to
three different possible terms in the sum. The implementation
works directly on the tabulated data by computing all the three possible
terms and then forming a weighted sum according to the number of times each
term is present.</p>
<div class="sourceCode" id="cb361"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">empFisher</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">par</span>, <span class="va">x</span>, <span class="va">grad</span>, <span class="va">center</span> <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">grad_MLE</span> <span class="op">&lt;-</span> <span class="fl">0</span> <span class="co">## is supposed to be 0 in the MLE</span>
  <span class="kw">if</span> <span class="op">(</span><span class="va">center</span><span class="op">)</span> 
     <span class="va">grad_MLE</span> <span class="op">&lt;-</span>  <span class="fu">grad</span><span class="op">(</span><span class="va">par</span>, <span class="va">x</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
   <span class="va">grad1</span> <span class="op">&lt;-</span> <span class="fu">grad</span><span class="op">(</span><span class="va">par</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="va">grad_MLE</span>
   <span class="va">grad2</span> <span class="op">&lt;-</span> <span class="fu">grad</span><span class="op">(</span><span class="va">par</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="va">grad_MLE</span>
   <span class="va">grad3</span> <span class="op">&lt;-</span> <span class="fu">grad</span><span class="op">(</span><span class="va">par</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="va">grad_MLE</span>
   <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">grad1</span><span class="op">)</span> <span class="op">%*%</span> <span class="va">grad1</span> <span class="op">+</span> 
     <span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">grad2</span><span class="op">)</span> <span class="op">%*%</span> <span class="va">grad2</span> <span class="op">+</span> 
     <span class="va">x</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">grad3</span><span class="op">)</span> <span class="op">%*%</span> <span class="va">grad3</span> 
<span class="op">}</span></code></pre></div>
<p>We test the implementation with and without centering and compare
the result to a numerically computed hessian using <code>optimHess</code> (it is
possible to get <code>optim</code> to compute the Hessian numerically in the minimizer
as a final step, but <code>optimHess</code> does this computation separately).</p>
<div class="sourceCode" id="cb362"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## The gradient of Q (equivalently the log-likelihood) was </span>
<span class="co">## implemented earlier as 'grad_loglik'.</span>
<span class="va">grad</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">par</span>, <span class="va">x</span><span class="op">)</span> <span class="fu">grad_loglik</span><span class="op">(</span><span class="va">par</span>, <span class="va">x</span>, <span class="va">prob</span>, <span class="va">Dprob</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>
<span class="fu">empFisher</span><span class="op">(</span><span class="va">phat</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">85</span>, <span class="fl">196</span>, <span class="fl">341</span><span class="op">)</span>, <span class="va">grad</span><span class="op">)</span></code></pre></div>
<pre><code>##           [,1]     [,2]
## [1,] 18487.558 1384.626
## [2,]  1384.626 6816.612</code></pre>
<div class="sourceCode" id="cb364"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">empFisher</span><span class="op">(</span><span class="va">phat</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">85</span>, <span class="fl">196</span>, <span class="fl">341</span><span class="op">)</span>, <span class="va">grad</span>, center <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<pre><code>##           [,1]     [,2]
## [1,] 18487.558 1384.626
## [2,]  1384.626 6816.612</code></pre>
<div class="sourceCode" id="cb366"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optimHess</a></span><span class="op">(</span><span class="va">phat</span>, <span class="va">loglik</span>, <span class="va">grad_loglik</span>, x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">85</span>, <span class="fl">196</span>, <span class="fl">341</span><span class="op">)</span>, 
          prob <span class="op">=</span> <span class="va">prob</span>, Dprob <span class="op">=</span> <span class="va">Dprob</span>, group <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##           [,1]     [,2]
## [1,] 18490.938 1384.629
## [2,]  1384.629 6816.769</code></pre>
<p>Note that the numerically computed Hessian (the <em>observed</em> Fisher information)
and the empirical Fisher information are different
estimates of the same quantity. Thus they are <em>not</em> supposed to be identical on
a given data set, but they are supposed to be estimates of the same thing
and thus to be similar.</p>
<p>An alternative to the empirical Fisher information or a direct computation of
the observed Fisher information is supplemented EM (SEM). This is a general method
for computing the observed Fisher
information that relies only on EM steps and a numerical differentiation scheme.
Define the EM map <span class="math inline">\(\Phi : \Theta \mapsto \Theta\)</span> by</p>
<p><span class="math display">\[\Phi(\theta') = \textrm{arg max}_{\theta} \ Q(\theta \mid \theta').\]</span></p>
<p>A global maximum of the likelihood is a fixed point of <span class="math inline">\(\Phi\)</span>, and the
EM algorithm searches for a fixed point for <span class="math inline">\(\Phi\)</span>, that is, a solution to</p>
<p><span class="math display">\[\Phi(\theta) = \theta.\]</span></p>
<p>Variations of the EM-algorithm can often be seen as other ways to
find a fixed point for <span class="math inline">\(\Phi\)</span>. From
<span class="math display">\[\ell(\theta) = Q(\theta \mid \theta') + H(\theta \mid \theta')\]</span>
it follows that the observed Fisher information equals</p>
<p><span class="math display">\[\hat{i}_X := - D^2_{\theta} \ell(\hat{\theta}) = 
\underbrace{-D^2_{\theta} Q(\hat{\theta} \mid \theta')}_{= \hat{i}_Y(\theta')} - D
\underbrace{^2_{\theta} H(\hat{\theta} \mid \theta')}_{= \hat{i}_{Y \mid X}(\theta')}.\]</span></p>
<p>It is possible to compute <span class="math inline">\(\hat{i}_Y := \hat{i}_Y(\hat{\theta})\)</span>.
For peppered moths (and exponential families)
it is as difficult as computing the Fisher information for complete observations.</p>
<p>We want to compute <span class="math inline">\(\hat{i}_X\)</span> but <span class="math inline">\(\hat{i}_{Y \mid X} := \hat{i}_{Y \mid X}(\hat{\theta})\)</span>
is not computable either. It can, however, be shown that</p>
<p><span class="math display">\[D_{\theta} \Phi(\hat{\theta})^T = \hat{i}_{Y\mid X} \left(\hat{i}_Y\right)^{-1}.\]</span></p>
<p>Hence
<span class="math display">\[\begin{align}
\hat{i}_X &amp; = \left(I - \hat{i}_{Y\mid X} \left(\hat{i}_Y\right)^{-1}\right) \hat{i}_Y \\
&amp; = \left(I - D_{\theta} \Phi(\hat{\theta})^T\right) \hat{i}_Y.
\end{align}\]</span></p>
<p>Though the EM map <span class="math inline">\(\Phi\)</span> might not have a simple analytic expression,
its Jacobian, <span class="math inline">\(D_{\theta} \Phi(\hat{\theta})\)</span>, can be computed via numerical
differentiation once we have implemented <span class="math inline">\(\Phi\)</span>. We also need the
hessian of the map <span class="math inline">\(Q\)</span>, which we implement as an R function as well.</p>
<div class="sourceCode" id="cb368"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">Q</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">p</span>, <span class="va">pp</span>, <span class="va">x</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">85</span>, <span class="fl">196</span>, <span class="fl">341</span><span class="op">)</span>, <span class="va">group</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">p</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">p</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">p</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>
  <span class="va">pp</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">pp</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="va">pp</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>
  <span class="op">-</span> <span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">group</span><span class="op">]</span> <span class="op">*</span> <span class="fu">prob</span><span class="op">(</span><span class="va">pp</span><span class="op">)</span> <span class="op">/</span> <span class="fu">M</span><span class="op">(</span><span class="fu">prob</span><span class="op">(</span><span class="va">pp</span><span class="op">)</span>, <span class="va">group</span><span class="op">)</span><span class="op">[</span><span class="va">group</span><span class="op">]</span><span class="op">)</span> <span class="op">%*%</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu">prob</span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>The R package numDeriv contains functions that compute numerical derivatives.</p>
<div class="sourceCode" id="cb369"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://optimizer.r-forge.r-project.org/">numDeriv</a></span><span class="op">)</span></code></pre></div>
<p>The Hessian of <span class="math inline">\(Q\)</span> can be computed using this package.</p>
<div class="sourceCode" id="cb370"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">iY</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/numDeriv/man/hessian.html">hessian</a></span><span class="op">(</span><span class="va">Q</span>, <span class="va">phat</span>, pp <span class="op">=</span> <span class="va">phat</span>, group <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>Supplemented EM can then be implemented by computing the Jacobian of
<span class="math inline">\(\Phi\)</span> using numDeriv as well.</p>
<div class="sourceCode" id="cb371"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">Phi</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">pp</span><span class="op">)</span> <span class="fu">MStep</span><span class="op">(</span><span class="fu">EStep</span><span class="op">(</span><span class="va">pp</span>, x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">85</span>, <span class="fl">196</span>, <span class="fl">341</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="va">DPhi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/numDeriv/man/jacobian.html">jacobian</a></span><span class="op">(</span><span class="va">Phi</span>, <span class="va">phat</span><span class="op">)</span>  <span class="co">## Using numDeriv function 'jacobian'</span>
<span class="va">iX</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">DPhi</span><span class="op">)</span><span class="op">)</span> <span class="op">%*%</span> <span class="va">iY</span>
<span class="va">iX</span></code></pre></div>
<pre><code>##           [,1]     [,2]
## [1,] 18487.558 1384.626
## [2,]  1384.626 6816.612</code></pre>
<p>For statistics, we actually need the inverse Fisher information, which can
be computed by inverting <span class="math inline">\(\hat{i}_X\)</span>, but we also have the following
interesting identity</p>
<p><span class="math display">\[\begin{align}
\hat{i}_X^{-1} &amp; = \hat{i}_Y^{-1} \left(I - D_{\theta} \Phi(\hat{\theta})^T\right)^{-1} \\
 &amp; = \hat{i}_Y^{-1} \left(I + \sum_{n=1}^{\infty} \left(D_{\theta} \Phi(\hat{\theta})^T\right)^n \right) \\
 &amp; = \hat{i}_Y^{-1} + \hat{i}_Y^{-1} D_{\theta} \Phi(\hat{\theta})^T \left(I - D_{\theta} \Phi(\hat{\theta})^T\right)^{-1}
\end{align}\]</span></p>
<p>where the second identity follows by the
<a href="https://en.wikipedia.org/wiki/Neumann_series">Neumann series</a>.</p>
<p>The last formula above explicitly gives the asymptotic variance for the incomplete
observation <span class="math inline">\(X\)</span> as the asymptotic variance for the complete observation <span class="math inline">\(Y\)</span> plus
a correction term.</p>
<div class="sourceCode" id="cb373"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">iYinv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="va">iY</span><span class="op">)</span>
<span class="va">iYinv</span> <span class="op">+</span> <span class="va">iYinv</span> <span class="op">%*%</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span> <span class="op">-</span> <span class="va">DPhi</span>, <span class="va">DPhi</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##               [,1]          [,2]
## [1,]  5.492602e-05 -1.115686e-05
## [2,] -1.115686e-05  1.489667e-04</code></pre>
<div class="sourceCode" id="cb375"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/solve-methods.html">solve</a></span><span class="op">(</span><span class="va">iX</span><span class="op">)</span> <span class="co">## SEM-based, but different use of inversion</span></code></pre></div>
<pre><code>##               [,1]          [,2]
## [1,]  5.492602e-05 -1.115686e-05
## [2,] -1.115686e-05  1.489667e-04</code></pre>
<p>The SEM implementation above relies on the <code>hessian</code> and <code>jacobian</code> functions from the
numDeriv package for numerical differentiation.</p>
<p>It is possible to implement the computation of the hessian of <span class="math inline">\(Q\)</span> analytically
for the peppered moths, but to illustrate functionality of the numDeriv package
we implemented the computation numerically above.</p>
<p>Variants on the strategy for computing <span class="math inline">\(D_{\theta} \Phi(\hat{\theta})\)</span> via
numerical differentiation have been suggested in the literature, specifically
using difference quotient approximations along the
sequence of EM steps. This is not going to work as well as standard numerical
differentiation since this method ignores numerical errors, and when the algorithm
gets sufficiently close to the MLE, the numerical errors will dominate in
the difference quotients.</p>
</div>
<div id="revisiting-gaussian-mixtures" class="section level2" number="8.4">
<h2>
<span class="header-section-number">8.4</span> Revisiting Gaussian mixtures<a class="anchor" aria-label="anchor" href="#revisiting-gaussian-mixtures"><i class="fas fa-link"></i></a>
</h2>
<p>In a two-component Gaussian mixture model the marginal density of the
distribution of <span class="math inline">\(Y\)</span> is
<span class="math display">\[ f(y) = p \frac{1}{\sqrt{2 \pi \sigma_1^2}} e^{-\frac{(y - \mu_1)^2}{2 \sigma_1^2}} + 
(1 - p)\frac{1}{\sqrt{2 \pi \sigma_2^2}}e^{-\frac{(y - \mu_2)^2}{2 \sigma_2^2}}.\]</span>
The following is a simulation of data from such a mixture model.</p>
<div class="sourceCode" id="cb377"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sigma1</span> <span class="op">&lt;-</span> <span class="fl">1</span>
<span class="va">sigma2</span> <span class="op">&lt;-</span> <span class="fl">2</span>
<span class="va">mu1</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">0.5</span>
<span class="va">mu2</span> <span class="op">&lt;-</span> <span class="fl">4</span>
<span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>
<span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span>
<span class="va">z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>, <span class="va">n</span>, replace <span class="op">=</span> <span class="cn">TRUE</span>, prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">p</span>, <span class="fl">1</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span><span class="op">)</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span>
<span class="va">n1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">z</span><span class="op">)</span>
<span class="va">y</span><span class="op">[</span><span class="va">z</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n1</span>, <span class="va">mu1</span>, <span class="va">sigma1</span><span class="op">)</span>
<span class="va">y</span><span class="op">[</span><span class="op">!</span><span class="va">z</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="va">n1</span>, <span class="va">mu2</span>, <span class="va">sigma2</span><span class="op">)</span></code></pre></div>
<p>We implement the log-likelihood assuming that the variances are known. Note
that the implementation takes just one single parameter argument, which is
then supposed to be a vector of all parameters in the model. Internally to
the function one has to decide for each entry in the parameter vector what
parameter in the model it corresponds to.</p>
<div class="sourceCode" id="cb378"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">loglik</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">par</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">p</span> <span class="op">&lt;-</span> <span class="va">par</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>
  <span class="kw">if</span><span class="op">(</span><span class="va">p</span> <span class="op">&lt;</span> <span class="fl">0</span> <span class="op">||</span> <span class="va">p</span> <span class="op">&gt;</span> <span class="fl">1</span><span class="op">)</span>
    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="cn">Inf</span><span class="op">)</span>
  
  <span class="va">mu1</span> <span class="op">&lt;-</span> <span class="va">par</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>
  <span class="va">mu2</span> <span class="op">&lt;-</span> <span class="va">par</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span>
  <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="va">mu1</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">sigma1</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="va">sigma1</span> <span class="op">+</span> 
             <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="va">mu2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">sigma2</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="va">sigma2</span><span class="op">)</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>Without further implementations, <code>optim</code> can find the
maximum-likelihood estimate if we have a sensible initial parameter guess.
In this case we use the true parameters, which can be used when
algorithms are tested, but they are, of course, not available for
real applications.</p>
<div class="sourceCode" id="cb379"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">4</span><span class="op">)</span>, <span class="va">loglik</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">]</span></code></pre></div>
<pre><code>## $par
## [1]  0.4934452 -0.5495679  4.0979106
## 
## $value
## [1] 1384.334</code></pre>
<p>However, if we initialize the optimization badly, it does not find the maximum
but a local maximum instead.</p>
<div class="sourceCode" id="cb381"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.9</span>, <span class="fl">3</span>, <span class="fl">1</span><span class="op">)</span>, <span class="va">loglik</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">]</span></code></pre></div>
<pre><code>## $par
## [1] 0.2334382 5.6763596 0.6255516
## 
## $value
## [1] 1509.86</code></pre>
<p>We will implement the EM algorithm for the Gaussian mixture model by
implementing and E-step and an M-step function. We know from Section
<a href="four-examples.html#Gaus-mix-ex">6.4.1</a> how the complete log-likelihood looks, and the E-step
becomes a matter of computing
<span class="math display">\[p_i(\mathbf{y}) = E(1(Z_i = 1) \mid \mathbf{Y} = \mathbf{y}) = P(Z_i = 1 \mid  \mathbf{Y} = \mathbf{y}).\]</span>
The M-step becomes identical to the MLE, which can be found explicitly,
but where the indicators <span class="math inline">\(1(Z_i = 1)\)</span> and <span class="math inline">\(1(Z_i = 2) = 1 - 1(Z_i = 1)\)</span> are
replaced by the conditional probabilities <span class="math inline">\(p_i(\mathbf{y})\)</span> and
<span class="math inline">\(1 - p_i(\mathbf{y})\)</span>, respectively.</p>
<div class="sourceCode" id="cb383"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">EStep</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">par</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">p</span> <span class="op">&lt;-</span> <span class="va">par</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>
  <span class="va">mu1</span> <span class="op">&lt;-</span> <span class="va">par</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>
  <span class="va">mu2</span> <span class="op">&lt;-</span> <span class="va">par</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span>
  <span class="va">a</span> <span class="op">&lt;-</span> <span class="va">p</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span> <span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="va">mu1</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">sigma1</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="va">sigma1</span> 
  <span class="va">b</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span> <span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="va">mu2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">sigma2</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="va">sigma2</span>
  <span class="va">b</span> <span class="op">/</span> <span class="op">(</span><span class="va">a</span> <span class="op">+</span> <span class="va">b</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">MStep</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">y</span>, <span class="va">pz</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>
  <span class="va">N2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">pz</span><span class="op">)</span>
  <span class="va">N1</span> <span class="op">&lt;-</span> <span class="va">n</span> <span class="op">-</span> <span class="va">N2</span>
  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">N1</span> <span class="op">/</span> <span class="va">n</span>, <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">pz</span><span class="op">)</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span> <span class="op">/</span> <span class="va">N1</span>, <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">pz</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span> <span class="op">/</span> <span class="va">N2</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">EM</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">par</span>, <span class="va">y</span>, <span class="va">epsilon</span> <span class="op">=</span> <span class="fl">1e-12</span><span class="op">)</span> <span class="op">{</span>
  <span class="kw">repeat</span><span class="op">{</span>
    <span class="va">par0</span> <span class="op">&lt;-</span> <span class="va">par</span>
    <span class="va">par</span> <span class="op">&lt;-</span> <span class="fu">MStep</span><span class="op">(</span><span class="va">y</span>, <span class="fu">EStep</span><span class="op">(</span><span class="va">par</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span>
    <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="op">(</span><span class="va">par</span> <span class="op">-</span> <span class="va">par0</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">&lt;=</span> <span class="va">epsilon</span> <span class="op">*</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">par</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span> <span class="va">epsilon</span><span class="op">)</span><span class="op">)</span>
      <span class="kw">break</span>
  <span class="op">}</span> 
  <span class="va">par</span>  <span class="co">## Remember to return the parameter estimate</span>
<span class="op">}</span>

<span class="fu">EM</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">4</span><span class="op">)</span>, <span class="va">y</span><span class="op">)</span></code></pre></div>
<pre><code>## [1]  0.4934443 -0.5497060  4.0982383</code></pre>
<p>The EM algorithm may, just as any other optimization algorithm,
end up in a <em>local</em> maximum, if it is started wrongly.</p>
<div class="sourceCode" id="cb385"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">EM</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.9</span>, <span class="fl">3</span>, <span class="fl">1</span><span class="op">)</span>, <span class="va">y</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 0.2334722 5.6759456 0.6256279</code></pre>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="numopt.html"><span class="header-section-number">7</span> Numerical optimization</a></div>
<div class="next"><a href="StochOpt.html"><span class="header-section-number">9</span> Stochastic Optimization</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#em"><span class="header-section-number">8</span> Expectation maximization algorithms</a></li>
<li>
<a class="nav-link" href="#basic-properties"><span class="header-section-number">8.1</span> Basic properties</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#incomplete-data-likelihood"><span class="header-section-number">8.1.1</span> Incomplete data likelihood</a></li>
<li><a class="nav-link" href="#monotonicity-of-the-em-algorithm"><span class="header-section-number">8.1.2</span> Monotonicity of the EM algorithm</a></li>
<li><a class="nav-link" href="#peppered-moths"><span class="header-section-number">8.1.3</span> Peppered moths</a></li>
</ul>
</li>
<li><a class="nav-link" href="#EM-exp"><span class="header-section-number">8.2</span> Exponential families</a></li>
<li><a class="nav-link" href="#fisher-information"><span class="header-section-number">8.3</span> Fisher information</a></li>
<li><a class="nav-link" href="#revisiting-gaussian-mixtures"><span class="header-section-number">8.4</span> Revisiting Gaussian mixtures</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/nielsrhansen/CSwR/blob/master/25-EM.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/nielsrhansen/CSwR/edit/master/25-EM.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Computational Statistics with R</strong>" was written by Niels Richard Hansen. It was last built on 2021-10-12, Git version: 6b05821.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>

[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"book developed graduate level course computational\nstatistics. used primary material course within MSc\nprogram statistics University Copenhagen.book assumes solid mathematical background, reader \nexpected reasonable command mathematical analysis, linear\nalgebra mathematical statistics – exemplified maximum likelihood\nestimation multivariate parameters asymptotic properties multivariate\nestimators. reader also expected understanding \nalgorithm , numerical computations differ symbolic computations,\nable write small computer programs.material covered supposed comprehensive treatment \ncomputational statistics. intended pedagogical introduction \ncore aspects computational statistics bridges gap \ntheory implementation. presentation driven selection \nstatistical examples computational challenges. examples \ntied together practical experimental approaches solving \ncomputational challenges.Contemporary research computational statistics revolves around\nlarge scale computations, either amount data massive \nwant apply ever complicated sophisticated models\nmethods analysis visualization data. Compared \nresearch challenges, examples treated book modest\ncomplexity. serve means learn necessary computational\ncraftsmanship needed complex problems solved.book based R several reasons. First ,\ntarget audience statisticians expected familiar R, \nlearn use programming language optimal way.\nincludes knowledge infrastructure offered R RStudio \nsupports good software development. addition, infrastructure combined\nR Markdown bookdown makes bliss write book systematically\nintegrates code software development theory. Finally, possible\nwrite performant R code proper use R high-level programming\nlanguage interfacing compiled code via Rcpp package. Statisticians \nprogram R master skills.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"Computational statistics turning theory methods algorithms \nactual numerical computations data. solving real computational\nproblems arise visualize, analyze model data.Computational statistics single coherent topic large\nnumber vaguely related computational techniques use statistics.\nbook attempting comprehensive.\nInstead, selected statistical topics methodologies \ntreated detail intention good computational practice\ncan learned topics transferred statistical\nmethodologies needed. Though topics arguably fundamental, reflect \nknowledge interests author, different topics clearly \nchosen.demarcation line statistical methodology computational statistics\n, moreover, blurred. methodology involves mathematical formulas even\nalgorithms computing estimates statistics interest data,\nevaluating probabilities integrals numerically via simulations.\nbook, transition methodology \ncomputational statistics happens methodology implemented.\n, formulas, algorithms pseudo code transformed actual\ncode statistical software. transition number \npractical challenges reveal , actual run time \nmemory usage, limitations finite precision arithmetic. \ndealing actual implementations learn appreciate \nvalue approximate solution may theoretically suboptimal \nsufficiently accurate practical purpose.Statistical software development also requires basic software engineering\nskills knowledge common programming paradigms. Implementing \nsingle algorithm specific problem one thing, developing piece \nstatistical software others use something quite different. book\nintroduction statistical software development , \nprocess developing good software plays role throughout. Good implementations\npresented code manifests divine insight, rather\ncode derived experimental analytic cycles – somewhat\nresembling software development actually takes place.notable practical experimental component software\ndevelopment. However important theoretical considerations regarding\ncorrectness complexity algorithms, say, actual code strike\nbalance generality, readability, efficiency, accuracy, ease usage\nease development among things. Finding good balance requires \none able reason benefits deficiencies different\nimplementations. important point book reasoning \nrely experiments empirical facts speculations.R RStudio used throughout, reader expected \nbasic knowledge R programming. RStudio requirement \nbook, recommendable IDE (integrated development environment)\nR, offers convenient framework developing, benchmarking,\nprofiling, testing, documenting experimenting statistical software.\nAppendix covers basic important aspects R programming\ncan serve survey quick reference. \n-depth treatment R programming language reader referred\nAdvanced R Hadley Wickham. fact, direct\nreferences book given throughout detailed explanations \nmany R programming language concepts, Appendix contains \nbrief overview core R concepts used.book organized three parts smoothing,\nMonte Carlo methods\noptimization. part introduced following\nthree sections give reader overview topics covered, \nrelated related main trends \nchallenges contemporary computational statistics. introduction,\nseveral R functions various packages used illustrate smoothing,\nsimulation random variables optimization play roles statistics.\nThus introduction relies largely already implemented solutions, \ndata analysts never want move beyond use R. However, \nremaining part book written want move learn\ndevelop solutions just use interfaces plethora\nalready existing implementations.","code":""},{"path":"intro.html","id":"intro-smooth","chapter":"1 Introduction","heading":"1.1 Smoothing","text":"Smoothing descriptive statistical tool summarizing data, practical\nvisualization technique, well nonparametric estimation methodology.\nbasic idea data representative underlying distribution\nsmoothness properties, like approximate \nestimate underlying distribution data.two related slightly different approaches. Either attempt estimate\nsmooth density observed variables, attempt estimate smooth\nconditional density one variable given others. latter can principle\ndone computing conditional density smooth estimate \njoint density. Thus appears really just need way \ncomputing smooth density estimates. practice may, however, better\nsolve conditional smoothing problem directly instead solving \nstrictly complicated problem. particularly , \nconditioning variables fixed e.g. design, main interest \nconditional mean median, say, entire conditional distribution.\nConditional smoothing dealt Chapter 3.introduction focus univariate case, really \none problem: smooth density estimation. Moreover, \nbasic problem, one viewpoint simply need “smooth \njumps histogram.” Indeed, need made \nsophisticated ! Humans able quite well using just\npen printed histogram, bit complicated automatize\nsmoothing procedure. Moreover, automatized procedure likely\nneed calibration yield good tradeoff smoothness\ndata fit. something humans can quite well\neyeballing visualizations, approach scale,\nneither terms number density estimates want consider,\nterms going univariate multivariate densities.want really discuss smoothing\nprocedure works just heuristic also estimator \nunderlying density, necessary formalize quantify\nperformance procedure. increases level \nmathematical sophistication, allows us discuss optimality, \nlets us develop fully automatized procedures rely human\ncalibration. human inspection visualizations always good\nidea, computational statistics also offloading humans \ncomputational tasks can automatized. true smoothing\nwell, hence need automatic robust smoothing\nprocedures produce well calibrated results minimum human effort.","code":""},{"path":"intro.html","id":"intro-angles","chapter":"1 Introduction","heading":"1.1.1 Angle distributions in proteins","text":"illustrate smoothing using small data set angles formed \ntwo subsequent peptide planes 3D protein structures. data set selected\nangle distributions multimodal slightly non-standard, \nproperties well suited illustrating fundamental considerations\nregarding smooth density estimation practice.\nFigure 1.1: 3D structure proteins largely given \\(\\phi\\)- \\(\\psi\\)-angles peptide planes. (Dcrjsr, CC 3.0 via Wikimedia Commons.)\nhydrogen atom binds nitrogen (N) oxygen atom binds \ncarbon without \\(\\alpha\\) subscript (C), see Figure 1.1,\nfour atoms form together known peptide bond two\nalpha-carbon atoms (C\\(_{\\alpha}\\)). C\\(_{\\alpha}\\) atom binds hydrogen\natom amino acid side chain. 20\nnaturally occurring amino acids genetically encoded proteins,\nthree letter code (Gly Glycine, Pro Proline, etc.).\nprotein typically form complicated 3D structure determined \namino acids, turn determine \\(\\phi\\)- \\(\\psi\\)-angles \npeptide planes shown Figure 1.1.consider small data set, phipsi, experimentally determined angles \nsingle protein, human protein 1HMP,\ncomposed two chains (denoted B). Figure 1.2 shows\n3D structure protein.\nFigure 1.2: 3D structure atoms constituting protein 1HMP. colors indicate two different chains.\ncan use base R functions hist density visualize \nmarginal distributions two angles.\nFigure 1.3: Histograms equipped rug plot smoothed density estimate (red line) distribution \\(\\phi\\)-angles (left) \\(\\psi\\)-angles (right).\nsmooth red density curve shown Figure 1.3 can thought\nsmooth version histogram. surprisingly difficult find\nautomatic smoothing procedures perform uniformly\nwell – even quite difficult automatically select number \npositions breaks used histograms. one important\npoints taken book: implement good default choices\nvarious tuning parameters required smoothing procedure.","code":"\nhead(phipsi)##   chain  AA pos        phi        psi\n## 1     A Pro   5 -1.6218794  0.2258685\n## 2     A Gly   6  1.1483709 -2.8314426\n## 3     A Val   7 -1.4160220  2.1190570\n## 4     A Val   8 -1.4926720  2.3941331\n## 5     A Ile   9 -2.1814653  1.4877618\n## 6     A Ser  10 -0.7525375  2.5676186\nhist(phipsi$phi, prob = TRUE)\nrug(phipsi$phi)\ndensity(phipsi$phi) %>% lines(col = \"red\", lwd = 2)\n\nhist(phipsi$psi, prob = TRUE)\nrug(phipsi$psi)\ndensity(phipsi$psi) %>% lines(col = \"red\", lwd = 2)"},{"path":"intro.html","id":"using-ggplot2","chapter":"1 Introduction","heading":"1.1.2 Using ggplot2","text":"also possible use ggplot2 achieve similar results.\nFigure 1.4: Histograms density estimates \\(\\phi\\)-angles (left) \\(\\psi\\)-angles (right) made ggplot2.\nHistograms produced ggplot2 non adaptive default number bins equal\n30 (number breaks equal 31), different hist uses\nSturges’ formula\n\\[\\text{number breaks} = \\lceil \\log_2(n) + 1 \\rceil\\]\n\\(n\\) number observations data set. addition, number \nmodified \nfunction pretty generates “nice” breaks, results 14 breaks\nangle data. easier comparison, number bins used \ngeom_histogram set 13, though \nnoticed breaks chosen exactly \nway geom_histogram hist. Automatic data adaptive bin\nselection difficult, geom_histogram implements simple fixed, \nlikely suboptimal, default notifying user default choice\ncan improved setting binwidth.density, geom_density actually relies density function \ndefault choices much smooth. Thus figure\nmay slightly different appearance, estimated density obtained \ngeom_density identical one obtained density.","code":"\nlibrary(ggplot2)\nggplot(phipsi, aes(x = phi)) + \n  geom_histogram(aes(y = ..density..), bins = 13) + \n  geom_density(col = \"red\", size = 1) + \n  geom_rug()\n\nggplot(phipsi, aes(x = psi)) + \n  geom_histogram(aes(y = ..density..), bins = 13) + \n  geom_density(col = \"red\", size = 1) + \n  geom_rug()"},{"path":"intro.html","id":"changing-the-defaults","chapter":"1 Introduction","heading":"1.1.3 Changing the defaults","text":"range angle data known \\((-\\pi, \\pi]\\), neither\nhistogram density smoother take advantage . pretty function,\nused hist chooses, instance, breaks \\(-3\\) \\(3\\), results \ntwo extreme\nbars histogram misleading. Note also \\(\\psi\\)-angle\nappears defaults result oversmoothing density\nestimate. , density \nsmoothed data (histogram) appears support.obtain different – perhaps better – results, can try change \ndefaults histogram density functions. two important\ndefaults consider bandwidth kernel.\nPostponing mathematics Chapter 2, kernel controls \nneighboring data points weighted relatively , \nbandwidth controls size neighborhoods. bandwidth can specified\nmanually specific numerical value, fully automatic procedure,\nselected bandwidth selection algorithm. density default\nrather simplistic algorithm known Silverman’s rule--thumb.\nFigure 1.5: Histograms various density estimates \\(\\psi\\)-angles. colors indicate different choices bandwidth adjustments using otherwise default bandwidth selection (left) different choices kernels using Sheather-Jones bandwidth selection (right).\nFigure\n1.5 shows examples several different density estimates\ncan obtained changing defaults density. breaks \nhistogram also chosen manually make sure match\nrange data. Note, particular, Sheather-Jones bandwidth\nselection appears work better default bandwidth example.\ngenerally \ncase multimodal distributions, default bandwidth tends oversmooth.\nNote also choice bandwidth far consequential \nchoice kernel, latter mostly affecting wiggly density\nestimate locally.noted defaults arise \ncombination historically sensible choices backward compatibility. Thought\ngo choosing good, robust default, default chosen,\nchanged haphazardly, might break existing code. \ndefaults used R today’s standards best known\nchoices. see argument made documentation density regarding \ndefault bandwidth selection, Sheather-Jones suggested \nbetter default current, compatibility reasons Silverman’s\nrule--thumb default likely remain .","code":"\nhist(phipsi$psi, breaks = seq(-pi, pi, length.out = 15), prob = TRUE)\nrug(phipsi$psi)\ndensity(phipsi$psi, adjust = 1, cut = 0) %>% lines(col = \"red\", lwd = 2)\ndensity(phipsi$psi, adjust = 0.5, cut = 0) %>% lines(col = \"blue\", lwd = 2)\ndensity(phipsi$psi, adjust = 2, cut = 0) %>% lines(col = \"purple\", lwd = 2)\n\nhist(phipsi$psi, breaks = seq(-pi, pi, length.out = 15), prob = TRUE)\nrug(phipsi$psi)\n# Default kernel is \"gaussian\"\ndensity(phipsi$psi, bw = \"SJ\", cut = 0) %>% lines(col = \"red\", lwd = 2)\ndensity(phipsi$psi, kernel = \"epanechnikov\", bw = \"SJ\", cut = 0) %>% \n  lines(col = \"blue\", lwd = 2)\ndensity(phipsi$psi, kernel = \"rectangular\", bw = \"SJ\", cut = 0) %>% \n  lines(col = \"purple\", lwd = 2)"},{"path":"intro.html","id":"multivariate-smoothing","chapter":"1 Introduction","heading":"1.1.4 Multivariate methods","text":"section provides single illustration use \nbivariate kernel smoother kde2d MASS package\nbivariate density estimation \\((\\phi, \\psi)\\)-angle distribution.\nscatter plot \\(\\phi\\) \\(\\psi\\) angles known Ramachandran plot, provides\nclassical important way visualizing local structural\nconstraints proteins structural biochemistry. density estimate\ncan understood estimate distribution \\((\\phi, \\psi)\\)-angles\nnaturally occuring proteins small sample angles \ndata set.compute density estimate grid size 100 100 using bandwidth\n2 using kde2d function uses bivariate normal kernel.recompute density estimate grid size using\nsmaller bandwidth 0.5.\nFigure 1.6: Bivariate density estimates protein backbone angles using bivariate Gaussian kernel bandwiths \\(2\\) (left) \\(0.5\\) (right).\nRamachandran plot Figure 1.6 shows structural constraints\nprotein, steric effects, induce non-standard bivariate distribution\n\\((\\phi, \\psi)\\)-angles.","code":"\ndenshat <- MASS::kde2d(phipsi$phi, phipsi$psi, h = 2, n = 100)\ndenshat <- data.frame(\n  cbind(\n    denshat$x, \n    rep(denshat$y, each = length(denshat$x)), \n    as.vector(denshat$z)\n    )\n)\ncolnames(denshat) <- c(\"phi\", \"psi\", \"dens\")\np <- ggplot(denshat, aes(phi, psi)) +\n  geom_tile(aes(fill = dens), alpha = 0.5) +\n  geom_contour(aes(z = sqrt(dens))) + \n  geom_point(data = phipsi, aes(fill = NULL)) +\n  scale_fill_gradient(low = \"white\", high = \"darkblue\", trans = \"sqrt\")\ndenshat <- MASS::kde2d(phipsi$phi, phipsi$psi, h = 0.5, n = 100)"},{"path":"intro.html","id":"large-scale-smoothing","chapter":"1 Introduction","heading":"1.1.5 Large scale smoothing","text":"small data sets less 10,000 data points, say, univariate\nsmooth density estimation requires modest amount computation. true\neven rather naive implementations standard methods. R function\ndensity implemented using number computational tricks like\nbinning fast Fourier transform, can compute density\nestimates million data points (around 8 MB) within fraction second.\nunclear ever need truly large scale univariate density\nestimation terabytes data points, say. amount \n(heterogeneous) data likely better breaking data \nsmaller homogeneous groups. , turn big data computation\nlarge number small data computations. remove \ncomputational challenge diminish somewhat e.g. parallelization.Deng Wickham review 2011 Density estimation R,\nassessed performance number R packages including \ndensity function. KernSmooth\npackage singled terms speed well accuracy\ncomputing smooth density estimates density performing quite well . (Histograms\nnon-smooth density estimates generally faster compute).\nassessment based using defaults different packages, meaningful\nsense representing \nperformance occasional user experience. , however,\nalso evaluation combination default choices implementation,\ndifferent packages rely e.g. different bandwidth selection algorithms,\nassessment complete story. bkde function KernSmooth\npackage, well density, solid choices, \npoint performance assessment multifaceted problem.little specific computational complexity density\nestimators, suppose \\(n\\) data points want evaluate \ndensity \\(m\\) points. naive implementation kernel smoothing,\nSection 2.2, \\(O(mn)\\) time complexity, \nnaive implementation best bandwidth selection algorithms \n\\(O(n^2)\\) time complexity. simple rule--thumb, anything beyond \\(O(n)\\)\nscale large data sets. quadratic time complexity bandwidth\nselection , particular, serious bottleneck. Kernel smoothing\nillustrates perfectly literal implementation mathematics behind\nstatistical method may always computationally viable. Even\n\\(O(mn)\\) time complexity may quite bottleneck reflects\n\\(mn\\) kernel evaluations, potentially computationally\nrelatively expensive operation.binning trick, number bins set \\(m\\), grouping data\npoints \\(m\\) sets neighbor points (bins) bin\nrepresenting points bin via single point weight. \\(m \\ll n\\),\ncan reduce time complexity substantially \\(O(m^2) + O(n)\\). fast\nFourier transform may reduce \\(O(m^2)\\) term even \\(O(m\\log(m))\\).\napproximations involved, importance\nevaluate tradeoff time memory complexity one\nside accuracy side.Multivariate smoothing different story. possible \ngeneralize basic ideas univariate density estimation arbitrary dimensions, \ncurse--dimensionality\nhits unconstrained smoothing hard – statistically well \ncomputationally. Multivariate smoothing therefore still active research\narea developing computationally tractable novel ways fitting smooth\ndensities conditional densities multivariate\neven high-dimensional data. key technique make structural\nassumptions alleviate challenge large dimension, many\ndifferent assumptions possible, makes body methods theory\nricher practical choices much difficult.","code":""},{"path":"intro.html","id":"intro-mc","chapter":"1 Introduction","heading":"1.2 Monte Carlo methods","text":"Broadly speaking, Monte Carlo methods computations rely form\nrandom input order carry computation. actual\nrandom input generated (pseudo)random number generator\naccording distributional specifications, precise value \ncomputation depend precise value random input way\nunderstand magnitude dependence quite well. \ncases can make dependence diminish increasing amount random\ninput.statistics quite interesting can make computer\nsimulate data can draw example data set statistical\nmodel. However, real usage simulations almost always \npart Monte Carlo computation, repeat simulation \ndata sets large number times compute distributional properties\nvarious statistics. just one obvious applications \nMonte Carlo methods statistics many others. Disregarding\nspecific computation interest applications, core problem \nMonte Carlo methods efficient simulation random variables \ngiven target distribution.","code":""},{"path":"intro.html","id":"vM","chapter":"1 Introduction","heading":"1.2.1 Univariate von Mises distributions","text":"exemplify Monte Carlo computations considering angle distributions\njust Section 1.1. angles take values interval\n\\((-\\pi, \\pi]\\), consider models based von Mises distribution\ninterval, density\\[f(x) = \\frac{1}{\\varphi(\\theta)} e^{\\theta_1 \\cos(x) + \\theta_2 \\sin(x)}\\]\\(\\theta = (\\theta_1, \\theta_2)^T \\\\mathbb{R}^2\\). common\nalternative parametrization obtained introducing\n\\(\\kappa = \\|\\theta\\|_2 = \\sqrt{\\theta_1^2 + \\theta_2^2}\\), \n(whenever \\(\\kappa \\neq 0\\))\n\\(\\nu = \\theta / \\kappa = (\\cos(\\mu), \\sin(\\mu))^T\\) \\(\\mu \\(-\\pi, \\pi]\\).\nUsing \\((\\kappa, \\mu)\\)-parametrization density becomes\\[f(x) = \\frac{1}{\\varphi(\\kappa \\nu)} e^{\\kappa \\cos(x - \\mu)}.\\]\nformer parametrization terms \n\\(\\theta\\) , however, canonical\nparametrization family distributions exponential family, \nparticularly useful various likelihood estimation algorithms.\nnormalization constant\\[\\begin{align*}\n\\varphi(\\kappa \\nu) & = \\int_{-\\pi}^\\pi e^{\\kappa \\cos(x - \\mu)}\\mathrm{d} x \\\\\n& = 2 \\pi \\int_{0}^{1} e^{\\kappa \\cos(\\pi x)}\\mathrm{d} x = 2 \\pi I_0(\\kappa) \n\\end{align*}\\]given terms modified Bessel function\n\\(I_0\\). can easily compute plot density using R’s besselI implementation \nmodified Bessel function.\nFigure 1.7: Density von Mises distribution parameters \\(\\kappa = 1\\) \\(\\nu = 0\\) (black), \\(\\kappa = 2\\) \\(\\nu = 1\\) (red), \\(\\kappa = 0.5\\) \\(\\nu = - 1.5\\) (blue).\nentirely obvious go simulating data points\nvon Mises distribution. demonstrated Section\n4.3 implement rejection sampler, one\nuseful algorithm simulating samples distribution density.section simply use rmovMF function movMF package,\nimplements functions working (finite mixtures ) von\nMises distributions, even general von Mises-Fisher distributions\ngeneralizations von Mises distribution \\(p\\)-dimensional\nunit spheres.\nFigure 1.8: Histogram 500 simulated data points von Mises distribution parameters \\(\\kappa = 0.5\\) \\(\\nu = - 1.5\\). smoothed density estimate (red) true density (blue) added plot.\n","code":"\nphi <- function(k) 2 * pi * besselI(k, 0)\ncurve(exp(cos(x)) / phi(1), -pi, pi, lwd = 2, ylab = \"density\", ylim = c(0, 0.52))\ncurve(exp(2 * cos(x - 1)) / phi(2), col = \"red\", lwd = 2, add = TRUE)\ncurve(exp(0.5 * cos(x + 1.5)) / phi(0.5), col = \"blue\", lwd = 2, add = TRUE)\nlibrary(\"movMF\")\nxy <- rmovMF(500, 0.5 * c(cos(-1.5), sin(-1.5)))\n# rmovMF represents samples as elements on the unit circle\nx <- acos(xy[, 1]) * sign(xy[, 2])\nhist(x, breaks = seq(-pi, pi, length.out = 15), prob = TRUE)\nrug(x)\ndensity(x, bw = \"SJ\", cut = 0) %>% lines(col = \"red\", lwd = 2)\ncurve(exp(0.5 * cos(x + 1.5)) / phi(0.5), col = \"blue\", lwd = 2, add = TRUE)"},{"path":"intro.html","id":"mixtures-of-von-mises-distributions","chapter":"1 Introduction","heading":"1.2.2 Mixtures of von Mises distributions","text":"von Mises distributions unimodal distributions \\((-\\pi, \\pi]\\). Thus \nfind good model bimodal angle data, say, move beyond \ndistributions. standard approach constructing multimodal distributions\nmixtures unimodal distributions. mixture two von Mises distributions\ncan constructed flipping (biased) coin decide two\ndistributions sample . use exponential family parametrization\nfollowing.rmovMF actually implements simulation mixture distribution\ndirectly, thus need construct “coin flips” explicitly.compare simulated data two mixture components model \nsmoothed density, implement R function computes density\nangle argument using function dmovMF takes unit circle\nargument.Note dmovMF uses normalized spherical measure\nunit circle reference\nmeasure, thus need \\(2\\pi\\) division want result \ncomparable histograms density estimates use Lebesgue measure \n\\((-\\pi, \\pi]\\) reference measure.\nFigure 1.9: Histograms 500 simulated data points mixture two von Mises distributions using either explicit construction mixture (left) functionality rmovMF simulate mixtures directly (right). smoothed density estimate (red) true density (blue) added plot.\nSimulation data distribution finds many applications. technique\nwidely used whenever want investigate statistical methodology \nterms frequentistic performance various data sampling models, \nsimulation tool fundamental importance \npractical application Bayesian statistical methods. Another important\napplication tool computing approximations integrals. \nusually called Monte Carlo integration form numerical\nintegration. Computing probabilities distribution functions, say,\nnotable examples integrals, consider computation \nprobability interval \\((0, 1)\\) mixture two von Mises\ndistributions.straightforward compute probability via Monte Carlo integration\nsimple average. Note use large number samples,\n50,000 case, simulated angles computation. Increasing\nnumber even make result accurate. Chapter 5\ndeals assessment accuracy Monte Carlo integrals, \nrandom error can estimated, bounded minimized.probability , course, expressed using \ndistribution function mixture von Mises distributions, \nturn can computed terms integrals von Mises densities.\nSpecifically, probability \n\\[p = \\frac{\\alpha}{\\varphi(\\theta_A)} \\int_0^1 e^{\\theta_{, 1} \\cos(x) + \\theta_{, 2} \\sin(x)} \\mathrm{d} x +  \n\\frac{1 - \\alpha}{\\varphi(\\theta_B)} \\int_0^1 e^{\\theta_{B, 1} \\cos(x) + \\theta_{B, 2} \\sin(x)} \\mathrm{d} x,\\]\nintegrals simple analytic representation – just \ndistribution function von Mises distribution doesn’t simple analytic\nexpression. Thus computation probability requires numerical\ncomputation integrals.R function integrate can used numerical integration univariate\nfunctions using standard numerical integration techniques. can thus\ncompute probability integrating density mixture,\nimplemented R function dvM. Note arguments passed\nintegrate . first argument density function, follows\nlower upper limits integration, follows additional\narguments density – case parameter values.integrate function R interface couple classical\nQUADPACK Fortran routines \nnumerical integration via adaptive quadrature.\nSpecifically, computations rely approximations form\n\\[\\int_a^b f(x) \\mathrm{d} x \\simeq \\sum_i w_i f(x_i)\\]\ncertain grid points \\(x_i\\) weights \\(w_i\\), computed using\nGauss-Kronrod quadrature.\nmethod provides estimate approximation error addition \nnumerical approximation integral .noteworthy integrate function implemented R\ntakes another function, case density dvM, argument. R\nfunctional programming language\nfunctions first-class citizens.\nimplies, instance, functions can passed arguments \nfunctions using variable name – just like variable can passed\nargument function. parlance functional programming,\nintegrate functional:\nhigher-order function takes function argument returns \nnumerical value. One themes book \nmake good use functional (object oriented) programming features R\nwrite clear, expressive modular code without sacrificing computational\nefficiency.Returning specific problem computation integral, may ask\npurpose Monte Carlo integration ? Apparently can\njust numerical integration using e.g. integrate. least two\nreasons Monte Carlo integration sometimes preferable. First, straightforward\nimplement often works quite well multivariate even high-dimensional\nintegrals, whereas grid-based numerical integration schemes scale poorly \ndimension. Second, require analytic representation\ndensity. common statistical applications interested\ndistribution statistic, complicated transformation \ndata, whose density difficult impossible find analytically. Yet \ncan just simulate data, can simulate distribution statistic,\ncan use Monte Carlo integration compute whatever\nprobability integral w.r.t. distribution statistic \ninterested .","code":"\nthetaA <- c(3.5, -2)\nthetaB <- c(-4.5, 4)\nalpha <- 0.55 # Probability of von Mises distribution A\n# The sample function implements the \"coin flips\"\nu <- sample(c(1, 0), 500, replace = TRUE, prob = c(alpha, 1 - alpha))\nxy <- rmovMF(500, thetaA) * u + rmovMF(500, thetaB) * (1 - u)\nx <- acos(xy[, 1]) * sign(xy[, 2])\ntheta <- rbind(thetaA, thetaB)\nxy <- rmovMF(length(x), theta, c(alpha, 1 - alpha))\nx_alt <- acos(xy[, 1]) * sign(xy[, 2])\ndvM <- function(x, theta, alpha) {\n  xx <- cbind(cos(x), sin(x))\n  dmovMF(xx, theta, c(alpha, 1 - alpha)) / (2 * pi)\n}\nhist(x, breaks = seq(-pi, pi, length.out = 15), prob = TRUE, ylim = c(0, 0.5))\nrug(x)\ndensity(x, bw = \"SJ\", cut = 0) %>% lines(col = \"red\", lwd = 2)\ncurve(dvM(x, theta, alpha), col = \"blue\", lwd = 2, add = TRUE)\n\nhist(x_alt, breaks = seq(-pi, pi, length.out = 15), prob = TRUE, ylim = c(0, 0.5))\nrug(x_alt)\ndensity(x_alt, bw = \"SJ\", cut = 0) %>% lines(col = \"red\", lwd = 2)\ncurve(dvM(x, theta, alpha), col = \"blue\", lwd = 2, add = TRUE)\nxy <- rmovMF(50000, theta, c(alpha, 1 - alpha))\nx <- acos(xy[, 1]) * sign(xy[, 2])\nmean(x > 0 & x < 1)  # Estimate of the probability of the interval (0, 1)## [1] 0.08508\nintegrate(dvM, 0, 1, theta = theta, alpha = alpha)## 0.08635171 with absolute error < 9.6e-16"},{"path":"intro.html","id":"large-scale-monte-carlo-methods","chapter":"1 Introduction","heading":"1.2.3 Large scale Monte Carlo methods","text":"Monte Carlo methods used pervasively statistics many \nsciences today. nowadays trivial \nsimulate millions data points simple univariate distribution like\nvon Mises distribution, Monte Carlo methods generally attractive\nallow us solve computational problems approximately\nmany cases exact analytic computations impossible alternative\ndeterministic numerical computations require adaptation specific problem.Monte Carlo methods thus really good --shelf methods, scaling\nmethods can challenge contemporary research topic.\nsimulation multivariate \\(p\\)-dimensional data can, \ninstance, make big difference whether algorithms scale linearly \\(p\\)\nlike \\(O(p^)\\) \\(> 1\\). Likewise, Monte Carlo methods (e.g. \nBayesian computations) depend data set size \\(n\\), large\nscale data sets, algorithms scale like \\(O(n)\\) really \nalgorithms can used practice.","code":""},{"path":"intro.html","id":"intro-op","chapter":"1 Introduction","heading":"1.3 Optimization","text":"third part book optimization. huge research field\nfocus book relatively narrow application \nparameter estimation statistics. , statistical model \ngiven parametrized family probability distributions, optimization \ncriterion – often likelihood function – used find model \nfits data.Classical generic optimization algorithms based first second order\nderivatives can often\nused, important understand convergence can measured monitored,\ndifferent algorithms scale size data set \ndimension parameter space. choice right data structure,\nsparse matrices, can pivotal efficient implementations \ncriterion function optimized.mixture von Mises distributions possible compute likelihood\noptimize using standard algorithms. However, can also \noptimized using EM-algorithm, clever algorithm \noptimizing likelihood functions models can formulated\nterms latent variables.section demonstrates fit mixture von Mises distributions \nangle data using EM-algorithm implemented R package movMF. \nillustrate many practical choices make using numerical\noptimization : choice starting value; stopping criterion;\nprecise specification steps algorithm use; \neven whether use randomized steps.","code":""},{"path":"intro.html","id":"the-em-algorithm","chapter":"1 Introduction","heading":"1.3.1 The EM-algorithm","text":"movMF() function implements EM-algorithm mixtures von Mises\ndistributions. code shows one example call movMF() one\nalgorithmic control arguments specified. common R \narguments bundled together single list argument called control.\ncan make function call appear bit complicated needed ,\nallows easier reuse – sometimes fairly long – list control\narguments. Note movMF package works data elements \nunit circle, angle data must transformed using cos sin.function movMF() returns object class movMF documentation\nsays (see ?movMF). case means vM_fit list class\nlabel movMF, controls generic functions work particular list.\nobject printed, instance, content list\nformatted printed follows.see estimated \\(\\theta\\)-parameters two mixture\ncomponents printed matrix, mixture proportions (alpha) \nvalue log-likelihood function (L) estimated parameters.can compare fitted model data using density function\nimplemented parameters estimated EM-algorithm.can see figure, looks like fairly good fit data, \nreassuring two reasons. First, shows two-component mixture\nvon Mises distributions good model.\nstatistical reassurance. Second, shows optimization \nparameter space found good fit. numerical reassurance. \noptimize parameter space subsequently find model\nfit data well, either numerical optimization\nfailed model wrong.Numerical optimization can fail number reasons. , algorithm\ncan get stuck local optimum, can also stop prematurely either \nmaximal number iterations reached stopping criterion\nfulfilled (even though algorithm reached optimum). information\nrelated two last problems can gathered algorithm\n, movMF() information found list entry vM_fit.see number iterations used 16 (less maximal\nnumber 100), stopping criterion (small relative improvement)\nactive (converge TRUE, FALSE algorithm run \nfixed number iterations). tolerance parameter used \nstopping criterion \\(1.49 \\times 10^{-8}\\).small relative improvement criterion stopping iteration \\(n\\) \n\\[ |L(\\theta_{n-1}) - L(\\theta_n)| < \\varepsilon (|L(\\theta_{n-1})| + \\varepsilon)\\]\n\\(L\\) log-likelihood \\(\\varepsilon\\) tolerance parameter .\ndefault tolerance parameter square root machine epsilon, see also ?.Machine.\ncommonly encountered default “small----small-number,”\noutside numerical differentiation default may supported \nmuch theory.choosing different control arguments can change numerical\noptimization proceeds. different method setting \nstarting value chosen , contains random component. consider\nresults four different runs entire algorithm different random\nstarting values. also decrease number maximal\niterations 10 make algorithm print information progress\nalong way.four cases appears algorithm approaching value \nlog-likelihood found , though last run starts \nmuch lower value takes iterations reach large log-likelihood value.\nNote also runs log-likelihood increasing. feature \nEM-algorithm every step algorithm increase likelihood.Variations EM-algorithm possible, like movMF \ncontrol argument E determines -called E-step algorithm.\ndefault (softmax)\ngives actual EM-algorithm whereas hardmax stochmax give alternatives.\nalternatives guarantee log-likelihood increases every\niteration can numerically beneficial. rerun algorithm \nfour starting values using hardmax E-step. Note\ncan reuse control list just adding single element .striking runs now stopped maximal number iterations\nreached, run four particularly noteworthy jumps low\nstarting value 190 just two iterations. However, hardmax \nheuristic algorithm, whose fixed points necessarily stationary points\nlog-likelihood. can also see four runs stopped \nvalues clearly smaller log-likelihood 193.30 \nreached using real EM-algorithm. Whether problem \nstatistical viewpoint different matter; using hardmax give\nestimator just efficient maximum likelihood estimator.optimization algorithm use? general \ndifficult question answer, non-trivial correctly assess\nalgorithm “best.” application movMF() illustrates,\noptimization algorithms may number different parameters\ncan tweaked, comes actually implementing optimization\nalgorithm need : make easy tweak parameters; investigate \nquantify effect parameters algorithm; \nchoose sensible defaults. Without work impossible \nmeaningful discussion benefits deficits various algorithms.","code":"\npsi_circle <- cbind(cos(phipsi$psi), sin(phipsi$psi))\nvM_fit <- movMF(\n  x = psi_circle, \n  k = 2,              # The number of mixture components\n  control = list(\n    start = \"S\"       # Determines how starting values are chosen\n  )\n)\nvM_fit## theta:\n##        [,1]      [,2]\n## 1  3.472846 -1.935807\n## 2 -4.508098  3.872847\n## alpha:\n## [1] 0.5486586 0.4513414\n## L:\n## [1] 193.3014\nhist(phipsi$psi, breaks = seq(-pi, pi, length.out = 15), prob = TRUE)\nrug(phipsi$psi)\ndensity(phipsi$psi, bw = \"SJ\", cut = 0) %>% lines(col = \"red\", lwd = 2)\ncurve(dvM(x, vM_fit$theta, vM_fit$alpha[1]), add = TRUE, col = \"blue\", lwd = 2)\nvM_fit$details## $reltol\n## [1] 1.490116e-08\n## \n## $iter\n##    iter maxiter \n##      16     100 \n## \n## $logLiks\n## [1] 193.3014\n## \n## $E\n## [1] \"softmax\"\n## \n## $kappa\n## NULL\n## \n## $minalpha\n## [1] 0\n## \n## $converge\n## [1] TRUE\nvM_control <- list(\n    verbose = TRUE,   # Print output showing algorithmic progress\n    maxiter = 10,     \n    nruns = 4         # Effectively 4 runs with randomized starting values\n  )\nvM_fit <- movMF(psi_circle, 2, control = vM_control)\n## Run: 1\n## Iteration: 0 *** L: 158.572\n## Iteration: 1 *** L: 190.999\n## Iteration: 2 *** L: 193.118\n## Iteration: 3 *** L: 193.238\n## Iteration: 4 *** L: 193.279\n## Iteration: 5 *** L: 193.293\n## Iteration: 6 *** L: 193.299\n## Iteration: 7 *** L: 193.3\n## Iteration: 8 *** L: 193.301\n## Iteration: 9 *** L: 193.301\n## Run: 2\n## Iteration: 0 *** L: 148.59\n## Iteration: 1 *** L: 188.737\n## Iteration: 2 *** L: 192.989\n## Iteration: 3 *** L: 193.197\n## Iteration: 4 *** L: 193.264\n## Iteration: 5 *** L: 193.288\n## Iteration: 6 *** L: 193.297\n## Iteration: 7 *** L: 193.3\n## Iteration: 8 *** L: 193.301\n## Iteration: 9 *** L: 193.301\n## Run: 3\n## Iteration: 0 *** L: 168.643\n## Iteration: 1 *** L: 189.946\n## Iteration: 2 *** L: 192.272\n## Iteration: 3 *** L: 192.914\n## Iteration: 4 *** L: 193.157\n## Iteration: 5 *** L: 193.249\n## Iteration: 6 *** L: 193.282\n## Iteration: 7 *** L: 193.295\n## Iteration: 8 *** L: 193.299\n## Iteration: 9 *** L: 193.301\n## Run: 4\n## Iteration: 0 *** L: 4.43876\n## Iteration: 1 *** L: 5.33851\n## Iteration: 2 *** L: 6.27046\n## Iteration: 3 *** L: 8.54826\n## Iteration: 4 *** L: 14.4429\n## Iteration: 5 *** L: 29.0476\n## Iteration: 6 *** L: 61.3225\n## Iteration: 7 *** L: 116.819\n## Iteration: 8 *** L: 172.812\n## Iteration: 9 *** L: 190.518\nvM_control$E <- \"hardmax\"\nvM_fit <- movMF(psi_circle, 2, control = vM_control)\n## Run: 1\n## Iteration: 0 *** L: 158.572\n## Iteration: 1 *** L: 193.052\n## Iteration: 2 *** L: 193.052\n## Run: 2\n## Iteration: 0 *** L: 148.59\n## Iteration: 1 *** L: 192.443\n## Iteration: 2 *** L: 192.812\n## Iteration: 3 *** L: 193.052\n## Iteration: 4 *** L: 193.052\n## Run: 3\n## Iteration: 0 *** L: 168.643\n## Iteration: 1 *** L: 191.953\n## Iteration: 2 *** L: 192.812\n## Iteration: 3 *** L: 193.052\n## Iteration: 4 *** L: 193.052\n## Run: 4\n## Iteration: 0 *** L: 4.43876\n## Iteration: 1 *** L: 115.34\n## Iteration: 2 *** L: 191.839\n## Iteration: 3 *** L: 192.912\n## Iteration: 4 *** L: 192.912"},{"path":"intro.html","id":"large-scale-optimization","chapter":"1 Introduction","heading":"1.3.2 Large scale optimization","text":"Numerical optimization tool made statistical models useful \nreal data analysis – importantly making possible compute maximum\nlikelihood estimators practice. works well model can given\nparametrization concave log-likelihood, optimization \ncomplicated log-likelihood surfaces can numerically challenging lead \nstatistically dubious results.contemporary statistics machine learning, numerical optimization\ncome play even important role structural model constraints\nnow often also part optimization, instance via penalty terms \ncriterion function. Instead working simple models selected \nspecific problem parameters, use complex models \nthousands millions parameters. flexible adaptable need\ncareful, regularized optimization overfit data. large amounts\ndata regularization can turned can discover aspects data \nsimple models never show. However, need pay computational price.dimension parameter\nspace, \\(p\\), number data points, \\(n\\), large, evaluation \nlog-likelihood gradient can become prohibitively costly.\nOptimization algorithms based higher order derivatives completely \nquestion, even (penalized) log-likelihood gradient\ncan computed \\(O(pn)\\)-operations may still much optimization\nalgorithm iteration. large scale\noptimization problems spurred substantial development stochastic\ngradient methods related stochastic optimization algorithms use\nparts data iteration, currently way fit sufficiently\ncomplicated models data.","code":""},{"path":"density.html","id":"density","chapter":"2 Density estimation","heading":"2 Density estimation","text":"chapter nonparametric density estimation. classical\nnonparametric estimator density histogram, provides\ndiscontinuous piecewise constant estimates. focus chapter \nalternatives provide continuous even smooth estimates\ninstead.Kernel methods form important class smooth density estimators \nimplemented R function density(). estimators essentially\njust locally weighted averages, computation relatively\nstraightforward theory. practice, different choices implement\ncomputations can, however, big effect actual computation time,\nimplementation kernel density estimators illustrate three points:possible, choose vectorized implementations R,small loss accuracy acceptable, approximate solution\ncan orders magnitudes faster literal implementation,time takes numerically evaluate different\nelementary functions\ncan depend lot function implement computation.first point emphasized results implementations \nshort, expressive easier understand just much typically results\ncomputationally efficient implementations. Note also every\ncomputation can vectorized beneficial way, one never go\nhoops vectorize computation.Kernel methods rely one regularization parameters must \nselected achieve right balance adapting\ndata without adapting much random variation data.\nChoosing right amount regularization just important choosing\nmethod use first place. may, fact, important.\nactually complete implementation nonparametric estimator\nimplemented data driven automatic way choosing \namount regularization. Implementing computations \nevaluating kernel estimator, say, leaving completely\nuser choose bandwidth job half done. Methods implementations\nchoosing bandwidth therefore treated detail chapter.final section likelihood analysis carried .\ndone clarify regularized estimators needed\navoid overfitting data, general nonparametric\nmaximum likelihood estimator density. Regularization likelihood\ncan achieved constraining density estimates belong family\nincreasingly flexible parametric densities fitted data. \nknown method sieves. Another approach based basis expansions,\neither case, automatic selection amount regularization just\nimportant kernel methods.","code":""},{"path":"density.html","id":"unidens","chapter":"2 Density estimation","heading":"2.1 Univariate density estimation","text":"Recall data \\(\\phi\\)- \\(\\psi\\)-angles \npolypeptide backbone structures, considered Section 1.1.1.\nFigure 2.1: Histograms equipped rug plot distribution \\(\\phi\\)-angles (left) \\(\\psi\\)-angles (right) peptide planes protein human protein 1HMP.\nsection treat methods smooth density\nestimation univariate data data either \\(\\phi\\)- \n\\(\\psi\\)-angle.let \\(f_0\\) denote unknown density want estimate.\n, imagine data points \\(x_1, \\ldots, x_n\\) \nobservations drawn probability measure \ndensity \\(f_0\\) w.r.t. Lebesgue measure \\(\\mathbb{R}\\).Suppose first \\(f_0\\) belongs parametrized statistical model\n\\((f_{\\theta})_{\\theta}\\), \\(f_{\\theta}\\) density w.r.t.\nLebesgue measure \\(\\mathbb{R}\\). \\(\\hat{\\theta}\\) estimate \nparameter, \\(f_{\\hat{\\theta}}\\) estimate unknown\ndensity \\(f_0\\). parametric family can always try use MLE\n\\[\\hat{\\theta} = \\text{arg max}_{\\theta} \\sum_{j=1}^n \\log f_{\\theta}(x_j)\\]\nestimate \\(\\theta\\). Likewise, might compute empirical mean variance\ndata plug numbers density Gaussian\ndistribution, way obtain Gaussian density estimate \\(f_0\\).\nFigure 2.2: Gaussian density (red) fitted \\(\\psi\\)-angles.\nFigure 2.2 shows, fit Gaussian distribution \\(\\psi\\)-angle\ndata get density estimate clearly match \nhistogram. Gaussian density matches data first second moments,\nhistogram shows clear bimodality Gaussian distribution \ndefinition match. Thus need flexible parametric model \nGaussian want fit density data set.nonparametric density estimating want estimate target density, \\(f_0\\),\nwithout assuming belongs particular parametrized family densities.","code":"\npsi_mean <- mean(psi)\npsi_sd <- sd(psi)\nhist(psi, prob = TRUE)\nrug(psi)\ncurve(dnorm(x, psi_mean, psi_sd), add = TRUE, col = \"red\")"},{"path":"density.html","id":"kernel-density","chapter":"2 Density estimation","heading":"2.2 Kernel methods","text":"simple approach nonparametric density estimation relies approximation\n\\[P(X \\(x-h, x+h)) = \\int_{x-h}^{x+h} f_0(z) \\ dz \\simeq f_0(x) 2h,\\]\nvalid continuous density \\(f_0\\). Inverting approximation\nusing law large numbers,\\[\\begin{align*}\nf_0(x) & \\simeq \\frac{1}{2h}P(X \\(x-h, x+h)) \\\\\n& \\simeq \\frac{1}{2hn} \\sum_{j=1}^n 1_{(x-h, x+h)}(x_j) \\\\\n& =  \\underbrace{\\frac{1}{2hn} \\sum_{j=1}^n 1_{(-h, h)}(x - x_j)}_{\\hat{f}_h(x)}\n\\end{align*}\\]..d. observations \\(x_1, \\ldots, x_n\\) distribution \\(f_0 \\cdot m\\).\nfunction \\(\\hat{f}_h\\) defined example kernel\ndensity estimator rectangular kernel. immediately note \\(h\\) \nchosen appropriately. \\(h\\) large, \\(\\hat{f}_h\\) flat close \nconstant. \\(h\\) small, \\(\\hat{f}_h\\) make large jumps close observations.mean “appropriate” choice \\(h\\) ? answer \nmust prior assumptions expect\n\\(f_0\\) look like. Typically, expect \\(f_0\\) oscillations \nfairly smooth, want \\(\\hat{f}_h\\) reflect . large \\(h\\) oversmooth\ndata relative \\(f_0\\) effectively ignoring data, small \\(h\\) undersmooth\ndata relative \\(f_0\\) allowing individual data points large local effects \nmake estimate wiggly. formally, can look mean variance\n\\(\\hat{f}_h\\). Letting \\(p(x, h) = P(X \\(x-h, x+h))\\), follows \n\\(f_h(x) = E(\\hat{f}_h(x)) = p(x, h) / (2h)\\) \\[\\begin{equation} \nV(\\hat{f}_h(x)) = \\frac{p(x, h) (1 - p(x, h))}{4h^2 n} \\simeq f_h(x) \\frac{1}{2hn}.\n\\tag{2.1}\n\\end{equation}\\]see computations \\(\\hat{f}_h(x)\\) approximately unbiased \\(x\\)\nneed \\(h\\) small – ideally letting \\(h \\0\\) since \\(f_h(x) \\f_0(x)\\).\nHowever, make variance blow , minimize variance \ninstead choose \\(h\\) large possible. One way define “appropriate” \nstrike balance bias variance function\n\\(h\\) minimize mean squared error \\(\\hat{f}_h(x)\\).find optimal tradeoff rectangular kernel Section 2.3\nbandwidth selection. ’s difficult, encouraged\ntry finding point. section focus \ncomputational aspects kernel density estimation, first generalize\nestimator allowing kernels.estimate \\(\\hat{f}_h(x)\\) unbiased \\(f_0\\) constantly equal \\(f_0(x)\\)\nentire interval \\((x-h, x+h)\\). atypical can happen \\(x\\)\n\\(f_0\\) constant. expect typical situation \n\\(f_0\\) deviates \\(f_0(x)\\) close \\(x \\pm h\\), \ncauses bias \\(\\hat{f}_h(x).\\)\nObservations falling close \\(x + h\\), say, thus count less\nobservations falling close \\(x\\)? rectangular kernel makes sharp\ncut; either data point . use smooth weighting\nfunction instead sharp cut, might able include \ndata points lower variance keeping bias small. \nprecisely idea kernel estimators, defined generally \\[\\begin{equation}\n\\hat{f}_h(x) = \\frac{1}{hn} \\sum_{j=1}^n K\\left(\\frac{x - x_j}{h}\\right)\n\\tag{2.2}\n\\end{equation}\\]kernel \\(K : \\mathbb{R} \\\\mathbb{R}\\). parameter \\(h > 0\\) known\nbandwidth. Examples kernels include uniform rectangular kernel\n\\[K(x) = \\frac{1}{2} 1_{(-1,1)}(x),\\]\nGaussian kernel\n\\[K(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}.\\]One direct benefit considering kernels rectangular \n\\(\\hat{f}_h\\) inherits smoothness properties \\(K\\). Whereas rectangular\nkernel even continuous, Gaussian kernel \\(C^{\\infty}\\) \nresulting kernel density estimate.","code":""},{"path":"density.html","id":"dens-implement","chapter":"2 Density estimation","heading":"2.2.1 Implementation","text":"computed compute kernel density estimate? , fact,\ngood question, definition actually just specifies evaluate\n\\(\\hat{f}_h\\) given point \\(x\\), really anything compute\nneed evaluate \\(\\hat{f}_h\\). Thus implement kernel density\nestimation really implement algorithms evaluating density estimate\nfinite number points.first implementation fairly low-level implementation returns\nevaluation density estimate given number equidistant\npoints. function mimics defaults density() \nactually evaluates estimate points density().Note function returns list containing grid points (x) \ndensity estimate evaluated well estimated density\nevaluations (y). Note also argument m sets number \ngrid points, whereas density() uses argument n . \nlatter can bit confusing \\(n\\) often used denote \nnumber data points.immediately test implementation works expected – \ncase comparing reference implementation density().\nFigure 2.3: Kernel density estimates Gaussian kernel (left) using R’s implementation (black) implementation (red) together differences estimates (right).\nFigure 2.3 suggests estimates computed \nimplementation density() just visually compare \nplotted densities. However, look differences instead,\nsee large \\(4 \\times 10^{-4}\\) absolute value. \nway expect rounding errors alone using\ndouble precision arithmetic. Thus two implementations compute\napproximately , , fact, density() relies\ncertain approximations run time efficiency.R can often beneficially implement computations vectorized way\ninstead using explicit loop. fairly easy change implementation\nvectorized computing evaluation one single line using\nsum() function fact exp() squaring vectorized.test new implementation comparing previous\nimplementation.magnitude differences order \\(10^{-16}\\), \ncan expected due rounding errors. Thus conclude rounding\nerrors, kern_dens() kern_dens_vec() return data set. ,\ncourse, comprehensive test, example one among \nnumber tests considered.several ways get completely rid explicit loops write\nentirely vectorized implementation R. One solutions use \nsapply() function, belongs family *apply() functions\napply function element vector list. \nparlance functional programming *apply() functions variations\nfunctional, higher-order-function, known map.sapply() call apply function function(z) sum(dnorm(...\nevery element vector xx return result vector. \nfunction example anonymous function get name \nexists sapply() evaluation. Instead sapply() possible\nuse lapply() returns list. fact, sapply() \nsimple wrapper around lapply() attempts “simplify” result \nlist array (case vector).alternative, also completely vectorized, solution can based \nfunctions outer() rowMeans().outer() function evaluates kernel combinations\ngrid data points returns matrix \ndimensions \\(m \\times n\\). function rowMeans() computes\nmeans row returns vector length \\(m\\)., course, also remember test two last implementations.natural question choose different implementations?\nBesides correct important code easy read\nunderstand. four implementations best \nrespect may depend lot background reader. strip\nimplementations comments, four arguably quite readable, \nkern_dens() double loop might appeal bit people\nbackground imperative programming, kern_dens_apply() might\nappeal people preference functional programming.\nfunctional vectorized solution also\nbit closer mathematical notation \ne.g. sum sign \\(\\Sigma\\) mapped directly sum() function\ninstead incremental addition -loop. \nspecific implementations differences mostly\naesthetic nuances preferences may subjective substantial.make qualified choice implementations \ninvestigate differ terms run time memory consumption.","code":"\n# This is an implementation of the function 'kern_dens' that computes \n# evaluations of Gaussian kernel density estimates in a grid of points.\n#\n# The function has three formal arguments: 'x' is the numeric vector of data \n# points, 'h' is the bandwidth and 'm' is the number of grid points. \n# The default value of 512 is chosen to match the default of 'density()'. \nkern_dens <- function(x, h, m = 512) {\n  rg <- range(x)\n  # xx is equivalent to grid points in 'density()'\n  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)\n  y <- numeric(m) # The evaluations, initialized as a vector of zeros\n  # The actual computation is done using nested for-loops. The outer loop\n  # is over the grid points, and the inner loop is over the data points.\n  for (i in seq_along(xx))\n    for (j in seq_along(x))\n      y[i] <- y[i] + exp(- (xx[i] - x[j])^2 / (2 * h^2))  \n  y <- y / (sqrt(2 * pi) * h * length(x))\n  list(x = xx, y = y)\n}\nf_hat <- kern_dens(psi, 0.2)\nf_hat_dens <- density(psi, 0.2)\nplot(f_hat, type = \"l\", lwd = 4, xlab = \"x\", ylab = \"Density\")\nlines(f_hat_dens, col = \"red\", lwd = 2)\nplot(f_hat$x, f_hat$y - f_hat_dens$y, \n  type = \"l\", \n  lwd = 2,\n  xlab = \"x\", \n  ylab = \"Difference\"\n)\nkern_dens_vec <- function(x, h, m = 512) {\n  rg <- range(x)\n  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)\n  y <- numeric(m) \n  # The inner loop from 'kern_dens' has been vectorized, and only the \n  # outer loop over the grid points remains. \n  const <- (sqrt(2 * pi) * h * length(x))\n  for (i in seq_along(xx))\n      y[i] <- sum(exp(-(xx[i] - x)^2 / (2 * h^2))) / const\n  list(x = xx, y = y)\n}\nrange(kern_dens(psi, 0.2)$y - kern_dens_vec(psi, 0.2)$y)## [1] -5.551115e-16  3.885781e-16\nkern_dens_apply <- function(x, h, m = 512) {\n  rg <- range(x)\n  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)\n  const <- sqrt(2 * pi) * h * length(x)\n  y <- sapply(xx, function(z) sum(exp(-(z - x)^2 / (2 * h^2))) / const)\n  list(x = xx, y = y)\n}\nkern_dens_outer <- function(x, h, m = 512) {\n  rg <- range(x)\n  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)\n  y <- outer(xx, x, function(zz, z) exp(-(zz - z)^2 / (2 * h^2)))\n  y <- rowMeans(y) / (sqrt(2 * pi) * h)\n  list(x = xx, y = y)\n}\nrange(kern_dens(psi, 0.2)$y - kern_dens_apply(psi, 0.2)$y)## [1] -5.551115e-16  3.885781e-16\nrange(kern_dens(psi, 0.2)$y - kern_dens_outer(psi, 0.2)$y)## [1] -4.996004e-16  3.885781e-16"},{"path":"density.html","id":"benchmarking","chapter":"2 Density estimation","heading":"2.2.2 Benchmarking","text":"Benchmarking measuring comparing performance. software\noften means measuring run time memory usage, though clearly\nmany aspects software benchmarked general. \nincludes user experience, energy consumption implementation maintenance\ntime. section focus benchmarking run time.function system.time R provides simple way benchmarking run time\nmeasured seconds.“elapsed” time total run time experienced, “user” \n“system” times long CPU spent executing code \noperating system code behalf code, respectively.simple benchmark, kern_dens() clearly substantially slower\nthree implementations. systematic benchmarking run\ntime, R package microbenchmark useful.result stored kern_bench() data frame two columns. first\ncontains R expressions evaluated, second evaluation\ntime measured nanoseconds. four expressions evaluated 100 times,\ndata frame thus 400 rows. times argument microbenchmark()\ncan used change number evaluations per expression needed.may immediately obvious kern_bench()\ndata frame, printing automatically summarize \ndata, actual data structure revealed R function str().total 400 evaluations done benchmark, microbenchmark()\nevaluations random order default. Measuring evaluation time \ncomplex system like modern computer empirical science, \norder evaluation can potentially affect results \nconditions evaluation change time. purpose \nrandomization avoid ordering causes systematically\nmisleading results.microbenchmark package implements methods summarizing \nprinting results following summary table times \nmilliseconds.summary table shows key statistics like median mean evaluation\ntimes also extremes upper lower quartiles. distributions\nrun times can investigated using autoplot() function, \nbased ggplot2 thus easy modify.refined benchmark study change initial impression \nusing system.time() substantially. function kern_dens() notably\nslower three vectorized implementations, now able \nclearly see minor differences among . instance, kern_dens_vec()\nkern_dens_apply() similar run time distributions, \nkern_dens_outer() clearly larger median run time also \nrun time distribution spread right.many cases benchmark run time interest\ninvestigate run time depends various parameters. \nkernel density estimation, want understand changes \nnumber data points, \\(n\\), number grid points, \\(m\\), affect\nrun time. can still use microbenchmark() running benchmark\nexperiment, typically process plot benchmark data afterwards\ncustomized way.\nFigure 2.4: Median run times four different implementations kernel density estimation. dashed gray line reference line slope 1.\nFigure 2.4 shows median run times experiment \n28 combinations parameters four different implementations\nyielding total 112 different R expressions benchmarked. number replications\nexpression set 40. results confirm kern_dens()\nsubstantially slower vectorized implementations combinations\n\\(n\\) \\(m\\). However, Figure 2.4 also reveals new\npattern; kern_dens_outer() appears scale \\(n\\) slightly different way\ntwo vectorized implementations small \\(n\\). comparable \neven bit faster \nkern_dens_vec() kern_dens_apply() small data sets, becomes\nslower larger data sets.Run time many algorithms good approximation dominating\npower law behavior function typical size parameters, , \nrun time scale approximately like \\(n \\mapsto C n^\\) constants \\(C\\) \\(\\)\n\\(n\\) denoting generic size parameter. Therefore \nbeneficial plot run time using log-log scales design benchmark studies\nsize parameters equidistant log-scale. approximate power\nlaw scaling, log run time behaves like\n\\[\\log(C) + \\log(n),\\]\n, log-log scale see approximate straight lines. slope\nreveals exponent \\(\\), two different algorithms solving \nproblem might different exponents thus different slopes\nlog-log-scale. Two different implementations \nalgorithm approximately slope\nmay differ constant \\(C\\) depending upon efficient \nparticular implementation particular programming language used.\nDifferences \\(C\\) correspond vertical translations log-log scale.practice, see deviations straight lines log-log plot\nnumber reasons. Writing run time \\(C n^+ R(n)\\),\nresidual term \\(R(n)\\) often noticeable even dominating positive\nsmall \\(n\\). large enough \\(n\\)\npower law term, \\(C n^\\), dominate. addition, run time\ncan affected hardware constraints cache memory sizes, \ncan cause abrupt jumps run time.Using microbenchmark() system.time() two main benefits. First,\nhandles replication randomization automatically, \nconvenient. Second, attempts provide accurate timings. latter\nmostly important benchmark fast computations.can debated median summary randomly ordered evaluations\nbest way summarize run time. due way R memory\nmanagement. R allocates deallocates memory automatically\nuses garbage collection\ndeallocation. means computations occasionally, somewhat\nunpredictable manner, trigger garbage collector, result small\nfraction evaluations may take substantially longer time \nrest. median typically almost unaffected, memory deallocation\nthus effectively (wrongly) disregarded run time median\nsummary used. argument using mean instead median,\ndue randomization computation triggered\ngarbage collector might one caused memory allocation\nfirst place. Using mean instead median therefore smear\ngarbage collection run time benchmarked expressions. Setting\nargument control = list(order = \"block\") microbenchmark() \nevaluate expressions blocks, combination mean\nsummary correctly accounts memory allocation deallocation \nrun time. downside without randomization results might\nsuffer artefacts. book use randomization\nmedian summaries throughout, keep mind \nunderestimate actual average run time depending upon much\nmemory given computation requires. Memory usage affects\nrun time triggering garbage collection dealt via\ncode profiling tools instead.","code":"\nsystem.time(kern_dens(psi, 0.2))\nsystem.time(kern_dens_vec(psi, 0.2))\nsystem.time(kern_dens_apply(psi, 0.2))\nsystem.time(kern_dens_outer(psi, 0.2))## kern_dens:\n##    user  system elapsed \n##   0.044   0.033   0.082 \n## kern_dens_vec:\n##    user  system elapsed \n##   0.004   0.000   0.005 \n## kern_dens_apply:\n##    user  system elapsed \n##   0.005   0.001   0.006 \n## kern_dens_outer:\n##    user  system elapsed \n##   0.005   0.002   0.007\nlibrary(microbenchmark)\nkern_bench <- microbenchmark(\n  kern_dens(psi, 0.2),\n  kern_dens_vec(psi, 0.2),\n  kern_dens_apply(psi, 0.2),\n  kern_dens_outer(psi, 0.2)\n)\nstr(kern_bench)## Classes 'microbenchmark' and 'data.frame':   400 obs. of  2 variables:\n##  $ expr: Factor w/ 4 levels \"kern_dens(psi, 0.2)\",..: 3 3 3 4 3 2 1 1 2 2 ...\n##  $ time: num  53000885 9967802 5594794 7379961 4926126 ...## Unit: milliseconds\n##                       expr   min    lq  mean median    uq    max neval\n##        kern_dens(psi, 0.2) 27.90 29.46 36.92  31.72 33.82 499.20   100\n##    kern_dens_vec(psi, 0.2)  2.81  3.31  5.10   3.60  3.94  79.36   100\n##  kern_dens_apply(psi, 0.2)  3.24  3.83  4.93   4.34  4.86  53.00   100\n##  kern_dens_outer(psi, 0.2)  3.47  5.30  5.79   5.59  6.31   9.06   100\nautoplot(kern_bench) + \n  geom_jitter(position = position_jitter(0.2, 0), \n              aes(color = expr), alpha = 0.4) + \n  aes(fill = I(\"gray\")) + \n  theme(legend.position = \"none\")"},{"path":"density.html","id":"bandwidth","chapter":"2 Density estimation","heading":"2.3 Bandwidth selection","text":"","code":""},{"path":"density.html","id":"rectangular","chapter":"2 Density estimation","heading":"2.3.1 Revisiting the rectangular kernel","text":"return rectangular kernel compute mean squared error. \nanalysis may helpful think \\(n\\) large \\(h\\) small. Indeed,\neventually choose \\(h = h_n\\) function \\(n\\) \n\\(n \\\\infty\\) \\(h_n \\0\\). also note \\(f_h(x) = E (\\hat{f}_h(x))\\)\ndensity, thus \\(\\int f_h(x) \\mathrm{d}x = 1\\).assume \\(f_0\\) sufficiently differentiable\nuse Taylor expansion distribution function \\(F_0\\) get \\[\\begin{align*}\nf_h(x) & = \\frac{1}{2h}\\left(F_0(x + h) - F_0(x - h)\\right) \\\\\n& = \\frac{1}{2h}\\left(2h f_0(x) + \\frac{h^3}{3} f_0''(x) + R_0(x,h) \\right) \\\\\n& = f_0(x) + \\frac{h^2}{6} f_0''(x) + R_1(x,h) \n\\end{align*}\\]\\(R_1(x, h) = o(h^2)\\). One note quadratic terms \\(h\\)\nTaylor expansion canceled. gives following formula \nsquared bias \\(\\hat{f}_h\\).\\[\\begin{align*}\n\\mathrm{bias}(\\hat{f}_h(x))^2 & = (f_h(x) - f_0(x))^2 \\\\\n& = \\left(\\frac{h^2}{6} f_0''(x) + R_1(x,h) \\right)^2 \\\\\n& = \\frac{h^4}{36} f_0''(x)^2 + R(x,h)\n\\end{align*}\\]\\(R(x,h) = o(h^4)\\). variance see (2.1)\n\n\\[V(\\hat{f}_h(x)) = f_h(x)\\frac{1}{2hn} - f_h(x)^2 \\frac{1}{n}.\\]\nIntegrating sum bias variance \\(x\\) gives \nintegrated mean squared error\\[\\begin{align*}\n\\mathrm{MISE}(h) & = \\int \\mathrm{bias}(\\hat{f}_h(x))^2 + V(\\hat{f}_h(x)) \\mathrm{d}x \\\\\n& = \\frac{h^4}{36} \\|f_0''\\|^2_2 + \\frac{1}{2hn} + \\int R(x,h) \\mathrm{d} x - \n\\frac{1}{n} \\int f_h(x)^2 \\mathrm{d} x.\n\\end{align*}\\]\\(f_h(x) \\leq C\\) (happens \\(f_0\\) bounded),\n\\[\\int f_h(x)^2 \\mathrm{d} x \\leq C \\int f_h(x) \\mathrm{d} x = C,\\]\nlast term \\(o((nh)^{-1})\\). second last term \\(o(h^4)\\)\ncan interchange limit integration order. conceivable\ncan suitable assumptions \\(f_0\\), pursue\nplace. sum two remaining asymptotically dominating terms \nformula MISE \\[\\mathrm{AMISE}(h) = \\frac{h^4}{36} \\|f_0''\\|^2_2 + \\frac{1}{2hn},\\]\nknown asymptotic mean integrated squared error. Clearly,\nuseful formula, must assume \\(\\|f_0''\\|_2^2 < \\infty\\).\ncase formula AMISE can used find asymptotic\noptimal tradeoff (integrated) bias variance. Differentiating w.r.t. \\(h\\) \nfind \n\\[\\mathrm{AMISE}'(h) = \\frac{h^3}{9} \\|f_0''\\|^2_2 - \\frac{1}{2h^2n},\\]\nsolving \\(\\mathrm{AMISE}'(h) = 0\\) yields\n\\[h_n = \\left(\\frac{9}{2 \\|f_0''\\|_2^2}\\right)^{1/5} n^{-1/5}.\\]\nAMISE regarded function \\(h\\) observe \ntends \\(\\infty\\) \\(h \\0\\) well \\(h \\\\infty\\), thus unique\nstationary point \\(h_n\\) unique global minimizer. Choosing bandwidth \n\\(h_n\\) therefore minimize asympototic mean integrated squared error, \nsense optimal choice bandwidth.see “wiggliness” \\(f_0\\) enters formula optimal\nbandwidth \\(h_n\\) via \\(\\|f_0''\\|_2\\). norm second derivative\nprecisely quantification much \\(f_0\\) oscillates. large value,\nindicating wiggly \\(f_0\\), drive optimal bandwidth whereas\nsmall value drive optimal bandwidth .also observe plug optimal bandwidth formula\nAMISE, get\n\\[\\begin{align*}\n\\mathrm{AMISE}(h_n) & = \\frac{h_n^4}{36} \\|f_0''\\|^2_2 + \\frac{1}{2h_n n} \\\\\n& = C n^{-4/5},\n\\end{align*}\\]\nindicates terms integrated mean squared error rate\ncan nonparametrically estimate\n\\(f_0\\) \\(n^{-4/5}\\). contrasted common parametric\nrate \\(n^{-1}\\) mean squared error.practical viewpoint one major problem optimal\nbandwidth \\(h_n\\); depends via \\(\\|f_0''\\|^2_2\\) upon unknown \\(f_0\\)\ntrying estimate. therefore refer \\(h_n\\) oracle\nbandwidth – bandwidth oracle knows \\(f_0\\) \ntell us use. practice, come estimate\n\\(\\|f_0''\\|^2_2\\) plug estimate formula \\(h_n\\).\npursue couple different options general kernel\ndensity estimators together methods rely \nAMISE formula.","code":""},{"path":"density.html","id":"ise-mise-and-mse-for-kernel-estimators","chapter":"2 Density estimation","heading":"2.3.2 ISE, MISE and MSE for kernel estimators","text":"Bandwidth selection general kernel estimators can studied\nasymptotically just . end useful formalize\nquantify quality estimate \\(\\hat{f}_h\\). One natural\nquantification integrated squared error,\n\\[\\mathrm{ISE}(\\hat{f}_h) = \\int (\\hat{f}_h(x) - f_0(x))^2 \\ \\mathrm{d}x = \\|\\hat{f}_h - f_0\\|_2^2.\\]quality estimation procedure producing \\(\\hat{f}_h\\) data\ncan quantified taking mean ISE,\n\\[\\mathrm{MISE}(h) = E(\\mathrm{ISE}(\\hat{f}_h)),\\]\nexpectation integral data. Using Tonelli’s theorem\nmay interchange expectation integration \\(x\\) \nget\n\\[\\mathrm{MISE}(h) = \\int \\mathrm{MSE}_x(h) \\ \\mathrm{d}x\\]\n\n\\[\\mathrm{MSE}_h(x) = \\mathrm{var}(\\hat{f}_h(x)) + \\mathrm{bias}(\\hat{f}_h(x))^2.\\]\npointwise mean squared error.Using kind Taylor expansion argument can show \n\\(K\\) square integrable probability density mean 0 \n\\[\\sigma_K^2 = \\int z^2 K(z) \\ \\mathrm{d}z > 0,\\]\n\n\\[\\mathrm{MISE}(h) = \\mathrm{AMISE}(h) + o((nh)^{-1} + h^4)\\]\nasymptotic mean integrated squared error \n\\[\\mathrm{AMISE}(h) = \\frac{\\|K\\|_2^2}{nh} + \\frac{h^4 \\sigma^4_K \\|f_0''\\|_2^2}{4}\\]\n\n\\[\\|g\\|_2^2 = \\int g(z)^2 \\ \\mathrm{d}z  \\quad (\\mathrm{squared } \\ L_2\\mathrm{-norm}).\\]\nregularity assumptions \\(f_0\\) necessary, result\nclearly need require \\(f_0''\\) meaningful square integrable.\nHowever, also enough. See Proposition .1 Tsybakov (2009) rigorous\nproof.minimizing \\(\\mathrm{AMISE}(h)\\) derive optimal oracle bandwidth\\[\\begin{equation}\n\\tag{2.3}\nh_n = \\left( \\frac{\\|K\\|_2^2}{ \\|f_0''\\|^2_2  \\sigma_K^4} \\right)^{1/5} n^{-1/5}.\n\\end{equation}\\]plug formula formula AMISE arrive asymptotic\nerror rate \\(\\mathrm{AMISE}(h_n) = C n^{-4/5}\\) constant \\(C\\) depending \n\\(f_0''\\) kernel. noteworthy asymptotic analysis can carried\neven \\(K\\) allowed take negative values, though resulting estimate\nmay valid density . Tsybakov (2009) demonstrates improve \nrate \\(n^{-4/5}\\) allowing kernels whose moments order two vanish.\nNecessarily, kernels must take negative values.observe rectangular kernel,\n\\[\\sigma_K^4 = \\left(\\frac{1}{2} \\int_{-1}^1 z^2 \\ \\mathrm{d} z\\right)^2 = \\frac{1}{9}\\]\n\n\\[\\|K\\|_2^2 = \\frac{1}{2^2} \\int_{-1}^1 \\ \\mathrm{d} z = \\frac{1}{2}.\\]\nPlugging numbers (2.3) find oracle bandwidth \nrectangular kernel derived Section 2.3.1. Gaussian\nkernel find \\(\\sigma_K^4 = 1\\), \n\\[\\|K\\|_2^2 = \\frac{1}{2 \\pi} \\int e^{-x^2} \\ \\mathrm{d} x = \\frac{1}{2 \\sqrt{\\pi}}.\\]","code":""},{"path":"density.html","id":"plug-in-estimation-of-the-oracle-bandwidth","chapter":"2 Density estimation","heading":"2.3.3 Plug-in estimation of the oracle bandwidth","text":"compute \\(\\|f_0''\\|^2_2\\) enters formula asymptotically optimal\nbandwidth know \\(f_0\\) trying estimate first place.\nresolve circularity make first guess \\(f_0\\) plug \nguess formula oracle bandwidth.first guess \\(f_0\\) Gaussian mean 0 variance \\(\\sigma^2\\).\n\\[\\begin{align*}\n\\|f_0''\\|^2_2 & = \\frac{1}{2 \\pi \\sigma^2} \\int \\left(\\frac{x^2}{\\sigma^4} -  \\frac{1}{\\sigma^2}\\right)^2 e^{-x^2/\\sigma^2} \\ \\mathrm{d}x \\\\\n& = \\frac{1}{2 \\sigma^9 \\sqrt{\\pi}} \\frac{1}{\\sqrt{\\pi \\sigma^2}} \\int (x^4 - 2 \\sigma^2 x^2 + \\sigma^4) e^{-x^2/\\sigma^2} \\ \\mathrm{d}x \\\\ \n& = \\frac{1}{2 \\sigma^9 \\sqrt{\\pi}} (\\frac{3}{4} \\sigma^4 - \\sigma^4 + \\sigma^4) \\\\\n& = \\frac{3}{8 \\sigma^5 \\sqrt{\\pi}}.\n\\end{align*}\\]Plugging expression squared 2-norm second derivative density\nformula oracle bandwidth gives\\[\\begin{equation}\n\\tag{2.4}\nh_n = \\left( \\frac{8 \\sqrt{\\pi} \\|K\\|_2^2}{3 \\sigma_K^4} \\right)^{1/5} \\sigma n^{-1/5},\n\\end{equation}\\]\\(\\sigma\\) quantity depending unknown density \\(f_0\\). can now\nsimply estimate \\(\\sigma\\), using e.g. empirical standard\ndeviation \\(\\hat{\\sigma}\\), plug estimate formula get \nestimate oracle bandwidth.well known empirical standard deviation sensitive outliers,\n\\(\\sigma\\) overestimated reason, bandwidth large\nresulting estimate oversmoothed. \nget robust bandwidth selection, Silverman (1986) suggested using\ninterquartile range estimate \\(\\sigma\\). fact, suggested\nestimating \\(\\sigma\\) \n\\[\\tilde{\\sigma} = \\min\\{\\hat{\\sigma}, \\mathrm{IQR} / 1.34\\}.\\]\nestimator, IQR denotes empirical interquartile range, \n\\(1.34\\) approximately interquartile range, \\(\\Phi^{-1}(0.75) - \\Phi^{-1}(0.25)\\),\nstandard Gaussian distribution. Curiously, interquartile range \nstandard Gaussian distribution \\(1.35\\) two decimals accuracy, \nuse \\(1.34\\) estimator \\(\\tilde{\\sigma}\\)\nprevailed. Silverman, moreover, suggested reduce kernel-dependent\nconstant formula (2.4) \\(h_n\\) reduce\noversmoothing.specialize Gaussian kernel, formula (2.4)\nsimplifies \n\\[\\hat{h}_n = \\left(\\frac{4}{3}\\right)^{1/5} \\tilde{\\sigma} n^{-1/5},\\]\n\\(\\tilde{\\sigma}\\) plugged robust estimate \\(\\sigma\\). Silverman (1986)\nmade ad hoc suggestion reduce factor \\((4/3)^{1/5} \\simeq 1.06\\) \\(0.9\\).\nresults bandwidth estimate\n\\[\\hat{h}_n = 0.9 \\tilde{\\sigma} n^{-1/5},\\]\nbecome known Silverman’s rule thumb.Silverman’s rule thumb default bandwidth estimator density()\nimplemented function bw.nrd0(). bw.nrd() function implements\nbandwidth selection using factor \\(1.06\\) instead \\(0.9\\). Though theoretical\nderivations behind implementations assume \\(f_0\\)\nGaussian, either give reasonable bandwidth selection range \nunimodal distributions. \\(f_0\\) multimodal, Silverman’s\nrule thumb known oversmooth density estimate.Instead computing \\(\\|f_0''\\|^2_2\\) assuming distribution Gaussian,\ncan compute norm pilot estimate, \\(\\tilde{f}\\), plug result\nformula \\(h_n\\). pilot estimate kernel estimate \nkernel \\(H\\) bandwidth \\(r\\) get\n\\[\\|\\tilde{f}''\\|^2_2 = \\frac{1}{n^2r^6} \\sum_{= 1}^n \\sum_{j=1}^n \n\\int H''\\left( \\frac{x - x_i}{r} \\right) H''\\left( \\frac{x - x_j}{r} \\right) \\mathrm{d} x.\\]\nproblem , course, now choose pilot bandwidth\n\\(r\\). using simple method like Silverman’s rule thumb \nstage typically bad idea. Thus arrive following\nplug-procedure using Gaussian kernel pilot estimate:Compute estimate, \\(\\hat{r}\\), pilot bandwidth using Silverman’s\nrule thumb.Compute \\(\\|\\tilde{f}''\\|^2_2\\) using Gaussian kernel pilot kernel \\(H\\) \nusing estimated pilot bandwidth \\(\\hat{r}\\).Plug \\(\\|\\tilde{f}''\\|^2_2\\) oracle bandwidth formula (2.3)\ncompute \\(\\hat{h}_n\\) kernel \\(K\\).Note use pilot kernel different Gaussian \nadjust constant 0.9 (1.06) Silverman’s rule thumb accordingly\ncomputing \\(\\|H\\|_2^2\\) \\(\\sigma_H^4\\) using (2.4).Sheather Jones (1991) took plug-ideas step analyzed detail\nchoose pilot bandwidth good data adaptive way. resulting\nmethod somewhat complicated implementable. skip details simply\nobserve method implemented R function bw.SJ(), \ncan selected using density() setting argument bw = \"SJ\".\nplug-method regarded solid default performs well \nmany different data generating densities \\(f_0\\).","code":""},{"path":"density.html","id":"cv","chapter":"2 Density estimation","heading":"2.3.4 Cross-validation","text":"alternative relying asymptotic optimality arguments \nintegrated mean squared error corresponding plug-estimates\nbandwidth known cross-validation. method mimics\nidea setting aside subset data set, \nused computing estimate validating estimator’s\nperformance. benefit cross-validation simply setting\naside validation data set “waste” data points\nvalidation . data points used ultimate computation\nestimate. deficit cross-validation usually\ncomputationally demanding.Suppose \\(I_1, \\ldots, I_k\\) form partition index set \\(\\{1, \\ldots, n\\}\\)\ndefine\n\\[^{-} = \\bigcup_{l: \\\\I_l} I_l.\\]\n, \\(^{-}\\) contains indices belong set \\(I_l\\)\ncontaining \\(\\). particular, \\(\\\\^{-}\\). Define also \\(n_i = |^{-}|\\)\n\n\\[\\hat{f}^{-}_h = \\frac{1}{h n_i} \\sum_{j \\^{-}} K\\left(\\frac{x_i - x_j}{h}\\right).\\]\n, \\(\\hat{f}^{-}_h\\) kernel density estimate based data \nindices \\(^{-}\\) evaluated \\(x_i\\). Since density\nestimate evaluated \\(x_i\\) based \\(x_i\\), quantity \\(\\hat{f}^{-}_h\\)\ncan used assess well density estimate computed using bandwidth\n\\(h\\) concur data point \\(x_i\\). can summarized using \nlog-likelihood\n\\[\\ell_{\\mathrm{CV}}(h) = \\sum_{=1}^n \\log (\\hat{f}^{-}_h),\\]\nrefer cross-validated log-likelihood, define\nbandwidth estimate \n\\[\\hat{h}_{\\mathrm{CV}} = \\textrm{arg max}_h \\ \\ \\ell_{\\mathrm{CV}}(h).\\]\ncross-validation based bandwidth can used computing kernel\ndensity estimates using entire data set.partition indices consists \\(k\\) subsets usually talk \n\\(k\\)-fold cross-validation. \\(k = n\\) subsets consist just single\nindex talk leave-one-cross-validation. leave-one-\ncross-validation one possible partition, \\(k < n\\)\nmany possible partitions. chosen ? practice,\nchoose partition sampling indices randomly without replacement\n\\(k\\) sets size roughly \\(n / k\\).also possible use cross-validation combination MISE. Rewriting\nfind \n\\[\\mathrm{MISE}(h) = E (\\| \\hat{f}_h\\|_2^2) - 2 E (\\hat{f}_h(X)) + E(\\|f_0^2\\|_2^2)\\]\n\\(X\\) random variable independent data distribution \ndensity \\(f_0\\). last term depend upon \\(h\\) can ignore \npoint view minimizing MISE. first term unbiased\nestimate \\(\\| \\hat{f}_h\\|_2^2\\). middle term can estimated\nwithout bias \n\\[\\frac{2}{n} \\sum_{=1}^n \\hat{f}^{-}_h,\\]\nleads statistic\n\\[\\mathrm{UCV}(h) = \\| \\hat{f}_h\\|_2^2 - \\frac{2}{n} \\sum_{=1}^n \\hat{f}^{-}_h\\]\nknown unbiased cross-validation criterion. corresponding\nbandwidth estimate \n\\[\\hat{h}_{\\mathrm{UCV}} = \\textrm{arg min}_h \\ \\ \\mathrm{UCV}(h).\\]\nContrary log-likelihood based criterion, criterion requires\ncomputation \\(\\| \\hat{f}_h\\|_2^2\\). Bandwidth selection using UCV\nimplemented R function bw.ucv() can \nused density() setting argument bw = \"ucv\".","code":""},{"path":"density.html","id":"likelihood","chapter":"2 Density estimation","heading":"2.4 Likelihood considerations","text":"Section 2.3.4 cross-validated likelihood used bandwidth selection.\nnatural ask go -simply maximized \nlikelihood possible densities find maximum likelihood estimator\ninstead using ad hoc idea behind kernel density estimation.log-likelihood\n\\[\\ell(f) = \\sum_{=j}^n \\log f(x_j)\\]\nwell defined function density \\(f\\) – even \\(f\\) restricted\nbelong finite-dimensional parametric model.\ninvestigate nonparametric MLE meaningful consider likelihood\nbehaves densities\n\\[\\overline{f}_h(x) = \\frac{1}{nh \\sqrt{2 \\pi}} \\sum_{j=1}^n e^{- \\frac{(x - x_j)^2}{2 h^2} }\\]\ndifferent choices \\(h\\). Note \\(\\overline{f}_h\\) simply \nkernel density estimator Gaussian kernel bandwidth \\(h\\).Figure 2.5 shows \ndensities look like compared histogram \\(\\psi\\)-angle data.\nClearly, large \\(h\\) densities smooth slowly oscillating, \n\\(h\\) gets smaller densities become wiggly. \\(h \\0\\)\ndensities become increasingly dominated tall narrow peaks\naround individual data points.\nFigure 2.5: densities \\(\\overline{f}_h\\) different choices \\(h\\).\nway densities adapt data points reflected \nlog-likelihood well.\nFigure 2.6: Log-likelihood, \\(\\ell(\\overline{f}_h)\\), densities \\(\\overline{f}_h\\) function \\(h\\). Note log-scale right.\nFigure 2.6 clear likelihood decreasing \n\\(h\\), appears unbounded \\(h \\0\\). clearly\nseen figure \\(h\\) plotted log-scale appears\nlog-likelihood approximately behaves \\(-\\log(h)\\) \\(h \\0\\).can show , indeed, case. \\(x_i \\neq x_j\\) \\(\\neq j\\)\\[\\begin{align*}\n\\ell(\\overline{f}_h) & = \\sum_{} \\log\\left(1 + \\sum_{j \\neq } e^{-(x_i - x_j)^2 / (2 h^2)} \\right) - \nn \\log(nh\\sqrt{2 \\pi}) \\\\\n& \\sim - n \\log(nh\\sqrt{2 \\pi})\n\\end{align*}\\]\\(h \\0\\). Hence, \\(\\ell(\\overline{f}_h) \\\\infty\\) \\(h \\0\\). demonstrates MLE\ndensity exist set distributions densities.sense weak convergence actually holds \n\\[\\overline{f}_h \\cdot m \\overset{\\mathrm{wk}}{\\longrightarrow} \n\\varepsilon_n = \\frac{1}{n} \\sum_{j=1}^n \\delta_{x_j}\\]\n\\(h \\0\\). empirical measure \\(\\varepsilon_n\\) can sensibly regarded \nnonparametric MLE distribution, empirical measure \ndensity. conclude directly define sensible\ndensity estimator maximum-likelihood estimator.","code":"\n## The densities can easily be implemented using the density implementation\n## of the Gaussian density in R\nf_h <- function(x, h) mean(dnorm(x, psi, h))\n## This function does not work as a vectorized function as it is, but there is \n## a convenience function, 'Vectorize', in R that turns the function into a \n## function that can actually be applied correctly to a vector. \nf_h <- Vectorize(f_h)\n# To plot the log-likelihood we need to evaluate it in a grid of h-values.\nhseq <- seq(1, 0.001, -0.001)\n# For the following computation of the log-likelihood it is necessary \n# that f_h is vectorized. There are other ways to implement this computation\n# in R, and some are more efficient, but the computation of the log-likelihood \n# for each h scales as O(n^2) with n the number of data points. \nll <- sapply(hseq, function(h) sum(log(f_h(psi, h))))\nqplot(hseq, ll, geom = \"line\") + xlab(\"h\") + ylab(\"Log-likelihood\")\nqplot(hseq, ll, geom = \"line\") + scale_x_log10(\"h\") + ylab(\"\")"},{"path":"density.html","id":"sieves","chapter":"2 Density estimation","heading":"2.4.1 Method of sieves","text":"sieve family models, \\(\\Theta_{h}\\), indexed \nreal valued parameter, \\(h \\\\mathbb{R}\\), \\(\\Theta_{h_1} \\subseteq \\Theta_{h_2}\\) \n\\(h_1 \\leq h_2\\). chapter \\(\\Theta_{h}\\) denote set probability densities.\nincreasing family models chosen sensible way, \nmay able compute MLE\n\\[\\hat{f}_h = \\text{arg max}_{f \\\\Theta_h} \\ell(f),\\]\nmay even able choose \\(h = h_n\\) function sample size \\(n\\)\n\\(\\hat{f}_{h_n}\\) becomes consistent estimator \\(f_0\\).possible take\n\\[\\Theta_h = \\{ \\overline{f}_{h'} \\mid h' \\leq h \\}\\]\n\\(\\overline{f}_{h'}\\) defined , case \\(\\hat{f}_h = \\overline{f}_h\\).\nsee following section simply kernel estimator.interesting example obtained letting\n\\[\\Theta_h = \\left\\{ x \\mapsto  \\frac{1}{h \\sqrt{2 \\pi}} \\int  e^{- \\frac{(x - z)^2}{2 h^2} } \\mathrm{d}\\mu(z) \\Biggm| \\mu \\textrm{ probability measure}   \\right\\},\\]\nknown convolution sieve. note \\(\\overline{f}_h \\\\Theta_h\\) \ntaking \\(\\mu = \\varepsilon_n\\), generally \\(\\hat{f}_h\\) \ndifferent \\(\\overline{f}_h\\).pursue general theory sieve estimators, refer\npaper Nonparametric Maximum Likelihood Estimation Method Sieves Geman Hwang.\nfollowing section work practical details\nparticular sieve estimator based basis expansion \nlog-density.","code":""},{"path":"density.html","id":"basis-density","chapter":"2 Density estimation","heading":"2.4.2 Basis expansions","text":"suppose section data points contained \ninterval \\([,b]\\) \\(, b \\\\mathbb{R}\\). true angle\ndata \\(= -\\pi\\) \\(b = \\pi\\) matter size data set,\n\\(f_0\\) bounded support may necessary let \\(\\)\n\\(b\\) change data. However, fixed data set can choose\nsufficiently large \\(\\) \\(b\\).section sieve indexed integers,\n\\(h \\\\mathbb{N}\\) suppose chosen continuous\nfunctions \\(b_1, \\ldots, b_h : [,b] \\\\mathbb{R}\\). \ncalled basis functions. define\n\\[\\Theta_h = \\left\\{ x \\mapsto  \\varphi(\\boldsymbol{\\beta})^{-1} \n\\exp\\left(\\sum_{k=1}^h \\beta_k b_k(x)\\right) \\Biggm| \\boldsymbol{\\beta} = (\\beta_1, \\ldots, \\beta_h)^T \\\\mathbb{R}^h \\right\\},\\]\n\n\\[\\varphi(\\boldsymbol{\\beta}) = \\int_a^b \\exp\\left(\\sum_{k=1}^h \\beta_k b_k(x)\\right) \\mathrm{d} x.\\]MLE \\(\\Theta_h\\) given \n\\[\\hat{f}_h(x) =  \\varphi(\\hat{\\boldsymbol{\\beta}})^{-1} \n\\exp\\left(\\sum_{k=1}^h \\hat{\\beta}_k b_k(x)\\right),\\]\n\\[\\begin{align*}\n\\hat{\\boldsymbol{\\beta}} & = \n\\text{arg max}_{\\boldsymbol{\\beta} \\\\mathbb{R}^h} \\ \\sum_{j=1}^n \\sum_{k=1}^h \\beta_k b_k(x_j) - \\log \\varphi(\\boldsymbol{\\beta}) \\\\\n& = \\text{arg max}_{\\boldsymbol{\\beta} \\\\mathbb{R}^h} \\ \\ \\mathbf{1}^T \\mathbf{B} \\boldsymbol{\\beta} - \\log \\varphi(\\boldsymbol{\\beta}).\n\\end{align*}\\]\\(\\mathbf{B}\\) \\(n \\times h\\) matrix \\(B_{jk} = b_k(x_j)\\). Thus \nfixed \\(h\\) model , fact, just ordinary parametric exponential\nfamily, though may entirely straightforward compute\n\\(\\varphi(\\boldsymbol{\\beta})\\).Many basis functions possible. Polynomials may used, splines \noften preferred. alternative selection trigonometric functions,\ninstance\\[b_1(x) = \\cos(x), b_2(x) = \\sin(x), \\ldots, b_{2h-1}(x) = \\cos(hx),  b_{2h}(x) = \\sin(hx)\\]\ninterval \\([-\\pi, \\pi]\\). Section 1.2.1 simple special case\nactually treated corresponding \\(h = 2\\), normalization constant\nidentified terms modified Bessel function.worth remembering following:“four parameters can fit elephant, five can make wiggle trunk.”— John von NeumannThe Normal-inverse Gaussian distribution four parameters \ngeneralized hyperbolic distribution extension five,\nvon Neumann probably thinking terms \nspline polynomial expansion four five suitably\nchosen basis functions.quote mathematical statement empirical observation.\nhandful parameters already quite flexible class \ndensities fit many real data sets well. remember\nreasonably good fit mean\nfound “true” data generating model. Though data \nsituations scarce resource today von Neumann made\nelephants wiggle trunks, quote still suggests \\(h\\) grow\nrather slowly \\(n\\) avoid overfitting.","code":""},{"path":"density.html","id":"density:ex","chapter":"2 Density estimation","heading":"2.5 Exercises","text":"","code":""},{"path":"density.html","id":"kernel-density-estimation","chapter":"2 Density estimation","heading":"Kernel density estimation","text":"Exercise 2.1  Epanechnikov kernel given \n\\[K(x) = \\frac{3}{4}(1 - x^2)\\]\n\\(x \\[-1, 1]\\) 0 elsewhere. Show probability density\nmean zero compute \\(\\sigma_K^2\\) well \\(\\|K\\|_2^2\\).following exercises use log(F12) variable considered \nExercise .4.Exercise 2.2  Use density() compute kernel density estimate Epanechnikov kernel\nlog(F12) data. Try different bandwidths.Exercise 2.3  Implement kernel density estimation using Epanechnikov kernel.\nTest implementation comparing density() using log(F12) data.","code":""},{"path":"density.html","id":"benchmarking-1","chapter":"2 Density estimation","heading":"Benchmarking","text":"Exercise 2.4  Construct following vectorThen use microbenchmark benchmark computation offor k ranging \\(2^5\\) \\(2^{13}\\). Summarize benchmarking results.Exercise 2.5  Benchmark implementation kernel density estimation using Epanechnikov\nkernel. Compare results obtained density().Exercise 2.6  Experiment different implementations kernel evaluation R using\nGaussian kernel Epanechnikov kernel. Use microbenchmark() compare\ndifferent implementations.","code":"\nx <- rnorm(2^13)\ndensity(x[1:k], 0.2)"},{"path":"bivariate.html","id":"bivariate","chapter":"3 Bivariate smoothing","heading":"3 Bivariate smoothing","text":"chapter estimation smooth relation two variables\n\\((X, Y)\\). focus direct estimation (aspects ) \nconditional distribution \\(Y\\) given \\(X\\). Thus variables treated\nasymmetrically. variables real valued, get good idea\nrelation scatter plot \\(Y\\) \\(X\\),\naim basically smooth relation scatter plot shows.methods considered, matter \\(X\\) represents \ndeterministic random variable. Thus \\(X\\) fixed experimental\ndesign, represent time observed variable.\nFigure 3.1 (left),\n\\(Y\\) annual average temperature Nuuk, Greenland, \\(X\\) \ntime calendar years. Figure 3.1 (right), \\(Y\\) \nmonthly average temperature Nuuk, \\(X\\) monthly average\ntemperature Qaqortoq – town south Nuuk Greenland. \nformer example, time deterministic interested \ntemperature Nuuk changes time. , instance, trend?\nsecond example, interested temperature Qaqortoq\npredictive temperature Nuuk. use two\ntemperature examples throughout, see Vinther et al. (2006). raw data available\nsite\nSW Greenland temperature data,\nalso available CSwR package.\nFigure 3.1: Nuuk average yearly temperature degrees Celsius (left) smoothed using loess (black), degree 10 polynomial (red) smooth spline (purple). Nuuk average monthly temperature Qaqortoq average montly temperature (right) smoothed using straight line (red) smooth spline (purple).\n","code":"\np_Nuuk <- ggplot(Nuuk_year, aes(x = Year, y = Temperature)) + \n  geom_point()\n\np_Nuuk + \n  geom_smooth(se = FALSE) +         # A loess smoother\n  geom_smooth(\n    method = \"lm\", \n    formula = y ~ poly(x, 10),      # A degree-10 polynomial expansion\n    color = \"red\", \n    se = FALSE\n  ) + \n  geom_smooth(\n    method = \"gam\", \n    formula = y ~ s(x, bs = \"cr\"),   # A spline smoother via 'mgcv::gam()'\n    color = \"purple\", \n    se = FALSE\n  )\n\np_Nuuk_Qaqortoq <- ggplot(Nuuk_Qaqortoq, aes(Temp_Qaqortoq, Temp_Nuuk)) + \n  geom_point()\n\np_Nuuk_Qaqortoq +\n  geom_smooth(\n    method = \"lm\", \n    formula = y ~ x,                 # A straight line smoother\n    color = \"red\", \n    se = FALSE\n  ) + \n  geom_smooth(\n    method = \"gam\", \n    formula = y ~ s(x, bs = \"cr\"),   # A spline smoother via 'mgcv::gam()'\n    color = \"purple\", \n    se = FALSE\n  )"},{"path":"bivariate.html","id":"nearest-neighbor-and-kernel-smoothers","chapter":"3 Bivariate smoothing","heading":"3.1 Nearest neighbor and kernel smoothers","text":"One basic ideas smoothing bivariate data use \nrunning mean moving average. particularly sensible \n\\(x\\)-values equidistant, e.g. observations constitute\ntime series Nuuk temperature data. running mean \nexample general nearest neighbor smoothers.Mathematically, \\(k\\) nearest neighbor smoother \\(x_i\\) defined \n\\[\\hat{f}_i = \\frac{1}{k} \\sum_{j \\N_i} y_j\\]\n\\(N_i\\) set indices \\(k\\) nearest neighbors \\(x_i\\).\nsimple idea actually general powerful. works long\n\\(x\\)-values lie metric space, letting \\(k\\) grow \n\\(n\\) possible construct consistent nonparametric estimators \nregression functions, \\(f(x) = E(Y \\mid X = x)\\), minimal assumptions.\npractical problem \\(k\\) must grow slowly high dimensions,\nestimator panacea.chapter focus mostly \\(x\\) real valued \nordinary metric used define nearest neighbors. total ordering \nreal line adds couple extra possibilities definition \\(N_i\\).\n\\(k\\) odd, symmetric nearest neighbor smoother takes \\(N_i\\) consist\n\\(x_i\\) together \\((k-1)/2\\) smaller \\(x_j\\)-s closest \n\\(x_i\\) \\((k-1)/2\\) larger \\(x_j\\)-s closest \\(x_i\\). \nalso possible choose one-sided smoother \\(N_i\\) corresponding\n\\(k\\) smaller \\(x_j\\)-s closest \\(x_i\\), case smoother \nknown causal filter.symmetric definition neighbors makes easy\nhandle neighbors computationally; don’t need compute keep\ntrack \\(n^2\\) pairwise distances \\(x_i\\)-s, need\nsort data according \\(x\\)-values. data sorted,\n\\[N_i = \\{- (k - 1) / 2, - (k - 1) / 2 + 1, \\ldots, - 1 , , + 1, \\ldots,   + (k - 1) / 2\\}\\]\n\\((k - 1) / 2 \\leq \\leq n - (k - 1) / 2\\). symmetric \\(k\\) nearest neighbor\nsmoother thus running mean \\(y\\)-values sorted according \n\\(x\\)-values. couple possibilities handling boundaries,\none simply define value \\(\\hat{f}_i\\) outside interval\n.\\(\\hat{\\mathbf{f}}\\) denoting vector smoothed values nearest\nneighbor smoother can observe always possible write\n\\(\\hat{\\mathbf{f}} = \\mathbf{S}\\mathbf{y}\\) matrix \\(\\mathbf{S}\\). symmetric\nnearest neighbor smoother data sorted according \\(x\\)-values,\nmatrix following band diagonal form\\[\n\\mathbf{S} = \\left( \\begin{array}{cccccccccc} \n\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & 0 & 0 & \\ldots & 0 & 0 \\\\\n0 & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & 0 & \\ldots & 0 & 0\\\\\n0 & 0 & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\ldots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ldots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\ldots & \\frac{1}{5} & \\frac{1}{5} \\\\\n\\end{array} \\right) \n\\]given \\(k = 5\\) dimensions \\((n - 4) \\times n\\) due \nundefined boundary values.","code":""},{"path":"bivariate.html","id":"linear-smoothers","chapter":"3 Bivariate smoothing","heading":"3.1.1 Linear smoothers","text":"smoother form \\(\\hat{\\mathbf{f}} = \\mathbf{S}\\mathbf{y}\\) smoother matrix \\(\\mathbf{S}\\),\nnearest neighbor smoother, known linear smoother.\nlinear form often beneficial theoretical arguments, many smoothers\nconsidered chapter linear smoothers. computing \\(\\mathbf{f}\\)\nmay, however, many alternatives forming matrix \\(\\mathbf{S}\\)\ncomputing matrix-vector product. Indeed, often best\nway compute smoothed values., hand, useful see \\(\\mathbf{S}\\) can constructed\nsymmetric nearest neighbor smoother.construction relies vector recycling w construction S\nfact w length \\(147 + 1\\), effectively cause w \ntranslated one right every time recycled new row. seen,\ncode triggers warning R, case get want.can use matrix smooth annual average temperature Nuuk using \nrunning mean window \\(k = 11\\) years. , smoothed temperature\ngiven year average temperatures period\nfive years five years . Note add smoothed\nvalues previous plot need pad values boundaries\nNAs get vector length 147.\nFigure 3.2: Annual average temperature Nuuk smoothed using running mean \\(k = 11\\) neighbors.\n","code":"\nw <- c(rep(1/11, 11), rep(0, 147 - 10))\nS <- matrix(w, 147 - 10, 147, byrow = TRUE)## Warning in matrix(w, 147 - 10, 147, byrow = TRUE): data length [148] is not a sub-multiple or\n## multiple of the number of rows [137]\nS\n# Check first if data is sorted correctly.\n# The test is backwards, but confirms that data isn't unsorted :-)\nis.unsorted(Nuuk_year$Year)## [1] FALSE\nf_hat <- c(rep(NA, 5), S %*% Nuuk_year$Temperature, rep(NA, 5))\np_Nuuk + geom_line(aes(y = f_hat), color = \"blue\")"},{"path":"bivariate.html","id":"implementing-the-running-mean","chapter":"3 Bivariate smoothing","heading":"3.1.2 Implementing the running mean","text":"running mean smoother fulfills following identity\n\\[\\hat{f}_{+1} = \\hat{f}_{} - y_{- (k-1)/2} / k + y_{+ (k + 1)/2} / k,\\]\ncan used much efficient implementation\nmatrix-vector multiplication. emphasized\nidentity implementation \nassume data sorted according \\(x\\)-values.\nFigure 3.3: Annual average temperature Nuuk smoothed using running mean \\(k = 11\\) neighbors. time using different implementation Figure 3.2.\nR function filter() (stats package) can used compute running\nmeans general moving averages using weight vector. compare two\nimplementations filter().Note filter() uses boundary convention used run_mean().benchmark comparison matrix-vector multiplication, run_mean() filter()\ngives following table median run time microseconds.matrix-vector computation clearly much slower two alternatives,\ntime construct \\(\\mathbf{S}\\)-matrix even included\nbenchmark . also difference matrix-vector\nmultiplication scales size data compared alternatives.\nWhenever data size doubles run time approximately doubles \nfilter() run_mean(), quadruples matrix-vector\nmultiplication. shows difference algorithm\nscales like \\(O(n)\\) algorithm scales like \\(O(n^2)\\)\nmatrix-vector product .Despite filter() implementing general\nalgorithm run_mean(), still faster, reflects \nimplemented C compiled.","code":"\n# The vector 'y' must be sorted according to the x-values\nrun_mean <- function(y, k) {\n  n <- length(y)\n  m <- floor((k - 1) / 2)\n  k <- 2 * m + 1           # Ensures k to be odd and m = (k - 1) / 2\n  y <- y / k\n  s <- rep(NA, n)\n  s[m + 1] <- sum(y[1:k])\n  for(i in (m + 1):(n - m - 1)) \n    s[i + 1] <- s[i] - y[i - m] + y[i + 1 + m]\n  s\n}\np_Nuuk + geom_line(aes(y = run_mean(Nuuk_year$Temperature, 11)), color = \"blue\")\nf_hat_filter <- stats::filter(Nuuk_year$Temperature, rep(1/11, 11))\nrange(f_hat_filter - f_hat, na.rm = TRUE)## [1] -4.440892e-16  4.440892e-16\nrange(f_hat_filter - run_mean(Nuuk_year$Temperature, 11), na.rm = TRUE)## [1] -1.332268e-15  4.440892e-16##                                       expr     median\n## 1                          S1 %*% y[1:512]   484.8890\n## 2                         S2 %*% y[1:1024]  1950.6210\n## 3                         S3 %*% y[1:2048]  8258.3855\n## 4                         S4 %*% y[1:4096] 33075.7655\n## 5               run_mean(y[1:512], k = 11)   112.4760\n## 6              run_mean(y[1:1024], k = 11)   197.7215\n## 7              run_mean(y[1:2048], k = 11)   376.1560\n## 8              run_mean(y[1:4096], k = 11)   734.0545\n## 9   stats::filter(y[1:512], rep(1/11, 11))   135.1490\n## 10 stats::filter(y[1:1024], rep(1/11, 11))   180.7525\n## 11 stats::filter(y[1:2048], rep(1/11, 11))   229.8480\n## 12 stats::filter(y[1:4096], rep(1/11, 11))   389.4675"},{"path":"bivariate.html","id":"choose-k-by-cross-validation","chapter":"3 Bivariate smoothing","heading":"3.1.3 Choose \\(k\\) by cross-validation","text":"Cross-validation relies predictions \\(y_i\\) \\(x_i\\)\ndata points \\((x_i, y_i)\\) left data set predictor\nfitted data. Many (linear) smoothers natural definition “--sample”\nprediction, , \\(\\hat{f}(x)\\) computed \\(x\\) \ndata. , becomes possible define\n\\[\\hat{f}^{-}_i = \\hat{f}^{-}(x_i)\\]\nprediction \\(x_i\\) using smoother computed data\n\\((x_i, y_i)\\) excluded. However, derectly define\n\\[\\hat{f}^{-}_i = \\sum_{j \\neq } \\frac{S_{ij}y_j}{1 - S_{ii}}\\]\nlinear smoother. definition concurs \n“--sample” predictor \\(x_i\\) smoothers,\nverified case--case.running mean little special respect. \nprevious section, \nrunning mean considered odd \\(k\\) using\nsymmetric neighbor definition. convenient\nconsidering running mean observations \\(x_i\\).\nconsidering running mean point, symmetric\nneighbor definition works better even \\(k\\). \nexactly definition \\(\\hat{f}^{-}_i\\) amounts .\n\\(\\mathbf{S}\\) running mean smoother matrix odd \\(k\\),\n\\(\\hat{f}^{-}_i\\) corresponds symmetric \\((k-1)\\)-nearest\nneighbor smoothing excluding \\((x_i, y_i)\\) data.Using definition , get leave-one-cross-validation\nsquared error criterion becomes\n\\[\\mathrm{LOOCV} = \\sum_{=1}^n (y_i - \\hat{f}^{-}_i)^2 = \n\\sum_{=1}^n \\left(\\frac{y_i - \\hat{f}_i}{1 - S_{ii}}\\right)^2.\\]\nimportant observation identity LOOCV\ncan computed without actually computing \\(\\hat{f}^{-}_i\\).running mean, diagonal elements smoother matrix identical.\ndisregard boundary values (NA value), get\ncomparable quantity across different choices \\(k\\) use mean() instead \nsum() implementation.\nFigure 3.4: leave-one-cross-validation criterion running mean function number neighbors \\(k\\).\noptimal choice \\(k\\) 15, LOOCV criterion jumps quite \nlot changing neighbor size, \\(k = 9\\) well \\(k = 25\\)\ngive rather low values well.\nFigure 3.5: \\(k\\)-nearest neighbor smoother optimal choice \\(k\\) based LOOCV (blue) \\(k = 9\\) (red) \\(k = 25\\) (purple).\n","code":"\nloocv <- function(k, y) {\n  f_hat <- run_mean(y, k)\n  mean(((y - f_hat) / (1 - 1/k))^2, na.rm = TRUE) \n}\nk <- seq(3, 40, 2)\nCV <- sapply(k, loocv, y = Nuuk_year$Temperature)\nk_opt <- k[which.min(CV)]\nqplot(k, CV) + geom_line() + geom_vline(xintercept = k_opt, color = \"red\")\np_Nuuk + \n  geom_line(aes(y = run_mean(Nuuk_year$Temperature, 9)), color = \"red\") +\n  geom_line(aes(y = run_mean(Nuuk_year$Temperature, k_opt)), color = \"blue\") +\n  geom_line(aes(y = run_mean(Nuuk_year$Temperature, 25)), color = \"purple\")"},{"path":"bivariate.html","id":"nadarayawatson-kernel-smoothing","chapter":"3 Bivariate smoothing","heading":"3.1.4 Nadaraya–Watson kernel smoothing","text":"section introduced basic idea nearest neighbor smoothing\nusing fixed number neighbors. similar idea use fixed\nneighborhood, \\(B_i\\), say, around \\(x_i\\). leads estimator\n\\[\\hat{f}_i = \\frac{\\sum_{j=1}^n y_j 1_{B_i}(x_j)}{\\sum_{j=1}^n 1_{B_i}(x_j)},\\]\nsimply average \\(y\\)-s corresponding \\(x\\)-s\nfall neighborhood \\(B_i\\) \\(x_i\\). Note contrary nearest\nneighbor estimator, denominator now also depends \\(x\\)-s.metric space natural\nchoice \\(B_i\\) ball, \\(B(x_i, h)\\), around \\(x_i\\) radius \\(h\\).\n\\(\\mathbb{R}\\) usual metric (even \\(\\mathbb{R}^p\\) equipped \nnorm-induced metric) \\[1_{B(x_i, h)}(x) = 1_{B(0, 1)}\\left(\\frac{x - x_i}{h}\\right),\\]\nthus since \\(B(0, 1) = [-1, 1]\\) \\(\\mathbb{R}\\)\n\\[\\hat{f}_i = \\frac{\\sum_{j=1}^n y_j 1_{[-1, 1]}\\left(\\frac{x_j - x_i}{h}\\right)}{\\sum_{l=1}^n 1_{[-1, 1]}\\left(\\frac{x_l - x_i}{h}\\right)}.\\]\nnonparametric estimator conditional expectation \\(E(Y \\mid X = x_i)\\)\nclosely related kernel density estimator \nrectangular kernel, see Section 2.2, just \nestimator natural generalization allowing arbitrary kernels\ninstead indicator function \\(1_{[-1, 1]}\\).\\(K : \\mathbb{R} \\mapsto \\mathbb{R}\\) fixed kernel corresponding\nkernel smoother bandwidth \\(h\\) becomes\n\\[\\hat{f}_i = \\frac{\\sum_{j=1}^n y_j K\\left(\\frac{x_j - x_i}{h}\\right)}{\\sum_{l=1}^n K\\left(\\frac{x_l - x_i}{h}\\right)}.\\]\nsmoother known Nadaraya–Watson kernel smoother Nadaraya–Watson\nestimator. See Exercise 3.1 additional\nperspective based bivariate kernel density estimation.Nadaraya–Watson kernel smoother can implemented much way\nkernel density estimator, run time inevitably scale\nlike \\(O(n^2)\\) unless exploit special properties kernel use\napproximation techniques binning. section focus \nimplementing bandwidth selection rather kernel smoother .\nbasic kernel smoothing computation also implemented \nksmooth function stats package.\nFigure 3.6: Nadaraya–Watson kernel smoother annual average temperature Nuuk computed using ksmooth Gaussian kernel bandwidth 10.\nkernel smoother clearly linear smoother, implement \ncomputation bandwidth selection using LOOCV direct computation\nsmoother matrix \\(\\mathbf{S}\\), given \n\\[S_{ij} = \\frac{K\\left(\\frac{x_j - x_i}{h}\\right)}{\\sum_{l=1}^n K\\left(\\frac{x_l - x_i}{h}\\right)}.\\]\nrows sum 1 construction, diagonal elements \n\\[S_{ii} = \\frac{K(0)}{\\sum_{l=1}^n K\\left(\\frac{x_l - x_i}{h}\\right)}.\\]\ncomputation \\(\\mathbf{S}\\) Gaussian kernel implemented\nusing outer rowSums.\nFigure 3.7: Nadaraya–Watson kernel smoother annual average temperature Nuuk computed using smoother matrix Gaussian kernel bandwidth 10.\nimplementation work kernel sequence \\(x\\)-s.\nexample, kernel symmetric \\(x\\)-s equidistant. Exercise\n3.2 explores exploit computation \nsmoother matrix well diagonal elements smoother matrix.smoother computed using ksmooth bandwidth 10,\nshown Figure 3.6, different smoother\ncomputed directly smoother matrix, see Figure 3.7.\nThough computations use Gaussian kernel allegedly bandwidth 10,\nresulting smoothers differ ksmooth internally rescales \nbandwidth. rescaling amounts multiplying \\(h\\) factor 0.3706506,\nmake kernel quartiles \\(\\pm 0.25 h\\) (see also ?ksmooth).LOOCV computation implemented function computes smoother\nmatrix corresponding LOOCV value function bandwidth.\ncomparison previous implementations mean computed instead \nsum.\nFigure 3.8: leave-one-cross-validation criterion kernel smoother using Gaussian kernel function bandwidth \\(h\\).\noptimal bandwith 1.55. compute resulting optimal smoother\ncompare smoother computed using ksmooth.differences order \\(10^{-5}\\), small larger \ncan explained rounding errors alone. fact, ksmooth truncates \ntails Gaussian kernel 0 beyond \\(4h\\). kernel becomes\nless \\(0.000335 \\times K(0) = 0.000134\\), practical purposes\neffectively equals zero. truncation explains relatively\nlarge differences two results otherwise equivalent. \nexample approximate solution computed ksmooth acceptable \nsubstantially reduces run time.\nFigure 3.9: Nadaraya–Watson kernel smoother annual average temperature Nuuk optimal bandwidth using LOOCV (blue) compared \\(k\\)-nearest neighbor smoother \\(k = 9\\) (red).\nFigure 3.9 shows optimal kernel smoother, actually\nsomewhat wiggly. locally smooth \\(k\\)-nearest neighbor smoother\noverall comparable smoother \\(k = 9\\).","code":"\nf_hat <- ksmooth(\n  Nuuk_year$Year, \n  Nuuk_year$Temperature, \n  kernel = \"normal\",\n  bandwidth = 10, \n  x.points = Nuuk_year$Year\n)\np_Nuuk + geom_line(aes(y = f_hat$y), color = \"blue\")\nkern <- function(x) exp(- x^2 / 2)  # The Gaussian kernel\nKij <- outer(Nuuk_year$Year, Nuuk_year$Year, function(x, y) kern((x - y) / 10))\nS <- Kij / rowSums(Kij)\nf_hat <- S %*% Nuuk_year$Temperature\np_Nuuk + geom_line(aes(y = f_hat), color = \"blue\")\nloocv <- function(h) {\n  Kij <- outer(Nuuk_year$Year, Nuuk_year$Year, function(x, y) kern((x - y) / h))\n  S <- Kij / rowSums(Kij)\n  mean(((Nuuk_year$Temperature - S %*% Nuuk_year$Temperature) / (1 - diag(S)))^2) \n}\nh <- seq(1, 5, 0.05)\nCV <- sapply(h, loocv)\nh_opt <- h[which.min(CV)]\nqplot(h, CV) + geom_line() + geom_vline(xintercept = h_opt, color = \"red\")\nKij <- outer(Nuuk_year$Year, Nuuk_year$Year, function(x, y) kern((x - y) / h_opt))\nS <- Kij / rowSums(Kij)\nf_hat <- S %*% Nuuk_year$Temperature\nf_hat_ksmooth <- ksmooth(\n  Nuuk_year$Year, Nuuk_year$Temperature, \n  kernel = \"normal\",\n  bandwidth = h_opt /  0.3706506,  # Rescaling!\n  x.points = Nuuk_year$Year\n)\nrange(f_hat - f_hat_ksmooth$y)## [1] -4.535467e-05  4.598776e-05\np_Nuuk + \n  geom_line(aes(y = run_mean(Nuuk_year$Temperature, 9)), color = \"red\") +\n  geom_line(aes(y = f_hat), color = \"blue\")"},{"path":"bivariate.html","id":"onb","chapter":"3 Bivariate smoothing","heading":"3.2 Orthogonal basis expansions","text":"","code":""},{"path":"bivariate.html","id":"polynomial-expansions","chapter":"3 Bivariate smoothing","heading":"3.2.1 Polynomial expansions","text":"Degree 19 polynomial fitted temperature data.can extract model matrix lm-object.\nFigure 3.10: model matrix columns functions\nmodel matrix (almost) orthogonal, estimation becomes quite simple.\northogonal model matrix normal equation reduces estimate\n\\[\\hat{\\beta} = \\Phi^T Y\\]\nsince \\(\\Phi^T \\Phi = \\). predicted (fitted) values \\(\\Phi \\Phi^T Y\\)\nsmoother matrix \\(\\mathbf{S} = \\Phi \\Phi^T\\) projection.homogeneous variance\n\\[\\hat{\\beta}_i \\overset{\\text{approx}}{\\sim} \\mathcal{N}(\\beta_i, \\sigma^2),\\]\n\\(\\beta_i = 0\\) \\(P(|\\hat{\\beta}_i| \\geq 1.96\\sigma) \\simeq 0.05.\\)Thresholding:\nFigure 3.11: Polynomial fit using 19 basis functions (blue) using degree 5 polynomial (red).\n","code":"\nintercept <- rep(1/sqrt(n), n)  # To make intercept column have norm one\npolylm <- lm(Temperature ~ intercept + poly(Year, 19) - 1, data = Nuuk_year)\nPhi <- model.matrix(polylm)\n(t(Phi) %*% Nuuk_year$Temperature)[1:10, 1]##       intercept poly(Year, 19)1 poly(Year, 19)2 poly(Year, 19)3 poly(Year, 19)4 \n##     -17.2469646       4.9002430      -1.7968913       0.8175400       5.9668689 \n## poly(Year, 19)5 poly(Year, 19)6 poly(Year, 19)7 poly(Year, 19)8 poly(Year, 19)9 \n##       1.4265091      -1.9258864      -0.2523581      -2.1355117      -0.8046267\ncoef(polylm)[1:10]##       intercept poly(Year, 19)1 poly(Year, 19)2 poly(Year, 19)3 poly(Year, 19)4 \n##     -17.2469646       4.9002430      -1.7968913       0.8175400       5.9668689 \n## poly(Year, 19)5 poly(Year, 19)6 poly(Year, 19)7 poly(Year, 19)8 poly(Year, 19)9 \n##       1.4265091      -1.9258864      -0.2523581      -2.1355117      -0.8046267"},{"path":"bivariate.html","id":"fourier-expansions","chapter":"3 Bivariate smoothing","heading":"3.2.2 Fourier expansions","text":"Introducing\n\\[x_{k,m} = \\frac{1}{\\sqrt{n}} e^{2 \\pi k m / n},\\]\n\\[\\sum_{k=0}^{n-1} |x_{k,m}|^2 = 1\\]\\(m_1 \\neq m_2\\)\n\\[\\sum_{k=0}^{n-1} x_{k,m_1}\\overline{x_{k,m_2}} = 0\\]Thus \\(\\Phi = (x_{k,m})_{k,m}\\) \\(n \\times n\\) unitary matrix;\n\\[\\Phi^*\\Phi = \\]\n\\(\\Phi^*\\) conjugate transposed \\(\\Phi\\).\\(\\hat{\\beta} = \\Phi^* y\\) discrete Fourier transform \\(y\\).\nbasis coefficients orthonormal basis given \\(\\Phi\\);\n\\[y_k = \\frac{1}{\\sqrt{n}} \\sum_{m=0}^{n-1} \\hat{\\beta}_m  e^{2 \\pi k m / n}\\]\\(y = \\Phi \\hat{\\beta}.\\)matrix \\(\\Phi\\) generates interesting pattern.Columns matrix \\(\\Phi\\):can estimate matrix multiplicationFor real \\(y\\) holds \\(\\hat{\\beta}_0\\) real, symmetry\n\\[\\hat{\\beta}_{n-m} = \\hat{\\beta}_m^*\\]\nholds \\(m = 1, \\ldots, n - 1\\). (\\(n\\) even, \\(\\hat{\\beta}_{n/2}\\) real ).Modulus distribution:Note \\(m \\neq 0, n/2\\), \\(\\beta_m = 0\\) \\(y \\sim \\mathcal{N}(\\Phi\\beta, \\sigma^2 I_n)\\) \\[(\\mathrm{Re}(\\hat{\\beta}_m), \\mathrm{Im}(\\hat{\\beta}_m))^T \\sim \\mathcal{N}\\left(0, \\frac{\\sigma^2}{2} I_2\\right),\\]hence\n\\[|\\hat{\\beta}_m|^2 = \\mathrm{Re}(\\hat{\\beta}_m)^2 + \\mathrm{Im}(\\hat{\\beta}_m)^2 \\sim \\frac{\\sigma^2}{2} \\chi^2_2,\\]\n, \\(P(|\\hat{\\beta}_m| \\geq 1.73 \\sigma) = 0.05.\\) clear case\nmultiple testing use threshold face value, expect\naround \\(0.05 \\times n/2\\) false positive signal . Lowering\nprobability using Bonferroni correction yields threshold around \\(2.7 \\sigma\\)\ninstead.Thresholding Fourier:coefficients independent (remember symmetry), one can alternatively\nconsider\n\\[\\hat{\\gamma}_m = \\sqrt{2} \\mathrm{Re}(\\hat{\\beta}_m) \\quad \\text{} \\quad \n\\hat{\\gamma}_{n' + m} = - \\sqrt{2} \\mathrm{Im}(\\hat{\\beta}_m)\\]\n\\(1 \\leq m < n / 2\\). \\(n' = \\lfloor n / 2 \\rfloor\\). , \\(\\hat{\\gamma}_0 = \\hat{\\beta}_0\\), \\(\\hat{\\gamma}_{n/2} = \\hat{\\beta}_{n/2}\\) \\(n\\) even.coefficients coefficients real cosine,\n\\(\\sqrt{2} \\cos(2\\pi k m / n)\\), sine, \\(\\sqrt{2} \\sin(2\\pi k m / n)\\), basis\nexpansion, ..d.\n\\(\\mathcal{N}(0, \\sigma^2)\\) distributed.Thresholding Fourier:\nFigure 3.12: Fourier based smoother thresholding (blue) polynomial fit degree 5 (red).\npoint using discrete Fourier transform?\npoint discrete Fourier transform can computed via \nfast Fourier transform (FFT), \\(O(n\\log(n))\\) time complexity.\nFFT works optimally \\(n = 2^p\\).","code":"\nPhi <- outer(\n  0:(n - 1), \n  0:(n - 1), \n  function(k, m) exp(2 * pi * 1i * (k * m) / n) / sqrt(n)\n)\nbetahat <- Conj(t(Phi)) %*% Nuuk_year$Temperature # t(Phi) = Phi for Fourier bases\nbetahat[c(1, 2:4, 73, n:(n - 2))]## [1] -17.2469646+0.0000000i  -2.4642887+2.3871189i   3.5481329+0.9099226i\n## [4]   1.6721444+0.7413580i   0.0321232+0.7089991i  -2.4642887-2.3871189i\n## [7]   3.5481329-0.9099226i   1.6721444-0.7413580i\nfft(Nuuk_year$Temperature)[1:4] / sqrt(n)## [1] -17.246965+0.000000i  -2.464289+2.387119i   3.548133+0.909923i\n## [4]   1.672144+0.741358i\nbetahat[1:4]## [1] -17.246965+0.000000i  -2.464289+2.387119i   3.548133+0.909923i\n## [4]   1.672144+0.741358i"},{"path":"bivariate.html","id":"splines","chapter":"3 Bivariate smoothing","heading":"3.3 Splines","text":"previous section orthogonality basis functions played \nimportant role computing basis function expansions efficiently well\nstatistical assessment estimated coefficients. section\ndeal bivariate smoothing via basis functions \nnecessarily orthogonal.Though material section apply choice \nbasis, restrict attention splines consider almost\nexclusively widely used B-splines (“B” basis).","code":""},{"path":"bivariate.html","id":"smoothing-splines","chapter":"3 Bivariate smoothing","heading":"3.3.1 Smoothing splines","text":"motivate splines briefly consider following penalized least squares\ncriterion finding smooth approximation bivariate data: minimize\n\\[\\begin{equation}\nL(f) = \\sum_{=1}^n (y_i - f(x_i))^2 + \\lambda \\|f''\\|_2^2\n\\tag{3.1}\n\\end{equation}\\]\ntwice differentiable functions \\(f\\). first term standard\nsquared error, can easily find smooth function interpolating \n\\(y\\)-values (\\(x\\)-values different), thus drive\nsquared error 0. squared 2-norm regularizes minimization\nproblem minimizer finds balance interpolation \nsmall second derivative (note \\(\\|f''\\|_2 = 0\\) \\(f\\) \naffine function). tuning parameter \\(\\lambda\\) controls balance.possible show minimizer (3.1) \nnatural cubic spline \nknots data points \\(x_i\\). , spline \\(C^2\\)-function\nequals third degree polynomial knots. knots,\ntwo polynomials meet fit together second derivative, \nmay differ third derivative. solution natural means\nzero second third derivative beyond two boundary knots.particularly difficult show space natural cubic\nsplines vector space dimension \\(n\\) \\(x\\)-values different.\ntherefore possible find basis splines, \\(\\varphi_1, \\ldots, \\varphi_n\\),\n\\(f\\) minimizes (3.1) form\n\\[f = \\sum_{=1}^n \\beta_i \\varphi_i.\\]\nremarkable basis (finite dimensional\nvector space spans) doesn’t depend upon \\(y\\)-values. Though \noptimization infinite dimensional space, penalization ensures\nminimizer always finite dimensional space nomatter\n\\(y_1, \\ldots, y_n\\) . Moreover, since (3.1) \nquite natural criterion minimize find smooth function fitting\nbivariate data, splines appear good candidates\nproducing smooth fits. top , splines several\ncomputational advantages widely used.let \\(\\hat{f}_i = \\hat{f}(x_i)\\) \\(\\hat{f}\\) minimizer \n(3.1), vector notation \n\\[\\hat{\\mathbf{f}} = \\boldsymbol{\\Phi}\\hat{\\beta}\\]\n\\(\\boldsymbol{\\Phi}_{ij} = \\varphi_j(x_i)\\). minimizer\ncan found observing \\[\\begin{align}\n L(\\mathbf{f}) & = (\\mathbf{y} - \\mathbf{f})^T (\\mathbf{y} - \\mathbf{f}) + \\lambda \\| f'' \\|_2^2 \\\\\n& = ( \\mathbf{y} -  \\boldsymbol{\\Phi}\\beta)^T (\\mathbf{y} -  \\boldsymbol{\\Phi}\\beta) + \\lambda \\beta^T \\mathbf{\\Omega} \\beta\n\\end{align}\\]\n\\[\\mathbf{\\Omega}_{ij} = \\langle \\varphi_i'', \\varphi_j'' \\rangle = \n\\int  \\varphi_i''(z) \\varphi_j''(z) \\mathrm{d}z.\\]\nmatrix \\(\\mathbf{\\Omega}\\) positive semidefinite construction, \nrefer penalty matrix. induces seminorm \\(\\mathbb{R}^n\\)\ncan express seminorm, \\(\\|f''\\|_2\\), \\(f\\) terms \nparameters basis expansion using \\(\\varphi_i\\).standard penalized least squares problem, whose solution \n\\[\\hat{\\beta} = (\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} + \\lambda \\mathbf{\\Omega})^{-1}\\boldsymbol{\\Phi}^T  \\mathbf{y}\\]\nresulting smoother\n\\[\\hat{\\mathbf{f}} = \\underbrace{\\boldsymbol{\\Phi} ((\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} + \\lambda \\mathbf{\\Omega})^{-1}\\boldsymbol{\\Phi}^T}_{\\mathbf{S}_{\\lambda}} \\mathbf{y}.\\]\nlinear smoother smoothing matrix \\(\\mathbf{S}_{\\lambda}\\) based\nnatural cubic splines gives known \nsmoothing spline \nminimizes (3.1). pursue\nspline based smoothing minimizing (3.1) \nusing various B-spline bases may less \\(n\\) elements.\nlinear algebra, actually doesn’t matter use spline\nbasis basis – long \n\\(\\boldsymbol{\\Phi}_{ij} = \\varphi_j(x_i)\\) \\(\\mathbf{\\Omega}\\) \ngiven terms \\(\\varphi''_i\\) .","code":""},{"path":"bivariate.html","id":"splines-in-r","chapter":"3 Bivariate smoothing","heading":"3.3.2 Splines in R","text":"splines package R implements basic functions needed work\nsplines. particular, splineDesign() function computes evaluations\nB-splines derivatives.\nFigure 3.13: B-spline basis computed splineDesign().\nbasis shown Figure 3.13 example cubic B-spline\nbasis 11 inner knots \\(0, 0.1, \\ldots, 0.9, 1\\). repeated\nboundary knots control spline basis behaves close boundaries\ninterval. basis 13 basis functions, 11, spans \nlarger space space natural cubic splines. \npossible compute basis based B-splines natural cubic splines\nusing function ns, practical purposes \nimportant, work exclusively B-spline basis .computation penalty matrix \\(\\mathbf{\\Omega}\\) constitutes \npractical problem, observing \\(\\varphi''_i\\) affine\nfunction knots leads simple way \ncomputing \\(\\mathbf{\\Omega}_{ij}\\). Letting \\(g_{ij} = \\varphi''_i \\varphi''_j\\)\nholds \\(g_{ij}\\) quadratic two consecutive knots \\(\\) \\(b\\),\ncase\n\\[\\int_a^b g_{ij}(z) \\mathrm{d}z = \\frac{b - }{6}\\left(g_{ij}() + 4 g_{ij}\\left(\\frac{+ b}{2}\\right) + g_{ij}(b)\\right).\\]\nidentity behind Simpson’s rule\nnumerical integration, fact identity quadratic\npolynomials, approximation, means Simpson’s rule\napplied appropriately leads exact computation \n\\(\\mathbf{\\Omega}_{ij}\\). need ability evaluate\n\\(\\varphi''_i\\) certain points, splineDesign() can used .laborious write good tests pen_mat(). work \nset example matrices means, e.g. hand. Alternatively, can\ncompare simpler numerical integration technique using Riemann sums.also test example non-equidistant knots.examples indicate pen_mat() computes \\(\\mathbf{\\Omega}\\) correctly,\nparticular increasing Riemann sum precision \nlowering number \\(10^{-5}\\) decrease relative error (shown).\ncourse, correctness ultimately depends splineDesign() computing\ncorrect second derivatives, hasn’t tested .can also test implementation smoothing splines works\ndata. implementing matrix-algebra directly\ncomputing \\(\\mathbf{S}_{\\lambda} \\mathbf{y}\\).Smoothing splines can computed using R function smooth.spline()\nstats package. possible manually specify amount \nsmoothing using one arguments lambda, spar df\n(latter trace smoother matrix). However,\ndue internal differences splineDesign() basis\n, lambda argument smooth.spline() match \n\\(\\lambda\\) parameter .amount smoothing manually set, smooth.spline() chooses\n\\(\\lambda\\) generalized cross validation (GCV), minimizes\n\\[\\mathrm{GCV} = \\sum_{=1}^n \\left(\\frac{y_i - \\hat{f}_i}{1 - \\mathrm{df} / n}\\right)^2,\\]\n\n\\[\\mathrm{df} = \\mathrm{trace}(\\mathbf{S}) = \\sum_{=1}^n S_{ii}.\\]\nGCV corresponds LOOCV diagonal entries, \\(S_{ii}\\),\nreplaced average \\(\\mathrm{df} / n\\). main reason \nusing GCV LOOCV smoothers, \nspline smoother, possible compute trace \\(\\mathrm{df}\\) easily\nwithout computing \\(\\mathbf{S}\\) even diagonal elements.compare results smooth.spline() optimize GCV criterion.\nFirst implement function computes GCV fixed value \\(\\lambda\\).\nimplementation relying computing smoother matrix, \nefficient implementation. Section 3.3.3\nprovides diagonalization smoother matrix jointly tuning\nparameters. representation allows efficient computation\nsplines, become clear necessary compute\n\\(\\mathbf{S}\\) even diagonal elements. trace nevertheless\neasily computable \\(\\mathbf{S}\\).apply function grid \\(\\lambda\\)-values \nchoose value \\(\\lambda\\) minimizes GCV.\nFigure 3.14: generalized cross-validation criterion smoothing splines function tuning parameter \\(\\lambda\\).\nFinally, can visualize resulting smoothing spline.\nFigure 3.15: smoothing spline minimizes GCV tuning parameter \\(\\lambda\\)\nsmoothing spline found minimizing GCV can compared \nsmoothing spline smooth.spline() computes minimizing GCV\nwell.\nFigure 3.16: smoothing spline minimizes GCV computed smooth.spline().\ndifferences smoothing spline computed implementation\nsmooth.spline() hardly detectable visually, \norder \\(10^{-3}\\) computed . possible \ndecrease differences finding optimal value \\(\\lambda\\) \nhigher precision, pursue .","code":"\nlibrary(splines)\n# Note the specification of repeated boundary knots\nknots <- c(0, 0, 0, seq(0, 1, 0.2), 1, 1, 1)\nxx <- seq(0, 1, 0.005)\nB_splines <- splineDesign(knots, xx)\nmatplot(xx, B_splines, type = \"l\", lty = 1)\npen_mat <- function(inner_knots) {\n  knots <- sort(c(rep(range(inner_knots), 3), inner_knots))\n  d <- diff(inner_knots)  # The vector of knot differences; b - a \n  g_ab <- splineDesign(knots, inner_knots, derivs = 2) \n  knots_mid <- inner_knots[-length(inner_knots)] + d / 2\n  g_ab_mid <- splineDesign(knots, knots_mid, derivs = 2)\n  g_a <- g_ab[-nrow(g_ab), ]\n  g_b <- g_ab[-1, ]\n  (crossprod(d * g_a,  g_a) + \n      4 * crossprod(d * g_ab_mid, g_ab_mid) + \n      crossprod(d * g_b, g_b)) / 6 \n}\nspline_deriv <- splineDesign(\n  c(0, 0, 0, 0, 0.5, 1, 1, 1, 1), \n  seq(0, 1, 1e-5), \n  derivs = 2\n)\nOmega_numeric <- crossprod(spline_deriv[-1, ]) * 1e-5  # Right Riemann sums\nOmega <- pen_mat(c(0, 0.5, 1))\nOmega_numeric / Omega##           [,1]      [,2]     [,3]     [,4]     [,5]\n## [1,] 0.9999700 0.9999673 0.999940 1.000000      NaN\n## [2,] 0.9999673 0.9999663 0.999955 1.000000 1.000000\n## [3,] 0.9999400 0.9999550 1.000000 1.000045 1.000060\n## [4,] 1.0000000 1.0000000 1.000045 1.000034 1.000033\n## [5,]       NaN 1.0000000 1.000060 1.000033 1.000030\nrange((Omega_numeric - Omega) / (Omega + 0.001))  #  Relative error## [1] -5.99967e-05  5.99983e-05\nspline_deriv <- splineDesign(\n  c(0, 0, 0, 0, 0.2, 0.3, 0.5, 0.6, 0.65, 0.7, 1, 1, 1, 1), \n  seq(0, 1, 1e-5), \n  derivs = 2\n)\nOmega_numeric <- crossprod(spline_deriv[-1, ]) * 1e-5  # Right Riemann sums\nOmega <- pen_mat(c(0, 0.2, 0.3, 0.5, 0.6, 0.65, 0.7, 1))\nrange((Omega_numeric - Omega) / (Omega + 0.001)) # Relative error## [1] -0.0001607084  0.0002545494\ninner_knots <- Nuuk_year$Year\nPhi <- splineDesign(c(rep(range(inner_knots), 3), inner_knots), inner_knots)\nOmega <- pen_mat(inner_knots)\nsmoother <- function(lambda) \n  Phi %*% solve(\n    crossprod(Phi) + lambda * Omega, \n    t(Phi) %*% Nuuk_year$Temperature\n  )\np_Nuuk + \n  geom_line(aes(y = smoother(10)), color = \"blue\") +      # Undersmooth\n  geom_line(aes(y = smoother(1000)), color = \"red\") +     # Smooth\n  geom_line(aes(y = smoother(100000)), color = \"purple\")  # Oversmooth\ngcv <- function(lambda, y) {\n  S <- Phi %*% solve(crossprod(Phi) + lambda * Omega, t(Phi))\n  df <- sum(diag(S))  # The trace of the smoother matrix\n  sum(((y - S %*% y) / (1 - df / length(y)))^2, na.rm = TRUE) \n}\nlambda <- seq(50, 250, 2)\nGCV <- sapply(lambda, gcv, y = Nuuk_year$Temperature)\nlambda_opt <- lambda[which.min(GCV)]\nqplot(lambda, GCV) + geom_vline(xintercept = lambda_opt, color = \"red\")\nsmooth_opt <- Phi %*% solve(\n  crossprod(Phi) + lambda_opt * Omega, \n  t(Phi) %*% Nuuk_year$Temperature\n)\np_Nuuk + geom_line(aes(y = smooth_opt), color = \"blue\")\nsmooth_splines <- smooth.spline(\n  Nuuk_year$Year, \n  Nuuk_year$Temperature, \n  all.knots = TRUE        # Don't use heuristic\n)  \nrange(smooth_splines$y - smooth_opt)## [1] -0.000775662  0.001072587\np_Nuuk + geom_line(aes(y = smooth_splines$y), color = \"blue\")"},{"path":"bivariate.html","id":"efficient-splines","chapter":"3 Bivariate smoothing","heading":"3.3.3 Efficient computation with splines","text":"Using full B-spline basis knots every observation computationally\nheavy practical viewpoint unnecessary. Smoothing using B-splines\ntherefore often done using knot-selection heuristic selects much fewer\nknots \\(n\\), particular \\(n\\) large. also smooth.spline()\nunless .knots = TRUE. heuristic selecting\nnumber knots bit complicated, implemented function .nknots.smspl(),\ncan inspected details. number knots gets 200 \ngrows extremely slowly \\(n\\). number knots selected,\ncommon heuristic selecting position use quantiles \ndistribution \\(x\\)-values. , 9 knots, say, knots\npositioned deciles (0.1-quantile, 0.2-quantile etc.). \neffectively also smooth.spline() , heuristic places \nknots data points.implemented knot-selection heuristic results \\(p\\) B-spline\nbasis functions, matrix \\(\\Phi\\) \\(n \\times p\\), typically \\(p < n\\)\n\\(\\Phi\\) full rank \\(p\\). case derive way computing \nsmoothing spline computationally efficient numerically \nstable relying matrix-algebraic solution . particularly\nneed compute smoother many different \\(\\lambda\\)-s optimize\nsmoother. show, \neffectively computing simultaneous diagonalization (symmetric) smoother\nmatrix \\(\\mathbf{S}_{\\lambda}\\) values \\(\\lambda\\).matrix \\(\\Phi\\) singular value decomposition\n\\[\\Phi = \\mathbf{U} D \\mathbf{V}^T\\]\n\\(D\\) diagonal entries \\(d_1 \\geq d_2 \\geq \\ldots \\geq d_p > 0\\),\n\\(\\mathbf{U}\\) \\(n \\times p\\), \\(\\mathbf{V}\\) \\(p \\times p\\) orthogonal matrices.\nmeans \n\\[\\mathbf{U}^T \\mathbf{U} = \\mathbf{V}^T \\mathbf{V} = \\mathbf{V} \\mathbf{V}^T =  \\]\n\\(p \\times p\\) dimensional identity matrix. find \\[\\begin{align}\n\\mathbf{S}_{\\lambda} & =  \\mathbf{U}D\\mathbf{V}^T(\\mathbf{V}D^2\\mathbf{V}^T + \\lambda \\mathbf{\\Omega})^{-1}\n\\mathbf{V}D\\mathbf{U}^T \\\\\n& = \\mathbf{U}D (D^2 + \\lambda  \\mathbf{V}^T \\mathbf{\\Omega} \\mathbf{V})^{-1} D \\mathbf{U}^T \\\\\n& = \\mathbf{U} (+ \\lambda D^{-1} \\mathbf{V}^T \\mathbf{\\Omega} \\mathbf{V} D^{-1})^{-1} \\mathbf{U}^T \\\\\n& = \\mathbf{U}(+ \\lambda  \\widetilde{\\mathbf{\\Omega}})^{-1} \\mathbf{U}^T,\n\\end{align}\\]\n\\(\\widetilde{\\mathbf{\\Omega}} = D^{-1} \\mathbf{V}^T \\mathbf{\\Omega} \\mathbf{V} D^{-1}\\) \npositive semidefinite \\(p \\times p\\) matrix. diagonalization,\n\\[\\widetilde{\\mathbf{\\Omega}} = \\mathbf{W} \\Gamma \\mathbf{W}^T,\\]\n\\(\\mathbf{W}\\) orthogonal \\(\\Gamma\\) diagonal matrix nonnegative\nvalues diagonal, find \\[\\begin{align}\n\\mathbf{S}_{\\lambda} & = \\mathbf{U} \\mathbf{W} (+ \\lambda  \\Gamma)^{-1}  \\mathbf{W}^T  \\mathbf{U}^T \\\\\n& = \\widetilde{\\mathbf{U}}  (+ \\lambda  \\Gamma)^{-1} \\widetilde{\\mathbf{U}}^T\n\\end{align}\\]\\(\\widetilde{\\mathbf{U}} = \\mathbf{U} \\mathbf{W}\\) orthogonal\n\\(n \\times p\\) matrix.interpretation representation follows.First, coefficients, \\(\\hat{\\beta} = \\widetilde{\\mathbf{U}}^Ty\\), computed \nexpanding \\(y\\) basis given columns \\(\\widetilde{\\mathbf{U}}\\).Second, \\(\\)-th coefficient shrunk towards 0,\n\\[\\hat{\\beta}_i(\\lambda) = \\frac{\\hat{\\beta}_i}{1 + \\lambda \\gamma_i}.\\]Third, smoothed values, \\(\\widetilde{\\mathbf{U}} \\hat{\\beta}(\\lambda)\\),\ncomputed expansion using shrunken coefficients.Thus smoother works shrinking coefficients toward zero orthonormal basis\ngiven columns \\(\\widetilde{\\mathbf{U}}\\). coefficients corresponding \nlargest eigenvalues \\(\\gamma_i\\) shrunk relatively toward zero\ncorresponding small eigenvalues. basis formed \ncolumns \\(\\widetilde{\\mathbf{U}}\\) known Demmler-Reinsch basis\nreference Demmler Reinsch (1975).implement computation diagonalization Nuuk temperature\ndata using \\(p = 20\\) basis functions (18 inner knots) equidistantly distributed\nrange years data.\nFigure 3.17: eigenvalues \\(\\gamma_i\\) determine much different basis coefficients orthonormal spline expansion shrunk toward zero. Left plot shows eigenvalues, right plot shows eigenvalues log-scale.\n\nFigure 3.18: columns \\(\\widetilde{\\mathbf{U}}\\) consitute orthonormal basis known Demmler-Reinsch basis computing spline based smoother.\nobserve Figures 3.17 3.18\ntwo relatively large eigenvalues corresponding \ntwo basis functions erratic behavior close boundaries, \ntwo eigenvalues effectively zero corresponding two affine\nbasis functions. addition, oscillating basis function ,\nlarger corresponding eigenvalue, corresponding\ncoefficient shrunk toward zero spline smoother.Observe also \n\\[\\mathrm{df}(\\lambda) = \\mathrm{trace}(\\mathbf{S}_\\lambda) \n= \\sum_{=1}^p \\frac{1}{1 + \\lambda \\gamma_i},\\]\nmakes possible implement GCV without even computing diagonal entries\n\\(\\mathbf{S}_{\\lambda}.\\)","code":"\ninner_knots <- seq(1867, 2013, length.out = 18)\nPhi <- splineDesign(c(rep(range(inner_knots), 3), inner_knots), Nuuk_year$Year)\nOmega <- pen_mat(inner_knots)\nPhi_svd <- svd(Phi)\nOmega_tilde <- t(crossprod(Phi_svd$v, Omega %*% Phi_svd$v)) / Phi_svd$d\nOmega_tilde <- t(Omega_tilde) / Phi_svd$d\n# It is safer to use the numerical singular value decomposition ('svd()')\n# for diagonalizing a positive semidefinite matrix than to use a \n# more general numerical diagonalization implementation such as 'eigen()'. \nOmega_tilde_svd <- svd(Omega_tilde)  \nU_tilde <- Phi_svd$u %*% Omega_tilde_svd$u"},{"path":"bivariate.html","id":"the-kalman-smoother","chapter":"3 Bivariate smoothing","heading":"3.4 The Kalman smoother","text":"Kalman smoother classical algorithm\nused smoothing time-indexed observations. can seen example\nGaussian prediction algorithms, imagine \\((X_1, Y_1), \\ldots, (X_n,Y_n)\\) follow joint Gaussian distribution,\nobserve \\(Y_1, \\ldots, Y_n\\) want predict\n\\(X_1, \\ldots, X_n\\). \\(X\\)-s form relatively smooth\nunobserved Gaussian process \\(Y\\)-s noisy measurements\n\\(X\\)-s, see predictions \\(X\\)-s \n\\(Y\\)-s can seen smoother. contrast bivariate\nsmoothers developed sections , approach taken\nsection fully probabilistic. , algorithms \nparameters justified entirely determined \nprobabilistic model data. said, see \nclear similarities smoothers.","code":""},{"path":"bivariate.html","id":"gaussian-processes","chapter":"3 Bivariate smoothing","heading":"3.4.1 Gaussian processes","text":"suppose time grid \\(t_1 < \\ldots < t_n\\) observe\n\\(Y_1, \\ldots, Y_n\\) \\(Y_i\\) observed time \\(t_i\\). \ndistribution \\(Y\\)-s given terms \\(X = (X_1, \\ldots, X_n)\\),\nsuppose \\(X \\sim \\mathcal{N}(\\xi_x, \\Sigma_{x})\\) \n\\[(\\Sigma_{x})_{,j} = \\mathrm{cov}(X_i, X_j) = K(t_i - t_j)\\]\nkernel function \\(K : \\mathbb{R} \\\\mathbb{R}\\).observation equation \\(Y_i = X_i + \\delta_i\\)\n\\(\\delta = (\\delta_1, \\ldots, \\delta_n) \\sim \\mathcal{N}(0, \\Omega)\\) \\(\\delta \\perp \\! \\! \\perp X\\) get\\[(X, Y) \\sim \\mathcal{N}\\left(\\left(\\begin{array}{c} \\xi_x \\\\ \\xi_x \\end{array}\\right),\n\\left(\\begin{array}{cc} \\Sigma_x & \\Sigma_x \\\\ \\Sigma_x & \\Sigma_x + \\Omega \\end{array} \\right) \\right).\\]Hence\n\\[E(X \\mid Y) = \\xi_x + \\Sigma_x (\\Sigma_x + \\Omega)^{-1} (Y - \\xi_x).\\]Assuming \\(\\xi_x = 0\\) conditional expectation linear smoother \nsmoother matrix\n\\[S = \\Sigma_x (\\Sigma_x + \\Omega)^{-1}.\\]also true \\(\\Sigma_x (\\Sigma_x + \\Omega)^{-1} \\xi_x = \\xi_x\\). \nidentity holds approximately, can argue computing \\(E(X \\mid Y)\\) \ndon’t need know \\(\\xi_x\\).observation variance \\(\\Omega = \\sigma^2 \\) smoother matrix\nsimplifies \n\\[S = \\Sigma_x (\\Sigma_x + \\sigma^2 )^{-1} = (+ \\sigma^2 \\Sigma_x^{-1})^{-1}.\\]\nThus knowing observation variance \\(\\sigma^2\\) covariance matrix\n\\(\\Sigma_x\\), determined entirely time-grid covariance\nkernel \\(K\\), can compute smoother matrix \\(S\\). Observing \\(Y = y\\), can\ncompute smoother solution equation\n\\[(+ \\sigma^2 \\Sigma_x^{-1})x = y.\\]\nStandard algorithms matrix inversion,\nneeded compute \\(S\\), time complexity \\(O(n^3)\\), standard algorithms solving\nlinear equation time complexity \\(O(n^2)\\).main point Kalman smoother \ntime complexity \\(O(n)\\), computation thus fast \nscales well \\(n\\). tradeoff Kalman smoother puts \nrestrictions \\(K\\) thus \\(\\Sigma_x\\). consider AR(1)-example\nsection , show \\(+ \\sigma^2 \\Sigma_x^{-1}\\)\ntridiagonal matrix, smoother can computed solution\n?? time linear \\(n\\).","code":""},{"path":"bivariate.html","id":"ar1-kalman-smoother","chapter":"3 Bivariate smoothing","heading":"3.4.2 AR(1) Kalman smoother","text":"simplest example efficient Kalman smoother AR(1)-model,\nassume equidistant time grid (, \\(t_i = \\)). Suppose \\(|\\alpha| < 1\\),\n\\(X_1 = \\epsilon_1 / \\sqrt{1 - \\alpha^2}\\) \n\\[X_i = \\alpha X_{-1} + \\epsilon_i\\]\n\\(= 2, \\ldots, n\\) \\(\\epsilon = (\\epsilon_1, \\ldots, \\epsilon_n) \\sim \\mathcal{N}(0, \\sigma^2 )\\).\\(\\mathrm{cov}(X_i, X_j) = \\alpha^{|-j|} / (1 - \\alpha^2)\\), thus can find\n\\(\\Sigma_x\\) compute\n\\[E(X_i \\mid Y) = ((+ \\sigma^2 \\Sigma_x^{-1})^{-1} Y)_i\\]\nFigure 3.19: Gaussian smoother matrix \\(S_{25,j}\\) \\(\\alpha = 0.3, 0.9\\), \\(\\sigma^2 = 2, 20\\)\nidentity \\(\\epsilon_i = X_i - \\alpha X_{-1}\\) follows \n\\(\\epsilon = X\\) \\[= \\left( \\begin{array}{cccccc}\n\\sqrt{1 - \\alpha^2} & 0 & 0 & \\ldots & 0 & 0 \\\\\n-\\alpha & 1 & 0 & \\ldots & 0 & 0 \\\\\n0 & -\\alpha & 1 & \\ldots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & 1 & 0 \\\\\n0 & 0 & 0 & \\ldots & -\\alpha & 1 \\\\\n\\end{array}\\right),\\]gives\n\\(= V(\\epsilon) = \\Sigma_x ^T\\), hence\n\\[\\Sigma_x^{-1} = (^{-1}(^T)^{-1})^{-1} = ^T .\\]shown \n\\[\\Sigma_x^{-1} = \\left( \\begin{array}{cccccc}\n1 & -\\alpha & 0 & \\ldots & 0 & 0 \\\\\n-\\alpha & 1 + \\alpha^2 & -\\alpha & \\ldots & 0 & 0 \\\\\n0 & -\\alpha & 1 + \\alpha^2 & \\ldots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & 1 + \\alpha^2 & -\\alpha \\\\\n0 & 0 & 0 & \\ldots & -\\alpha & 1 \\\\\n\\end{array}\\right).\\]Hence\n\\[+ \\sigma^2 \\Sigma_x^{-1} = \\left( \\begin{array}{cccccc}\n\\gamma_0 & \\rho & 0 & \\ldots & 0 & 0 \\\\\n\\rho & \\gamma & \\rho & \\ldots & 0 & 0 \\\\\n0 & \\rho & \\gamma & \\ldots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & \\gamma & \\rho \\\\\n0 & 0 & 0 & \\ldots & \\rho & \\gamma_0 \\\\\n\\end{array}\\right)\\]\\(\\gamma_0 = 1 + \\sigma^2\\), \\(\\gamma = 1 + \\sigma^2 (1 + \\alpha^2)\\) \\(\\rho = -\\sigma^2 \\alpha\\) tridiagonal matrix.equation\\[\\left( \\begin{array}{cccccc}\n\\gamma_0 & \\rho & 0 & \\ldots & 0 & 0 \\\\\n\\rho & \\gamma & \\rho & \\ldots & 0 & 0 \\\\\n0 & \\rho & \\gamma & \\ldots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & \\gamma & \\rho \\\\\n0 & 0 & 0 & \\ldots & \\rho & \\gamma_0 \\\\\n\\end{array}\\right) \n\\left( \\begin{array}{c} \nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \n\\end{array}\\right) = \\left(\\begin{array}{c}\ny_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n \n\\end{array}\\right)\\]can solved forward backward sweep.Forward sweep:Set \\(\\rho_1' = \\rho / \\gamma_0\\) \\(y_1' = y_1 / \\gamma_0\\),recursively\n\\[\\rho_i' = \\frac{\\rho}{\\gamma - \\rho \\rho_{-1}'} \\quad \\text{} \\quad y_i' = \\frac{y_i - \\rho y_{-1}'}{\\gamma - \\rho \\rho_{-1}'}\\]\n\\(= 2, \\ldots, n-1\\)finally\n\\[y_n' = \\frac{y_n - \\rho y_{n-1}'}{\\gamma_0 - \\rho \\rho_{n-1}'}.\\]forward sweep equation transformed \\[\\left( \\begin{array}{cccccc}\n1 & \\rho_1' & 0 & \\ldots & 0 & 0 \\\\\n0 & 1 & \\rho_2' & \\ldots & 0 & 0 \\\\\n0 & 0 & 1 & \\ldots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & 1 & \\rho_{n-1}' \\\\\n0 & 0 & 0 & \\ldots & 0 & 1 \\\\\n\\end{array}\\right) \n\\left( \\begin{array}{c}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n \n\\end{array}\\right) = \\left(\\begin{array}{c} \ny_1' \\\\ y_2' \\\\ y_3' \\\\ \\vdots \\\\ y_{n-1}' \\\\ y_n' \n\\end{array}\\right),\\]solved backsubstitution ; \\(x_n = y_n'\\) \n\\[x_{} = y_i' - \\rho_{}' x_{+1}, \\quad = n-1, \\ldots, 1.\\]","code":""},{"path":"bivariate.html","id":"implementation","chapter":"3 Bivariate smoothing","heading":"3.4.3 Implementation","text":"Result, \\(\\alpha = 0.95\\), \\(\\sigma^2 = 10\\)Comparing resultsNote forward sweep computes \\(x_n = E(X_n \\mid Y)\\), , backsubstitution solves smoothing problem computing \\(E(X \\mid Y)\\).Gaussian process used (AR(1)-process) smooth \nsmoothing data. related kernel function\n\\(K(s) = \\alpha^{|s|}\\) non-differentiable 0.Many smoothers equivalent Gaussian process smoother appropriate\nchoice kernel. simple inverse covariance matrix Kalman\nfilter algorithm.","code":"#include <Rcpp.h>\nusing namespace Rcpp;\n// [[Rcpp::export]]\nNumericVector KalmanSmooth(NumericVector y, double alpha, double sigmasq) {\n  double tmp, gamma0 = 1 + sigmasq, rho = - sigmasq * alpha;\n  double gamma = 1 + sigmasq * (1 + alpha * alpha);\n  int n = y.size();\n  NumericVector x(n), rhop(n - 1);\n  rhop[0] = rho / gamma0;\n  x[0] = y[0] / gamma0;\n  for(int i = 1; i < n - 1; ++i) { /* Forward sweep */\n    tmp = (gamma - rho * rhop[i - 1]);\n    rhop[i] = rho / tmp;\n    x[i] = (y[i] - rho * x[i - 1]) / tmp;\n  }\n  x[n - 1] = (y[n - 1] - rho * x[n - 2]) / (gamma0 - rho * rhop[n - 2]);\n  for(int i = n - 2; i >= 0; --i) { /* Backsubstitution */\n    x[i] = x[i] - rhop[i] * x[i + 1];\n  }\n  return x;\n}\nSigma <- outer(1:n, 1:n, \n               function(i, j) alpha^(abs(i - j))) / (1 - alpha^2)  \nSmooth <- Sigma %*% solve(Sigma + sigmasq * diag(n))\nqplot(1:n, Smooth %*% Nuuk_year$Temperature - ySmooth) + \n  ylab(\"Difference\")"},{"path":"bivariate.html","id":"the-kalman-filter","chapter":"3 Bivariate smoothing","heading":"3.4.4 The Kalman filter","text":"Result, \\(\\alpha = 0.95\\), \\(\\sigma^2 = 10\\)","code":"#include <Rcpp.h>\nusing namespace Rcpp;\n// [[Rcpp::export]]\nNumericVector KalmanFilt(NumericVector y, double alpha, double sigmasq) {\n  double tmp, gamma0 = 1 + sigmasq, rho = - sigmasq * alpha, yp;\n  double gamma = 1 + sigmasq * (1 + alpha * alpha);\n  int n = y.size();\n  NumericVector x(n), rhop(n);\n  rhop[0] = rho / gamma0;\n  yp = y[0] / gamma0;\n  x[0] = y[0] / (1 + sigmasq * (1 - alpha * alpha));\n  for(int i = 1; i < n; ++i) { \n    tmp = (gamma - rho * rhop[i - 1]);\n    rhop[i] = rho / tmp;\n    /* Note differences when compared to smoother */\n    x[i] = (y[i] - rho * yp) / (gamma0 - rho * rhop[i - 1]); \n    yp = (y[i] - rho * yp) / tmp;         \n  }\n  return x;\n}"},{"path":"bivariate.html","id":"bivariate:ex","chapter":"3 Bivariate smoothing","heading":"3.5 Exercises","text":"","code":""},{"path":"bivariate.html","id":"nearest-neighbors","chapter":"3 Bivariate smoothing","heading":"Nearest neighbors","text":"","code":""},{"path":"bivariate.html","id":"kernel-estimators","chapter":"3 Bivariate smoothing","heading":"Kernel estimators","text":"Exercise 3.1  Consider bivariate data set \\((x_1, y_1), \\ldots, (x_n, y_n)\\) let \\(K\\) \nprobability density mean 0. \n\\[\\hat{f}(x, y) = \\frac{1}{n h^2} \\sum_{=1}^n K\\left(\\frac{x - x_i}{h}\\right) K\\left(\\frac{y - y_i}{h}\\right)\\]\nbivariate kernel density estimator joint density \\(x\\) \\(y\\). Show\nkernel density estimator\n\\[\\hat{f}_1(x) = \\frac{1}{n h} \\sum_{=1}^n K\\left(\\frac{x - x_i}{h}\\right)\\]\nalso marginal distribution \\(x\\) \\(\\hat{f}\\), \nNadaraya-Watson kernel smoother conditional expectation \\(y\\)\ngiven \\(x\\) \\(\\hat{f}\\).","code":""},{"path":"univariate-random-variables.html","id":"univariate-random-variables","chapter":"4 Univariate random variables","heading":"4 Univariate random variables","text":"chapter deal algorithms simulating observations \ndistribution \\(\\mathbb{R}\\) subset thereof. can several\npurposes , instance:want investigate properties distribution.want simulate independent realizations univariate random variables \ninvestigate distribution transformation.want use Monte Carlo integration compute numerically integral\n(probability).chapter focus simulation single random variable\n..d. sequence random variables primarily via various transformations\npseudorandom numbers. pseudo\nrandom numbers approximate simulations ..d. random\nvariables uniformly distributed \\((0, 1)\\).","code":""},{"path":"univariate-random-variables.html","id":"pseudorandom-number-generators","chapter":"4 Univariate random variables","heading":"4.1 Pseudorandom number generators","text":"simulation algorithms based algorithms generating\npseudorandom uniformly distributed variables \\((0, 1)\\). arise \ndeterministic integer sequences initiated seed. classical\nexample pseudorandom integer generator \nlinear congruential generator. sequence numbers \ngenerator computed iteratively \n\\[x_{n+1} = (x_n + c)  \\text{ mod m}\\]\ninteger parameters \\(\\), \\(c\\) \\(m\\). seed \\(x_1\\) \nnumber \\(0\\) \\(m - 1\\), resulting\nsequence set \\(\\{0, \\ldots, m - 1\\}\\). ANSI C standard specifies\nchoices \\(m = 2^{31}\\), \\(= 1,103,515,245\\) \\(c = 12,345\\). generator\nsimple understand implement superseded much better\ngenerators.Pseudorandom number generators generally defined terms finite state space\n\\(\\mathcal{Z}\\) one--one map \\(f : \\mathcal{Z} \\\\mathcal{Z}\\). \ngenerator produces sequence \\(\\mathcal{Z}\\) iteratively \nseed \\(\\mathbf{z}_1 \\\\mathcal{Z}\\) \n\\[\\mathbf{z}_n = f(\\mathbf{z}_{n-1}).\\]\nPseudorandom integers typically obtained \n\\[x_n = h(\\mathbf{z}_n)\\]\ntransformation \\(h : \\mathcal{Z} \\mapsto \\mathbb{Z}\\). \nimage \\(h\\) set \\(\\{0, 1, \\ldots, 2^{w} - 1\\}\\) \\(w\\)-bit integers,\npseudorandom numbers \\([0, 1)\\) typically obtained \n\\[x_n = 2^{-w} h(\\mathbf{z}_n).\\]R, default pseudorandom number generator 32-bit\nMersenne Twister, generates integers range\n\\[\\{0, 1, \\ldots, 2^{32} -1\\}.\\] state space \n\\[\\mathcal{Z} =  \\{0, 1, \\ldots, 2^{32} -1\\}^{624},\\]\n, state 624 dimensional vector 32-bit integers.\nfunction \\(f\\) form\n\\[f(\\mathbf{z}) = (z_2, z_3, \\ldots, z_{623}, f_{624}(z_1, z_2, z_{m + 1})),\\]\n\\(1 \\leq m < 624\\), \\(h\\) function \\(z_{624}\\) .\nstandard choice \\(m = 397\\) used R implementation.\nfunction \\(f_{624}\\) bit complicated, includes\nknown twist transformation, requires\nadditional parameters. period generator\nastronomical number\n\\[2^{32 \\times 624 - 31} - 1 = 2^{19937} - 1,\\]\nMersenne prime.\nMoreover, combinations consecutive integers\ndimension 623 occur equally often period, empirical tests \ngenerator demonstrate good statistical properties, though \nknown fail tests.R can set seed using function set.seed takes \ninteger argument produces element state space. argument\ngiven set.seed actual seed, set.seed computes \nvalid seed pseudorandom number generator R using,\nwhether Mersenne Twister . Thus use set.seed \nsafe recommended way setting seed.actual seed (together additional information) can accessed\nvia vector .Random.seed. first entry, .Random.seed[1], encodes \npseudorandom number generator used well generator \nGaussian variables discrete uniform variables. \ninformation decoded RNGkind().Mersenne Twister,\n.Random.seed[3:626] contains vector state space, \n.Random.seed[2] contains “current position” state vector.\nimplementation needs position variable 624 updates \nstate vector time runs values sequentially\nnext update. equivalent efficient e.g.\nimplementing position shifts explicitly definition \\(f\\) .Every time random number generated, e.g. runif , underlying\nsequence pseudorandom numbers used, state vector stored \n.Random.seed updated accordingly.Resetting seed restart pseudorandom number generator \nseed result sequence random numbers.Note using standard R generators, value \\(0\\) \\(1\\)\nreturned underlying pseudorandom uniform generator \nadjusted \\((0,1)\\). Thus uniform random variables guaranteed \n\\((0, 1)\\).random number generators implemented R use one\npseudorandom number per variable. , instance, case simulate\nGamma distributed random variables.example , first Gamma variable required two pseudorandom\nnumbers, second required three pseudorandom numbers. detailed\nexplanation given Section 4.3, shown\ngenerate random variables Gamma distribution via\nrejection sampling. requires minimum two pseudorandom numbers\nevery Gamma variable generated.","code":"\nRNGkind()## [1] \"Mersenne-Twister\" \"Inversion\"        \"Rejection\"\nset.seed(27112015)           ## Computes a new seed from an integer\noldseed <- .Random.seed[-1]  ## The actual seed\n.Random.seed[1]              ## Encoding of generators used, will stay fixed## [1] 10403\n.Random.seed[2]              ## Start position after the seed has been set is 624## [1] 624\ntmp <- runif(1)\ntmp## [1] 0.7793288\nhead(oldseed, 5)## [1]         624 -1660633125 -1167670944  1031453153   815285806\nhead(.Random.seed[-1], 5)     ## The state vector and position has been updated## [1]           1  -696993996 -1035426662  -378189083  -745352065\nc(tmp, runif(1))          ## [1] 0.7793288 0.5613179\nhead(.Random.seed[-1], 5)     ## The state vector has not changed, only the position## [1]           2  -696993996 -1035426662  -378189083  -745352065\nset.seed(27112015)\nhead(.Random.seed[-1], 5)## [1]         624 -1660633125 -1167670944  1031453153   815285806\nhead(oldseed, 5)             ## Same as current .Random.seed## [1]         624 -1660633125 -1167670944  1031453153   815285806\nrunif(1)                     ## Same as tmp## [1] 0.7793288\nset.seed(27112015)\nrgamma(1, 1)                 ## A single Gamma distributed random number## [1] 1.192619\nhead(.Random.seed[-1], 5)    ## Position changed to 2## [1]           2  -696993996 -1035426662  -378189083  -745352065\nrgamma(1, 1)                 ## A single Gamma distributed random number## [1] 0.2794622\nhead(.Random.seed[-1], 5)    ## Position changed to 5## [1]           5  -696993996 -1035426662  -378189083  -745352065"},{"path":"univariate-random-variables.html","id":"implementing-a-pseudorandom-number-generator","chapter":"4 Univariate random variables","heading":"4.1.1 Implementing a pseudorandom number generator","text":"development high quality pseudorandom number generators \nresearch field . particularly true one needs\ntheoretical guarantees randomized algorithms cryptographically\nsecure generators. scientific computations \nsimulations correct statistical properties, reproducibility speed \nimportant cryptographic security, even , trivial\ninvent good generator, field still developing. \ngenerator seriously considered, mathematical properties \nwell understood, pass () tests standardized\ntest suites TestU01,\nsee L’Ecuyer Simard (2007).R provides couple alternatives Mersenne Twister,\nsee ?RNG, compelling reason switch ordinary\nuse. mostly available historical reasons.\nOne exception L’Ecuyer-CMRG generator, useful \nindependent pseudorandom sequences needed parallel computations.Though Mersenne Twister widely used pseudorandom number generator,\nwell known shortcomings.\nhigh quality alternatives simpler faster, \nfamily shift-register generators\nvariations, currently available base R package.Shift-register generators based linear transformations bit\nrepresentation integers. Three particular transformations typically\ncomposed; \\(\\mathrm{Lshift}\\) \\(\\mathrm{Rshift}\\) operators \nbitwise \\(\\mathrm{xor}\\) operator. Let \\(z = [z_{31}, z_{30}, \\ldots, z_0]\\)\n\\(z_i \\\\{0, 1\\}\\) denote bit\nrepresentation 32-bit (unsigned) integer \\(z\\) (ordered significant\nbit least significant bit). ,\n\\[z = z_{31} 2^{31} + z_{30} 2^{30} + \\ldots + z_2 2^2 + z_1 2^{1} + z_0.\\]\nleft shift operator defined \n\\[\\mathrm{Lshift}(z) = [z_{30}, z_{29}, \\ldots, z_0, 0],\\]\nright shift operator defined \n\\[\\mathrm{Rshift}(z) = [0, z_{31}, z_{30}, \\ldots, z_1].\\]\nbitwise xor operator defined \n\\[\\mathrm{xor}(z, z') = [\\mathrm{xor}(z_{31},z_{31}') , \n\\mathrm{xor}(z_{30}, z_{30}'), \\ldots, \\mathrm{xor}(z_0, z_0')]\\]\n\\(\\mathrm{xor}(0, 0) = \\mathrm{xor}(1, 1) = 0\\) \n\\(\\mathrm{xor}(1, 0) = \\mathrm{xor}(0, 1) = 1\\). Thus transformation \nform\n\\[\\mathrm{xor}(z, \\mathrm{Rshift}^2(z)) = [\\mathrm{xor}(z_{31}, 0) , \n\\mathrm{xor}(z_{30}, 0), \\mathrm{xor}(z_{29}, z_{31}), \\ldots, \\mathrm{xor}(z_0, z_2)].\\]One example shift-register based generator Marsaglia’s xorwow\nalgorithm, Marsaglia (2003). addition shift xor operations, output \ngenerator perturbed sequence integers period \\(2^{32}\\).\nstate space generator \n\\[\\{0, 1, \\ldots, 2^{32} -1\\}^{5}\\]\n\n\\[f(\\mathbf{z}) = (z_1 + 362437 \\ (\\mathrm{mod}\\ 2^{32}),\nf_1(z_5, z_2), z_2, z_3, z_4),\\]\n\n\\[h(\\mathbf{z}) = 2^{-32} (z_1 + z_2).\\]\nnumber 362437 Marsaglia’s choice generating calls \nWeyl sequence, odd number . function \\(f_1\\) given \n\\[f_1(z, z') = \\mathrm{xor}(\\mathrm{xor}(z, \\mathrm{Rshift}^2(z)), \\mathrm{xor}(z', \\mathrm{xor}(\\mathrm{Lshift}^4(z'), \\mathrm{Lshift}(\\mathrm{xor}(z, \\mathrm{Rshift}^2(z)))))).\\]may look intimidating, operations elementary.\nTake number \\(z = 123456\\), say, intermediate value\n\\(\\overline{z} = \\mathrm{xor}(z, \\mathrm{Rshift}^2(z))\\) computed follows:\\[\n\\begin{array}{ll}\nz & \\texttt{00000000 00000001 11100010 01000000} \\\\\n\\mathrm{Rshift}^2(z) & \\texttt{00000000 00000000 01111000 10010000} \\\\\n\\hline\n\\mathrm{xor} & \\texttt{00000000 00000001 10011010 11010000}\n\\end{array}\n\\]\\(z' = 87654321\\) value \\(f_1(z, z')\\) computed like :\\[\n\\begin{array}{ll}\n\\mathrm{Lshift}^4(z') & \\texttt{01010011 10010111 11111011 00010000} \\\\\n\\mathrm{Lshift}(\\overline{z}) & \\texttt{00000000 00000011 00110101 10100000} \\\\\n\\hline\n\\mathrm{xor} & \\texttt{01010011 10010100 11001110 10110000} \\\\\nz' & \\texttt{00000101 00111001 01111111 10110001} \\\\\n\\hline\n\\mathrm{xor} & \\texttt{01010110 10101101 10110001 00000001} \\\\\n\\overline{z} & \\texttt{00000000 00000001 10011010 11010000} \\\\\n\\hline\n \\mathrm{xor} & \\texttt{01010110 10101100 00101011 11010001}\n\\end{array}\n\\]Converted back 32-bit integer, result \\(f_1(z, z') = 1454123985\\).\nshift xor operations tedious hand extremely fast \nmodern computer architectures, shift-register based generators \nfastest generators good statistical properties.make R use xorwow generator need implement user supplied\ngenerator. requires writing C code implements generator,\ncompiling code shared object file, loading \nR dyn.load function, finally calling RNGkind(\"user\")\nmake R use pseudorandom number generator. See ?Random.user\ndetails example.Using Rcpp package, sourceCpp, particular, usually much preferred\nmanual compiling loading. However, case need make\nfunctions available internals R rather exporting functions \ncallable R console. , nothing needs exported C/C++.\nnothing exported, sourceCpp actually load shared object\nfile, need trick sourceCpp anyway. implementation\nachieve simply exporting direct interface xorwow generator.first test direct interface xorwow algorithm.set R’s pseudorandom number generator user supplied\ngenerator.R’s standard random number generators call RNGkind(\"user\")\nrely user provided generator, case xorwow generator.\nNote R “initial scrambling” argument given set.seed\npassed user defined initializer. \nscrambling turns 24102019 used 3573076633 used .code shows state vector xorwow algorithm seeded\nuser_unif_init function, also shows \nupdate state vector single iteration xorwow algorithm.Though xorwow algorithm fast simple, benchmark study (shown)\nreveals using xorwow instead Mersenne Twister doesn’t impact \nrun time notable way using e.g. runif. generator simply\nbottleneck. implementation\nxorwow experimental thoroughly tested, \nrely quickly reset random number generator default value.","code":"#include <Rcpp.h>\n#include <R_ext/Random.h>\n\n/* The Random.h header file contains the function declarations for the functions \n that R rely on internally for a user defined generator, and it also defines \n the type Int32 as an unsigned int. */\n\nstatic Int32 z[5];     // The state vector \nstatic double res;\nstatic int nseed = 5;  // Length of the state vector\n\n// Implementation of xorwow from Marsaglia's \"Xorshift RNGs\" \n// modified so as to return a double in [0, 1). The '>>' and '<<' \n// operators in C are bitwise right and left shift operators, and \n// the caret, '^', is the xor operator. \n\ndouble * user_unif_rand()\n{ \n  Int32 t = z[4];\n  Int32 s = z[1];\n  z[0] += 362437;\n  z[4] = z[3];\n  z[3] = z[2];\n  z[2] = s;\n  // Right shift t by 2, then bitwise xor between t and its shift\n  t ^= t >> 2;  \n  // Left shift t by 1 and s by 4, xor them, xor with s and xor with t\n  t ^= s ^ (s << 4) ^ (t << 1);\n  z[1] = t;\n  res = (z[0] + t) * 2.32830643653869e-10;\n  return &res;\n}\n\n// A seed initializer using Marsaglia's congruential PRNG\n\nvoid  user_unif_init(Int32 seed_in) { \n  z[0] = seed_in;\n  z[1] = 69069 * z[0] + 1;\n  z[2] = 69069 * z[1] + 1;\n  z[3] = 69069 * z[2] + 1;\n  z[4] = 69069 * z[3] + 1;\n}\n\n// Two functions to make '.Random.seed' in R reflect the state vector\n\nint * user_unif_nseed() { return &nseed; }\nint * user_unif_seedloc() { return (int *) &z; }\n\n// Wrapper to make 'user_unif_rand' callable from R\ndouble xor_runif() {\n  return *user_unif_rand();\n}\n\n// This module will export two functions to be directly available from R.\n// Note: if nothing is exported, `sourceCpp` will not load the shared \n// object file generated by the compilation of the code, and \n// 'user_unif_rand' will not become available to the internals of R. \nRCPP_MODULE(xorwow) {\n  Rcpp::function(\"xor_set.seed\", &user_unif_init, \"Seeds Marsaglia's xorwow\");\n  Rcpp::function(\"xor_runif\", &xor_runif, \"A uniform from Marsaglia's xorwow\");\n}\nxor_set.seed(3573076633)\nxor_runif()## [1] 0.9090892\ndefault_prng <- RNGkind(\"user\")\nset.seed(24102019)\n.Random.seed[-1]  ## The state vector as seeded ## [1] -721890663    9136518 -310030769 1191753796  194708085\nrunif(1)          ## As above since same unscrambled seed is used## [1] 0.9090892\n.Random.seed[-1]  ## The state vector after one update## [1] -721528226  331069150    9136518 -310030769 1191753796\n## Resetting the generator to the default\nRNGkind(default_prng[1])"},{"path":"univariate-random-variables.html","id":"rng-packages","chapter":"4 Univariate random variables","heading":"4.1.2 Pseudorandom number packages","text":"benefit recent developments pseudorandom number generators\ncan turn R packages dqrng\npackage. implements pcg64 PCG family \ngenerators well Xoroshiro128+ Xoshiro256+\nshift-register algorithms. Xoroshiro128+ default \ngenerators can chosen using dqRNGkind. usage generators dqrng\nsimilar usage base R generators.Using generators dqrng interfere base R generators\nstate vectors completely separated.addition uniform pseudorandom variables generated dqrunif \ndqrng package can generate exponential (dqrexp) Gaussian (dqrnorm)\nrandom variables well uniform discrete distributions (dqsample \ndqsample.int). based fast pseudorandom integer generators \npackage includes. addition, package C++ interface makes \npossible use generators compiled code well.benchmark shows, dqrunif six times faster \nrunif generating one million variables. generators provided\ndqrng package show similar improvements base R generators.","code":"\nlibrary(dqrng)\ndqset.seed(24102019)\ndqrunif(1)## [1] 0.6172152\nmicrobenchmark(\n  runif(1e6),\n  dqrunif(1e6)\n)## Unit: milliseconds\n##            expr  min   lq mean median   uq   max neval\n##    runif(1e+06) 25.7 28.3 32.5     29 30.8 151.9   100\n##  dqrunif(1e+06)  4.3  4.9  6.6      7  7.4   9.8   100"},{"path":"univariate-random-variables.html","id":"transformation-techniques","chapter":"4 Univariate random variables","heading":"4.2 Transformation techniques","text":"\\(T : \\mathcal{Z} \\\\mathbb{R}\\) map \\(Z \\\\mathcal{Z}\\) random\nvariable can simulate, can simulate \\(X = T(Z).\\)Theorem 4.1  \\(F^{\\leftarrow} : (0,1) \\mapsto \\mathbb{R}\\)\n(generalized) inverse distribution function \\(U\\) uniformly distributed\n\\((0, 1)\\) distribution \n\\[F^{\\leftarrow}(U)\\]\ndistribution function \\(F\\).proof Theorem 4.1 can found many\ntextbooks skipped. easiest use theorem \nanalytic formula inverse distribution function. even\ncases don’t might useful simulation anyway \naccurate approximation fast evaluate.call RNGkind() previous section revealed default R generating samples \\(\\mathcal{N}(0,1)\\) inversion. ,\nTheorem 4.1 used transform\nuniform random variables inverse distribution function \\(\\Phi^{-1}\\).\nfunction , however, non-standard, R implements \ntechnical approximation \\(\\Phi^{-1}\\) via rational functions.","code":""},{"path":"univariate-random-variables.html","id":"sampling-from-a-t-distribution","chapter":"4 Univariate random variables","heading":"4.2.1 Sampling from a \\(t\\)-distribution","text":"Let \\(Z = (Y, W) \\\\mathbb{R} \\times (0, \\infty)\\) \\(Z \\sim \\mathcal{N}(0, 1)\\) \n\\(W \\sim \\chi^2_k\\) independent.Define \\(T : \\mathbb{R} \\times (0, \\infty) \\\\mathbb{R}\\) \n\\[T(z,w) = \\frac{z}{\\sqrt{w/k}},\\]\n\n\\[X = T(Z, W) = \\frac{Z}{\\sqrt{W/k}} \\sim t_k.\\]R simulates \\(t\\)-distribution \\(W\\) generated \ngamma distribution shape parameter \\(k / 2\\) scale parameter \\(2\\).","code":""},{"path":"univariate-random-variables.html","id":"reject-samp","chapter":"4 Univariate random variables","heading":"4.3 Rejection sampling","text":"section deals general algorithm simulating variables\ndistribution density \\(f\\). call \\(f\\) target density\ncorresponding distribution called target distribution.\nidea simulate proposals different distribution\ndensity \\(g\\) (proposal distribution) according \ncriterion decide accept reject proposals. assumed\nthroughout proposal density \\(g\\) density fulfilling \\[\\begin{equation}\ng(x) = 0 \\Rightarrow f(x) = 0.\n\\tag{4.1}\n\\end{equation}\\]Let \\(Y_1, Y_2, \\ldots\\) ..d. density \\(g\\) \\(\\mathbb{R}\\) \\(U_1, U_2, \\ldots\\)\n..d. uniformly distributed \\((0,1)\\) independent \\(Y_i\\)-s. Define\n\\[T(\\mathbf{Y}, \\mathbf{U}) = Y_{\\sigma}\\]\n\n\\[\\sigma = \\inf\\{n \\geq 1 \\mid U_n \\leq \\alpha f(Y_n) / g(Y_n)\\},\\]\n\\(\\alpha \\(0, 1]\\) \\(f\\) density. Rejection sampling consists\nsimulating independent pairs \\((Y_n, U_n)\\) long reject \nproposals \\(Y_n\\) sampled \\(g\\),\n, long \n\\[U_n > \\alpha f(Y_n) / g(Y_n).\\]\nfirst time accept proposal \\(\\sigma\\), stop \nsampling return proposal \\(Y_{\\sigma}\\). result , indeed,\nsample distribution density \\(f\\) following theorem states.Theorem 4.2  \\(\\alpha f(y) \\leq g(y)\\) \\(y \\\\mathbb{R}\\) \\(\\alpha > 0\\)\ndistribution \\(Y_{\\sigma}\\) density \\(f\\). Proof.  Note \\(g\\) automatically fulfills (4.1). formal proof decomposes\nevent \\((Y_{\\sigma} \\leq y)\\) according value \\(\\sigma\\) follows\\[\\begin{align}\nP(Y_{\\sigma} \\leq y) & = \\sum_{n = 1}^{\\infty} P(Y_{n} \\leq y, \\ \\sigma = n) \\\\\n& = \\sum_{n = 1}^{\\infty} P(Y_{n} \\leq y, \\ U_n \\leq \\alpha f(Y_n) / g(Y_n)) P(\\sigma > n - 1) \\\\\n& = P(Y_{1} \\leq y, \\ U_1 \\leq \\alpha f(Y_1) / g(Y_1)) \\sum_{n = 1}^{\\infty} P(\\sigma > n - 1).\n\\end{align}\\]independence pairs \\((Y_n, U_n)\\) find \n\\[P(\\sigma > n - 1) = p^{(n-1)}\\]\n\\(p = P(U_1 > \\alpha f(Y_1) / g(Y_1))\\), \n\\[\\sum_{n = 1}^{\\infty} P(\\sigma > n - 1) = \\sum_{n = 1}^{\\infty} p^{(n-1)} = \\frac{1}{1 - p}.\\]find using Tonelli’s theorem \\[\\begin{align}\nP(Y_{1} \\leq y, \\ U_1 \\leq \\alpha f(Y_1) / g(Y_1)) & = \\int_{-\\infty}^y \\alpha \\frac{f(z)}{g(z)} g(z) \\mathrm{d}z \\\\\n& = \\alpha \\int_{-\\infty}^y f(z) \\mathrm{d} z.\n\\end{align}\\]also follows , taking \\(y = \\infty\\), \\(1 - p = \\alpha\\),\nconclude \n\\[P(Y_{\\sigma} \\leq y) = \\int_{-\\infty}^y f(z) \\mathrm{d} z,\\]\ndensity distribution \\(Y_{\\sigma}\\) , indeed, \\(f\\).Note \\(\\alpha f \\leq g\\) densities \\(f\\) \\(g\\), \n\\[\\alpha = \\int \\alpha f(x) \\mathrm{d}x \\leq \\int g(x) \\mathrm{d}x = 1,\\]\nwhence follows automatically \\(\\alpha \\leq 1\\) whenever \\(\\alpha f\\) \ndominated \\(g\\). function \\(g/\\alpha\\) called envelope \\(f\\).\ntighter envelope, smaller probability rejecting\nsample \\(g\\), quantified explicitly \\(\\alpha\\) \\(1 - \\alpha\\)\nrejection probability. Thus \\(\\alpha\\) preferably close \none possible.\\(f(y) = c q(y)\\) \\(g(y) = d p(y)\\) (unknown) normalizing constants\n\\(c, d > 0\\) \\(\\alpha' q \\leq p\\) \\(\\alpha' > 0\\) \n\\[\\underbrace{\\left(\\frac{\\alpha' d}{c}\\right)}_{= \\alpha} \\ f \\leq g.\\]\nconstant \\(\\alpha'\\) may larger 1, argument \nknow \\(\\alpha \\leq 1\\), Theorem 4.2 gives \n\\(Y_{\\sigma}\\) distribution density \\(f\\). appears need \ncompute normalizing constants implement rejection sampling. However,\nobserve \n\\[u \\leq \\frac{\\alpha f(y)}{g(y)} \\Leftrightarrow u \\leq \\frac{\\alpha' q(y)}{p(y)},\\]\nwhence rejection sampling can actually implemented knowledge\nunnormalized densities \\(\\alpha'\\) without computing \\(c\\) \\(d\\).\none great advantage rejection sampling. \nnote, though, don’t know normalizing constants, \\(\\alpha'\\) tell\nus anything tight envelope , thus small rejection\nprobability .Given two functions \\(q\\) \\(p\\), find \\(\\alpha'\\) \n\\(\\alpha' q \\leq p\\)? Consider function\n\\[y \\mapsto \\frac{p(y)}{q(y)}\\]\n\\(q(y) > 0\\). function lower bounded value strictly larger \nzero, can take\n\\[\\alpha' = \\inf_{y: q(y) > 0} \\frac{p(y)}{q(y)} > 0.\\]\ncan practice often find value minimizing \\(p(y)/q(y)\\). \nminimum zero, \\(\\alpha'\\), \\(p\\) \nused construct envelope. minimum strictly positive\nbest possible choice \\(\\alpha'\\).","code":""},{"path":"univariate-random-variables.html","id":"vMsim","chapter":"4 Univariate random variables","heading":"4.3.1 von Mises distribution","text":"Recall von Mises distribution Section 1.2.1. \ndistribution \\((-\\pi, \\pi]\\) density\n\\[f(x) \\propto e^{\\kappa \\cos(x - \\mu)}\\]\nparameters \\(\\kappa > 0\\) \\(\\mu \\(-\\pi, \\pi]\\). Clearly, \\(\\mu\\) \nlocation parameter, fix \\(\\mu = 0\\) following. Simulating random variables\n\\(\\mu \\neq 0\\) can achieved (wrapped) translation variables\n\\(\\mu = 0\\).Thus target density \\(f(x) \\propto e^{\\kappa \\cos(x)}\\). section\nuse uniform distribution \\((-\\pi, \\pi)\\) proposal distribution.\nconstant density \\(g(x) = (2\\pi)^{-1}\\), need , fact, \n\\(g(x) \\propto 1\\). Since \\(x \\mapsto 1 / \\exp(\\kappa \\cos(x)) = \\exp(-\\kappa \\cos(x))\\) attains \nminimum \\(\\exp(-\\kappa)\\) \\(x = 0\\), find \n\\[\\alpha' e^{\\kappa \\cos(x)} = e^{\\kappa(\\cos(x) - 1)} \\leq 1,\\]\n\\(\\alpha' = \\exp(-\\kappa)\\). rejection test \nproposal \\(Y \\sim g\\) can therefore carried \ntesting uniformly distributed random variable \\(U\\) \\((0,1)\\)\nsatisfies\n\\[U > e^{\\kappa(\\cos(Y) - 1)}.\\]\nFigure 4.1: Histograms 100,000 simulated data points von Mises distributions parameters \\(\\kappa = 0.5\\) (left) \\(\\kappa = 2\\) (right). true densities (blue) added plots.\nFigure 4.1 confirms implementation simulates\nvon Mises distribution.Though implementation can easily simulate 100,000 variables couple \nseconds, might still possible improve . investigate \nrun time spent use line profiling tool implemented \nprofvis package.profiling result shows almost time\nspent simulating uniformly distributed random variables. , perhaps,\nexpected take time, takes much time\ncomputing ratio, say, used rejection test bit surprising.\nmight even surprising large amount memory allocation\ndeallocation associated simulation variables.culprit runif overhead associated call.\nfunction performs much better called return vector \ncalled repeatedly return just single numbers. \nrewrite rejection sampler make better use runif, \nmake code bit complicated don’t know upfront\nmany uniform variables need. introduce bookkeeping\npossible abstract away implementation \nrejection sampler. Therefore implement generic\nwrapper random number generator cache suitable amount\nrandom variables. function take care bookkeeping\nvariables can extracted needed. also nicely\nillustrates use function factory.implementation function returns function. returned\nfunction, next_rn comes environment, stores \ncached variables extracts returns one variable whenever called.\ngenerates new vector random variables\nwhenever “runs .” first time , function estimates \nfactor many variables needed total based argument r, \ngenerates estimated number variables needed. may \nrepeated couple times.can reimplement vMsim using rng_stream. later usage add \npossibility printing tracing information., course, remember test new implementation still\ngenerates variables von Mises distribution.\nFigure 4.2: Histograms 100,000 simulated data points von Mises distributions parameters \\(\\kappa = 0.5\\) (left) \\(\\kappa = 2\\) (right), simulated using vectorized generation random variables.\ncan compare run time new implementation \nrun time first implementation.see time estimate , using vectorized call runif\nreduces run time factor 4-5. possible get factor 2-3\nrun time improvement (shown) implementing computations done \nrng_stream directly inside vMsim. However, prioritize \nmodular code can reuse rng_stream rejection samplers without\nrepeating code. pure R implementation based loop never able \ncompete C++ implementation anyway accept-reject step \nsimple computation.fact, write pure R function run time efficient, need \nturn entire rejection sampler vectorized computation. ,\njust generation random numbers need vectorized.\nway around form loop don’t known upfront many\nrejections . can, however, benefit ideas rng_stream\nestimate fraction acceptances first round, can \nused subsequent simulations. done following fully\nvectorized R implementation.implementation incrementally grows list, whose entries contain\nvectors accepted samples. usually advisable dynamically\ngrow objects (vectors list), lead lot memory\nallocation, copying deallocation. Thus better initialize vector\ncorrect size upfront. particular case list contain\nentries, inconsequential grown dynamically.Finally, C++ implementation via Rcpp given random variables \ngenerated one time via C-interface R’s random number\ngenerators. (substantial) overhead C++.\nFigure 4.3: Histograms 100,000 simulated data points von Mises distributions parameters \\(\\kappa = 0.5\\) (left) \\(\\kappa = 2\\) (right), simulated using Rcpp implementation (top) fully vectorized R implementation (bottom).\nFigure 4.3 shows results testing C++ implementation\nfast R implementation,\nconfirms implementations simulate von Mises distribution.\nconclude measuring run time implementations using\nsystem.time combined microbenchmark four different implementations.C++ implementation factor 1.5 faster fully\nvectorized R implementation, around factor 15 faster\nloop-based vMsim factor 85 faster \nfirst implementation vMsim_slow. Rejection sampling good example\nalgorithm naive loop-based R implementation\nperforms rather poorly terms run time, completely vectorized\nimplementation competitive Rcpp implementation.","code":"\nvMsim_slow <- function(n, kappa) {\n  y <- numeric(n)\n  for(i in 1:n) {\n    reject <- TRUE\n    while(reject) {\n      y0 <- runif(1, - pi, pi)\n      u <- runif(1)\n      reject <- u > exp(kappa * (cos(y0) - 1))\n    }\n    y[i] <- y0\n  }\n  y\n}\nf <- function(x, k) exp(k * cos(x)) / (2 * pi * besselI(k, 0))\nx <- vMsim_slow(100000, 0.5)\nhist(x, breaks = seq(-pi, pi, length.out = 20), prob = TRUE)\ncurve(f(x, 0.5), -pi, pi, col = \"blue\", lwd = 2, add = TRUE)\nx <- vMsim_slow(100000, 2)\nhist(x, breaks = seq(-pi, pi, length.out = 20), prob = TRUE)\ncurve(f(x, 2), -pi, pi, col = \"blue\", lwd = 2, add = TRUE)\nsystem.time(vMsim_slow(100000, kappa = 5))##    user  system elapsed \n##   2.507   0.158   2.693\nlibrary(profvis)\nprofvis(vMsim_slow(10000, 5))\nrng_stream <- function(m, rng, ...) {\n  args <- list(...)\n  cache <- do.call(rng, c(m, args))\n  j <- 0\n  fact <- 1\n  next_rn <- function(r = m) {\n    j <<- j + 1\n    if(j > m) {\n      if(fact == 1 && r < m) fact <<- m / (m - r)\n      m <<- floor(fact * (r + 1))\n      cache <<- do.call(rng, c(m, args))\n      j <<- 1\n    }\n    cache[j] \n  }\n  next_rn\n}\nvMsim <- function(n, kappa, trace = FALSE) {\n  count <- 0\n  y <- numeric(n)\n  y0 <- rng_stream(n, runif, - pi, pi)\n  u <- rng_stream(n, runif)\n  for(i in 1:n) {\n    reject <- TRUE\n    while(reject) {\n      count <- count + 1\n      z <- y0(n - i)\n      reject <- u(n - i) > exp(kappa * (cos(z) - 1))\n    }\n    y[i] <- z\n  }\n  if(trace)\n    cat(\"kappa =\", kappa, \":\", (count - n)/ count, \"\\n\")  ## Rejection frequency\n  y\n}\nx <- vMsim(100000, 0.5)\nhist(x, breaks = seq(-pi, pi, length.out = 20), prob = TRUE)\ncurve(f(x, 0.5), -pi, pi, col = \"blue\", lwd = 2, add = TRUE)\nx <- vMsim(100000, 2)\nhist(x, breaks = seq(-pi, pi, length.out = 20), prob = TRUE)\ncurve(f(x, 2), -pi, pi, col = \"blue\", lwd = 2, add = TRUE)\nsystem.time(vMsim(100000, kappa = 5))##    user  system elapsed \n##   1.076   0.718   2.052\nvMsim_vec <- function(n, kappa) {\n  fact <- 1\n  j <- 1\n  l <- 0  ## The number of accepted samples\n  y <- list()\n  while(l < n) {\n    m <- floor(fact * (n - l))  ## equals n the first time\n    y0 <- runif(m, - pi, pi)\n    u <- runif(m)\n    accept <- u <= exp(kappa * (cos(y0) - 1))\n    l <- l + sum(accept)\n    y[[j]] <- y0[accept]\n    j <- j + 1\n    if(fact == 1) fact <- n / l\n  }\n  unlist(y)[1:n]\n}#include <Rcpp.h>\nusing namespace Rcpp;\n// [[Rcpp::export]]\nNumericVector vMsim_cpp(int n, double kappa) {\n  NumericVector y(n);\n  double y0;\n  bool reject;\n  for(int i = 0; i < n; ++i) {\n    do {\n      y0 = R::runif(- M_PI, M_PI);\n      reject = R::runif(0, 1) > exp(kappa * (cos(y0) - 1));\n    } while(reject);\n    y[i] = y0;\n  }\n  return y;\n}\nsystem.time(vMsim_cpp(100000, kappa = 5))##    user  system elapsed \n##   0.045   0.001   0.047\nmicrobenchmark(\n  vMsim_slow(1000, kappa = 5),\n  vMsim(1000, kappa = 5),\n  vMsim_vec(1000, kappa = 5),\n  vMsim_cpp(1000, kappa = 5)\n)## Unit: microseconds\n##                         expr   min    lq  mean median    uq    max neval\n##  vMsim_slow(1000, kappa = 5) 17387 19161 26383  20872 23973 103838   100\n##       vMsim(1000, kappa = 5)  6304  7076  8596   7897  8690  67349   100\n##   vMsim_vec(1000, kappa = 5)   452   537   621    592   680   1040   100\n##   vMsim_cpp(1000, kappa = 5)   298   344   388    378   407    629   100"},{"path":"univariate-random-variables.html","id":"gamma-distribution","chapter":"4 Univariate random variables","heading":"4.3.2 Gamma distribution","text":"may possible find suitable envelope density \ngamma distribution \\((0, \\infty)\\), turns \nefficient rejection sampler non-standard distribution \ncan transformed gamma distribution simple transformation.Let \\(t(y) = (1 + )^3\\) \\(y \\(-b^{-1}, \\infty)\\), \\(t(Y) \\sim \\Gamma(r,1)\\) \\(r \\geq 1\\)\n\\(Y\\) density\n\\[f(y) \\propto t(y)^{r-1}t'(y) e^{-t(y)} = e^{(r-1)\\log t(y) + \\log t'(y) - t(y)}.\\]proof follows simple univariate density transformation theorem,\nsee also original paper Marsaglia Tsang (2000) proposed rejection\nsampler discussed section. density \\(f\\) target density\nrejection sampler.\n\\[f(y) \\propto e^{(r-1)\\log t(y) + \\log t'(y) - t(y)},\\]\n\\(= r - 1/3\\) \\(b = 1/(3 \\sqrt{})\\)\n\\[f(y) \\propto e^{\\log t(y)/- t(y) + \\log } \\propto \\underbrace{e^{\\log t(y)/- t(y) + }}_{q(y)}.\\]analysis \\(w(y) := - y^2/2 - \\log q(y)\\) shows convex \\((-b^{-1}, \\infty)\\)\nattains minimum \\(0\\) \\(w(0) = 0\\), whence\n\\[q(y) \\leq e^{-y^2/2}.\\]\ngives us envelope expressed terms unnormalized densities\n\\(\\alpha' = 1\\).implementation rejection sampler based analysis relatively\nstraightforward. rejection sampler simulate distribution\ndensity \\(f\\) simulating Gaussian distribution (envelope).\nrejection step need implement \\(q\\). Finally, also need\nimplement \\(t\\) transform result rejection sampler \ngamma distributed. rejection sampler otherwise implemented \nnon-vectorized von Mises distribution. investigate rejection probabilities \nadditionally implement possibility printing tracing\ninformation.test implementation simulating \\(100,000\\) values\nparameters \\(r = 8\\) well \\(r = 1\\) compare resulting histograms\nrespective theoretical densities.\nFigure 4.4: Histograms simulated gamma distributed variables shape parameters \\(r = 8\\) (left) \\(r = 1\\) (right) corresponding theoretical densities (blue).\nThough simple informal test, indicates implementation\ncorrectly simulates gamma distribution.Rejection sampling can computationally expensive many samples rejected.\ntight envelope lead fewer rejections, loose envelope \nlead many rejections. Using tracing option implemented obtain\nestimates rejection probability thus quantification \ntight envelope .observe rejection frequencies small \\(r = 1\\) \nworst case around 5% rejections. cases rejection\nfrequencies 1%, thus rejection rare.visual comparison \\(q\\) (unnormalized) Gaussian density also\nshows two (unnormalized) densities close except\ntails little probability mass.\nFigure 4.5: Comparisons Gaussian proposal (red) target density (blue) used eventually simulating gamma distributed variables via transformation.\n","code":"\n## r >= 1 \ntfun <- function(y, a) {\n  b <- 1 / (3 * sqrt(a))\n  (y > -1/b) * a * (1 + b * y)^3  ## 0 when y <= -1/b\n}\n\nqfun <- function(y, r) {\n  a <- r - 1/3\n  tval <- tfun(y, a)\n  exp(a * log(tval / a) - tval + a)\n}\n\ngammasim <- function(n, r, trace = FALSE) {\n  count <- 0\n  y <- numeric(n)\n  y0 <- rng_stream(n, rnorm)\n  u <- rng_stream(n, runif)\n  for(i in 1:n) {\n    reject <- TRUE\n    while(reject) {\n      count <- count + 1\n      z <- y0(n - i)\n      reject <- u(n - i) > qfun(z, r) * exp(z^2/2)\n    }\n    y[i] <- z\n  }\n  if(trace)\n    cat(\"r =\", r, \":\", (count - n)/ count, \"\\n\")  ## Rejection frequency\n  tfun(y, r - 1/3)\n}\ny <- gammasim(100000, 16, trace = TRUE)\ny <- gammasim(100000, 8, trace = TRUE)\ny <- gammasim(100000, 4, trace = TRUE)\ny <- gammasim(100000, 1, trace = TRUE)## r = 16 : 0.00161738 \n## r = 8 : 0.003745915 \n## r = 4 : 0.007966033 \n## r = 1 : 0.04789108"},{"path":"univariate-random-variables.html","id":"adaptive","chapter":"4 Univariate random variables","heading":"4.4 Adaptive envelopes","text":"good envelope tight, meaning \\(\\alpha\\) close one,\nfast simulate density \nfast evaluate. obvious find envelope\narbitrary target density \\(f\\).section develops general scheme construction envelopes\nlog-concave target densities. special class densities,\nuncommon practice. scheme can also extended work\ndensities combinations log-concave log-convex\nbehaviors. idea used constructing\nenvelopes can used bound \\(f\\) . accept-reject step\ncan avoid many evaluations \\(f\\), beneficial \\(f\\) \ncomputationally expensive evaluate.key idea scheme bound log-density piecewise\naffine functions. particularly easy density\nlog-concave. scheme leads analytically manageable\nformulas envelope, corresponding distribution function \ninverse, result fast simulate proposals \ncompute envelope needed accept-reject step.scheme requires choice finite number points determine\naffine bounds. given choice points scheme adapts \nenvelope target density automatically. possible implement\nfully adaptive scheme doesn’t even require choice points\ninitializes updates points dynamically rejection\nsamples computed. section focus scheme \ngiven fixed number points.continuously differentiable, strictly positive \nlog-concave target open interval \\(\\subseteq \\mathbb{R}\\)\nholds \n\\[\\log(f(x)) \\leq \\frac{f'(x_0)}{f(x_0)}(x - x_0) + \\log(f(x_0))\\]\n\\(x, x_0 \\\\).Let \\(x_1 < x_2 < \\ldots < x_{m} \\\\) let \\(I_1, \\ldots, I_m \\subseteq \\) \nintervals form partition \\(\\) \\(x_i \\I_i\\). Defining\n\\[a_i = (\\log(f(x_i)))' = \\frac{f'(x_i)}{f(x_i)} \\quad \\text{} \\quad b_i = \\log(f(x_i)) -  \\alpha_i x_i\\]\nfind upper bound\n\\[\\log(f(x)) \\leq V(x) = \\sum_{=1}^m  (a_i x + b_i) 1_{I_i}(x),\\]\n\n\\[f(x) \\leq e^{V(x)}.\\]\nNote log-concavity \\(f\\), \\(a_1 \\geq a_2 \\geq \\ldots \\geq a_m\\).\nupper bound integrable \\(\\) either \\(a_1 > 0\\) \\(a_m < 0\\),\n\\(a_m < 0\\) \\(\\) bounded left, \\(a_1 > 0\\) \\(\\) bounded\nright. cases define\n\\[c = \\int_I e^{V(x)} \\mathrm{d} x < \\infty\\]\n\\(g(x) = c^{-1} \\exp(V(x))\\), find \\(\\alpha = c^{-1}\\)\n\\(\\alpha f \\leq g\\) \\(g\\) envelope \\(f\\). Note \nactually necessary compute \\(c\\) (\\(\\alpha\\)) implement \nrejection step rejection sampler, \\(c\\) needed \nsimulating \\(g\\) described . assume following\n\\(c < \\infty\\).intervals \\(I_i\\) specified, , fact, implement\nrejection sampling choice intervals fulfilling conditions\n. interest maximizing\n\\(\\alpha\\) (minimizing \\(c\\)) thus minimizing rejection frequency, \nchoose \\(I_i\\) \\(a_i x + b_i\\) minimal \\(I_i\\) among affine\nupper bounds. result tightest envelope. means\n\\(= 1, \\ldots, m - 1\\), \\(I_i = (z_{-1}, z_i]\\) \\(z_i\\)\npoint \\(a_i x + b_i\\) \\(a_{+1} x + b_{+1}\\) intersect. find\nsolution \n\\[a_i x + b_i = a_{+1} x + b_{+1}\\]\n\n\\[z_i = \\frac{b_{+1} - b_i}{a_i - a_{+1}}\\]\nprovided \\(a_{+1} > a_i\\). two extremes, \\(z_0\\) \\(z_m\\), chosen\nendpoints \\(\\) may \\(- \\infty\\) \\(+ \\infty\\), respectively.One way simulate envelopes transformation\nuniform random variables inverse distribution function.\nrequires little bookkeeping, otherwise straightforward.\nDefine \\(x \\I_i\\)\n\\[F_i(x) = \\int_{z_{-1}}^x e^{a_i z + b_i} \\mathrm{d} z,\\]\nlet \\(R_i = F_i(z_i)\\). \\(c = \\sum_{=1}^m R_i\\), \ndefine \\(Q_i = \\sum_{k=1}^{} R_k\\) \\(= 0, \\ldots, m\\)\ninverse distribution function \\(q\\) given solution \nequation\n\\[F_i(x) = cq - Q_{-1}, \\qquad Q_{-1} < cq \\leq Q_{}.\\]\n, given \\(q \\(0, 1)\\), first determine interval\n\\((Q_{-1}, Q_{}]\\) \\(c q\\) falls , solve corresponding\nequation. Observe \\(a_i \\neq 0\\),\n\\[F_i(x) = \\frac{1}{a_i}e^{b_i}\\left(e^{a_i x} - e^{a_i z_{-1}}\\right).\\]","code":""},{"path":"univariate-random-variables.html","id":"beta-distribution","chapter":"4 Univariate random variables","heading":"4.4.1 Beta distribution","text":"illustrate envelope construction simple log-concave\ndensity consider Beta distribution \\((0, 1)\\) shape parameters \\(\\geq 1\\).\ndistribution density\n\\[f(x) \\propto x^{\\alpha - 1}(1-x)^{\\beta - 1},\\]\nlog-concave (shape parameters greater one).\nimplement rejection sampling algorithm density adaptive\nenvelope using two points.\nFigure 4.6: Histograms simulated variables Beta distributions using rejection sampler adaptive envelope based log-concavity. true density (blue) envelope (red) added plots.\nNote safeguard implemented test \\(a_i\\)-s check \nformulas used actually meaningful, specifically \ndivisions zero.","code":"\nBetasim <- function(n, x1, x2, alpha, beta) {\n  lf <- function(x) (alpha - 1) * log(x) + (beta - 1) * log(1 - x) \n  lf_deriv <- function(x) (alpha - 1)/x - (beta - 1)/(1 - x)\n  a1 <- lf_deriv(x1)\n  a2 <- lf_deriv(x2)\n  if(a1 == 0 || a2 == 0 || a1 - a2 == 0) \n    stop(\"\\nThe implementation requires a_1 and a_2 different \nand both different from zero. Choose different values of x_1 and x_2.\")\n  b1 <- lf(x1) - a1 * x1\n  b2 <- lf(x2) - a2 * x2\n  z1 <- (b2 - b1) / (a1 - a2)\n  Q1 <- exp(b1) * (exp(a1 * z1) - 1) / a1 \n  c <- Q1 + exp(b2) * (exp(a2 * 1) - exp(a2 * z1)) / a2\n\n  y <- numeric(n)\n  uy <- rng_stream(n, runif)\n  u <- rng_stream(n, runif)\n  for(i in 1:n) {\n    reject <- TRUE\n    while(reject) {\n      u0 <- c * uy(n - i)\n      if(u0 < Q1) {\n        z <- log(a1 * exp(-b1) * u0 + 1) / a1\n        reject <- u(n - i) > exp(lf(z) - a1 * z - b1)\n      } else {\n        z <- log(a2 * exp(-b2) * (u0 - Q1) + exp(a2 * z1)) / a2\n        reject <- u(n - i) > exp(lf(z) - a2 * z - b2)\n      }\n    }\n    y[i] <- z\n  }\n  y\n}\nBetasim(1, x1 = 0.25, x2 = 0.75, alpha = 4, beta = 2)  ## Error in Betasim(1, x1 = 0.25, x2 = 0.75, alpha = 4, beta = 2): ## The\nimplementation requires a_1 and a_2 different ## and both different from zero. Choose\ndifferent values of x_1 and x_2.```\n\n```r\nBetasim(1, x1 = 0.2, x2 = 0.75, alpha = 4, beta = 2)   ## Error in Betasim(1, x1 = 0.2, x2 = 0.75, alpha = 4, beta = 2): ## The implementation\nrequires a_1 and a_2 different ## and both different from zero. Choose different values\nof x_1 and x_2.```\n\n```r\nBetasim(1, x1 = 0.2, x2 = 0.8, alpha = 4, beta = 2)    ## [1] 0.761879"},{"path":"univariate-random-variables.html","id":"von-mises-distribution","chapter":"4 Univariate random variables","heading":"4.4.2 von Mises distribution","text":"von Mises rejection sampler Section 4.3.1 used \nuniform distribution proposal distribution. turns , uniform\ndensity particularly tight envelope. illustrate \nstudying proportion rejections previous implementation.rejection frequency high increases \\(\\kappa\\). \\(\\kappa = 5\\)\n80% proposals rejected, simulating \\(n = 10,000\\)\nvon Mises distributed variables thus requires simulation around \\(50,000\\)\nvariables proposal.von Mises density , unfortunately, log-concave \\((-\\pi, \\pi)\\),\n\\((-\\pi/2, \\pi/2)\\). , furthermore, log-convex \\((-\\pi, -\\pi/2)\\)\nwell \\((\\pi/2, \\pi)\\), implies two intervals \nlog-density corresponding chords. chords can \npieced together tangents give envelope.\nFigure 4.7: Histograms simulated variables von Mises distributions using rejection sampler adaptive envelope based combination log-concavity log-convexity. true density (blue) envelope (red) added plots.\nsee compared using uniform density envelope, adaptive\nenvelopes generally tighter leads fewer rejections. Even tighter\nenvelopes possible using four intervals, , course,\nalways good question added complexity bookkeeping induced \nusing advanced adaptive envelopes affect run time. even good question\ncurrent adaptive implementation outperform first, \nmuch simpler, implementation used uniform envelope.results benchmark show adaptive implementation \nrun time comparable using uniform proposal.\n\\(x_1 = -0.4\\) \\(x_2 = 0.4\\) \\(\\kappa = 5\\)\nfound rejection frequency 20% \nadaptive envelope, 80% using uniform\nenvelope. naive computation thus suggest speedup \nfactor 4, using adaptive envelope actually \nspeedup factor 2. vectorized solution still\nconsiderably faster. completely vectorized solution using\nadaptive envelope possible, entirely\nstraightforward implement complicated envelope\nefficiently, may better option case implement\nusing Rcpp.Even either implementation can improved terms run time,\nimportant point comparing algorithms don’t get \nfocused surrogate performance quantities. probability rejection\nsurrogate actual run time, might conceptually interest\nbring probability . expense additional\ncomputations might worth effort terms real run time.","code":"\ny <- vMsim(10000, 0.1, trace = TRUE)\ny <- vMsim(10000, 0.5, trace = TRUE)\ny <- vMsim(10000, 2, trace = TRUE)\ny <- vMsim(10000, 5, trace = TRUE)## kappa = 0.1 : 0.09156977 \n## kappa = 0.5 : 0.3561679 \n## kappa = 2 : 0.6932986 \n## kappa = 5 : 0.816507\nvMsim_adapt <- function(n, x1, x2, kappa, trace = FALSE) {\n  lf <- function(x) kappa * cos(x) \n  lf_deriv <- function(x) - kappa * sin(x)\n  a1 <- 2 * kappa / pi\n  a2 <- lf_deriv(x1)\n  a3 <- lf_deriv(x2)\n  a4 <- - a1\n  \n  b1 <- kappa\n  b2 <- lf(x1) - a2 * x1\n  b3 <- lf(x2) - a3 * x2\n  b4 <- kappa\n  \n  z0 <- -pi\n  z1 <- -pi/2\n  z2 <- (b3 - b2) / (a2 - a3)\n  z3 <- pi/2\n  z4 <- pi\n  \n  Q1 <- exp(b1) * (exp(a1 * z1) - exp(a1 * z0)) / a1 \n  Q2 <- Q1 + exp(b2) * (exp(a2 * z2) - exp(a2 * z1)) / a2\n  Q3 <- Q2 + exp(b3) * (exp(a3 * z3) - exp(a3 * z2)) / a3\n  c <- Q3 + exp(b4) * (exp(a4 * z4) - exp(a4 * z3)) / a4\n  \n  count <- 0\n  y <- numeric(n)\n  uy <- rng_stream(n, runif)\n  u <- rng_stream(n, runif)\n  for(i in 1:n) {\n    reject <- TRUE\n    while(reject) {\n      count <- count + 1\n      u0 <- c * uy(n - i)\n      if(u0 < Q1) {\n        z <- log(a1 * exp(-b1) * u0 + exp(a1 * z0)) / a1\n        reject <- u(n - i) > exp(lf(z) - a1 * z - b1)\n      } else if(u0 < Q2) {\n        z <- log(a2 * exp(-b2) * (u0 - Q1) + exp(a2 * z1)) / a2\n        reject <- u(n - i) > exp(lf(z) - a2 * z - b2)\n      } else if(u0 < Q3) {\n        z <- log(a3 * exp(-b3) * (u0 - Q2) + exp(a3 * z2)) / a3\n        reject <- u(n - i) > exp(lf(z) - a3 * z - b3)\n      } else {\n        z <- log(a4 * exp(-b4) * (u0 - Q3) + exp(a4 * z3)) / a4\n        reject <- u(n - i) > exp(lf(z) - a4 * z - b4)\n      }\n    }\n    y[i] <- z\n  }\n  if(trace)\n    cat(\"kappa =\", kappa, \", x1 =\", x1, \n        \", x2 =\", x2, \":\", (count - n) / count, \"\\n\")  \n  y\n}\ny <- vMsim_adapt(100000, -0.4, 0.4, 5, trace = TRUE)\ny <- vMsim_adapt(100000, -1, 1, 2, trace = TRUE)\ny <- vMsim_adapt(100000, -0.1, 0.1, 5, trace = TRUE)\ny <- vMsim_adapt(100000, -0.4, 0.4, 2, trace = TRUE)## kappa = 5 , x1 = -0.4 , x2 = 0.4 : 0.1996414 \n## kappa = 2 , x1 = -1 , x2 = 1 : 0.2412459 \n## kappa = 5 , x1 = -0.1 , x2 = 0.1 : 0.4844059 \n## kappa = 2 , x1 = -0.4 , x2 = 0.4 : 0.1557619\nmicrobenchmark(vMsim_adapt(100, -1, 1, 5),\n               vMsim_adapt(100, -0.4, 0.4, 5),\n               vMsim_adapt(100, -0.2, 0.2, 5),\n               vMsim_adapt(100, -0.1, 0.1, 5),\n               vMsim(100, 5),\n               vMsim_vec(100, 5)\n               )## Unit: microseconds\n##                            expr min  lq mean median   uq    max neval\n##      vMsim_adapt(100, -1, 1, 5) 772 915 1208   1080 1406   3439   100\n##  vMsim_adapt(100, -0.4, 0.4, 5) 410 463  738    526  581  18252   100\n##  vMsim_adapt(100, -0.2, 0.2, 5) 466 564  704    607  811   1419   100\n##  vMsim_adapt(100, -0.1, 0.1, 5) 573 673 3613    761  959 269242   100\n##                   vMsim(100, 5) 767 998 1292   1100 1472   3171   100\n##               vMsim_vec(100, 5)  77 116  202    140  179   1793   100"},{"path":"univariate-random-variables.html","id":"univariate:ex","chapter":"4 Univariate random variables","heading":"4.5 Exercises","text":"","code":""},{"path":"univariate-random-variables.html","id":"rejection-sampling-of-gaussian-random-variables","chapter":"4 Univariate random variables","heading":"4.5.1 Rejection sampling of Gaussian random variables","text":"exercise rejection sampling Gaussian distribution \nusing Laplace distribution envelope. Recall Laplace distribution density\n\\[g(x) = \\frac{1}{2} e^{-|x|}\\]\n\\(x \\\\mathbb{R}\\).Note \\(X\\) \\(Y\\) independent exponentially distributed mean one, \\(X - Y\\) Laplace\ndistribution. gives way easily sample Laplace distribution.Exercise 4.1  Implement rejection sampling standard Gaussian distribution density\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{- x^2 / 2}\\]\nsimulating Laplace random variables differences exponentially distributed random variables.\nTest implementation computing variance Gaussian distribution\nMC estimate comparing directly Gaussian distribution using\nhistograms QQ-plots.Exercise 4.2  Implement simulation Laplace distribution transforming uniform random variable \ninverse distribution function. Use method together rejection\nsampler implemented Exercise 4.1Note: Laplace distribution can seen simple version \nadaptive envelopes suggested Section 4.4.","code":""},{"path":"mci.html","id":"mci","chapter":"5 Monte Carlo integration","heading":"5 Monte Carlo integration","text":"typical usage simulation random variables Monte Carlo integration.\n\\(X_1, \\ldots, X_n\\) ..d. density \\(f\\)\n\\[\\hat{\\mu}_{\\textrm{MC}} := \\frac{1}{n} \\sum_{=1}^n h(X_i) \n\\rightarrow \\mu := E(h(X_1)) = \\int h(x) f(x) \\ \\mathrm{d}x\\]\n\\(n \\\\infty\\) law large numbers (LLN).Monte Carlo integration clever idea, use computer\nsimulate ..d. random variables compute average \napproximation integral. idea may applied statistical\ncontext, way also applications outside statistics \ndirect competitor numerical integration. increasing \\(n\\) LLN\ntells us average eventually become good approximation\nintegral. However, LLN quantify large \\(n\\) \n, fundamental question Monte Carlo integration therefore\nquantify precision average.chapter first deals quantification precision — mostly\nvia asymptotic variance central limit theorem. \none hand provide us quantification precision \nspecific Monte Carlo approximation, hand provide\nus way compare different Monte Carlo integration techniques. \ndirect use average requires can simulate \ndistribution density \\(f\\), might low precision \nmight just plain difficult. second half chapter treat importance\nsampling, technique simulating different distribution\nuse weighted average obtain approximation integral.","code":""},{"path":"mci.html","id":"assessment","chapter":"5 Monte Carlo integration","heading":"5.1 Assessment","text":"error analysis Monte Carlo integration differs \nordinary (deterministic) numerical integration methods. latter,\nerror analysis provides bounds error computable\napproximation terms properties function integrated. \nbounds provide guarantee error can . \ngenerally impossible provide guarantee using Monte\nCarlo integration computed approximation construction\n(pseudo)random. Thus error analysis assessment precision \n\\(\\hat{\\mu}_{\\textrm{MC}}\\) approximation \\(\\mu\\) probabilistic.two main approaches. can use approximations distribution\n\\(\\hat{\\mu}_{\\textrm{MC}}\\) assess precision computing confidence\ninterval, say. can provide finite sample upper bounds, known \nconcentration inequalities, probability\nerror \\(\\hat{\\mu}_{\\textrm{MC}}\\) larger given \\(\\varepsilon\\).\nconcentration inequality can turned confidence interval, needed, can used directly\nanswer question : want approximation error\nsmaller \\(\\varepsilon = 10^{-3}\\), large \\(n\\) need \nguarantee error bound probability least \\(99.99\\)%?Confidence intervals typically computed using central limit theorem \nestimated value asymptotic variance. notable practical\nproblem estimation asymptotic variance, otherwise \nmethod straightforward use. major deficit method \ncentral limit theorem provide bounds – approximations\nunknown precision finite \\(n\\). Thus without analysis, \nreally certain results central limit theorem\nreflect accuracy \\(\\hat{\\mu}_{\\textrm{MC}}\\). Concentration inequalities\nprovide actual guarantees, albeit probabilistic. , however, typically\nproblem specific harder derive, involve constants \ndifficult compute estimate, tend pessimistic\nreal applications. focus chapter therefore using \ncentral limit theorem, emphasize example Section 5.2.2\nshows potentially misleading confidence intervals can \nconvergence slow.","code":""},{"path":"mci.html","id":"CLT-gamma","chapter":"5 Monte Carlo integration","heading":"5.1.1 Using the central limit theorem","text":"CLT gives \n\\[\\hat{\\mu}_{\\textrm{MC}} = \\frac{1}{n} \\sum_{=1}^n h(X_i) \\overset{\\textrm{approx}} \\sim \n\\mathcal{N}(\\mu, \\sigma^2_{\\textrm{MC}} / n)\\]\n\n\\[\\sigma^2_{\\textrm{MC}} = V(h(X_1)) = \\int (h(x) - \\mu)^2 f(x) \\ \\mathrm{d}x.\\]can estimate \\(\\sigma^2_{\\textrm{MC}}\\) using empirical variance\n\\[\\hat{\\sigma}^2_{\\textrm{MC}} = \\frac{1}{n - 1} \\sum_{=1}^n (h(X_i) - \n\\hat{\\mu}_{\\textrm{MC}})^2,\\]variance \\(\\hat{\\mu}_{\\textrm{MC}}\\) estimated \n\\(\\hat{\\sigma}^2_{\\textrm{MC}} / n\\) standard 95%\nconfidence interval \\(\\mu\\) \n\\[\\hat{\\mu}_{\\textrm{MC}} \\pm 1.96 \\frac{\\hat{\\sigma}_{\\textrm{MC}}}{\\sqrt{n}}.\\]\nFigure 5.1: Sample path confidence band Monte Carlo integration mean gamma distributed random variable.\n","code":"\nn <- 1000\nx <- rgamma(n, 8) # h(x) = x\nmu_hat <- (cumsum(x) / (1:n)) # Cumulative average\nsigma_hat <- sd(x)\nmu_hat[n]  # Theoretical value 8## [1] 8.09556\nsigma_hat  # Theoretical value sqrt(8) = 2.8284## [1] 2.861755\nqplot(1:n, mu_hat) + \n  geom_ribbon(\n  mapping = aes(\n    ymin = mu_hat - 1.96 * sigma_hat / sqrt(1:n), \n    ymax = mu_hat + 1.96 * sigma_hat / sqrt(1:n)\n  ), fill = \"gray\") +\n  coord_cartesian(ylim = c(6, 10)) +\n  geom_line() + \n  geom_point() "},{"path":"mci.html","id":"concentration-inequalities","chapter":"5 Monte Carlo integration","heading":"5.1.2 Concentration inequalities","text":"\\(X\\) real valued random variable finite second moment, \\(\\mu = E(X)\\)\n\\(\\sigma^2 = V(X)\\), Chebychev’s\ninequality holds\n\\[P(|X - \\mu| > \\varepsilon) \\leq \\frac {\\sigma^2}{\\varepsilon^2}\\]\n\\(\\varepsilon > 0\\).\ninequality implies, instance, simple Monte Carlo average\ninequality\n\\[P(|\\hat{\\mu}_{\\textrm{MC}} - \\mu| > \\varepsilon) \\leq \\frac{\\sigma^2_{\\textrm{MC}}}{n\\varepsilon^2}.\\]\ncommon usage inequality qualitative statement known \nlaw large numbers: \\(\\varepsilon > 0\\)\n\\[P(|\\hat{\\mu}_{\\textrm{MC}} - \\mu| > \\varepsilon) \\rightarrow 0\\]\n\\(n \\\\infty\\). \\(\\hat{\\mu}_{\\textrm{MC}}\\) converges probability towards\n\\(\\mu\\) \\(n\\) tends infinity. However, inequality actually also provides\nquantitative statement accurate \\(\\hat{\\mu}_{\\textrm{MC}}\\) \napproximation \\(\\mu\\).Chebyshev’s inequality useful due minimal assumption finite\nsecond moment. However, typically doesn’t give tight bound \nprobability \\(P(|X - \\mu| > \\varepsilon)\\). Much better inequalities can obtained\nstronger assumptions, particular finite exponential moments.Assuming moment generating function \\(X\\) finite,\n\\(M(t) = E(e^{tX}) < \\infty\\), suitable \\(t \\\\mathbb{R}\\), follows \nMarkov’s inequality\n\n\\[P(X - \\mu > \\varepsilon) = P(e^{tX} > e^{t(\\varepsilon + \\mu)}) \\leq e^{-t(\\varepsilon + \\mu)}M(t),\\]\ncan provide tight upper bound minimizing bound \\(t\\). \nrequires knowledge moment generating function. illustrate \nusage inequality considering gamma distribution \nmoment generating function well known.","code":""},{"path":"mci.html","id":"exponential-tail-bound-for-gamma-distributed-variables","chapter":"5 Monte Carlo integration","heading":"5.1.3 Exponential tail bound for Gamma distributed variables","text":"\\(X\\) follows Gamma distribution shape parameter\n\\(\\lambda > 0\\) \\(t <1\\), \n\\[M(t) = \\frac{1}{\\Gamma(\\lambda)} \\int_0^{\\infty} x^{\\lambda - 1} e^{-(1-t) x} \\,\n\\mathrm{d} x = \\frac{1}{(1-t)^{\\lambda}}.\\]\nWhence\n\\[P(X-\\lambda > \\varepsilon) \\leq e^{-t(\\varepsilon + \\lambda)}\n\\frac{1}{(1-t)^{\\lambda}}.\\]\nMinimization \\(t\\) right hand side gives minimizer\n\\(t = \\varepsilon/(\\varepsilon + \\lambda)\\) upper bound\n\\[P(X-\\lambda > \\varepsilon) \\leq e^{-\\varepsilon} \\left(\\frac{\\varepsilon + \\lambda}{\\lambda\n    }\\right)^{\\lambda}.\\]\nCompare bound\n\\[P(|X-\\lambda| > \\varepsilon) \\leq \\frac{\\lambda}{\\varepsilon^2}\\]\nChebychev’s inequality.\nFigure 5.2: Actual tail probabilities (left) gamma distribution, computed via pgamma function, compared tight bound (red) weaker bound Chebychev’s inequality (blue). differences tail clearly seen log-probabilities (right)\n","code":""},{"path":"mci.html","id":"importance-sampling","chapter":"5 Monte Carlo integration","heading":"5.2 Importance sampling","text":"interested Monte Carlo integration, \nneed sample target distribution.Observe \n\\[\\begin{align}\n\\mu = \\int h(x) f(x) \\ \\mathrm{d}x & = \\int h(x) \\frac{f(x)}{g(x)} g(x) \\ \\mathrm{d}x \\\\\n& = \\int h(x) w^*(x) g(x) \\ \\mathrm{d}x\n\\end{align}\\]whenever \\(g\\) density fulfilling \n\\[g(x) = 0 \\Rightarrow f(x) = 0.\\]\\(X_1, \\ldots, X_n\\) ..d. density \\(g\\) define weights\n\\[w^*(X_i) = f(X_i) / g(X_i).\\]\nimportance sampling estimator \n\\[\\hat{\\mu}_{\\textrm{}}^* := \\frac{1}{n} \\sum_{=1}^n h(X_i)w^*(X_i).\\]\nmean \\(\\mu\\). LLN\n\\[\\hat{\\mu}_{\\textrm{}}^* \\rightarrow E(h(X_1) w^*(X_1)) = \\mu.\\]illustrate use importance sampling computing mean \ngamma distribution via simulations Gaussian distribution, cf. also\nSection 5.1.1.assess precision importance sampling estimate via CLT \nneed variance average just plain Monte Carlo integration.\nCLT\n\\[\\hat{\\mu}_{\\textrm{}}^* \\overset{\\textrm{approx}} \\sim \n\\mathcal{N}(\\mu, \\sigma^{*2}_{\\textrm{}} / n)\\]\n\n\\[\\sigma^{*2}_{\\textrm{}} = V (h(X_1)w^*(X_1)) = \\int (h(x) w^*(x) - \\mu)^2 g(x) \\ \\mathrm{d}x.\\]importance sampling variance can estimated similarly Monte Carlo variance\n\\[\\hat{\\sigma}^{*2}_{\\textrm{}} = \\frac{1}{n - 1} \\sum_{=1}^n (h(X_i)w^*(X_i) - \\hat{\\mu}_{\\textrm{}}^*)^2,\\]\n95% standard confidence interval computed \n\\[\\hat{\\mu}^*_{\\textrm{}} \\pm 1.96 \\frac{\\hat{\\sigma}^*_{\\textrm{}}}{\\sqrt{n}}.\\]\nFigure 5.3: Sample path confidence band importance sampling Monte Carlo integration mean gamma distributed random variable via simulations Gaussian distribution.\nmay happen \\(\\sigma^{*2}_{\\textrm{}} > \\sigma^2_{\\textrm{MC}}\\) \n\\(\\sigma^{*2}_{\\textrm{}} < \\sigma^2_{\\textrm{MC}}\\) depending \\(h\\) \\(g\\), \nchoosing \\(g\\) cleverly \\(h(x) w^*(x)\\) becomes constant possible,\nimportance sampling can often reduce variance compared plain\nMonte Carlo integration.mean gamma distribution , \\(\\sigma^{*2}_{\\textrm{}}\\)\n50% larger \\(\\sigma^2_{\\textrm{MC}}\\), loose precision\nusing importance sampling way compared plain Monte Carlo integration.\nSection 5.3 consider different example achieve\nconsiderable variance reduction using importance sampling.","code":"\nx <- rnorm(n, 10, 3)\nw_star <- dgamma(x, 8) / dnorm(x, 10, 3) \nmu_hat_IS <- (cumsum(x * w_star) / (1:n))\nmu_hat_IS[n]  # Theoretical value 8## [1] 7.995228\nsigma_hat_IS <- sd(x * w_star)\nsigma_hat_IS  # Theoretical value ??## [1] 3.499995"},{"path":"mci.html","id":"unknown-normalization-constants","chapter":"5 Monte Carlo integration","heading":"5.2.1 Unknown normalization constants","text":"\\(f = c^{-1} q\\) \\(c\\) unknown \n\\[c = \\int q(x) \\ \\mathrm{d}x = \\int \\frac{q(x)}{g(x)} g(x) \\ d x,\\]\n\n\\[\\mu = \\frac{\\int h(x) w^*(x) g(x) \\ d x}{\\int w^*(x) g(x) \\ d x},\\]\n\\(w^*(x) = q(x) / g(x).\\)\\(X_1, \\ldots, X_n\\) ..d. distribution density \\(g\\),\nimportance sampling estimate \\(\\mu\\) can computed \n\\[\\hat{\\mu}_{\\textrm{}} = \\frac{\\sum_{=1}^n h(X_i) w^*(X_i)}{\\sum_{=1}^n w^*(X_i)} = \\sum_{=1}^n h(X_i) w(X_i),\\]\n\\(w^*(X_i) = q(X_i) / g(X_i)\\) \n\\[w(X_i) = \\frac{w^*(X_i)}{\\sum_{=1}^n w^*(X_i)}\\]\nstandardized weights. works irrespectively value \nnormalizing constant \\(c\\), actually works also \\(g\\) unnormalized.Revisiting mean gamma distribution, can implement importance\nsampling via samples Gaussian distribution using weights computed\nwithout normalization constants.variance estimator standardized weights little \ncomplicated, estimator ratio random variables. \nmultivariate CLT\n\\[\\frac{1}{n} \\sum_{=1}^n \\left(\\begin{array}{c}\n h(X_i) w^*(X_i) \\\\\n w^*(X_i) \n\\end{array}\\right) \\overset{\\textrm{approx}}{\\sim} \n\\mathcal{N}\\left( c \\left(\\begin{array}{c} \\mu  \\\\   {1} \\end{array}\\right),\n\\frac{1}{n} \\left(\\begin{array}{cc} \\sigma^{*2}_{\\textrm{}} & \\gamma \\\\ \\gamma & \\sigma^2_{w^*}\n\\end{array} \\right)\\right),\\]\n\n\\[\\begin{align}\n\\sigma^{*2}_{\\textrm{}} & = V(h(X_1)w^*(X_1)) \\\\\n\\gamma & = \\mathrm{cov}(h(X_1)w^*(X_1), w^*(X_1)) \\\\\n\\sigma_{w^*}^2 & = V (w^*(X_1)).\n\\end{align}\\]can apply \\(\\Delta\\)-method \\(t(x, y) = x / y\\). Note \n\\(Dt(x, y) = (1 / y, - x / y^2)\\), whence\n\\[Dt(c\\mu, c)   \\left(\\begin{array}{cc} \\hat{\\sigma}^{*2}_{\\textrm{}} & \\gamma \\\\ \\gamma & \\sigma^2_{w^*}\n\\end{array} \\right) Dt(c\\mu, c)^T = c^{-2} (\\sigma^{*2}_{\\textrm{}} + \\mu^2 \\sigma_{w^*}^2 - 2 \\mu \\gamma).\\]\\(\\Delta\\)-method\n\\[\\hat{\\mu}_{\\textrm{}} \\overset{\\textrm{approx}}{\\sim} \n\\mathcal{N}(\\mu, c^{-2} (\\sigma^{*2}_{\\textrm{}} + \\mu^2 \\sigma_{w^*}^2 - 2 \\mu \\gamma) / n).\\]\nunknown quantities asymptotic variance must estimated using e.g. \nempirical equivalents, \\(c \\neq 1\\) (used unnormalized densities)\nnecessary estimate \\(c\\) \n\\(\\hat{c} = \\frac{1}{n} \\sum_{=1}^n w^*(X_i)\\)\ncompute estimate variance.example mean gamma distribution, find following\nestimate variance.\nFigure 5.4: Sample path confidence band importance sampling Monte Carlo integration mean gamma distributed random variable via simulations Gaussian distribution using standardized weights.\nexample, variance using standardized weights little lower\nusing unstandardized weights, still larger variance \nplain Monte Carlo average.","code":"\nw_star <- numeric(n)\nx_pos <- x[x > 0]\nw_star[x > 0] <- exp((x_pos - 10)^2 / 18 - x_pos + 7 * log(x_pos))\nmu_hat_IS <- cumsum(x * w_star) / cumsum(w_star)\nmu_hat_IS[n]  # Theoretical value 8## [1] 8.102177\nc_hat <- mean(w_star)\nsigma_hat_IS <- sd(x * w_star)\nsigma_hat_w_star <- sd(w_star)\ngamma_hat <- cov(x * w_star, w_star)\nsigma_hat_IS_w <- sqrt(sigma_hat_IS^2 + mu_hat_IS[n]^2 * sigma_hat_w_star^2 - \n                         2 * mu_hat_IS[n] * gamma_hat) / c_hat\nsigma_hat_IS_w## [1] 3.198314"},{"path":"mci.html","id":"hd-int","chapter":"5 Monte Carlo integration","heading":"5.2.2 Computing a high-dimensional integral","text":"illustrate usage also limitations Monte Carlo\nintegration, consider following \\(p\\)-dimensional integral\n\\[ \\int e^{-\\frac{1}{2}\\left(x_1^2 + \\sum_{=2}^p (x_i - \\alpha x_{-1})^2\\right)}  \\mathrm{d} x.\\]\nNow integral even expressed expectation w.r.t. distribution\nfirst place – integral w.r.t. Lebesgue measure \\(\\mathbb{R}^p\\).\nuse idea importance sampling rewrite integral\nexpectation w.r.t. probability distribution. might many ways\n, following just one.Rewrite exponent \n\\[||x||_2^2 + \\sum_{= 2}^p \\alpha^2 x_{-1}^2 - 2\\alpha x_i x_{-1}\\]\n\\[\\begin{align*}\n\\int e^{-\\frac{1}{2}\\left(x_1^2 + \\sum_{=2}^p (x_i - \\alpha x_{-1})^2\\right)}  \\mathrm{d} x & =  \\int e^{- \\frac{1}{2} \\sum_{= 2}^p \\alpha^2 \nx_{-1}^2 - 2\\alpha x_i x_{-1}}  e^{-\\frac{||x||_2^2}{2}} \\mathrm{d} x \\\\\n& =  (2 \\pi)^{p/2} \\int e^{- \\frac{1}{2} \\sum_{= 2}^p \\alpha^2 \nx_{-1}^2 - 2\\alpha x_i x_{-1}} f(x) \\mathrm{d} x\n\\end{align*}\\]\\(f\\) density \\(\\mathcal{N}(0, I_p)\\) distribution. Thus \n\\(X \\sim \\mathcal{N}(0, I_p)\\),\n\\[ \\int e^{-\\frac{1}{2}\\left(x_1^2 + \\sum_{=2}^p (x_i - \\alpha x_{-1})^2\\right)}  \\mathrm{d} x = (2 \\pi)^{p/2}  E\\left( e^{- \\frac{1}{2} \\sum_{= 2}^p \\alpha^2 \nX_{-1}^2 - 2\\alpha X_i X_{-1}} \\right).\\]Monte Carlo integration computes\n\\[\\mu = E\\left( e^{- \\frac{1}{2} \\sum_{= 2}^p \\alpha^2 \nX_{-1}^2 - 2\\alpha X_i X_{-1}} \\right)\\]\ngenerating \\(p\\)-dimensional random\nvariables \\(\\mathcal{N}(0, I_p)\\). can actually shown \\(\\mu = 1\\),\nskip proof .First, implement function want integrate.specify various parameters.actual computation implemented using apply function. \nfirst look case \\(\\alpha = 0.1\\).can plot cumulative average compare \nactual value integral know 1.want control error probability 0.95 can use Chebychev’s inequality\nsolve \\(\\varepsilon\\) using estimated variance.confidence bands provided \ncentral limit theorem typically accurate estimates actual uncertainty\nupper bounds provided Chebychev’s inequality.illustrate limitations Monte Carlo integration increase \\(\\alpha\\) \n\\(\\alpha = 0.4\\).sample path carefully selected pathological. Due \noccasional large values, typical sample path show occasional large jumps,\nvariance may easily grossly underestimated.\nFigure 5.5: Four sample paths cumulative average \\(\\alpha = 0.4\\).\nfair, choice standard multivariate normal distribution \nreference distribution large \\(\\alpha\\) problematic rather \nMonte Carlo integration importance sampling . However, high\ndimensions can difficult choose suitable distribution sample\n.difficulty specific integral due exponent integrand,\ncan become large positive \\(x_i \\simeq x_{-1}\\) enough coordinates.\nhappens rarely independent random variables, large values rare\nevents can, nevertheless, contribute notably integral. \nlarger \\(\\alpha \\(0, 1)\\) , pronounced problem occasional\nlarge values integrand. possible use importance sampling \ninstead sample distribution large values likely. \nparticular example need simulate distribution \nvariables dependent, pursue .","code":"\nh <- function(x, alpha = 0.1){ \n  p <- length(x)\n  tmp <- alpha * x[1:(p - 1)] \n  exp( - sum((tmp / 2 - x[2:p]) * tmp))\n}\nn <- 10000 # The number of random variables to generate\np <- 100   # The dimension of each random variable\nx <- matrix(rnorm(n * p), n, p)\nevaluations <- apply(x, 1, h)\nmu_hat <- cumsum(evaluations) / 1:n\nplot(mu_hat, pch = 20, xlab = \"n\")\nabline(h = 1, col = \"red\")\nplot(mu_hat, pch = 20, xlab = \"n\")\nabline(h = 1, col = \"red\")\nsigma_hat <- sd(evaluations)\nepsilon <- sigma_hat / sqrt((1:n) * 0.05)\nlines(1:n, mu_hat + epsilon)\nlines(1:n, mu_hat - epsilon)\nplot(mu_hat, pch = 20, xlab = \"n\")\nabline(h = 1, col = \"red\")\nlines(1:n, mu_hat + 2 * sigma_hat / sqrt(1:n))\nlines(1:n, mu_hat - 2 * sigma_hat / sqrt(1:n))\nevaluations <- apply(x, 1, h, alpha = 0.4)"},{"path":"mci.html","id":"network","chapter":"5 Monte Carlo integration","heading":"5.3 Network failure","text":"section consider serious application importance\nsampling. Though still toy example, can find exact solution,\nexample illustrates well type application want \napproximate small probability using Monte Carlo average. Importance sampling can\nincrease probability rare event result make \nvariance Monte Carlo average smaller.consider following network consisting ten nodes \nnodes connected.network computer network ten computers. different\nconnections (edges) may “fail” independently probability \\(p\\), \nask question: probability node 1 node 10 disconnected?can answer question computing integral indicator\nfunction, , computing sum\n\\[\\mu = \\sum_{x \\\\{0,1\\}^{18}} 1_B(x) f_p(x)\\]\n\\(f_p(x)\\) point probability \\(x\\), \\(x\\) representing 18 edges\ngraph fail, \\(B\\) represents set edges node 1 node 10 \ndisconnected. simulating edge failures can approximate sum \nMonte Carlo average.network nodes can represented graph adjacency matrix \\(\\) \n\\(A_{ij} = 1\\) edge \\(\\) \\(j\\) (\\(A_{ij} = 0\\)\notherwise).compute probability 1 10 disconnected Monte Carlo\nintegration, need sample (sub)graphs randomly removing \nedges. implemented using upper triangular part (symmetric)\nadjacency matrix.core implementation uses sample() function, \ncan sample replacement set \\(\\{0, 1\\}\\). vector ones\ncontains indices (upper triangular part ) adjacency matrix\ncontaining 1, positions replaced sampled values \nmatrix returned.fairly fast sample even large number random graphs way.second function implement checks network connectivity based \nupper triangular part adjacency matrix.\nrelies fact path node 1 node 10 consisting\n\\(k\\) edges \\((^k)_{1,10} > 0\\). see directly \npath needs consist least \\(k = 3\\) edges. Also, don’t need \ncheck paths \\(k = 9\\) edges contain one node\nmultiple times can thus shortened.obtain following estimate probability nodes 1 10 \ndisconnected using Monte Carlo integration.random approximation, report Monte Carlo\nestimate also confidence interval. Since estimate average \n0-1-variables, can estimate variance, \\(\\sigma^2\\), individual terms\nusing \\(\\sigma^2 = \\mu (1 - \\mu)\\). just well used \nempirical variance, give almost numerical value\n\\(\\hat{\\mu} (1 - \\hat{\\mu})\\), use latter estimator illustrate\n(good) estimator \\(\\sigma^2\\) can used estimating asymptotic\nvariance.estimated probability low 1 3000 simulated graphs\nnode 1 10 disconnected. suggests importance sampling\ncan useful sample probability distribution larger\nprobability edge failure.implement importance sampling note point probabilities\n(density w.r.t. counting measure) sampling 18 independent 0-1-variables\n\\(x = (x_1, \\ldots, x_{18})\\) \\(P(X_i = 0) = p\\) \\[f_p(x) = p^{18 - s} (1- p)^{s}\\]\\(s = \\sum_{=1}^{18} x_i\\). implementation,\nweights computed correspond using probability \\(p_0\\) (density \\(g = f_{p_0}\\))\ninstead \\(p\\), weights computed node 1\n10 disconnected.implementation uses formula\\[\nw(x) = \\frac{f_p(x)}{f_{p_0}(x)} = \\frac{p^{18 - s} (1- p)^{s}}{p_0^{18 - s} (1- p_0)^{s}} = \n\\left(\\frac{p}{p_0}\\right)^{18} \\left(\\frac{p_0 (1- p)}{p (1- p_0)}\\right)^s.\n\\]importance sampling estimate \\(\\mu\\) computed.obtain following confidence interval using \nempirical variance estimate \\(\\hat{\\sigma}^2\\).ratio estimated variances plain Monte Carlo estimate \nimportance sampling estimate isThus need around 11 times samples\nusing plain Monte Carlo integration compared importance sampling obtain precision.\nbenchmark show extra computing time importance sampling small compared\nreduction variance.\ntherefore worth coding effort used repeatedly, \none-computation.graph , fact, small enough complete enumeration thus computation\nexact solution. \\(2^{18} = 262,144\\) different networks \nnumber edges failing. \nsystematically walk possible combinations edges failing,\nuse function intToBits converts integer binary\nrepresentation integers 0 262,143. quick convenient\nway representing different fail non-fail combinations\nedges.probability nodes 1 10 disconnected can computed \nsum probabilities prob.number compared estimates computed . complete\ncomparison, used importance sampling edge fail probability\nranging 0.1 0.4, see Figure 5.6. results\nshow failure probability 0.2 close optimal terms \ngiving importance sampling estimate minimal variance. smaller\nvalues, event 1 10 become disconnected rare, \nlarger values importance weights become variable. choice 0.2\nstrikes good balance.\nFigure 5.6: Confidence intervals importance sampling estimates network nodes 1 10 disconnected independent edge failures probability 0.05. red line true probability computed complete enumeration.\n","code":"\nA  # Graph adjacency matrix##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n##  [1,]    0    1    0    1    1    0    0    0    0     0\n##  [2,]    1    0    1    0    0    1    0    0    0     0\n##  [3,]    0    1    0    0    0    1    1    1    0     1\n##  [4,]    1    0    0    0    1    0    0    1    0     0\n##  [5,]    1    0    0    1    0    0    0    1    1     0\n##  [6,]    0    1    1    0    0    0    1    0    0     1\n##  [7,]    0    0    1    0    0    1    0    0    0     1\n##  [8,]    0    0    1    1    1    0    0    0    1     0\n##  [9,]    0    0    0    0    1    0    0    1    0     1\n## [10,]    0    0    1    0    0    1    1    0    1     0\nsim_net <- function(Aup, p) {\n  ones <- which(Aup == 1)\n  Aup[ones] <- sample(\n    c(0, 1), \n    length(ones), \n    replace = TRUE,\n    prob = c(p, 1 - p)\n  )\n  Aup \n}\nAup <- A\nAup[lower.tri(Aup)] <- 0\nsystem.time(replicate(1e5, {sim_net(Aup, 0.5); NULL}))##    user  system elapsed \n##   1.854   0.975   2.297\ndiscon <- function(Aup) {\n  A <- Aup + t(Aup)\n  i <- 3\n  Apow <- A %*% A %*% A # A%^%3\n  while(Apow[1, 10] == 0 & i < 9) {\n    Apow <- Apow %*% A\n    i <- i + 1    \n  }\n  Apow[1, 10] == 0  # TRUE if nodes 1 and 10 not connected\n}\nseed <- 27092016\nset.seed(seed)\nn <- 1e5\ntmp <- replicate(n, discon(sim_net(Aup, 0.05)))\nmu_hat <- mean(tmp)\nmu_hat + 1.96 * sqrt(mu_hat * (1 - mu_hat) / n) * c(-1, 0, 1)## [1] 0.000226 0.000340 0.000454\nweights <- function(Aup, Aup0, p0, p) {\n  w <- discon(Aup0)\n  if (w) {\n    s <- sum(Aup0)\n    w <- (p / p0)^18 * (p0 * (1 - p) / (p * (1 - p0)))^s\n  }\n  as.numeric(w)\n}\nset.seed(seed)\ntmp <- replicate(n, weights(Aup, sim_net(Aup, 0.2), 0.2, 0.05))\nmu_hat_IS <- mean(tmp)\nmu_hat_IS + 1.96 * sd(tmp) / sqrt(n) * c(-1, 0, 1)## [1] 0.000262 0.000296 0.000330\nmu_hat * (1 - mu_hat) / var(tmp)## [1] 11.22476\nones <- which(Aup == 1)\nAtmp <- Aup\np <- 0.05\nprob <- numeric(2^18)\nfor(i in 0:(2^18 - 1)) {\n  on <- as.numeric(intToBits(i)[1:18])\n  Atmp[ones] <- on\n  if (discon(Atmp)) {\n    s <- sum(on)\n    prob[i + 1] <- p^(18 - s) * (1 - p)^s\n  }\n}\nsum(prob)## [1] 0.000288295"},{"path":"mci.html","id":"object-oriented-implementations","chapter":"5 Monte Carlo integration","heading":"5.3.1 Object oriented implementations","text":"Implementations algorithms handle graphs simulate graphs, Monte Carlo\ncomputations , can benefit using object oriented approach. \nend first implement -called constructor, function\ntakes adjacency matrix edge failure probability arguments \nreturns list class label network.use constructor function network() construct object class\nnetwork specific adjacency matrix.network object contains, addition p, two precomputed components:\nupper triangular part adjacency matrix; indices matrix\ncontaining 1.intention write two methods network class. method sim() \nsimulate graph edges failed, method failure() \nestimate probability node 1 10 disconnected Monte Carlo\nintegration. need define two corresponding generic functions.method simulation implemented function name\nsim.network.implemented using essentially implementation sim_net()\nexcept Aup p extracted components object x\ninstead arguments, ones extracted x well instead\ncomputed. One argue sim() method return object\nclass network — natural. However, need call \nconstructor full adjacency matrix, take time \nwant default. Thus simply return upper triangular\npart adjacency matrix sim().failure() method implements plain Monte Carlo integration well\nimportance sampling returns vector containing estimate \nwell 95% confidence interval. implementation relies \nalready implemented functions discon() weights().test implementation previously computed results.find numbers computed , thus object\noriented implementation concurs non-object oriented example.benchmark object oriented implementation measure \nrun time loss improvement due using objects.\nOne expect small computational overhead due method dispatching,\n, procedure R uses look appropriate sim() method\nobject class network. hand, sim() recompute\nones every time.benchmark, object oriented solution using sim() appears \nbit slower sim_net() despite latter recomputing ones, \ncan explained method dispatching taking order 1 microsecond\nbenchmark computations.taken object oriented approach, can also implement methods\nstandard generic functions, e.g. print function. generic\nfunction already exists, simply need implement method class network.print method now prints summary information graph instead\njust raw list.want work seriously graphs, likely want use\nexisting R package instead reimplementing many graph algorithms. One \npackages igraph, also illustrates well object oriented implementation\ngraph classes R.start constructing new graph object adjacency matrix.igraph package supports vast number graph computation, manipulation \nvisualization tools. illustrate igraph can used plot \ngraph can implement simulation method objects class igraph.can use plot(net), call plot method objects class igraph.\n, specify layout graph.layout specified makes easy recognize graph.use two functions igraph package implement simulation \nnew graph. need gsize(), gives number edges graph,\nneed delete_edges(), removes edges graph. Otherwise\nsimulation still based sample().Note method also called sim(), yet conflict \nmethod objects class network generic sim() function\ndelegate call correct method based objects class label.combine new sim() method igraphs plot method, can\nplot simulated graph.implementation using igraph turns little slower using\nmatrix representation alone sim_net().One also implement function testing nodes 1 10 \ndisconnected using shortest_paths() function, faster\nsimple matrix multiplications used discon() either. However, one\ncareful draw general conclusions . graph \nadmittedly small toy example, implemented solutions largely\nhandle particular graph.","code":"\nnetwork <- function(A, p) {\n  Aup <- A\n  Aup[lower.tri(Aup)] <- 0 \n  ones <- which((Aup == 1))\n  structure(\n    list(\n      A = A, \n      Aup = Aup, \n      ones = ones, \n      p = p\n    ), \n    class = \"network\"\n  )\n}\nmy_net <- network(A, p = 0.05)\nstr(my_net) ## List of 4\n##  $ A   : num [1:10, 1:10] 0 1 0 1 1 0 0 0 0 0 ...\n##  $ Aup : num [1:10, 1:10] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ ones: int [1:18] 11 22 31 41 44 52 53 63 66 73 ...\n##  $ p   : num 0.05\n##  - attr(*, \"class\")= chr \"network\"\nclass(my_net)## [1] \"network\"\nsim <- function(x, ...)\n  UseMethod(\"sim\")\nfailure <- function(x, ...)\n  UseMethod(\"failure\")\nsim.network <- function(x) {\n  Aup <- x$Aup\n  Aup[x$ones] <- sample(\n    c(0, 1), \n    length(x$ones), \n    replace = TRUE, \n    prob = c(x$p, 1 - x$p)\n  )\n  Aup\n}\nfailure.network <- function(x, n, p0 = NULL) {\n  if (is.null(p0)) { \n    # Plain Monte Carlo simulation\n    tmp <- replicate(n, discon(sim(x)))\n    mu_hat <- mean(tmp)\n    se <- sqrt(mu_hat * (1 - mu_hat) / n)\n  } else { \n    # Importance sampling\n    p <- x$p\n    x$p <- p0\n    tmp <- replicate(n, weights(x$Aup, sim(x), p0, p))\n    se <- sd(tmp) / sqrt(n)\n    mu_hat <- mean(tmp)\n  } \n  value <- mu_hat + 1.96 * se * c(-1, 0, 1)  \n  names(value) <- c(\"low\", \"estimate\", \"high\")\n  value\n}\nset.seed(seed)  # Resetting seed\nfailure(my_net, n)##      low estimate     high \n## 0.000226 0.000340 0.000454\nset.seed(seed)  # Resetting seed\nfailure(my_net, n, p0 = 0.2)##      low estimate     high \n## 0.000262 0.000296 0.000330\nmicrobenchmark(\n  sim_net(Aup, 0.05),\n  sim(my_net),\n  times = 1e4\n)## Unit: microseconds\n##                expr  min   lq mean median   uq   max neval\n##  sim_net(Aup, 0.05) 8.21 12.6 19.1   14.1 15.7 11529 10000\n##         sim(my_net) 9.22 14.0 19.3   15.2 16.8  5915 10000\nprint.network <- function(x) {\n  cat(\"#vertices: \", nrow(x$A), \"\\n\")\n  cat(\"#edges:\", sum(x$Aup), \"\\n\")\n  cat(\"p = \", x$p, \"\\n\")\n}\nmy_net  # Implicitly calls 'print'## #vertices:  10 \n## #edges: 18 \n## p =  0.05\nnet <- graph_from_adjacency_matrix(A, mode = \"undirected\")\nclass(net)## [1] \"igraph\"\nnet # Illustrates the print method for objects of class 'igraph'## IGRAPH ea76a4a U--- 10 18 -- \n## + edges from ea76a4a:\n##  [1] 1-- 2 1-- 4 1-- 5 2-- 3 2-- 6 3-- 6 3-- 7 3-- 8 3--10 4-- 5 4-- 8 5-- 8 5-- 9 6-- 7 6--10\n## [16] 7--10 8-- 9 9--10\n# You can generate a layout ...\nnet_layout <- layout_(net, nicely())\n# ... or you can specify one yourself \nnet_layout <- matrix(\n  c(-20,  1,\n    -4,  3,\n    -4,  1,\n    -4, -1,\n    -4, -3,\n     4,  3,\n     4,  1,\n     4, -1,\n     4, -3,\n     20,  -1),\n  ncol = 2, nrow = 10, byrow = TRUE)\nplot(net, layout = net_layout, asp = 0)\nsim.igraph <- function(x, p) {\n  deledges <- sample(\n    c(TRUE, FALSE), \n    gsize(x),        # 'gsize()' returns the number of edges \n    replace = TRUE, \n    prob = c(p, 1 - p)\n  )\n  delete_edges(x, which(deledges))\n}\nplot(sim(net, 0.25), layout = net_layout, asp = 0)\nsystem.time(replicate(1e5, {sim(net, 0.05); NULL}))##    user  system elapsed \n##   6.092   7.149  13.955"},{"path":"mci.html","id":"particle-filters","chapter":"5 Monte Carlo integration","heading":"5.4 Particle filters","text":"","code":""},{"path":"four-examples.html","id":"four-examples","chapter":"6 Four Examples","heading":"6 Four Examples","text":"chapter treats four examples non-trivial statistical models \ndetail. parametric models, central computational\nchallenge fit models data via (penalized) likelihood\nmaximization. actual optimization algorithms implementations\ntopics Chapters 7 8.\nfocus chapter structure statistical models\nprovide necessary background later chapters.Statistical models come forms shapes, possible \ntake general abstract mathematical approach; statistical\nmodels parametrized families probability distributions. \nsay anything interest, need structure structure\nparameter set, properties parametrized distributions,\nproperties mapping parameter set \ndistributions. specific model ample structure\noften also overwhelming amount irrelevant details\ndistracting clarifying. intention \nfour examples treated illustrate breath \nstatistical models share important\nstructures without getting lost wasteland abstractions.one emphasize single abstract idea \ntheoretical value well practical importance, \nidea exponential families. Statistical models \nexponential families much structure \ngeneral theory provides number results details\npractical value individual models. Exponential\nfamilies exemplary statistical models, widely\nused models data, central building\nblocks complicated models data. reason,\ntreatment examples preceded treatment \nexponential families.","code":""},{"path":"four-examples.html","id":"exp-fam","chapter":"6 Four Examples","heading":"6.1 Exponential families","text":"section introduces exponential families concise way. \ncrucial observation log-likelihood concave, \ncan derive general formulas derivatives. \nimportant optimization algorithms developed\nlater computing maximum-likelihood estimates \nanswering standard asymptotic inference questions.exponential families extremely well behaved \nmathematical well computational viewpoint, may\ninadequate modeling data cases. \ntypical practical problem heterogeneous variation data\nbeyond can captured single exponential family.\nfairly common technique build exponential family\nmodel observed variables well latent variables.\nlatent variables serve purpose modeling \nheterogeneity. resulting model observed variables \nconsequently marginalization exponential family, \ngenerally exponential family many ways\nless well behaved. nevertheless possible exploit exponential\nfamily structure underlying marginalized model many\ncomputations statistical importance. EM-algorithm \ntreated Chapter 8 one particularly good example, \nBayesian computations can similar ways exploit structure.","code":""},{"path":"four-examples.html","id":"full-exponential-families","chapter":"6 Four Examples","heading":"6.1.1 Full exponential families","text":"section consider statistical models abstract\nproduct sample space\n\\[\\mathcal{Y} = \\mathcal{Y}_1 \\times \\ldots \\times \\mathcal{Y}_m.\\]\ninterested models \nobservations \\(y_1 \\\\mathcal{Y}_1, \\ldots, y_m \\\\mathcal{Y}_m\\)\nindependent necessarily identically distributed.exponential family defined terms two ingredients:maps \\(t_j : \\mathcal{Y}_j \\\\mathbb{R}^p\\) \\(j = 1, \\ldots, m\\),non-trivial \\(\\sigma\\)-finite measures \\(\\nu_j\\) \\(\\mathcal{Y}_j\\) \\(j = 1, \\ldots, m\\).maps \\(t_j\\) called sufficient statistics, terms\nbase measures \\(\\nu_j\\) define\n\\[\\varphi_j(\\theta) = \\int e^{\\theta^T t_j(u)} \\nu_j(\\mathrm{d}u).\\]\nfunctions well defined functions\n\\[\\varphi_j : \\mathbb{R}^p \\(0,\\infty].\\]\ndefine\n\\[\\Theta_j = \\mathrm{int}(\\{ \\theta \\\\mathbb{R}^p \\mid \\varphi_j(\\theta) < \\infty \\}),\\]\ndefinition open set . can shown\n\\(\\Theta_j\\) convex \\(\\varphi_j\\) log-convex function.\nDefining\n\\[\\Theta = \\bigcap_{j=1}^m \\Theta_j,\\]\n\\(\\Theta\\) likewise open convex, define exponential family\ndistributions parametrized \\(\\theta \\\\Theta\\) densities\\[\\begin{equation}\nf(\\mathbf{y} \\mid \\theta) = \\prod_{j=1}^m \\frac{1}{\\varphi_j(\\theta)} e^{\\theta^T t_j(y_j)} = e^{\\theta^T \\sum_{j=1}^m t_j(y_j) - \\sum_{j=1}^m \\log \\varphi_j(\\theta)}, \\quad \\mathbf{y} \\\\mathcal{Y},\n\\tag{6.1}\n\\end{equation}\\]w.r.t. \\(\\otimes_{j=1}^m \\nu_j\\). \ncase \\(\\Theta = \\emptyset\\) interest, thus assume\nparameter set \\(\\Theta\\) non-empty. parameter \\(\\theta\\) called\ncanonical parameter \\(\\Theta\\) canonical parameter space.\nmay also say exponential family canonically parametrized \n\\(\\theta\\). important realize exponential family may come\nnon-canonical parametrization doesn’t reveal right away \nexponential family. Thus bit work needed show\nparametrized family distributions can, indeed, reparametrized\nexponential family. non-canonical parametrization, \nfamily example curved exponential family defined .Example 6.1  von Mises distributions \\(\\mathcal{Y} = (-\\pi, \\pi]\\) form exponential family\n\\(m = 1\\). sufficient statistic \\(t_1 : (-\\pi, \\pi] \\mapsto \\mathbb{R}^2\\)\n\n\\[t_1(y) = \\left(\\begin{array}{c} \\cos(y) \\\\ \\sin(y) \\end{array}\\right),\\]\n\n\\[\\varphi(\\theta) = \\int_{-\\pi}^{\\pi} e^{\\theta_1 \\cos(u) + \\theta_2 \\sin(u)} \\mathrm{d}u < \\infty\\]\n\\(\\theta = (\\theta_1, \\theta_2)^T \\\\mathbb{R}^2\\). Thus\ncanonical parameter space \\(\\Theta = \\mathbb{R}^2\\).mentioned Section 1.2.1, function \\(\\varphi(\\theta)\\) can \nexpressed terms modified Bessel function, doesn’t \nexpression terms elementary functions. Likewise Section 1.2.1,\nalternative parametrization (polar coordinates) given;\n\\[(\\kappa, \\mu) \\mapsto \\theta = \\kappa \\left(\\begin{array}{c} \\cos(\\mu) \\\\ \\sin(\\mu) \\end{array}\\right)\\]\nmaps \\([0,\\infty) \\times (-\\pi, \\pi]\\) onto \\(\\Theta\\). von Mises\ndistributions form curved exponential family \n\\((\\kappa, \\mu)\\)-parametrization, parametrization several problems.\nFirst, \\(\\mu\\) parameter identifiable \\(\\kappa = 0\\), \nreflected fact reparametrization one--one map.\nSecond, parameter space open, can quite nuisance\ne.g. maxmimum-likelihood estimation. circumvent problems restricting\nattention \\((\\kappa, \\mu) \\(0,\\infty) \\times (-\\pi, \\pi)\\), \nmiss distributions exponential family –\nnotably uniform distribution corresponding \\(\\theta = 0\\). conclusion,\ncanonical parametrization family distributions \nexponential family preferable mathematical computational reasons.Example 6.2  family Gaussian distributions \\(\\mathbb{R}\\) example exponential\nfamily defined \\(m = 1\\) \\(\\mathcal{Y} = \\mathbb{R}\\).\ndensity \\(\\mathcal{N}(\\mu, \\sigma^2)\\) distribution \n\\[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right) = \n  \\frac{1}{\\sqrt{\\pi}} \\exp\\left(\\frac{\\mu}{\\sigma^2} y - \\frac{1}{2\\sigma^2} y^2 - \\frac{\\mu^2}{2\\sigma^2} - \n                                     \\frac{1}{2}\\log (2\\sigma^2) \\right).\\]\nLetting base measure \\(\\nu_1\\) Lebesgue measure scaled \\(1/\\sqrt{\\pi}\\),\n\\(t_1 : \\mathbb{R} \\mapsto \\mathbb{R}^2\\) \n\\[t_1(y) = \\left(\\begin{array}{c} y \\\\ - y^2 \\end{array}\\right)\\]\nidentify family distributions exponential family canonical\nparameter\n\\[\\theta = \\left(\\begin{array}{c} \\frac{\\mu}{\\sigma^2} \\\\ \\frac{1}{2 \\sigma^2} \\end{array}\\right).\\]can express mean variance terms \\(\\theta\\) \n\\[\\sigma^2 = \\frac{1}{2\\theta_2} \\quad \\text{} \\quad \\mu = \\frac{\\theta_1}{2\\theta_2},\\]\nfind \n\\[\\log \\varphi_1(\\theta) = \\frac{\\mu^2}{2\\sigma^2} + \\frac{1}{2} \\log(2 \\sigma^2) = \n  \\frac{\\theta_1^2}{4\\theta_2} - \\frac{1}{2} \\log \\theta_2.\\]note reparametrization \\((\\mu, \\sigma^2) \\mapsto \\theta\\) maps\n\\(\\mathbb{R} \\times (0,\\infty)\\) bijectively onto open set \\(\\mathbb{R} \\times (0,\\infty)\\),\nformula \\(\\log \\varphi_1(\\theta)\\) holds set.\nnatural ask canonical parameter space actually larger \nexponential family. , \\(\\varphi_1(\\theta) < \\infty\\) \\(\\theta_2 \\leq 0\\)?\nend observe \\(\\theta_2 \\leq 0\\)\n\\[\\varphi_1(\\theta) = \\frac{1}{\\sqrt{2\\pi}} \\int e^{\\theta_1 u - \\theta_2 \\frac{u^2}{2}} \\mathrm{d}u\n\\geq \\frac{1}{\\sqrt{2\\pi}} \\int e^{\\theta_1 u} \\mathrm{d} u = \\infty,\\]\nconclude \n\\[\\Theta = \\mathbb{R} \\times (0,\\infty).\\]family Gaussian distributions example family distributions\nwhose commonly used parametrization terms mean variance differs \ncanonical parametrization exponential family. mean \nvariance easy interpret, terms mean\nvariance, Gaussian distributions form curved exponential family. \nmathematical computational purposes canonical parametrization preferable.general exponential families may seem restrictive \nsufficient statistics, \\(t_j\\), take values \n\\(p\\)-dimensional space, marginal distributions share common\nparameter vector \\(\\theta\\). , however, restriction. Say \ntwo distributions sufficient statistics \\(\\tilde{t}_1 : \\mathcal{Y}_1 \\\\mathbb{R}^{p_1}\\)\n\\(\\tilde{t}_2 : \\mathcal{Y}_2 \\\\mathbb{R}^{p_2}\\) corresponding\nparameters \\(\\theta_1\\) \\(\\theta_2\\), construct\n\\[t_1(y_1) = \\left(\\begin{array}{c} \\tilde{t}_1(y_1) \\\\ 0 \\end{array}\\right) \\quad \n\\text{} \\quad t_2(y_2) = \\left(\\begin{array}{c} 0 \\\\ \\tilde{t}_2(y_2) \\end{array}\\right).\\]\nNow \\(t_1\\) \\(t_2\\) map \\(\\mathbb{R}^{p}\\) \\(p = p_1 + p_2\\), \ncan bundle parameters together vector\n\\[\\theta = \\left(\\begin{array}{c} \\theta_1 \\\\ \\theta_2 \\end{array}\\right) \\\\mathbb{R}^p.\\]\nClearly, construction can generalized number distributions\nallows us always assume common parameter space. sufficient\nstatistics take care selecting\ncoordinates parameter vector used \nparticular marginal distribution.","code":""},{"path":"four-examples.html","id":"bayes-net","chapter":"6 Four Examples","heading":"6.1.2 Exponential family Bayesian networks","text":"exponential family defined parametrized family distributions\n\\(\\mathcal{Y}\\) independent variables. , joint density\n(6.1) factorizes w.r.t. product measure. Without really\nchanging notation assumption can relaxed considerably.sufficient statistic \\(t_j\\), instead fixed map, \nallowed depend \\(y_1, \\ldots, y_{j-1}\\), \\(f(\\mathbf{y} \\mid \\theta)\\) \ndefined (6.1)\nstill joint density. difference factor\n\\[f_j(y_j \\mid y_1, \\ldots, y_{j-1}, \\theta) = e^{\\theta^T t_j(y_j) - \\log \\varphi_j(\\theta)}\\]\nnow conditional density \\(y_j\\) given \\(y_1, \\ldots, y_{j-1}\\).\nnotation let dependence \\(t_j\\) \\(y_1, \\ldots, y_{j-1}\\)\nimplicit affect abstract theory. concrete\nexample though, clear \\(t_j\\) actually depends upon , \nnone variables. Note \\(\\varphi_j\\) may now also depend\nupon data \\(y_1, \\ldots, y_{j-1}\\).observation powerful. allows us develop unified\napproach based exponential families majority statistical\nmodels applied practice. models consider make two essential\nassumptionsthe variables model can ordered \\(y_j\\) \ndepends \\(y_1, \\ldots, y_{j-1}\\) \\(j = 1, \\ldots, m\\),conditional distributions form exponential families ,\nconditioning variables entering \\(t_j\\)-maps.first assumptions equivalent joint distribution\nBayesian network, , distribution whose density factorizes\naccording directed acyclic graph. includes time series models\nhierarchical models. second assumption restrictive, \ncommon practice applied work. Moreover, many standard statistical\nmodels univariate discrete \ncontinuous variables , fact, exponential families, building\njoint distribution Bayesian network via conditional\nbinomial, Poisson, beta, Gamma Gaussian distributions among others\nrather flexible framework, yet \nresult model density factorizes (6.1).Bayesian networks large interesting subject , unfair\ngloss details. One main points many\ncomputations possible develop efficient algorithms exploiting \ngraph structure. seminal paper Lauritzen Spiegelhalter (1988) demonstrated \ncomputation conditional distributions. mere fact \nvariables can ordered way aligns \nvariables depend useful, can \nmany ways , just specifying ordering ignores\nimportant details graph. , however, beyond scope \nbook get graph algorithms required thorough\ngeneral treatment Bayesian networks.","code":""},{"path":"four-examples.html","id":"exp-fam-deriv","chapter":"6 Four Examples","heading":"6.1.3 Likelihood computations","text":"simplify notation introduce\n\\[t(\\mathbf{y}) = \\sum_{j=1}^m t_j(y_j),\\]\nrefer sufficient statistic, \n\\[\\kappa(\\theta) = \\sum_{j=1}^m \\log \\varphi_j(\\theta),\\]\nconvex \\(C^{\\infty}\\)-function \\(\\Theta\\).observed \\(\\mathbf{y} \\\\mathcal{Y}\\) evident log-likelihood\nexponential family specified (6.1) \n\\[\\ell(\\theta) = \\log f(\\mathbf{y} \\mid \\theta) = \\theta^T t(\\mathbf{y}) - \\kappa(\\theta).\\]\nfollows gradient log-likelihood \n\\[\\nabla \\ell(\\theta) = t(\\mathbf{y}) - \\nabla \\kappa(\\theta)\\]\nHessian \n\\[D^2 \\ell(\\theta) = - D^2 \\kappa(\\theta),\\]\nalways negative semidefinite. maximum-likelihood estimator exists\nsolution score equation\n\\[t(\\mathbf{y}) = \\nabla \\kappa(\\theta),\\]\nunique solution, \\(\\hat{\\theta}\\), \n\\((\\hat{\\theta}) = D^2 \\kappa(\\hat{\\theta})\\) positive definite. \ncall \\((\\hat{\\theta})\\) observed Fisher information.Note independence assumption,\n\\[\\nabla \\log \\varphi_j(\\theta) = \\frac{1}{\\varphi_j(\\theta)} \\int t_j(u) e^{\\theta^T t_j(u)} \\nu_i(\\mathrm{d} u) = E_{\\theta}(t_j(Y)), \\]\nmeans score equation can expressed \n\\[t(\\mathbf{y}) = \\sum_{j=1}^m E_{\\theta}(t_j(Y)).\\]\nBayesian network setup \\(\\nabla \\log \\varphi_j(\\theta) = E_{\\theta}(t_j(Y) \\mid Y_1, \\ldots, Y_{j-1}),\\)\nscore equation \n\\[t(\\mathbf{y}) = \\sum_{j=1}^m E_{\\theta}(t_j(Y) \\mid y_1, \\ldots, y_{j-1}),\\]\nbit complicated right-hand-side depends \nobservations.definition exponential family Section 6.1 encompasses\nsituation \\(y_1, \\ldots, y_m\\) ..d. taking \\(t_j\\) \nindependent \\(j\\). case, \\(\\kappa(\\theta) = m \\kappa_1(\\theta)\\),\nscore equation can rewritten \n\\[\\frac{1}{m} \\sum_{j=1}^m t_1(y_j) = \\kappa_1(\\theta),\\]\nFisher information becomes\n\\[(\\hat{\\theta}) = m D^2 \\kappa_1(\\hat{\\theta}).\\]However, point general formulation \nincludes regression models, , via Bayesian networks extension\n, models dependence structures. , course, \n..d. replications \\(\\mathbf{y}_1, \\ldots, \\mathbf{y}_n\\) \nentire \\(m\\)-dimensional vector \\(\\mathbf{y}\\), get\nscore equation\n\\[\\frac{1}{n} \\sum_{=1}^n t(\\mathbf{y}_i) = \\kappa(\\theta),\\]\ncorresponding Fisher information\n\\[(\\hat{\\theta}) = n D^2 \\kappa(\\hat{\\theta}).\\]","code":""},{"path":"four-examples.html","id":"curved-exponential-families","chapter":"6 Four Examples","heading":"6.1.4 Curved exponential families","text":"curved exponential family consists exponential family together \n\\(C^2\\)-map \\(\\theta : \\Psi \\\\Theta\\) set \\(\\Psi \\subseteq \\mathbb{R}^q\\).\nset \\(\\Psi\\) provides parametrization\nsubset \\(\\theta(\\Psi) \\subseteq \\Theta\\) full exponential family,\nlog-likelihood \\(\\psi\\)-parameter \n\\[\\ell(\\psi) = \\theta(\\psi)^T t(\\mathbf{y}) - \\kappa(\\theta(\\psi)).\\]\ngradient\n\\[\\nabla \\ell(\\psi) = D \\theta(\\psi)^T t(\\mathbf{y}) - \\nabla \\kappa(\\theta(\\psi)) D \\theta(\\psi)\\]\nHessian\n\\[D^2 \\ell(\\psi) =  \\sum_{k=1}^p D^2\\theta_k(\\psi) (t(\\mathbf{y})_k - \\partial_k \\kappa(\\theta(\\psi))) -   \nD \\theta(\\psi)^T D^2 \\kappa(\\theta(\\psi)) D \\theta(\\psi).\\]","code":""},{"path":"four-examples.html","id":"multinomial-models","chapter":"6 Four Examples","heading":"6.2 Multinomial models","text":"multinomial model model probability distributions \nfinite set \\(\\mathcal{Y} = \\{1, \\ldots, K\\}\\). model parametrized\nsimplex\n\\[\\Delta_K = \\left\\{(p_1, \\ldots, p_K)^T \\\\mathbb{R}^K \\mid p_k \\geq 0, \\sum_{k=1}^K p_k = 1\\right\\}.\\]\ndistributions parametrized relative interior \\(\\Delta_K\\) form\nexponential family parametrization\n\\[p_k = \\frac{e^{\\theta_k}}{\\sum_{l=1}^K e^{\\theta_l}} \\(0,1)\\]\n\\((\\theta_1, \\ldots, \\theta_K)^T \\\\mathbb{R}^K.\\) ,\nsufficient statistic \\(k \\mapsto (\\delta_{1k}, \\ldots, \\delta_{Kk})^T \\\\mathbb{R}^{K-1}\\)\n(\\(\\delta_{lk} \\\\{0, 1\\}\\) Kronecker delta 1 \n\\(l = k\\)), \n\\[\\varphi(\\theta_1, \\ldots, \\theta_K) = \\sum_{l=1}^K e^{\\theta_l}.\\]\ncall exponential family parametrization symmetric parametrization.\ncanonical parameter symmetric parametrization identifiable,\nresolve lack identifiability\ntradition fixing last parameter \\(\\theta_K = 0\\). \nresults canonical parameter \\(\\theta = (\\theta_1, \\ldots, \\theta_{K-1})^T \\\\mathbb{R}^{K-1},\\)\nsufficient statistic \\(t_1(k) = (\\delta_{1k}, \\ldots, \\delta_{(K-1)k})^T \\\\mathbb{R}^p\\),\\[\\varphi(\\theta) = 1 + \\sum_{l=1}^{K-1} e^{\\theta_l}\\]probabilities\\[p_k = \\left\\{\\begin{array}{ll} \\frac{e^{\\theta_k}}{1 + \\sum_{l=1}^{K-1} e^{\\theta_l}}  & \\quad \\text{} k = 1, \\ldots, K-1 \\\\  \\frac{1}{1 + \\sum_{l=1}^{K-1} e^{\\theta_l}} & \\quad \\text{} k = K. \\end{array}\\right.\\]see parametrization \\(p_k = e^{\\theta_k}p_K\\) \\(k = 1, \\ldots, K-1\\),\nprobability last element \\(K\\) acts baseline reference probability,\nfactors \\(e^{\\theta_k}\\) act multiplicative modifications \nbaseline. Moreover,\n\\[\\frac{p_k}{p_l} = e^{\\theta_k - \\theta_l},\\]\nindependent chosen baseline.special case \\(K = 2\\) two elements \\(\\mathcal{Y}\\) often given\nlabels \\(1\\) \\(2\\). common labels \\(\\{0, 1\\}\\) \\(\\{-1, 1\\}\\).\nuse \\(0\\)-\\(1\\)-labels convention use \\(p_0\\) baseline \nthus\n\\[p_1 = \\frac{e^{\\theta}}{1 + e^{\\theta}} = e^{\\theta} p_0 = e^{\\theta} (1 - p_1).\\]\nparametrized \\(\\theta \\\\mathbb{R}\\). function \\(\\theta\\) \nknown logistic function, parametrization probability\ndistributions \\(\\{0,1\\}\\) often referred logistic model. \nsee directly \n\\[\\theta = \\log \\frac{p_1}{1 - p_1}\\]\nlog-odds.use \\(\\pm 1\\)-labels, alternative exponential family parametrization \n\\[p_k = \\frac{e^{k\\theta}}{e^\\theta + e^{-\\theta}}\\]\n\\(\\theta \\\\mathbb{R}\\) \\(k \\\\{-1, 1\\}\\). gives symmetric treatment two\nlabels retaining identifiability...d. observations multinomial distribution find\nlog-likelihood \\[\\begin{align*}\n\\ell(\\theta) & = \\sum_{=1}^n \\sum_{k=1}^K \\delta_{k y_i} \\log(p_k(\\theta)) \\\\\n& = \\sum_{k=1}^K \\underbrace{\\sum_{=1}^n \\delta_{k y_i}}_{= n_k} \\log(p_k(\\theta)) \\\\ \n& = \\theta^T  \\mathbf{n} - n \\log \\left(1 + \\sum_{l=1}^{K-1} e^{\\theta_l} \\right).\n\\end{align*}\\]\\(\\mathbf{n} = (n_1, \\ldots, n_K)^T = \\sum_{=1}^n t(y_i)\\) sufficient\nstatistic, simply tabulation times different elements \n\\(\\{1, \\ldots, K\\}\\) observed.","code":""},{"path":"four-examples.html","id":"pep-moth","chapter":"6 Four Examples","heading":"6.2.1 Peppered Moths","text":"example color peppered Moth (Birkemåler Danish).\ncolor moth \nprimarily determined one gene occur three different alleles denoted C,\nT. alleles ordered terms dominance C > > T. Moths genotype\nincluding C dark. Moths\ngenotype TT light colored. Moths genotypes II mottled.\nThus total six different genotypes (CC, CI, CT, II, TT) \nthree different phenotypes (black, mottled, light colored).peppered moth provided early demonstration evolution\n19th century England, light colored moth outnumbered dark\ncolored variety. dark color became dominant due increased\npollution, trees darkened soot, reason selective\nadvantage. nice collection\nmoth different colors Danish Zoological Museum, \nexplanation role played understanding evolution.denote allele frequencies \\(p_C\\), \\(p_I\\), \\(p_T\\) \\(p_C + p_I + p_T = 1.\\)\nAccording Hardy-Weinberg principle,\ngenotype frequencies \\[p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2,  2p_Ip_T, p_T^2.\\]observe genotypes, complete multinomial log-likelihood \\[\\begin{align*}\n & 2n_{CC} \\log(p_C) + n_{CI} \\log (2 p_C p_I) + n_{CT} \\log(2 p_C p_I) \\\\\n& \\ \\ + 2 n_{II} \\log(p_I) + n_{} \\log(2p_I p_T) + 2 n_{TT} \\log(p_T) \\\\\n& = 2n_{CC} \\log(p_C) + n_{CI} \\log (2 p_C p_I) + n_{CT} \\log(2 p_C p_I) \\\\\n& \\ \\ + 2 n_{II} \\log(p_I) + n_{} \\log(2p_I (1 - p_C - p_I)) + 2 n_{TT} \\log(1 - p_C - p_I). \n\\end{align*}\\]log-likelihood given terms genotype counts two probability parameters\n\\(p_C, p_I \\geq 0\\) \\(p_C + p_I \\leq 1\\), interior \nparameter set model curved exponential family.Collecting moths determining color , however, identify phenotype,\ngenotype. Thus observe \\((n_C, n_T, n_I)\\), \n\\[n = \\underbrace{n_{CC} + n_{CI} + n_{CT}}_{= n_C} + \n\\underbrace{n_{} + n_{II}}_{=n_I} + \\underbrace{n_{TT}}_{=n_T}.\\]maximum-likelihood estimation parameters model \nobservation \\((n_C, n_T, n_I)\\), need likelihood,\n, likelihood marginal distribution observed\nvariables.peppered Moth example example cell collapsing multinomial model.\ngeneral, let \\(A_1 \\cup \\ldots \\cup A_{K_0} = \\{1, \\ldots, K\\}\\) partition let\n\\[M : \\mathbb{N}_0^K \\\\mathbb{N}_0^{K_0}\\]\nmap given \n\\[M((n_1, \\ldots, n_K))_j = \\sum_{k \\A_j} n_k.\\]\\(Y \\sim \\textrm{Mult}(p, n)\\) \\(p = (p_1, \\ldots, p_K)\\) \n\\[X = M(Y) \\sim \\textrm{Mult}(M(p), n).\\]\n\\(\\theta \\mapsto p(\\theta)\\) parametrization cell probabilities\nlog-likelihood collapsed multinomial model generally\\[\\begin{equation}\n\\ell(\\theta) = \\sum_{j = 1}^{K_0} x_j \\log (M(p(\\theta))_j) = \\sum_{j = 1}^{K_0} x_j \\log \\left(\\sum_{k \\A_j} p_k(\\theta)\\right).\n\\tag{6.2} \n\\end{equation}\\]peppered Moths, \\(K = 6\\) corresponding six genotypes, \\(K_0 = 3\\) \npartition corresponding phenotypes \n\\[\\{1, 2, 3\\} \\cup \\{4, 5\\} \\cup \\{6\\} = \\{1, \\ldots, 6\\},\\]\n\n\\[M(n_1, \\ldots, n_6) = (n_1 + n_2 + n_3, n_4 + n_5, n_6).\\]terms \\((p_C, p_I)\\) parametrization, \\(p_T = 1 - p_C - p_I\\) \n\\[p = (p_C^2, 2p_Cp_I, 2p_Cp_T, p_I^2,  2p_Ip_T, p_T^2).\\]Hence\n\\[M(p) = (p_C^2 + 2p_Cp_I + 2p_Cp_T, p_I^2 +2p_Ip_T, p_T^2).\\]log-likelihood \\[\\begin{align}\n\\ell(p_C, p_I) & = n_C \\log(p_C^2 + 2p_Cp_I + 2p_Cp_T) + n_I \\log(p_I^2 +2p_Ip_T) + n_T \\log (p_T^2).\n\\end{align}\\]can implement log-likelihood problem specific way.\nNote parameter constraints encoded via return value \\(\\infty\\).possible use optim R just implementation \ncompute maximum-likelihood estimate allele parameters.optim function uses algorithm called Nelder-Mead\ndefault algorithm relies log-likelihood evaluations . \nslow fairly robust, though bit thought go initial\nparameter choice.log-likelihood implemented multinomial cell\ncollapsing via M two problem specific arguments \nloglik function. One vector specifying\ngrouping structure collapsing. function\ndetermines \nparametrization maps parameter optimizing \ncell probabilities. implementation assumed\nprob function addition encodes parameter constraints\nreturning NULL parameter constraints violated.function prob implemented specifically peppered moths\nfollows.test new implementation gives result \noptimized using problem specific implementation.found estimate parameters, can compute prediction\nunobserved genotype counts phenotype counts using\nconditional distribution genotypes given phenotypes.\nstraightforward conditional distribution \\(Y_{A_j} = (Y_k)_{k \\A_j}\\),\nconditionally \\(X\\) also multinomial distribution;\n\\[Y_{A_j} \\mid X = x \\sim \\textrm{Mult}\\left( \\frac{p_{A_j}}{M(p)_j}, x_j \\right).\\]\nprobability parameters simply \\(p\\) restricted \\(A_j\\) renormalized\nprobability vector. Observe gives\n\\[E (Y_k \\mid X = x) = \\frac{x_j p_k}{M(p)_j}\\]\n\\(k \\A_j\\). Using estimated parameters M function implemented\n, can compute prediction genotype counts follows.interest , computing conditional expectations\nalso central EM algorithm Chapter 8.","code":"\n## par = c(pC, pI), pT = 1 - pC - pI\n## x is the data vector of length 3 of counts \nloglik <- function(par, x) {\n  pT <- 1 - par[1] - par[2]\n  \n  if (par[1] > 1 || par[1] < 0 || par[2] > 1 \n      || par[2] < 0 || pT < 0)\n    return(Inf)\n  \n  PC <- par[1]^2 + 2 * par[1] * par[2] + 2 * par[1] * pT\n  PI <- par[2]^2 + 2 * par[2] * pT\n  PT <- pT^2\n  ## The function returns the negative log-likelihood \n  - (x[1] * log(PC) + x[2] * log(PI) + x[3]* log(PT))\n}\noptim(c(0.3, 0.3), loglik, x = c(85, 196, 341))## $par\n## [1] 0.07084643 0.18871900\n## \n## $value\n## [1] 600.481\n## \n## $counts\n## function gradient \n##       71       NA \n## \n## $convergence\n## [1] 0\n## \n## $message\n## NULL\noptim(c(0, 0), loglik, x = c(85, 196, 341))## Error in optim(c(0, 0), loglik, x = c(85, 196, 341)): function cannot be evaluated\nat initial parameters```\n\nStarting the algorithm in a boundary value where the negative log-likelihood attains\nthe value $\\infty$ does not work. \n\nThe computations can beneficially be implemented in greater \ngenerality. The function `M` sums the cells that are collapsed, \nwhich has to be specified by the `group` argument.\n\n\n```r\nM <- function(x, group)\n  as.vector(tapply(x, group, sum))\nloglik <- function(par, x, prob, group, ...) {\n  p <- prob(par)\n  if(is.null(p)) return(Inf)\n  - sum(x * log(M(prob(par), group)))\n}\nprob <- function(p) {\n  p[3] <- 1 - p[1] - p[2]\n  if (p[1] > 1 || p[1] < 0 || p[2] > 1 || p[2] < 0 || p[3] < 0)\n    return(NULL)\n  c(p[1]^2, 2 * p[1] * p[2], 2* p[1] * p[3], \n    p[2]^2, 2 * p[2] * p[3], p[3]^2)\n}\noptim(c(0.3, 0.3), loglik, x = c(85, 196, 341), \n      prob = prob, group = c(1, 1, 1, 2, 2, 3))## $par\n## [1] 0.07084643 0.18871900\n## \n## $value\n## [1] 600.481\n## \n## $counts\n## function gradient \n##       71       NA \n## \n## $convergence\n## [1] 0\n## \n## $message\n## NULL\nx <- c(85, 196, 341)\ngroup <- c(1, 1, 1, 2, 2, 3)\np <- prob(c(0.07084643, 0.18871900))\nx[group] * p / M(p, group)[group]## [1]   3.121549  16.630211  65.248241  22.154520 173.845480 341.000000"},{"path":"four-examples.html","id":"regression","chapter":"6 Four Examples","heading":"6.3 Regression models","text":"Regression models models one variable, called response,\nconditionally one variables, called predictors.\nTypically use models assume conditional independence responses\ngiven predictors, particularly convenient class regression\nmodels based exponential families.let \\(y_i \\\\mathbb{R}\\) denote \\(\\)th response \\(x_i \\\\mathbb{R}^p\\)\n\\(\\)th predictor can consider exponential family models joint density\n\\[f(\\mathbf{y} \\mid \\mathbf{X}, \\beta) = \\prod_{=1}^n e^{\\theta^T t_i(y_i \\mid x_i) - \\log \\varphi_i(\\theta)}\\]\nsuitably chosen base measures. \n\\[\\mathbf{X} = \\left( \\begin{array}{c} x_1^T \\\\ x_2^T \\\\ \\vdots \\\\ x_{n-1}^T \\\\ x_n^T \\end{array} \\right)\\]\ncalled model matrix \\(n \\times p\\) matrix.Example 6.3  \\(y_i \\\\mathbb{N}_0\\) counts often use Poisson regression model\npoint probabilities (density w.r.t. counting measure)\n\\[p_i(y_i \\mid x_i) = e^{-\\mu(x_i)} \\frac{\\mu(x_i)^{y_i}}{y_i!}.\\]\nmean depends predictors log-linear way, \\(\\log(\\mu(x_i)) = x_i^T \\beta\\),\n\n\\[p_i(y_i \\mid x_i) = e^{\\beta^T x_i y_i - \\exp( x_i^T \\beta)} \\frac{1}{y_i!}.\\]\nfactor \\(1/y_i!\\) can absorbed base measure, recognize\nPoisson regression model exponential family sufficient statistics\n\\[t_i(y_i) = x_i y_i\\]\n\n\\[\\log \\varphi_i(\\beta) =  \\exp( x_i^T \\beta).\\]implement numerical optimization algorithms computing \nmaximum-likelihood estimate note \n\\[t(\\mathbf{y}) = \\sum_{=1}^n x_i y_i = \\mathbf{X}^T \\mathbf{y} \\quad \\text{} \\quad\n\\kappa(\\beta) = \\sum_{=1}^n e^{x_i^T \\beta},\\]\n\n\\[\\nabla \\kappa(\\beta) = \\sum_{=1}^n x_i e^{x_i^T \\beta},\\]\n\n\\[D^2 \\kappa(\\beta) = \\sum_{=1}^n x_i x_i^T e^{x_i^T \\beta} = \\mathbf{X}^T \\mathbf{W}(\\beta) \\mathbf{X}\\]\n\\(\\mathbf{W}(\\beta)\\) diagonal matrix \\(\\mathbf{W}(\\beta)_{ii} = \\exp(x_i^T \\beta).\\)\nformulas follow directly general formulas exponential\nfamilies.illustrate use Poisson regression model consider\nfollowing data set major Swedish supermarket chain. contains\nnumber bags frozen vegetables sold given week given store\nmarketing campaign. predicted number items sold normal week\n(without campaign) based historic sales included. interest \nrecalibrate number give good prediction number items\nactually sold.natural model number sold items using Poisson regression\nmodel, consider following simple log-linear model:\\[\\log(E(\\text{sale} \\mid \\text{normalSale})) = \\beta_0 + \\beta_1 \\log(\\text{normalSale}).\\]example exponential family regression model \nPoisson response distribution. standard regression model can\nfitted data using glm function using formula interface\nspecify response depend normal sale. model \nfitted computing maximum-likelihood estimate two parameters\n\\(\\beta_0\\) \\(\\beta_1\\).fitted model expected sale function normal sale, \\(x\\), \\[x \\mapsto e^{1.46 + 0.92 \\times \\log(x)} = (4.31 \\times x^{-0.08}) \\times x.\\]\nmodel roughly predicts four-fold increase sale \ncampaign, though effect decays increasing \\(x\\). normal\nsale 10 items factor \\(4.31 \\times 10^{-0.08} = 3.58\\), \nnormal sale 100 items factor reduces \\(4.31 \\times 100^{-0.08} = 2.98\\).entirely satisfied model. fitted across \nstores data set, obvious effect \napply across stores. Thus like fit model form\\[\\log(E(\\text{sale})) = \\beta_{\\text{store}} + \\beta_1 \\log(\\text{normalSale})\\]instead. Fortunately, also straightforward using glm.print individual store effects 352\nindividual stores.note coefficient \\(\\log(\\text{normalSale})\\) \nchanged considerably, model somewhat different model now\nindividual stores.computational viewpoint important thing changed \nnumber parameters increased lot. first simple\nmodel two parameters. second model 353 parameters.\nComputing maximum-likelihood estimate considerably difficult\nproblem second case.","code":"\nvegetables <- read_csv(\"data/vegetables.csv\", col_types = cols(store = \"c\"))\nsummary(vegetables)##       sale          normalSale        store          \n##  Min.   :  1.00   Min.   :  0.20   Length:1066       \n##  1st Qu.: 12.00   1st Qu.:  4.20   Class :character  \n##  Median : 21.00   Median :  7.25   Mode  :character  \n##  Mean   : 40.29   Mean   : 11.72                     \n##  3rd Qu.: 40.00   3rd Qu.: 12.25                     \n##  Max.   :571.00   Max.   :102.00\n# A Poisson regression model\npois_model_null <- glm(\n  sale ~ log(normalSale), \n  data = vegetables, \n  family = poisson\n)  \nsummary(pois_model_null)## \n## Call:\n## glm(formula = sale ~ log(normalSale), family = poisson, data = vegetables)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -16.218   -2.677   -0.864    1.324   21.730  \n## \n## Coefficients:\n##                 Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)        1.461      0.015    97.2   <2e-16 ***\n## log(normalSale)    0.922      0.005   184.4   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 51177  on 1065  degrees of freedom\n## Residual deviance: 17502  on 1064  degrees of freedom\n## AIC: 22823\n## \n## Number of Fisher Scoring iterations: 5\n## Note, variable store is a factor! The 'intercept' is removed \n## in the formula to obtain a parametrization as above. \npois_model <- glm(\n  sale ~ store + log(normalSale) - 1, \n  data = vegetables, \n  family = poisson\n)\nsummary(pois_model)$coefficients[c(1, 2, 3, 4, 353), ]##                 Estimate Std. Error z value   Pr(>|z|)\n## store1            2.7182    0.12756  21.309 9.416e-101\n## store10           4.2954    0.12043  35.668 1.254e-278\n## store100          3.4866    0.12426  28.060 3.055e-173\n## store101          3.3007    0.11791  27.993 1.989e-172\n## log(normalSale)   0.2025    0.03127   6.474  9.536e-11"},{"path":"four-examples.html","id":"finite-mixture-models","chapter":"6 Four Examples","heading":"6.4 Finite mixture models","text":"finite mixture model model pair random variables \\((Y, Z)\\)\n\\(Z \\\\{1, \\ldots, K\\}\\), \\(P(Z = k) = p_k\\), conditional distribution\n\\(Y\\) given \\(Z = k\\) density \\(f_k( \\cdot \\mid \\theta_k)\\). joint\ndensity \n\\[(y, k) \\mapsto f_k(y \\mid \\theta_k) p_k,\\]\nmarginal density distribution \\(Y\\) \n\\[f(y \\mid \\theta) =  \\sum_{k=1}^K f_k(y \\mid \\theta_k) p_k\\]\n\\(\\theta\\) vector parameters. say \nmodel \\(K\\) mixture components call \\(f_k( \\cdot \\mid \\theta_k)\\)\nmixture distributions \\(p_k\\) mixture weights.main usage mixture models situations \\(Z\\) observed.\npractice, \\(Y\\) observed, parameter estimation \nbased marginal distribution \\(Y\\) density \\(f(\\cdot \\mid \\theta)\\),\nweighted sum mixture distributions.set probability measures \\(\\{1, \\ldots, K\\}\\) exponential\nfamily sufficient statistic\n\\[\\tilde{t}_0(k) = (\\delta_{1k}, \\delta_{2k}, \\ldots, \\delta_{(K-1)k}) \\\\mathbb{R}^{K-1},\\]\ncanonical parameter \\(\\alpha = (\\alpha_1, \\ldots, \\alpha_{K-1}) \\\\mathbb{R}^{K-1}\\)\n\n\\[p_k = \\frac{e^{\\alpha_k}}{1 + \\sum_{l=1}^{K-1} e^{\\alpha_l}}.\\]\\(f_k\\) exponential family well sufficient statistic\n\\(\\tilde{t}_k : \\mathcal{Y} \\\\mathbb{R}^{p_k}\\) \\(\\theta_k\\) \ncanonical parameter, bundle \\(\\alpha, \\theta_1, \\ldots, \\theta_K\\)\n\\(\\theta\\) define\n\\[t_1(y) = \\left(\\begin{array}{c}\n\\tilde{t}_0 \\\\\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{array}\n\\right)\\]\ntogether \n\\[t_2(y \\mid k) = \\left(\\begin{array}{c}\n0 \\\\\n\\delta_{1k} \\tilde{t}_1(y) \\\\\n\\delta_{2k} \\tilde{t}_2(y) \\\\\n\\vdots \\\\\n\\delta_{Kk} \\tilde{t}_K(y)\n\\end{array}\n\\right)\\]\nsee exponential family joint distribution \\((Y, Z)\\)\n\\(p = K-1 + p_1 + \\ldots + p_K\\)-dimensional canonical parameter \\(\\theta\\),\nsufficient statistic \\(t_1\\) determining marginal distribution \\(Z\\)\nsufficient statistic \\(t_2\\) determining conditional distribution\n\\(Y\\) given \\(Z\\). made conditioning variable explicit.marginal density \\(Y\\) exponential family parametrization \nbecomes\n\\[f(y \\mid \\theta) = \\sum_{k=1}^K  e^{\\theta^T (t_1(k) + t_2(y \\mid k)) - \\log \\varphi_1(\\theta) - \\log \\varphi_2(\\theta \\mid k)}.\\]\nsmall \\(K\\) usually unproblematic implement computation \nmarginal density using formula , computation derivatives\ncan likewise implemented based formulas derived Section 6.1.3.","code":""},{"path":"four-examples.html","id":"Gaus-mix-ex","chapter":"6 Four Examples","heading":"6.4.1 Gaussian mixtures","text":"Gaussian mixture model mixture model mixture distributions \nGaussian distributions potentially different means variances. \nsection focus simplest Gaussian mixture model \\(K = 2\\) mixture\ncomponents.\\(K = 2\\), Gaussian mixture model parametrized five parameters:\nmixture weight \\(p = P(Z = 1) \\(0, 1)\\), two means \\(\\mu_1, \\mu_2 \\\\mathbb{R}\\),\ntwo variances \\(\\sigma_1, \\sigma_2 > 0\\). parametrization\nusing canonical exponential family parameters, return . First\nsimply simulate random variables model.simulation generated 5000 samples two-component\nGaussian mixture model mixture distributions \\(\\mathcal{N}(-0.5, 1)\\)\n\\(\\mathcal{N}(4, 4)\\), component weight \\(0.5\\).\ngives bimodal distribution illustrated histogram Figure\n6.1.\nFigure 6.1: Histogram density estimate (red) data simulated two-component Gaussian mixture distribution. true mixture distribution \npossible give mathematically different representation \nmarginal distribution \\(Y\\) sometimes useful. Though gives\nmarginal distribution components, provide \ndifferent interpretation mixture model model , \nprovide different way simulating mixture distribution.let \\(Y_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)\\) \\(Y_2 \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)\\)\nindependent, independent \\(Z\\), can define\\[\\begin{equation}\nY = 1(Z = 1) Y_1 + 1(Z = 2) Y_2 = Y_{Z}.\n\\tag{6.3}\n\\end{equation}\\]Clearly, conditional distribution \\(Y\\) given \\(Z = k\\) \\(f_k\\).\n(6.3) follows directly \n\\[E(Y^n) = P(Z = 1) E(Y_1^n) + P(Z = 2) E(Y_2^n) = p m_1(n) + (1 - p) m_2(n)\\]\n\\(m_k(n)\\) denotes \\(n\\)th non-central moment \\(k\\)th mixture\ncomponent. particular,\\[E(Y) = p \\mu_1 + (1 - p) \\mu_2.\\]variance can found second moment \\[\\begin{align}\nV(Y) & = p(\\mu_1^2 + \\sigma_1^2) + (1-p)(\\mu_2^2 + \\sigma_2^2) -  (p \\mu_1 + (1 - p) \\mu_2)^2 \\\\\n& = p\\sigma_1^2 + (1 - p) \\sigma_2^2 + p(1-p)(\\mu_1^2 + \\mu_2^2 - 2 \\mu_1 \\mu_2).\n\\end{align}\\]certainly possible derive formula means, using\n(6.3) gives simple argument based \nelementary properties expectation independence\n\\((Y_1, Y_2)\\) \\(Z\\).construction via (6.3) interpretation differs\nmixture model defined first place. Though \\((Y, Z)\\) \ncorrect joint distribution, \\(Y\\) (6.3) result \\(Z\\) selecting one\ntwo possible observations. difference can best illustrated \nexample. Suppose large population consisting married couples entirely.\ncan draw sample individuals (ignoring marriage relations\ncompletely) population let \\(Z\\) denote sex \\(Y\\) \nheight individual. \\(Y\\) follows mixture distribution two\ncomponents corresponding males females according definition.\nrisk heteronormative, suppose couples consist\none male one female. also draw sample\nmarried couples, couple flip coin decide whether report\nmale’s female’s height. corresponds construction\n\\(Y\\) (6.3). get marginal mixture\ndistribution heights though.Arguably heights individuals marriage \nindependent, actually immaterial. dependence structure \n\\(Y_1\\) \\(Y_2\\) lost transformation (6.3), \ncan just well assume independent mathematical convenience.\nwon’t able tell difference observing \\(Y\\) (\\(Z\\)) anyway.illustrate (6.3) can used alternative\nimplementations ways simulate mixture model. compare empirical\nmeans variances theoretical values test implementations\nactually simulate Gaussian mixture model.terms run time big difference three \nways simulating mixture model. benchmark study (shown) \nreveal first third methods comparable terms run time\nslightly faster fourth, second using ifelse takes \ntwice much time others. \nunsurprising ifelse method takes (6.3) literally\ngenerates twice number Gaussian variables actually needed.marginal density \\(Y\\) \n\\[f(y) = p \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} e^{-\\frac{(y - \\mu_1)^2}{2 \\sigma_1^2}} + \n(1 - p)\\frac{1}{\\sqrt{2 \\pi \\sigma_2^2}}e^{-\\frac{(y - \\mu_2)^2}{2 \\sigma_2^2}}\\]\ngiven terms parameters \\(p\\), \\(\\mu_1, \\mu_2\\), \\(\\sigma_1^2\\) \\(\\sigma_2^2\\).Returning canonical parameters see given \nfollows:\n\\[\\theta_1  = \\log \\frac{p}{1 - p}, \\quad \n\\theta_2  = \\frac{\\mu_1}{2\\sigma_1^2}, \\quad\n\\theta_3  = \\frac{1}{2\\sigma_1^2}, \\quad\n\\theta_4  = \\frac{\\mu_2}{\\sigma_2^2}, \\quad\n\\theta_5  = \\frac{1}{2\\sigma_2^2}.\\]joint density parametrization becomes\n\\[(y,k) \\mapsto \\left\\{ \\begin{array}{ll} \n\\exp\\left(\\theta_1 + \\theta_2 y - \\theta_3 y^2 - \\log (1 + e^{\\theta_1}) - \\frac{ \\theta_2^2}{4\\theta_3}+ \\frac{1}{2} \\log(\\theta_3) \\right) & \\quad \\text{} k = 1 \\\\\n\\exp\\left(\\theta_4 y - \\theta_5 y^2 - \\log (1 + e^{\\theta_1}) - \\frac{\\theta_4^2}{4\\theta_5} + \\frac{1}{2}\\log(\\theta_5) \\right) &  \\quad \\text{} k = 2 \n\\end{array} \\right. \\]\nmarginal density \\(Y\\) \\[\\begin{align}\nf(y \\mid \\theta) & = \\exp\\left(\\theta_1 + \\theta_2 y - \\theta_3 y^2 - \\log (1 + e^{\\theta_1}) - \\frac{ \\theta_2^2}{4\\theta_3}+ \\frac{1}{2} \\log(\\theta_3) \\right) \\\\ \n& + \\exp\\left(\\theta_4 y - \\theta_5 y^2 - \\log (1 + e^{\\theta_1}) - \\frac{\\theta_4^2}{4\\theta_5} + \\frac{1}{2}\\log(\\theta_5) \\right).\n\\end{align}\\]apparent benefit canonical parametrization considering\nmarginal density. , however, value need logarithm \njoint density EM-algorithm.","code":"\nsigma1 <- 1\nsigma2 <- 2\nmu1 <- -0.5\nmu2 <- 4\np <- 0.5\nn <- 5000\nz <- sample(c(TRUE, FALSE), n, replace = TRUE, prob = c(p, 1 - p))\n## Conditional simulation from the mixture components\ny <- numeric(n)\nn1 <- sum(z)\ny[z] <- rnorm(n1, mu1, sigma1)\ny[!z] <- rnorm(n - n1, mu2, sigma2)\nyy <- seq(-3, 11, 0.1)\ndens <- p * dnorm(yy, mu1, sigma1) + (1 - p) * dnorm(yy, mu2, sigma2)\nggplot(mapping = aes(y, ..density..)) + \n  geom_histogram(bins = 20) + \n  geom_line(aes(yy, dens), color = \"blue\", size = 1) + \n  stat_density(bw = \"SJ\", geom=\"line\", color = \"red\", size = 1)\n## Mean\nmu <- p * mu1 + (1 - p) * mu2\n\n## Variance \nsigmasq <- p * sigma1^2 + (1 - p) * sigma2^2 + \n  p * (1-p)*(mu1^2 + mu2^2 - 2 * mu1 * mu2)\n\n## Simulation using the selection formulation via 'ifelse'\ny2 <- ifelse(z, rnorm(n, mu1, sigma1), rnorm(n, mu2, sigma2))\n\n## Yet another way of simulating from a mixture model\n## using arithmetic instead of 'ifelse' for the selection \n## and with Y_1 and Y_2 actually being dependent\ny3 <- rnorm(n)\ny3 <- z * (sigma1 * y3 + mu1) + (!z) * (sigma2 * y3 + mu2)\n\n## Returning to the definition again, this last method simulates conditionally \n## from the mixture components via transformation of an underlying Gaussian\n## variable with mean 0 and variance 1\ny4 <- rnorm(n)\ny4[z] <- sigma1 * y4[z] + mu1\ny4[!z] <- sigma2 * y4[!z] + mu2\n\n## Tests\ndata.frame(mean = c(mu, mean(y), mean(y2), mean(y3), mean(y4)),\n      variance = c(sigmasq, var(y), var(y2), var(y3), var(y4)), \n      row.names = c(\"true\", \"conditional\", \"ifelse\", \"arithmetic\", \n                    \"conditional2\" ))##                  mean variance\n## true         1.750000 7.562500\n## conditional  1.728195 7.381843\n## ifelse       1.713098 7.513963\n## arithmetic   1.708420 7.566312\n## conditional2 1.700052 7.463941"},{"path":"four-examples.html","id":"mixed-models","chapter":"6 Four Examples","heading":"6.5 Mixed models","text":"mixed model regression model observations allows \nrandom variation two different levels. section \nfocus mixed models exponential family context. Mixed models can \nconsidered greater generality little shared\nstructure one deal models much case--case\nmanner.mixed model observations \\(y_j \\\\mathcal{Y}\\)\n\\(z \\\\mathcal{Z}\\) :distribution \\(z\\) exponential family canonical parameter\n\\(\\theta_0\\)conditionally \\(z\\) \\(y_j\\)s independent distribution exponential\nfamily sufficient statistics \\(t_j( \\cdot \\mid z)\\).definition emphasizes \\(y_j\\)s variation two levels.\nvariation underlying \\(z\\), first level variation\n(often called random effect),\nvariation among \\(y_j\\)s given \\(z\\), \nsecond level variation. special case hierarchical models\n(Bayesian networks tree graphs) also known multilevel models, \nmixed model two levels. observe data model\ntypically observe independent replications, \\((y_{ij})_{j=1, \\ldots, m_i}\\)\n\\(= 1, \\ldots, n\\), \\(y_j\\)s . Note allow different\nnumber, \\(m_i\\), \\(y_j\\)s \\(\\).simplest class mixed models obtained \\(t_j = t\\) \ndepending \\(j\\), \n\\[t(y_j \\mid z) = \\left(\\begin{array}{cc} t_1(y_j) \\\\ t_2(y_j, z) \\end{array} \\right)\\]\nfixed maps \\(t_1 : \\mathcal{Y} \\mapsto \\mathbb{R}^{p_1}\\) \\(t_2 : \\mathcal{Y} \\times \\mathcal{Z} \\mapsto \\mathbb{R}^{p_2}\\).\ncalled random effects model (model fixed effects \nsense \\(t_j\\) depend \\(j\\), given random effect\n\\(z\\) \\(y_j\\)s ..d.). canonical parameters associated model\n\\(\\theta_0\\) enters distribution random effect,\n\\(\\theta_1 \\\\mathbb{R}^{p_1}\\) \\(\\theta_2 \\\\mathbb{R}^{p_2}\\) enter\nconditional distribution \\(y_j\\) given \\(z\\).Example 6.4  special case Gaussian, linear random effects model model \n\\(z\\) \\(\\mathcal{N}(0, 1)\\) distributed, \\(\\mathcal{Y} = \\mathbb{R}\\)\n(base measure proportional Lebesgue measure)\nsufficient statistic \n\\[t(y_j \\mid z) = \\left(\\begin{array}{cc} y_j \\\\ - y_j^2 \\\\ zy_j \\end{array}\\right).\\]\nfree parameters distribution \\(z\\).Example 6.2\nfollows conditional variance \\(y\\) given \\(z\\) \n\\[\\sigma^2 = \\frac{1}{2\\theta_2}\\]\nconditional mean \\(y\\) given \\(z\\) \n\\[\\frac{\\theta_1 + \\theta_3 z}{2 \\theta_2} = \\sigma^2 \\theta_1 + \\sigma^2 \\theta_3 z.\\]\nReparametrizing terms \\(\\sigma^2\\), \\(\\beta_0 = \\sigma^2 \\theta_1\\) \n\\(\\nu = \\sigma^2 \\theta_3\\) see conditional distribution \\(y_j\\)\ngiven \\(z\\) \\(\\mathcal{N}(\\beta_0 + \\nu z, \\sigma^2)\\). clear\nmixed model \\(y_j\\) conditionally \\(z\\) regression model. However,\nobserve \\(z\\) practice.Using distributional result can see \nGaussian random effects model observations \\(y_{ij}\\) can\nequivalently stated \\[Y_{ij} = \\beta_0 + \\nu Z_i + \\varepsilon_{ij}\\]\\(= 1, \\ldots, n\\) \\(j = 1, \\ldots, m_i\\) \n\\(Z_1, \\ldots, Z_n\\) ..d. \\(\\mathcal{N}(0, 1)\\)-distributed\nindependent \n\\(\\varepsilon_{11}, \\varepsilon_{12}, \\ldots, \\varepsilon_{1n_1}, \\ldots, \\varepsilon_{mn_m}\\)\n..d. \\(\\mathcal{N}(0, \\sigma^2)\\)-distributed.","code":""},{"path":"numopt.html","id":"numopt","chapter":"7 Numerical optimization","heading":"7 Numerical optimization","text":"main application numerical optimization statistics \ncomputation parameter estimates. Typically maximizing likelihood\nfunction maximizing minimizing another estimation criterion. focus \nchapter optimization algorithms (penalized) maximum likelihood\nestimation. tradition \nformulate results algorithms terms minimization \nmaximization.generic optimization problem considered minimization \n\\(H : \\Theta \\\\mathbb{R}\\) \\(\\Theta \\subseteq \\mathbb{R}^p\\) \nopen set \\(H\\) twice differentiable. applications, \\(H = -\\ell\\),\nnegative log-likelihood function, \\(H = -\\ell + J\\), \\(J : \\Theta \\\\mathbb{R}\\)\npenalty function, likewise twice differentiable, \ndepend upon data.Statistical optimization problems share properties\npay attention . term\n\\[-\\ell(\\theta) = - \\sum_{=1}^N \\log(f_{\\theta}(x_i))\\]\nobjective function minimized sum \ndata, data computationally demanding\nevaluate \\(H\\) derivatives. exceptions, though,\nsufficient statistic can computed upfront, generally\nmust expect run time grow data size. Additionally, high precision \ncomputed (local) minimizer typically necessary. numerical\nerror already orders magnitudes smaller statistical\nuncertainty parameter estimate computed, \noptimization makes relevant difference.fact, blindly pursuing global maximum likelihood can\nlead astray model well behaved. situations \\(H\\) \nconvex negative log-likelihood may unbounded, e.g. finite mixtures, yet\ngood estimate can found local minimizer. even phrase\nobjective global optimization objective, may equally interested local\nminima even just stationary points; solutions \\(\\nabla H(\\theta) = 0\\). Optimization computational\ntool used statistics adapting models data – minimizer \\(H\\)\nparticular data set intrinsically interesting. may \nlittle gained computing minimizer \\(H\\) high numerical precision,\nassessment model fit quantification model uncertainty \ngreater practical importance highly accurately determined minimizer.generic minimization problem considered chapter, practical\nchallenge implementing algorithms R typically\nimplement efficient evaluation \\(H\\) derivatives. particular,\nefficient evaluations \\(-\\ell\\). Several choices standard optimization\nalgorithms possible already implemented available R.\nmany practical purposes BFGS-algorithms implemented via optim()\nwork well require computation gradients. , course,\nparamount \\(H\\) \\(\\nabla H\\) correctly implemented, \nefficiency algorithms largely determined efficiency \nimplementation \\(H\\) \\(\\nabla H\\) also choice parametrization.optimization problems, Newton-type algorithms preferable, standard\nimplementations available R nlm() nls() different\ninterfaces optim().","code":""},{"path":"numopt.html","id":"algorithms-and-convergence","chapter":"7 Numerical optimization","heading":"7.1 Algorithms and convergence","text":"numerical optimization algorithm computes initial value\n\\(\\theta_0 \\\\Theta\\) sequence \\(\\theta_1, \\theta_2, \\ldots \\\\Theta\\).\nOne hope \n\\[\\theta_n \\rightarrow \\text{arg min}_{\\theta} H(\\theta)\\]\n\\(n \\\\infty\\), less can typically guaranteed. First,\nglobal minimizer may exist may unique, \ncase convergence undefined ambiguous. Second, \\(\\theta_n\\)\ncan general shown converge local minimizer anything.\nThird, \\(\\theta_n\\) may converge, even \\(H(\\theta_n)\\)\nconverges local minimum.discuss properties optimization algorithms greater detail \nneed ways quantify, analyze monitor convergence. can \ndone terms \\(\\theta_n\\), \\(H(\\theta_n)\\) \\(\\nabla H(\\theta_n)\\).\nFocusing \\(H(\\theta_n)\\) say algorithm descent algorithm \n\\[H(\\theta_0) \\geq H(\\theta_1) \\geq H(\\theta_2) \\geq \\ldots.\\]\ninequalities sharp (except \\(\\theta_i\\) local minimizer),\nalgorithm called strict descent algorithm. \nsequence \\(H(\\theta_n)\\) convergent descent algorithm \\(H\\)\nbounded , e.g. level set\n\\[\\mathrm{lev}(\\theta_0) =  \\{\\theta \\\\Theta \\mid H(\\theta) \\leq H(\\theta_0)\\}\\]\ncompact. However, even strict descent algorithm,\n\\(H(\\theta_n)\\) may descent smaller smaller steps toward limit \n(local) minimum. good optimization algorithm needs \nguarantee descent – needs guarantee sufficient descent\nstep.Many algorithms can phrased \n\\[\\theta_n = \\Phi(\\theta_{n-1})\\]\nmap \\(\\Phi : \\Theta \\\\Theta\\).\n\\(\\Phi\\) continuous \\(\\theta_n \\rightarrow \\theta_{\\infty}\\)\nfollows \n\\[\\Phi(\\theta_n) \\rightarrow \\Phi(\\theta_{\\infty}).\\]\nSince \\(\\Phi(\\theta_n) = \\theta_{n+1} \\rightarrow \\theta_{\\infty}\\) see \n\\[\\Phi(\\theta_{\\infty}) = \\theta_{\\infty}.\\]\n, \\(\\theta_{\\infty}\\) fixed point \\(\\Phi\\). \\(\\theta_n\\) \nconvergent, accumulation point \\(\\theta_n\\) still \nfixed point \\(\\Phi\\). Thus can search potential accumulation\npoints searching fixed points map \\(\\Phi: \\Theta \\\\Theta\\).Mathematics full fixed point theorems : ) give conditions map\nfixed point; ii) cases guarantee iterates \\(\\Phi^{\\circ n}(\\theta_0)\\)\nconverge fixed point. prominent result Banach’s fixed point\ntheorem.\nstates \\(\\Phi\\) contraction, ,\n\\[\\| \\Phi(\\theta) - \\Phi(\\theta')\\| \\leq c \\|\\theta - \\theta'\\|\\]\nconstant \\(c \\[0,1)\\) (using norm), \\(\\Phi\\) unique\nfixed point \\(\\theta_n = \\Phi^{\\circ n}(\\theta_0)\\) converges fixed\npoint starting point \\(\\theta_0 \\\\Theta\\).show simple gradient descent algorithm can \nanalyzed – descent algorithm Banach’s\nfixed point theorem. demonstrate basic proof techniques \nwell typical conditions \\(H\\) can give convergence results\noptimization algorithms.\nscratch surface intention can motivate\nalgorithms introduced subsequent sections chapters well\nempirical techniques practical assessment \nconvergence.Indeed, theory rarely provides us sharp quantitative results \nrate convergence need computational techniques monitor measure convergence algorithms practice. Otherwise compare\nefficiency different algorithms.","code":""},{"path":"numopt.html","id":"grad-descent","chapter":"7 Numerical optimization","heading":"7.1.1 Gradient descent","text":"assume section \\(\\Theta = \\mathbb{R}^p\\). \nsimplify analysis bit, minor modifications can\ngeneralized case \\(\\Theta\\) open convex.Suppose \\(D^2H(\\theta)\\) numerical radius uniformly bounded \\(L\\), ,\n\\[|\\gamma^T D^2H(\\theta) \\gamma| \\leq L \\|\\gamma\\|_2^2\\]\n\\(\\theta \\\\Theta\\) \\(\\gamma \\\\mathbb{R}^p\\). \ngradient descent algorithm fixed step length \\(1/(L + 1)\\) given \n\\[\\begin{align}\n\\theta_{n} & = \\theta_{n-1} - \\frac{1}{L + 1} \\nabla H(\\theta_{n-1}).  \\tag{7.1}\n\\end{align}\\]\nFixing \\(n\\) Taylor’s theorem \n\\(\\tilde{\\theta} = \\alpha \\theta_{n} + (1- \\alpha)\\theta_{n-1}\\) (\\(\\alpha \\[0,1]\\))\nline \\(\\theta_n\\) \\(\\theta_{n-1}\\) \\[\\begin{align*}\nH(\\theta_n) & = H(\\theta_{n-1}) - \\frac{1}{L+1} \\|\\nabla H(\\theta_{n-1})\\|_2^2 +  \n \\\\  & \\qquad \\frac{1}{(L+1)^2} \\nabla H(\\theta_{n-1})^T D^2H(\\tilde{\\theta}) \\nabla H(\\theta_{n-1}) \\\\\n& \\leq H(\\theta_{n-1}) - \\frac{1}{L+1} \\|\\nabla H(\\theta_{n-1})\\|_2^2 + \n   \\frac{L}{(L+1)^2} \\|\\nabla H(\\theta_{n-1})\\|_2^2 \\\\\n& = H(\\theta_{n-1}) - \\frac{1}{(L+1)^2} \\|\\nabla H(\\theta_{n-1})\\|_2^2.\n\\end{align*}\\]shows \\(H(\\theta_n) \\leq H(\\theta_{n-1})\\), \\(\\theta_{n-1}\\) \nstationary point, \\(H(\\theta_n) < H(\\theta_{n-1})\\). , algorithm\ndescent algorithm, unless \nhits stationary point strict descent algorithm.furthermore follows \\[\\begin{align}\nH(\\theta_n) & = \n(H(\\theta_n) - H(\\theta_{n-1})) + (H(\\theta_{n-1}) - H(\\theta_{n-2})) + ... \\\\\n& \\qquad + (H(\\theta_1) - H(\\theta_0)) + H(\\theta_0) \\\\\n& \\leq H(\\theta_0) - \\frac{1}{(L + 1)^2} \\sum_{k=1}^n \\|\\nabla H(\\theta_{k-1})\\|_2^2.\n\\end{align}\\]\\(H\\) bounded , \\(\\sum_{k=1}^{\\infty} \\|\\nabla H(\\theta_{k-1})\\|_2^2 < \\infty\\)\nhence\n\\[\\|\\nabla H(\\theta_{n})\\|_2 \\rightarrow 0\\]\n\\(n \\\\infty\\). accumulation point, \\(\\theta_{\\infty}\\), \nsequence \\(\\theta_n\\), follows continuity \\(\\nabla H\\) \n\\[\\nabla H(\\theta_{\\infty}) = 0,\\]\n\\(\\theta_{\\infty}\\) stationary point. \\(H\\) unique\nstationary point \\(\\mathrm{lev}(\\theta_0)\\), \\(\\theta_{\\infty}\\) (local)\nminimizer, \n\\[\\theta_n \\rightarrow \\theta_{\\infty}\\]\n\\(n \\\\infty\\).gradient descent algorithm (7.1) given\nmap\n\\[\\Phi_{\\nabla}(\\theta) = \\theta - \\frac{1}{L + 1} \\nabla H(\\theta).\\]\ngradient descent map, \\(\\Phi_{\\nabla}\\), \\(\\theta\\) fixed point \\[\\nabla H(\\theta) = 0,\\]\n, \\(\\theta\\) stationary point.show \\(\\Phi_{\\nabla}\\) contraction \\(\\Theta\\)\nassumption eigenvalues (symmetric) matrix\n\\(D^2H(\\theta)\\) \\(\\theta \\\\Theta\\) contained interval \\([l, L]\\)\n\\(0 < l \\leq L\\). \\(\\theta, \\theta' \\\\Theta\\) find\nTaylor’s theorem \n\\[\\nabla H(\\theta) = \\nabla H(\\theta') + D^2H(\\tilde{\\theta})(\\theta - \\theta')\\]\n\\(\\tilde{\\theta}\\) line \\(\\theta\\) \\(\\theta'\\).\ngradient descent map gives \\[\\begin{align*}\n\\|\\Phi_{\\nabla}(\\theta) - \\Phi_{\\nabla}(\\theta')\\|_2 & = \n\\left\\|\\theta - \\theta' - \\frac{1}{L+1}\\left(\\nabla H(\\theta) - \\nabla H(\\theta')\\right)\\right\\|_2 \\\\\n& = \n\\left\\|\\theta - \\theta' - \\frac{1}{L+1}\\left( D^2H(\\tilde{\\theta})(\\theta - \\theta')\\right)\\right\\|_2 \\\\\n& = \n\\left\\|\\left(- \\frac{1}{L+1} D^2H(\\tilde{\\theta}) \\right) (\\theta - \\theta')\\right\\|_2 \\\\\n& \\leq \\left(1 - \\frac{l}{L + 1}\\right) \\|\\theta - \\theta'\\|_2,\n\\end{align*}\\]since eigenvalues \\(- \\frac{1}{L+1} D^2H(\\tilde{\\theta})\\) \n\\(1 - L/(L + 1)\\) \\(1 - l/(L+1)\\). shows \\(\\Phi_{\\nabla}\\) \ncontraction \\(2\\)-norm \\(c = 1 - l/(L + 1) < 1\\). Banach’s fixed\npoint theorem follows unique stationary point, \\(\\theta_{\\infty}\\),\n\\(\\theta_n \\rightarrow \\theta_{\\infty}\\). Since \\(D^2H(\\theta_{\\infty})\\) \npositive definite, \\(\\theta_{\\infty}\\) (global) minimizer.summarize, \\(D^2H(\\theta)\\) uniformly bounded numerical radius, \n\\(H\\) unique stationary point \\(\\theta_{\\infty}\\) compact set \\(\\mathrm{lev}(\\theta_0)\\), gradient descent algorithm strict descent\nalgorithm converges toward (local) minimizer \\(\\theta_{\\infty}.\\)\nfixed step length key analysis.\nupper bound numerical radius\n\\(D^2H(\\theta)\\) guarantees descent fixed step length, \nfixed step length guarantees sufficient descent.eigenvalues \\(D^2H(\\theta)\\) \\([l, L]\\) \\(0 < l \\leq L\\),\n\\(H\\) strongly convex function unique global minimizer\n\\(\\theta_{\\infty}\\) level sets \\(\\mathrm{lev}(\\theta_0)\\) compact.\nConvergence can also established via Banach’s fixed point theorem.\nconstant \\(c = 1 - l/(L + 1) = 1 - \\kappa^{-1}\\) \\(\\kappa = (L + 1) / l\\)\nquantifies rate convergence, discussed greater\ndetail next section. constant \\(\\kappa\\) upper bound \nconditioning number\nmatrix \\(D^2H(\\theta)\\) uniformly \\(\\theta\\), large value \n\\(\\kappa\\) means \\(c\\) close 1 convergence can slow.\nlarge conditioning number second derivative indicates graph\n\\(H\\) looks like narrow ravine, case can expect \ngradient descent algorithm slow.","code":""},{"path":"numopt.html","id":"convergence-rate","chapter":"7 Numerical optimization","heading":"7.1.2 Convergence rate","text":"\\(\\theta_n = \\Phi^{\\circ n}(\\theta_0)\\) contraction \\(\\Phi\\),\nBanach’s fixed point theorem tells us unique fixed point\n\\(\\theta_{\\infty}.\\) \\(\\Phi\\) contraction implies \\[\\|\\theta_n - \\theta_{\\infty}\\| = \\|\\Phi(\\theta_{n-1}) - \\theta_{\\infty}\\| \\leq c \\|\\theta_{n-1} - \\theta_{\\infty}\\| \\leq c^n \\|\\theta_0 - \\theta_{\\infty}\\|.\\]\n, \\(\\|\\theta_n - \\theta_{\\infty}\\| \\0\\) least geometric rate \\(c < 1\\).numerical optimization literature, convergence geometric rate \nknown linear convergence (number correct digits increases linearly\nnumber iterations), linearly convergent algorithm often\nquantified terms asymptotic convergence rate.Definition 7.1  Let \\((\\theta_n)_{n \\geq 1}\\) convergent sequence \\(\\mathbb{R}^p\\) \nlimit \\(\\theta_{\\infty}\\). Let \\(\\| \\cdot \\|\\) norm \\(\\mathbb{R}^p\\). say\nconvergence linear \n\\[\\limsup_{n \\\\infty} \\frac{\\|\\theta_{n} - \\theta_{\\infty}\\|}{\\|\\theta_{n-1} - \\theta_{\\infty}\\|} = r\\]\n\\(r \\(0, 1)\\), case \\(r\\) called asymptotic rate.Convergence algorithm can faster slower linear. \\[\\limsup_{n \\\\infty} \\frac{\\|\\theta_{n} - \\theta_{\\infty}\\|}{\\|\\theta_{n-1} - \\theta_{\\infty}\\|} = 1\\]\nsay convergence sublinear, \n\\[\\limsup_{n \\\\infty} \\frac{\\|\\theta_{n} - \\theta_{\\infty}\\|}{\\|\\theta_{n-1} - \\theta_{\\infty}\\|} = 0\\]\nsay convergence superlinear. also refined\nnotion convergence order superlinearly convergent algorithms\nprecisely describes convergence.contraction, \\(\\theta_n = \\Phi^{\\circ n}(\\theta_0)\\) converges linearly\nsuperlinearly. converges linearly, asymptotic rate bounded \n\\(c\\), global constant may pessimistic. following\nlemma provides local upper bound asymptotic rate terms \nderivative \\(\\Phi\\) limit \\(\\theta_{\\infty}\\).Lemma 7.1  Let \\(\\Phi : \\mathbb{R}^p \\\\mathbb{R}^p\\) twice continuously differentiable. \n\\(\\theta_n = \\Phi^{\\circ n}(\\theta_0)\\) converges linearly toward fixed point \\(\\theta_{\\infty}\\)\n\\(\\Phi\\) \n\\[r_{\\max} = \\sup_{\\gamma: \\|\\gamma \\| = 1} \\|D \\Phi(\\theta_{\\infty}) \\gamma\\|\\]\nupper bound asymptotic rate.Proof. Taylor’s theorem,\n\\[\\begin{align*}\n\\theta_{n} = \\theta_{\\infty} + D \\Phi(\\theta_{\\infty})(\\theta_{n-1} - \\theta_{\\infty}) +\no(\\|\\theta_{n-1} - \\theta_{\\infty}\\|_2).\n\\end{align*}\\]\nRearranging yields\n\\[\\begin{align*}\n\\limsup_{n \\\\infty} \n\\frac{\\| \\theta_{n} - \\theta_{\\infty}\\|}{\\| \\theta_{n-1} - \\theta_{\\infty}\\|} \n& = \\limsup_{n \\\\infty} \n\\frac{\\| D \\Phi(\\theta_{\\infty})(\\theta_{n-1} - \\theta_{\\infty})\\|}{\\| \\theta_{n-1} - \\theta_{\\infty}\\|} \\\\\n& = \\limsup_{n \\\\infty} \n\\| D \\Phi(\\theta_{\\infty})\\left(\\frac{\\theta_{n-1} - \\theta_{\\infty}}{\\| \\theta_{n-1} - \\theta_{\\infty}\\|} \\right)\\| \\\\\n& \\leq r_{\\max}.\n\\end{align*}\\]Note possible \\(r_{\\max} \\geq 1\\) even convergence linear,\ncase \\(r_{\\max}\\) useless upper bound. Moreover, actual rate\ncan depend starting point \\(\\theta_0\\), even \\(r_{\\max} < 1\\) \nquantifies worst case convergence rate.estimate actual convergence rate, note linear\nconvergence rate \\(r\\) implies \\(\\delta > 0\\)\n\\[\n\\| \\theta_n - \\theta_{\\infty} \\| \\leq (r + \\delta) \\| \\theta_{n-1} - \\theta_{\\infty} \\| \\leq \\ldots \n\\leq (r + \\delta)^{n - n_0} \\| \\theta_{n_0} - \\theta_{\\infty} \\|\n\\]\n\\(n \\geq n_0\\) \\(n_0\\) sufficiently large (depending \\(\\delta\\)). ,\n\\[\\log \\|\\theta_{n} - \\theta_{\\infty}\\| \\leq n \\log(r + \\delta) + d,\\]\n\\(n \\geq n_0\\). practice, run algorithm \\(M\\)\niterations, \\(\\theta_M = \\theta_{\\infty}\\) computer precision,\nplot \\(\\log \\|\\theta_{n} - \\theta_{M}\\|\\) function \\(n\\).\ndecay approximately linear \\(n \\geq n_0\\) \\(n_0\\), \ncase slope \\(\\log(r) < 0\\), can estimated least squares.\nslower--linear faster--linear decay indicate \nalgorithm converges sublinearly superlinearly, respectively., course, possible quantify convergence\nterms sequences \\(H(\\theta_n)\\) \\(\\nabla H(\\theta_n)\\) instead.\ndescent algorithm \\(H(\\theta_n) \\rightarrow H(\\theta_\\infty)\\) linearly \n\\(n \\\\infty\\), can plot\n\\[\\log(H(\\theta_n) - H(\\theta_M))\\]\nfunction \\(n\\), use least squares estimate asymptotic rate \nconvergence \\(H(\\theta_n)\\).Using \\(\\nabla H(\\theta_n)\\) quantify convergence particularly appealing\nknow limit \\(0\\). also means can monitor\nconvergence algorithm running \nconverged. \\(\\nabla H(\\theta_n)\\) converges linearly norm \\(\\|\\cdot\\|\\), ,\n\\[\\limsup_{n \\\\infty} \\frac{\\|\\nabla H(\\theta_n)\\|}{\\|\\nabla H(\\theta_{n-1})\\|} = r,\\]\n\\(\\delta > 0\\) \n\\[\\log \\|\\nabla H(\\theta_n)\\| \\leq n \\log(r + \\delta) + d\\]\n\\(n \\geq n_0\\) \\(n_0\\) sufficiently large. , least squares\ncan used estimate asymptotic rate convergence \n\\(\\|\\nabla H(\\theta_n)\\|\\).concepts linear convergence asymptotic rate , unfortunately, \nindependent norm used, linear convergence \\(\\theta_n\\) \\(\\|\\cdot\\|\\)\nimply linear convergence \\(H(\\theta_n)\\) \\(\\|\\nabla H(\\theta_n)\\|\\).\ncan shown, though, \\(\\theta_n\\) converges linearly norm, two\nsequences converge linearly along arithmetic subsequence. Similarly, can shown\nnorm \\(\\| \\cdot \\|\\), \\(\\theta_n\\) linearly convergent\nalong arithmetic subsequence.Theorem 7.1  Suppose \\(H\\) three times continuously\ndifferentiable \\(\\nabla H (\\theta_{\\infty}) = 0\\) \\(D^2 H(\\theta_{\\infty})\\)\npositive definite. \\(\\theta_n\\) converges linearly toward \\(\\theta_{\\infty}\\)\nnorm \\(k \\geq 1\\), subsequences \\(H(\\theta_{nk})\\) \n\\(\\|\\nabla H(\\theta_{nk})\\|\\) converge linearly toward \\(H(\\theta_{\\infty})\\)\n\\(0\\), respectively.Proof. Let \\(G = D^2 H(\\theta_{\\infty})\\) \\(g_n = \\theta_n - \\theta_{\\infty}\\).\nSince \\(G\\) positive definite defines norm, since norms\n\\(\\mathbb{R}^p\\) equivalent constants \\(, b > 0\\) \n\\[\\| x \\|^2 \\leq x^T G x \\leq b \\| x \\|^2.\\]Taylor’s theorem,\n\\[H(\\theta_{(n + 1)k}) - H(\\theta_{\\infty}) = g_{(n + 1)k}^T G  g_{(n + 1)k} + o(\\|g_{(n + 1)k}\\|^2).\\]\n,\n\\[\\begin{align*}\n\\limsup_{n \\\\infty} \\frac{H(\\theta_{(n + 1)k}) - H(\\theta_{\\infty})}{H(\\theta_{nk}) - H(\\theta_{\\infty})}\n& = \\limsup_{n \\\\infty} \\frac{g_{(n + 1)k}^T G  g_{(n + 1)k}}{g_{nk}^T G  g_{nk}} \\\\\n& \\leq \\limsup_{n \\\\infty} \\frac{b \\| g_{(n + 1)k}\\|^2}{\\| g_{nk}\\|^2} \n= \\frac{b}{} r^{2k}\n\\end{align*}\\]\n\\(r \\(0, 1)\\) asymptotic rate \\(\\theta_n \\\\theta_{\\infty}\\).\nchoosing \\(k\\) sufficiently large, right hand side strictly\nsmaller 1, gives upper bound rate along \nsubsequence \\(H(\\theta_{nk})\\). similar argument gives lower bound strictly larger\n0, shows convergence superlinear.Using Taylor’s theorem \\(\\nabla H(\\theta_{nk})\\) yields similar\nproof subsequence \\(\\|\\nabla H(\\theta_{nk})\\|\\) – \\(G\\) replaced\n\\(G^2\\) bounds.bounds proof often pessimistic since \\(g_n\\) \\(g_{n+1}\\)\ncan point completely different directions.\nExercise ?? shows, however, \\(\\theta_n\\) approaches \\(\\theta_{\\infty}\\) nicely\nalong fixed direction (making \\(g_n\\) \\(g_{n+1}\\) almost collinear),\n\\(H(\\theta_n)\\) well \\(\\|\\nabla H(\\theta_n)\\|\\)\ninherit linear convergence \\(\\theta_n\\) even rate.rates discussed hithereto per iteration,\nnatural investigating single algorithm. However, different\nalgorithms may spend different amounts time per iteration, \nmake sense make comparison per iteration rates across different\nalgorithms. therefore need able convert per iteration\nper time unit rates. one iteration takes \\(\\delta\\) time units (seconds, say)\nper time unit rate \n\\[r^{1/\\delta}.\\]\n\\(t_n\\) denotes run time \\(n\\) iterations, \nestimate \\(\\delta\\) \\(t_M / M\\) \\(M\\) sufficiently large.throughout systematically investigate convergence\nfunction time \\(t_n\\) instead iterations \\(n\\), estimate\nrates per time unit directly least squares regression \\(t_n\\)\ninstead \\(n\\).","code":""},{"path":"numopt.html","id":"stopping-criteria","chapter":"7 Numerical optimization","heading":"7.1.3 Stopping criteria","text":"One important practical question remains unanswered even\nunderstand theoretical convergence properties \nalgorithm well good methods measuring convergence rates.\nalgorithm can run infinite number\niterations, thus algorithms need criterion \nstop, choice appropriate stopping criterion \nnotoriously difficult. present four commonly used criteria \ndiscuss benefits deficits .Maximal number iterations: Stop \n\\[n = M\\]\nfixed maximal number iterations \\(M\\).\narguably simplest criterion, , obviously,\nreaching maximal number iterations provides evidence \n\\(H(\\theta_M)\\) sufficiently close (local) minimum. \nspecific problem experience know sufficiently large\n\\(M\\) algorithm typically converged \\(M\\) iterations, \nimportant use criterion combination another\ncriterion works safeguard infinite loop.three stopping criteria depend choosing \ntolerance parameter \\(\\varepsilon > 0\\), play different roles three\ncriteria. three criteria can used individually combinations,\nunfortunately neither combination provide convergence\nguarantees. nevertheless common say algorithm “converged”\nstopping criterion satisfied, since none criteria\nsufficient convergence can bit misleading.Small relative change: Stop \n\\[\\|\\theta_n - \\theta_{n-1}\\| \\leq \\varepsilon(\\|\\theta_n\\| + \\varepsilon).\\]\nidea \\(\\theta_n \\simeq \\theta_{n-1}\\) sequence approximately\nreached limit \\(\\theta_{\\infty}\\) can stop. possible use \nabsolute criterion \\(\\|\\theta_n - \\theta_{n-1}\\| < \\varepsilon\\), \ncriterion sensitive rescaling parameters. Thus fixing \nreasonable tolerance parameter across many problems makes sense \nrelative absolute criterion. main reason adding \\(\\varepsilon\\)\nright hand size make criterion well behaved even \n\\(\\|\\theta_n\\|\\) close zero.main benefit criterion require evaluations\nobjective function. algorithms, EM-algorithm, \nneed evaluate objective function, may even difficult .\ncases criterion can used. main deficit \nsingle iteration little change parameter can happen many\nreasons besides convergence, imply neither\n\\(\\|\\theta_n - \\theta_{\\infty}\\|\\) \\(H(\\theta_{n}) - H(\\theta_{\\infty})\\)\nsmall.Small relative descent: Stop \n\\[H(\\theta_{n-1}) - H(\\theta_n) \\leq \\varepsilon (|H(\\theta_n)| + \\varepsilon).\\]\ncriterion makes sense algorithm descent algorithm.\ndiscussed , absolute criterion sensitive rescaling \nobjective function, added \\(\\varepsilon\\)\nright hand side ensure reasonable behavior \\(H(\\theta_n)\\)\nclose zero.criterion natural descent algorithms; stop algorithm\ndecrease value objective function sufficiently.\nuse relative criterion makes possible choose tolerance\nparameter works reasonably well many problems. conventional choice\n\\(\\varepsilon \\simeq 10^{-8}\\) (often chosen square root \nmachine epsilon), though\ntheoretical support choice weak. deficit \nalgorithm criterion : small descent imply \\(H(\\theta_{n}) - H(\\theta_{\\infty})\\) small, happen \nalgorithm enters part \\(\\Theta\\) \\(H\\) flat, say.Small gradient: Stop \n\\[\\|\\nabla H(\\theta_n)\\| \\leq \\varepsilon.\\]\ncriterion directly measures \\(\\theta_n\\) close stationary\npoint (\\(\\theta_n\\) close stationary point).\nsmall value \\(\\|\\nabla H(\\theta_n)\\|\\) still guarantee \n\\(\\|\\theta_n - \\theta_{\\infty}\\|\\) \\(H(\\theta_{n}) - H(\\theta_{\\infty})\\) \nsmall. criterion also requires computation gradient. addition,\ndifferent norms, \\(\\|\\cdot\\|\\), can used, \ndifferent coordinates gradient different orders \nmagnitude reflected norm. Alternatively, parameters\nrescaled.Neither four criteria gives theoretical convergence guarantee\nterms close \\(H(\\theta_{n})\\) \\(H(\\theta_{\\infty})\\). \nspecial cases possible develop criteria stronger\ntheoretical support. continuous function \\(\\tilde{H}\\) satisfying\n\\(H(\\theta_{\\infty}) = \\tilde{H}(\\theta_{\\infty})\\) \n\\[H(\\theta_{\\infty}) \\geq \\tilde{H}(\\theta)\\]\n\\(\\theta \\\\Theta\\) (just \\(\\mathrm{lev}(\\theta_0)\\) descent\nalgorithm) \n\\[0 \\leq H(\\theta_{n}) - H(\\theta_{\\infty}) \\leq \nH(\\theta_{n}) - \\tilde{H}(\\theta_{n}),\\]\nright hand side directly quantifies convergence. Convex duality\ntheory gives many convex optimization problems function, \\(\\tilde{H}\\),\ncases convergence criterion based \n\\(H(\\theta_{n}) - \\tilde{H}(\\theta_{n})\\) actually comes theoretical guarantee \nclose minimum. pursue\nnecessary convex duality theory , useful know\ncases can better ad hoc criteria .","code":""},{"path":"numopt.html","id":"descent-direction-algorithms","chapter":"7 Numerical optimization","heading":"7.2 Descent direction algorithms","text":"negative gradient \\(H\\) \\(\\theta\\) direction steepest descent.\nSince goal minimize \\(H\\), natural move\naway \\(\\theta\\) direction \\(-\\nabla H(\\theta)\\).\nHowever, directions negative gradient can also suitable\ndescent directions.Definition 7.2  descent direction \\(\\theta\\) vector \\(\\rho \\\\mathbb{R}^p\\) \n\\[\\nabla H(\\theta)^T \\rho < 0.\\]\\(\\theta\\) stationary point,\\[\\nabla H(\\theta)^T(-\\nabla H(\\theta)) = - \\| \\nabla H(\\theta)^T \\|_2^2 < 0\\]\n\\(-\\nabla H(\\theta)^T\\) descent direction \\(\\theta\\) according \ndefinition.Given \\(\\theta_n\\) descent direction \\(\\rho_n\\) \\(\\theta_n\\) can define\n\\[\\theta_{n+1} = \\theta_n + \\gamma \\rho_n\\]\nsuitably chosen \\(\\gamma > 0\\). Taylor’s theorem\n\\[H(\\theta_{n+1}) = H(\\theta_n) + \\gamma \\nabla H(\\theta_n) \\rho_n + o(\\gamma),\\]\nmeans \n\\[H(\\theta_{n+1}) < H(\\theta_n)\\]\n\\(\\gamma\\) small enough.One strategy choosing \\(\\gamma\\) minimize univariate\nfunction\n\\[\\gamma \\mapsto H(\\theta_n + \\gamma \\rho_n),\\]\nexample line search method. minimization\ngive maximal possible descent direction \\(\\rho_n\\),\nargued, \\(\\rho_n\\) descent direction, minimizer \\(\\gamma > 0\\)\nguarantees descent \\(H\\). However, unless minimization can \ndone analytically often computationally expensive.\nLess also , shown Section 7.1.1,\nHessian uniformly bounded numerical radius possible \nfix one (sufficiently small) step length guarantee descent.","code":""},{"path":"numopt.html","id":"line-search","chapter":"7 Numerical optimization","heading":"7.2.1 Line search","text":"consider algorithms form\n\\[\\theta_{n+1} = \\theta_n + \\gamma_{n} \\rho_n\\]\nstarting \\(\\theta_n\\) \\(\\rho_n\\) descent direction \\(\\theta_n\\).\nstep lengths, \\(\\gamma_n\\), chosen give\nsufficient descent iteration.function \\(h(\\gamma) = H(\\theta_{n} + \\gamma \\rho_{n})\\)\nunivariate differentiable function,\n\\[h : [0,\\infty) \\\\mathbb{R},\\]\ngives value \\(H\\) descent direction\n\\(\\rho_n\\). find \n\\[h'(\\gamma) = \\nabla H(\\theta_{n} + \\gamma \\rho_{n})^T \\rho_{n},\\]\nmaximal descent direction \\(\\rho_n\\) can found solving\n\\(h'(\\gamma) = 0\\) \\(\\gamma\\). mentioned , less . First note\n\n\\[h'(0) = \\nabla H(\\theta_{n})^T \\rho_{n} < 0,\\]\n\\(h\\) negative slope \\(0\\). descents sufficiently\nsmall interval \\([0, \\varepsilon)\\), even true \\(c \\(0, 1)\\)\n\\(\\varepsilon > 0\\) \n\\[h(\\gamma) \\leq h(0) + c \\gamma h'(0)\\]\n\\(\\gamma \\[0, \\varepsilon)\\). note inequality can\nchecked easily given \\(\\gamma > 0\\), known \nsufficient descent condition. Sufficient descent enough\nstep length arbitrarily small, algorithm\neffectively get stuck.prevent small steps can enforce another condition. close\n\\(0\\), \\(h\\) almost slope, \\(h'(0)\\), \\(0\\). \ntherefore require slope \\(\\gamma\\) larger \\(\\tilde{c} h'(0)\\)\n\\(\\tilde{c} \\(0, 1)\\), \\(\\gamma\\) forced away \\(0\\). \nknown curvature condition.combined conditions \\(\\gamma\\),\n\\[h(\\gamma) \\leq h(0) + c \\gamma h'(0)\\]\n\\(c \\(0, 1)\\) \n\\[h'(\\gamma) \\geq \\tilde{c} h'(0)\\]\n\\(\\tilde{c} \\(c, 1)\\) known collectively \nWolfe conditions. can shown \\(h\\) bounded \nexists step length satisfying Wolfe conditions (Lemma 3.1 Nocedal Wright (2006)).Even choosing \\(\\gamma_{n}\\) fulfill\nWolfe conditions guarantee \\(\\theta_n\\)\nconverge let alone converge toward global minimizer. best \ncan hope general \n\\[\\|\\nabla H(\\theta_n)\\|_2 \\rightarrow 0\\]\n\\(n \\\\infty\\), happen relatively weak\nconditions \\(H\\) (Theorem 3.2 Nocedal Wright (2006)) assumption\n\n\\[\\frac{\\nabla H(\\theta_n)^T \\rho_n}{\\|\\nabla H(\\theta_n)\\|_2 \\| \\rho_n\\|_2} \\leq - \\delta < 0.\\]\n, angle descent direction gradient \nuniformly bounded away \\(90^{\\circ}\\).practical way searching step length via backtracking.\nChoosing \\(\\gamma_0\\) constant \\(d \\(0, 1)\\) \ncan search sequence step lengths\n\\[\\gamma_0, d \\gamma_0, d^2 \\gamma_0, d^3 \\gamma_0, \\ldots\\]\nstop first time find step length satisfying Wolfe\nconditions.Using backtracking, can actually dispense curvature condition\nsimply check sufficient descent condition\\[H(\\theta_{n} + d^k \\gamma_0 \\rho_{n}) \\leq H(\\theta_n) + cd^k \\gamma_0 \\nabla H(\\theta_{n})^T \\rho_{n}\\]\\(c \\(0, 1)\\). implementation backtracking requires choice\nthree parameters: \\(\\gamma_0 > 0\\), \\(d \\(0, 1)\\) \\(c \\(0, 1)\\).\ngood choice depends quite lot algorithm used choosing\ndescent direction, choosing \\(c\\) close 1 can make algorithm\ntake small steps, taking \\(d\\) small can likewise\ngenerate small step lengths. Thus \\(d = 0.8\\) \\(d = 0.9\\)\n\\(c = 0.1\\) even smaller sensible choices. algorithms,\nlike Newton algorithm dealt , natural\nchoice \\(\\gamma_0 = 1\\). algorithms good choice depends\ncrucially scale parameters, general\nadvice choosing \\(\\gamma_0\\).","code":""},{"path":"numopt.html","id":"gradient-descent","chapter":"7 Numerical optimization","heading":"7.2.2 Gradient descent","text":"implement gradient descent backtracking function GD().\ngradient descent, sufficient descent condition amounts \nchoosing smallest \\(k \\geq 0\\) \\[H(\\theta_{n} - d^k \\gamma_0 \\nabla H(\\theta_{n})) \\leq H(\\theta_n) -  cd^k \\gamma_0 \\|\\nabla H(\\theta_{n})\\|_2^2.\\]GD() function takes starting point, \\(\\theta_0\\), objective function,\n\\(H\\), gradient, \\(\\nabla H\\), arguments. four parameters \\(d\\), \\(c\\), \\(\\gamma_0\\) \n\\(\\varepsilon\\) control algorithm can also specified additional arguments,\ngiven reasonable default values. implementation uses squared norm \ngradient stopping criterion, also maximal number \niterations safeguard. Note maximal number reached, \nwarning printed.Finally, include callback argument (cb argument).\nfunction passed argument, evaluated iteration\nalgorithm. gives us possibility logging \nprinting values variables evaluation, can highly useful\nunderstanding inner workings algorithm. Monitoring logging\nintermediate values evaluation code referred \ntracing.use Poisson regression example illustrate use \ngradient descent optimization algorithms, need \nimplement functions R computing negative log-likelihood \ngradient.implementation uses function factory produce \nlist containing parameter vector, negative log-likelihood function\ngradient. anticipate Newton algorithm Section\n7.3 also need implementation Hessian, \nthus included well.exploit model.matrix() function construct model matrix\ndata via formula, sufficient statistic also\ncomputed. implementations use linear algebra vectorized computations\nrelying access model matrix sufficient statistics \nenclosing environment.choose normalize log-likelihood number observations \\(n\\) (number\nrows model matrix). small computational\ncost, resulting numerical values become less dependent\nupon \\(n\\), makes easier choose sensible default values\nvarious parameters numerical optimization algorithms.\nGradient descent slow large Poisson model individual\nstore effects, consider simple model two parameters.gradient descent implementation tested comparing minimizer \nestimated parameters computed glm().get result first two decimals. convergence\ncriterion gradient descent algorithm quite loose (\\(\\varepsilon = 10^{-4}\\),\nmeans norm gradient smaller \\(10^{-2}\\) \nalgorithm stops). choice \\(\\varepsilon\\) combination \\(\\gamma_0 = 0.01\\)\nimplies algorithm stops gradient small changes\nnorm \\(10^{-4}\\).Comparing resulting values negative log-likelihood shows agreement\nfirst five decimals, notice negative log-likelihood\nparameters fitted using glm() just slightly smaller.investigate actually went inside gradient descent\nalgorithm use callback argument trace internals\ncall GD(). tracer() function CSwR package\ncan used construct tracer object tracer() function can pass callback\nargument. tracer object tracer() function work storing\ninformation enclosing environment tracer(). used \ncallback argument e.g. GD() tracer() function look \nvariables evaluation environment GD(), store \nprint requested, store run time information well.GD() returned, trace information can accessed\nvia summary() method tracer object. tracer objects\ntracer() function confused trace()\nfunction R base package, tracer object’s tracer() function\ncan passed tracer argument trace() interactively\ninject tracing code R function. , tracer objects \nused together callback argument.use tracer object gradient descent implementation\nprint trace information every 50th iteration.see gradient descent algorithm runs little 350\niterations, can observe value negative log-likelihood \ndescending. can also see step length \\(\\gamma\\) bounces \n\\(0.004096 = 0.8^4 \\times 0.01\\) \n\\(0.00512 = 0.8^3 \\times 0.01\\), thus backtracking\ntakes 3 4 iterations find step length sufficient descent.printed trace reveal run time information. \nrun time measured iteration algorithm \ncumulative run time greater interest. information\ncan computed inspected algorithm converged,\nreturned summary method tracer objects.trace information stored list. summary method transforms\ntrace information data frame one row per iteration. can also access\nindividual entries list trace information via subsetting.\nFigure 7.1: Gradient norm (left) value negative log-likelihood (right) limit value \\(H(\\theta_{\\infty})\\). straight line fitted data points except first ten using least squares, rate computed estimate reported per ms.\n","code":"\nGD <- function(\n  par, \n  H,\n  gr,\n  d = 0.8, \n  c = 0.1, \n  gamma0 = 0.01, \n  epsilon = 1e-4, \n  maxiter = 1000,\n  cb = NULL\n) {\n  for(i in 1:maxiter) {\n    value <- H(par)\n    grad <- gr(par)\n    h_prime <- sum(grad^2)\n    if(!is.null(cb)) cb()\n    # Convergence criterion based on gradient norm\n    if(h_prime <= epsilon) break\n    gamma <- gamma0\n    # Proposed descent step\n    par1 <- par - gamma * grad\n    # Backtracking while descent is insufficient\n    while(H(par1) > value - c * gamma * h_prime) {\n      gamma <- d * gamma\n      par1 <- par - gamma * grad\n    }\n    par <- par1\n  }\n  if(i == maxiter)\n    warning(\"Maximal number, \", maxiter, \", of iterations reached\")\n  par\n}\npoisson_model <- function(form, data, response) {\n  X <- model.matrix(form, data)\n  y <- data[[response]]\n  # The function drop() drops the dim attribute and turns, for instance,\n  # a matrix with one column into a vector\n  t_map <- drop(crossprod(X, y))  # More efficient than drop(t(X) %*% y)\n  n <- nrow(X)\n  p <- ncol(X)\n  \n  H <- function(beta) \n    drop(sum(exp(X %*% beta)) - beta %*% t_map) /n\n  \n  grad_H <- function(beta) \n    (drop(crossprod(X, exp(X %*% beta))) - t_map) / n\n  \n  Hessian_H <- function(beta)\n    crossprod(X, drop(exp(X %*% beta)) * X) / n\n  \n  list(par = rep(0, p), H = H, grad_H = grad_H, Hessian_H = Hessian_H)\n}\nveg_pois <- poisson_model(~ log(normalSale), vegetables, response = \"sale\")\npois_GD <- GD(veg_pois$par, veg_pois$H, veg_pois$grad_H)\nrbind(pois_glm = coefficients(pois_model_null), pois_GD)##          (Intercept) log(normalSale)\n## pois_glm    1.461440       0.9215699\n## pois_GD     1.460352       0.9219358\nveg_pois$H(coefficients(pois_model_null))\nveg_pois$H(pois_GD)## [1] -124.406827879897\n## [1] -124.406825325047\nGD_tracer <- tracer(c(\"value\", \"h_prime\", \"gamma\"), N = 50)\npois_GD <- GD(veg_pois$par, veg_pois$H, veg_pois$grad_H, cb = GD_tracer$tracer)## n = 1: value = 1; h_prime = 14268.59; gamma = NA; \n## n = 50: value = -123.9395; h_prime = 15.45722; gamma = 0.004096; \n## n = 100: value = -124.3243; h_prime = 3.133931; gamma = 0.00512; \n## n = 150: value = -124.3935; h_prime = 0.601431; gamma = 0.00512; \n## n = 200: value = -124.4048; h_prime = 0.09805907; gamma = 0.00512; \n## n = 250: value = -124.4065; h_prime = 0.0109742; gamma = 0.00512; \n## n = 300: value = -124.4068; h_prime = 0.002375827; gamma = 0.00512; \n## n = 350: value = -124.4068; h_prime = 0.000216502; gamma = 0.004096;\ntail(summary(GD_tracer))##         value      h_prime    gamma      .time\n## 372 -124.4068 1.125779e-04 0.005120 0.06706513\n## 373 -124.4068 1.218925e-04 0.005120 0.06720427\n## 374 -124.4068 1.323878e-04 0.005120 0.06734928\n## 375 -124.4068 1.441965e-04 0.005120 0.06749018\n## 376 -124.4068 1.574572e-04 0.005120 0.06764097\n## 377 -124.4068 7.600796e-05 0.004096 0.06785127\nGD_tracer[377] ## $value\n## [1] -124.4068\n## \n## $h_prime\n## [1] 7.600796e-05\n## \n## $gamma\n## [1] 0.004096\n## \n## $.time\n## [1] 0.0002102931"},{"path":"numopt.html","id":"conjugate-gradients","chapter":"7 Numerical optimization","heading":"7.2.3 Conjugate gradients","text":"gradient direction best descent direction. \nlocal, convergence can quite slow. One better algorithms \nstill first order algorithm (using gradient information) nonlinear conjugate\ngradient algorithm.\nFletcher–Reeves version algorithm\ndescent direction initialized negative gradient\n\\(\\rho_0 = - \\nabla H(\\theta_{0})\\) updated \n\\[\\rho_{n} = - \\nabla H(\\theta_{n}) + \\frac{\\|\\nabla H(\\theta_n)\\|_2^2}{\\|\\nabla H(\\theta_{n-1})\\|_2^2} \\rho_{n-1}.\\]\n, descent direction, \\(\\rho_{n}\\), negative gradient modified according \nprevious descent direction. plenty opportunity vary prefactor\n\\(\\rho_{n-1}\\), one presented makes Fletcher–Reeves\nversion. versions go names inventors Polak–Ribière\nHestenes–Stiefel.fact, \\(\\rho_{n}\\) need descent direction unless put \nrestrictions step lengths. One possibility require \nstep length \\(\\gamma_{n}\\) satisfies strong curvature condition\n\\[|h'(\\gamma)| = |\\nabla H(\\theta_n + \\gamma \\rho_n)^T \\rho_n | \\leq \\tilde{c} |\\nabla H(\\theta_n)^T \\rho_n| = \\tilde{c} |h'(0)|\\]\n\\(\\tilde{c} < \\frac{1}{2}\\). \\(\\rho_{n + 1}\\) can shown descent\ndirection \\(\\rho_{n}\\) .implement conjugate gradient method slightly different way. Instead\nintroducing advanced curvature condition, simply reset \nalgorithm use gradient direction case non-descent direction\nchosen. Resets descent direction every \\(p\\)-th iteration recommended\nanyway nonlinear conjugate gradient algorithm.\nFigure 7.2: Gradient norms (top) negative log-likelihoods (bottom) gradient descent (left) conjugate gradient (right).\nalgorithm fast enough fit large Poisson regression model.Using optim() conjugate gradient method.","code":"\nCG <- function(\n  par, \n  H,\n  gr,\n  d = 0.8, \n  c = 0.1, \n  gamma0 = 1, \n  epsilon = 1e-6,\n  maxiter = 1000,\n  cb = NULL\n) {\n  p <- length(par)\n  m <- 1\n  rho0 <- numeric(p)\n  for(i in 1:maxiter) {\n    value <- H(par)\n    grad <- gr(par)\n    grad_norm_sq <- sum(grad^2)\n    if(!is.null(cb)) cb()\n    if(grad_norm_sq <= epsilon) break\n    gamma <- gamma0\n    # Descent direction\n    rho <- - grad + grad_norm_sq * rho0\n    h_prime <- drop(t(grad) %*% rho)\n    # Reset to gradient descent if m > p or rho is not a descent direction\n    if(m > p || h_prime >= 0) {\n      rho <- - grad\n      h_prime <- - grad_norm_sq \n      m <- 1\n    }\n    par1 <- par + gamma * rho\n    # Backtracking\n    while(H(par1) > value + c * gamma * h_prime) {\n      gamma <- d * gamma\n      par1 <- par + gamma * rho\n    }\n    rho0 <- rho / grad_norm_sq\n    par <- par1\n    m <- m + 1\n  }\n  if(i == maxiter)\n    warning(\"Maximal number, \", maxiter, \", of iterations reached\")\n  par\n}\nCG_tracer <- tracer(c(\"value\", \"gamma\", \"grad_norm_sq\"), N = 10)\npois_CG <- CG(veg_pois$par, veg_pois$H, veg_pois$grad_H, cb = CG_tracer$tracer)## n = 1: value = 1; gamma = NA; grad_norm_sq = 14269; \n## n = 10: value = -123.15; gamma = 0.018014; grad_norm_sq = 129.78; \n## n = 20: value = -123.92; gamma = 0.022518; grad_norm_sq = 77.339; \n## n = 30: value = -124.23; gamma = 0.018014; grad_norm_sq = 22.227; \n## n = 40: value = -124.41; gamma = 0.0092234; grad_norm_sq = 0.179; \n## n = 50: value = -124.41; gamma = 0.10737; grad_norm_sq = 0.028232; \n## n = 60: value = -124.41; gamma = 0.0092234; grad_norm_sq = 0.00021747; \n## n = 70: value = -124.41; gamma = 0.0092234; grad_norm_sq = 1.7488e-06;\nveg_pois <- poisson_model(~ store + log(normalSale) - 1, vegetables, response = \"sale\")\nCG_tracer <- tracer(c(\"value\", \"gamma\", \"grad_norm_sq\"), N = 100)\npois_CG <- CG(veg_pois$par, veg_pois$H, veg_pois$grad_H, cb = CG_tracer$tracer)## n = 1: value = 1; gamma = NA; grad_norm_sq = 12737; \n## n = 100: value = -127.9; gamma = 0.018014; grad_norm_sq = 1.676; \n## n = 200: value = -128.28; gamma = 0.011529; grad_norm_sq = 2.5128; \n## n = 300: value = -128.55; gamma = 0.0037779; grad_norm_sq = 0.068176; \n## n = 400: value = -128.59; gamma = 0.022518; grad_norm_sq = 0.0028747; \n## n = 500: value = -128.59; gamma = 0.0092234; grad_norm_sq = 0.0652; \n## n = 600: value = -128.59; gamma = 0.018014; grad_norm_sq = 0.00020555; \n## n = 700: value = -128.59; gamma = 0.0019343; grad_norm_sq = 5.3952e-06; \n## n = 800: value = -128.59; gamma = 0.005903; grad_norm_sq = 3.7118e-06; \n## n = 900: value = -128.59; gamma = 0.0037779; grad_norm_sq = 1.0621e-06;\ntail(summary(CG_tracer))##         value       gamma grad_norm_sq    .time\n## 899 -128.5894 0.005902958 5.214667e-06 12.26816\n## 900 -128.5894 0.003777893 1.062092e-06 12.28321\n## 901 -128.5894 0.068719477 2.119915e-05 12.29143\n## 902 -128.5894 0.107374182 2.047029e-04 12.30012\n## 903 -128.5894 0.004722366 7.056181e-06 12.31577\n## 904 -128.5894 0.003777893 8.881615e-07 12.33146\nsystem.time(\n  pois_optim_CG <- optim(\n    veg_pois$par, \n    veg_pois$H, \n    veg_pois$grad_H, \n    method = \"CG\", \n    control = list(maxiter = 10000)\n  )\n)## Warning in optim(veg_pois$par, veg_pois$H, veg_pois$grad_H, method = \"CG\", : unknown names in\n## control: maxiter##    user  system elapsed \n##   0.500   0.375   0.845\npois_optim_CG[c(\"value\", \"counts\")]## $value\n## [1] -127.3596\n## \n## $counts\n## function gradient \n##      237      101"},{"path":"numopt.html","id":"pep-moth-descent","chapter":"7 Numerical optimization","heading":"7.2.4 Peppered Moths","text":"Returning peppered moth Section 6.2.1 implemented\nsection log-likelihood general multinomial cell collapsing\napplied implementation compute maximum-likelihood estimate.\nsection implement gradient well. \nexpression log-likelihood (6.2) follows\ngradient equals\\[\\nabla \\ell(\\theta) = \\sum_{j = 1}^{K_0}  \\frac{ x_j }{ M(p(\\theta))_j}\\nabla M(p(\\theta))_j = \\sum_{j = 1}^{K_0} \\sum_{k \\A_j}  \\frac{ x_j}{ M(p(\\theta))_j} \\nabla p_k(\\theta).\\]Letting \\(j(k)\\) defined \\(k \\A_{j(k)}\\) see gradient\ncan also written \n\\[\\nabla \\ell(\\theta) = \\sum_{k=1}^K    \\frac{x_{j(k)}}{ M(p(\\theta))_{j(k)}} \\nabla p_k(\\theta) = \\mathbf{\\tilde{x}}(\\theta) \\mathrm{D}p(\\theta),\\]\n\\(\\mathrm{D}p(\\theta)\\) Jacobian parametrization \\(\\theta \\mapsto p(\\theta)\\),\n\\(\\mathbf{\\tilde{x}}(\\theta)\\) vector \n\\[\\mathbf{\\tilde{x}}(\\theta)_k = \\frac{ x_{j(k)}}{M(p(\\theta))_{j(k)}}.\\]Jacobian needs implemented specific example\npeppered moths.can use conjugate gradient algorithm compute \nmaximum-likelihood estimate.peppered Moth example simple.\nlog-likelihood can easily computed, used \nproblem illustrate ways implementing \nlikelihood R use optim maximize .One likelihood implementations problem specific\nabstract general, used general abstract\napproach implement gradient . gradient \nused optimization algorithms, still using optim, \nconjugate gradient. fact, can use conjugate gradient without\ncomputing implementing gradient.implement gradient, numerical gradient \nused optim(). can result slower algorithm \ngradient implemented, seriously, can result convergence\nproblems. subtle tradeoff numerical\naccuracy accuracy finite difference approximation used \napproximate gradient. \nexperience convergence problems example , one way remedy problems\nset parscale fnscale entries control\nlist argument optim().following chapter peppered moth example used illustrate\nEM algorithm. important understand EM algorithm \nrely ability compute likelihood gradient \nlikelihood matter. many real applications\nEM algorithm computation likelihood challenging even\nimpossible, thus standard optimization algorithms directly\napplicable.","code":"\ngrad_loglik <- function(par, x, prob, Dprob, group) {\n  p <- prob(par)\n  if(is.null(p)) return(rep(NA, length(par)))\n  - (x[group] / M(p, group)[group]) %*% Dprob(par)\n}\nDprob <- function(p) {\n  p[3] <- 1 - p[1] - p[2]\n  matrix(\n    c(2 * p[1],             0, \n      2 * p[2],             2 * p[1], \n      2* p[3] - 2 * p[1],  -2 * p[1],\n      0,                    2 * p[2],         \n      -2 * p[2],            2 * p[3] - 2 * p[2], \n      -2 * p[3],           -2 * p[3]),\n    ncol = 2, nrow = 6, byrow = TRUE)\n}\noptim(c(0.3, 0.3), loglik, grad_loglik, x = c(85, 196, 341), \n      prob = prob, Dprob = Dprob, group = c(1, 1, 1, 2, 2, 3), \n      method = \"CG\")## $par\n## [1] 0.07083691 0.18873652\n## \n## $value\n## [1] 600.481\n## \n## $counts\n## function gradient \n##       92       19 \n## \n## $convergence\n## [1] 0\n## \n## $message\n## NULL\noptim(c(0.3, 0.3), loglik, x = c(85, 196, 341), \n      prob = prob, group = c(1, 1, 1, 2, 2, 3), \n      method = \"CG\")## $par\n## [1] 0.07084109 0.18873718\n## \n## $value\n## [1] 600.481\n## \n## $counts\n## function gradient \n##      107       15 \n## \n## $convergence\n## [1] 0\n## \n## $message\n## NULL"},{"path":"numopt.html","id":"Newton","chapter":"7 Numerical optimization","heading":"7.3 Newton-type algorithms","text":"Newton algorithm similar gradient descent\nexcept gradient descent direction replaced \n\\[\\rho_n = - D^2 H(\\theta_n)^{-1} \\nabla H(\\theta_n).\\]Newton algorithm typically much efficient gradient\ndescent converge iterations. However, storage \n\\(p \\times p\\) Hessian, computation, solution equation\ncompute \\(\\rho_n\\) scale like \\(p^2\\) can make algorithm useless\nlarge \\(p\\).variety alternatives Newton algorithm exist replace\nHessian another matrix can easier compute update.\nnoted choose matrix \\(B_n\\) \\(n\\)-th\niteration, \\(- B_n \\nabla H(\\theta_n)\\)\ndescent direction whenever \\(B_n\\) positive definite matrix.Newton implementation (trace).","code":"\nNewton <- function(\n  par, \n  H,\n  gr,\n  hess,\n  d = 0.8, \n  c = 0.1, \n  gamma0 = 1, \n  epsilon = 1e-10, \n  maxiter = 50,\n  cb = NULL\n) {\n  for(i in 1:maxiter) {\n    value <- H(par)\n    grad <- gr(par)\n    if(!is.null(cb)) cb()\n    if(sum(grad^2) <= epsilon) break\n    Hessian <- hess(par) \n    rho <- - drop(solve(Hessian, grad)) \n    gamma <- gamma0\n    par1 <- par + gamma * rho\n    h_prime <- t(grad) %*% rho \n    while(H(par1) > value +  c * gamma * h_prime) { \n      gamma <- d * gamma \n      par1 <- par + gamma * rho\n    }\n    par <- par1 \n  }\n  if(i == maxiter)\n    warning(\"Maximal number, \", maxiter, \", of iterations reached\")\n  par\n}"},{"path":"numopt.html","id":"poisson-regression","chapter":"7 Numerical optimization","heading":"7.3.1 Poisson regression","text":"use implementation Hessian matrix.R function glm.fit() uses Newton algorithm (without backtracking)\nfactor five faster example.One careful comparing run times different optimization\nalgorithms, case achieved precision\nfaster glm.fit() even obtained smallest negative\nlog-likelihood value two.","code":"\nNewton_tracer <- tracer(c(\"value\", \"h_prime\", \"gamma\"), N = 0)\npois_Newton <- Newton(\n  veg_pois$par, \n  veg_pois$H, \n  veg_pois$grad_H, \n  veg_pois$Hessian_H, \n  cb = Newton_tracer$tracer\n)\nrange(pois_Newton - coefficients(pois_model))## [1] -4.979266e-10  1.298776e-06\nrbind(\n  pois_Newton = veg_pois$H(pois_Newton),\n  pois_glm = veg_pois$H(coefficients(pois_model))\n)##                               [,1]\n## pois_Newton -128.58945047446991339\n## pois_glm    -128.58945047447093657\nsummary(Newton_tracer)##         value       h_prime    gamma     .time\n## 1     1.00000            NA       NA 0.0000000\n## 2   -14.83270 -4.140563e+03 0.022518 0.1883374\n## 3   -64.81635 -4.029847e+02 0.262144 0.3562165\n## 4  -111.33647 -7.636275e+01 1.000000 0.5206488\n## 5  -124.24937 -2.104160e+01 1.000000 0.6841749\n## 6  -127.71116 -5.652483e+00 1.000000 0.8590845\n## 7  -128.49729 -1.332600e+00 1.000000 1.0345645\n## 8  -128.58733 -1.647034e-01 1.000000 1.2165784\n## 9  -128.58945 -4.159696e-03 1.000000 1.3956716\n## 10 -128.58945 -3.288913e-06 1.000000 1.5609791\nenv_pois <- environment(veg_pois$H)\nsystem.time(glm.fit(env_pois$X, env_pois$y, family = poisson()))##    user  system elapsed \n##   0.512   0.036   0.406"},{"path":"numopt.html","id":"quasi-newton-algorithms","chapter":"7 Numerical optimization","heading":"7.3.2 Quasi-Newton algorithms","text":"turn descent direction algorithms efficient\ngradient descent choosing descent direction \nclever way less computationally demanding Newton\nalgorithm requires computation full Hessian \niteration.consider application BFGS algorithm via \nimplementation R function optim().","code":"\nsystem.time(\n  pois_BFGS <- optim(\n    veg_pois$par, \n    veg_pois$H, \n    veg_pois$grad_H, \n    method = \"BFGS\", \n    control = list(maxiter = 10000)\n  )\n)## Warning in optim(veg_pois$par, veg_pois$H, veg_pois$grad_H, method = \"BFGS\", : unknown names in\n## control: maxiter##    user  system elapsed \n##   0.324   0.180   0.475\nrange(pois_BFGS$par - coefficients(pois_model))## [1] -0.2638641  0.6364195\npois_BFGS[c(\"value\", \"counts\")]## $value\n## [1] -128.5888\n## \n## $counts\n## function gradient \n##      104      100"},{"path":"numopt.html","id":"sparsity","chapter":"7 Numerical optimization","heading":"7.3.3 Sparsity","text":"One benefits implementations \\(H\\) derivatives\nwell descent algorithms can exploit sparsity\n\\(\\mathbf{X}\\) almost free. implementations done \nprevious computations, \\(\\mathbf{X}\\) stored dense matrix. \nreality, \\(\\mathbf{X}\\) sparse matrix (vast majority matrix\nentries zero),\nconvert sparse matrix, matrix-vector products\nrun time efficient. Sparse matrices implemented \nR package Matrix.Without changing code, get immediate\nrun time improvement using e.g. optim() BFGS algorithm.real applications avoid constructing dense intermediate\nmodel matrix step toward constructing sparse model matrix. \npossible constructing sparse model matrix directly using\nfunction R package MatrixModels. Ideally, reimplement\npois_model() support option using sparse matrices, \nfocus run time benefits sparse matrices, simply\nchange matrix appropriate environment directly.Newton implementation benefits enormously using sparse matrices\nbottleneck computation Hessian.summarize run times measured Poisson regression example,\nfound conjugate gradient algorithms took order 10 seconds\nconverge. Newton-type algorithms section faster took\n0.3 1.7 seconds converge. use sparse matrices reduced \nrun time quasi-Newton algorithm BFGS factor 3, reduced \nrun time Newton algorithm factor 50 0.03 seconds. One \nconcerned construction sparse model matrix takes time (\nmeasure), measured turns example\ntakes time construct dense model matrix takes \nconstruct sparse one.Run time efficiency argument using sparse matrices \nalso memory efficient. memory (time) inefficient use dense intermediates,\ntruly large scale problems impossible. Using sparse model matrices\nregression models allows us work larger models \nvariables, factor levels observations use dense\nmodel matrices. Poisson regression model memory used either representation can found.see dense matrix uses around factor 30 memory \nsparse representation. case means using around 3 MB storing \ndense matrix instead around 100 kB, won’t problem\ncontemporary computer. However, going using 3 GB \nstoring matrix using 100 Mb difference \nable work matrix standard laptop \nrunning computations problems. Using model.Matrix makes\npossible construct sparse model matrices directly avoid\ndense intermediates.function glm4() MatrixModels package fitting regression models,\ncan exploit sparse model matrices direction, can\nthus useful cases model matrix becomes large sparse.\ntwo main applications model matrix becomes sparse.\nmodel response using one factors, possibly \ninteractions, model matrix become particularly sparse \nfactors many levels. Another case model response\nvia basis expansions quantitative predictors use basis functions\nlocal support. B-splines form important example basis\nlocal support results sparse model matrix.","code":"\nlibrary(Matrix)\nenv_pois$X <- Matrix(env_pois$X)\nclass(env_pois$X)## [1] \"dgCMatrix\"\n## attr(,\"package\")\n## [1] \"Matrix\"\nsystem.time(\n  pois_BFGS_sparse <- optim(\n    veg_pois$par, \n    veg_pois$H, \n    veg_pois$grad_H, \n    method = \"BFGS\", \n    control = list(maxiter = 10000)\n  )\n)## Warning in optim(veg_pois$par, veg_pois$H, veg_pois$grad_H, method = \"BFGS\", : unknown names in\n## control: maxiter##    user  system elapsed \n##   0.135   0.014   0.080\nenv_pois$X <- MatrixModels::model.Matrix(\n  ~ store + log(normalSale) - 1, \n  data = vegetables, \n  sparse = TRUE\n)\nclass(env_pois$X)## [1] \"dsparseModelMatrix\"\n## attr(,\"package\")\n## [1] \"MatrixModels\"\nNewton_tracer <- tracer(c(\"value\", \"h_prime\", \"gamma\"), N = 0)\n  pois_Newton <- Newton(\n  veg_pois$par, \n  veg_pois$H, \n  veg_pois$grad_H, \n  veg_pois$Hessian_H, \n  cb = Newton_tracer$tracer\n)\nsummary(Newton_tracer)##         value       h_prime    gamma       .time\n## 1     1.00000            NA       NA 0.000000000\n## 2   -14.83270 -4.140563e+03 0.022518 0.009095301\n## 3   -64.81635 -4.029847e+02 0.262144 0.014059242\n## 4  -111.33647 -7.636275e+01 1.000000 0.017209904\n## 5  -124.24937 -2.104160e+01 1.000000 0.020465703\n## 6  -127.71116 -5.652483e+00 1.000000 0.023446856\n## 7  -128.49729 -1.332600e+00 1.000000 0.026573878\n## 8  -128.58733 -1.647034e-01 1.000000 0.030414141\n## 9  -128.58945 -4.159696e-03 1.000000 0.033323748\n## 10 -128.58945 -3.288913e-06 1.000000 0.036309897\nobject.size(env_pois$X)\nobject.size(as.matrix(env_pois$X))## Sparse matrix memory usage:\n## 123440 bytes\n## Dense matrix memory usage:\n## 3103728 bytes"},{"path":"numopt.html","id":"misc.","chapter":"7 Numerical optimization","heading":"7.4 Misc.","text":"\\(\\Phi\\) just nonexpansive (\nconstant \\(c\\) one), longer true, replacing \\(\\Phi\\)\n\\(\\alpha \\Phi + (1 - \\alpha) \\) \\(\\alpha \\(0,1)\\) get\nKrasnoselskii-Mann iterates form\n\\[\\theta_n = \\alpha \\Phi(\\theta_{n-1}) + (1 - \\alpha) \\theta_{n-1}\\]\nconverge fixed point \\(\\Phi\\) provided one.Banach’s fixed point theorem implies convergence least fast \nlinear convergence asymptotic rate \\(c\\).\nMoreover, \\(\\Phi\\) just contraction \\(n \\geq n_0\\) \\(n_0\\),\n\\(n \\geq n_0\\)\n\\[\\|\\theta_{n + 1} - \\theta_{n}\\| \\leq c \\| \\theta_{n} - \\theta_{n-1}\\|.\\]\nconvergence may superlinear, linear, rate bounded \\(c\\).\nindicate \\(\\Phi\\) asymptotically contraction, can introduce\n\\[R_n = \\frac{\\|\\theta_{n + 1} - \\theta_{n}\\|}{\\|\\theta_{n} - \\theta_{n- 1}\\|}\\]\nmonitor behavior \\(n \\\\infty\\). constant\n\\[r = \\limsup_{n \\\\infty} R_n\\]\nasymptotically smallest possible contraction constant.\nconvergence sublinear \\(R_n \\1\\) sequence \ncalled logarithmically convergent (definition), \\(r \\(0,1)\\) \nindication linear convergence rate \\(r\\), \\(r = 0\\) indication \nsuperlinear convergence.practice, can plot ratio \\(R_n\\) algorithm running, \nuse \\(R_n\\) estimate rate \\(r\\) large \\(n\\). However,\ncan quite unstable method estimating rate.Finally, also possible \nestimate order \\(q\\) well rate \\(r\\) using \n\\[\\log \\|\\theta_{n} - \\theta_{N}\\| \\simeq q \\log \\|\\theta_{n-1} - \\theta_{N}\\| + \\log(r)\\]\n\\(n \\geq n_0\\). can estimate \\(q\\) \\(\\log(r)\\)\nfitting linear function least squares log-log transformed norms\nerrors.Iteration, fixed points, convergence criteria. Ref Nonlinear Parameter Optimization Using R Tools.","code":""},{"path":"em.html","id":"em","chapter":"8 Expectation maximization algorithms","heading":"8 Expectation maximization algorithms","text":"Somewhat surprisingly, possible develop algorithm, known \nexpectation-maximization algorithm, computing maximizer likelihood\nfunction situations computing likelihood quite\ndifficult. possible situations model defined \nterms certain unobserved components, likelihood computations \noptimization relatively easy complete observation. EM algorithm\nexploits special structure, thus general optimization\nalgorithm, situation applies common enough statistics\none core optimization algorithms used computing\nmaximum-likelihood estimates.chapter shown algorithm generally descent algorithm\nnegative log-likelihood, examples implementation given \nmultinomial cell collapsing Gaussian mixtures. theoretical results needed\nEM algorithm special case mixed models given well. Finally,\ntheoretical results well practical implementations computing\nestimates Fisher information presented.","code":""},{"path":"em.html","id":"basic-properties","chapter":"8 Expectation maximization algorithms","heading":"8.1 Basic properties","text":"section EM algorithm formulated shown descent algorithm\nnegative log-likelihood. Allele frequency estimation peppered moth\nconsidered simple example showing algorithm can implemented.","code":""},{"path":"em.html","id":"incomplete-data-likelihood","chapter":"8 Expectation maximization algorithms","heading":"8.1.1 Incomplete data likelihood","text":"Suppose \\(Y\\) random variable \\(X = M(Y)\\). Suppose \\(Y\\) density\n\\(f(\\cdot \\mid \\theta)\\) \\(X\\) marginal density \\(g(x \\mid \\theta)\\).marginal density typically form\n\\[g(x \\mid \\theta) = \\int_{\\{y: M(y) = x\\}} f(y \\mid \\theta) \\ \\mu_x(\\mathrm{d} y)\\]\nsuitable measure \\(\\mu_x\\) depending \\(M\\) \\(x\\) \\(\\theta\\).\ngeneral argument marginal density relies coarea formula.log-likelihood observing \\(X = x\\) \n\\[\\ell(\\theta) = \\log g(x \\mid \\theta).\\]\nlog-likelihood often\nimpossible compute analytically difficult expensive compute\nnumerically. complete log-likelihood, \\(\\log f(y \\mid \\theta)\\), often easy \ncompute, don’t know \\(Y\\), \\(M(Y) = x\\).cases possible compute\n\\[Q(\\theta \\mid \\theta') := E_{\\theta'}(\\log f(Y \\mid \\theta) \\mid X = x),\\]\nconditional expectation complete log-likelihood given\nobserved data computed using probability measure given \\(\\theta'\\).\nThus fixed \\(\\theta'\\) computable function \\(\\theta\\) depending\nobserved data \\(x\\).One get following idea: initial guess \n\\(\\theta' = \\theta_0\\) compute iteratively\n\\[\\theta_{n + 1} = \\textrm{arg max} \\ Q(\\theta \\mid \\theta_n)\\]\n\\(n = 0, 1, 2, \\ldots\\). idea EM algorithm:E-step: Compute conditional expectation \\(Q(\\theta \\mid \\theta_n )\\).M-step: Maximize \\(\\theta \\mapsto Q(\\theta \\mid \\theta_n )\\).bit weird present algorithm two-step algorithm abstract\nformulation. Even though can regard \\(Q(\\theta \\mid \\theta_n)\\) \nsomething can compute abstractly \\(\\theta\\) given \\(\\theta_n\\),\nmaximization practice really done using evaluations. \ncomputed either analytic formula involving \\(x\\) \\(\\theta_n\\), \nnumerical algorithm computes certain evaluations \\(Q( \\cdot \\mid \\theta_n)\\)\nperhaps gradient Hessian. computing specific evaluations\n, course, need computation conditional expectations,\ncompute needed upfront.However, important applications EM algorithm, particularly\nexponential families covered Section 8.2, makes lot sense regard\nalgorithm two-step algorithm. case whenever\n\\(Q(\\theta \\mid \\theta_n) = q(\\theta, t(x, \\theta_n))\\) given\nterms \\(\\theta\\) function \\(t(x, \\theta_n )\\) \\(x\\) \\(\\theta_n\\)\ndoesn’t depend \\(\\theta\\). E-step becomes computation \n\\(t(x, \\theta_n )\\), M-step, \\(Q(\\cdot \\mid \\theta_n )\\) \nmaximized maximizing \\(q(\\cdot, t(x, \\theta_n ))\\),\nmaximum function \\(t(x, \\theta_n )\\).","code":""},{"path":"em.html","id":"monotonicity-of-the-em-algorithm","chapter":"8 Expectation maximization algorithms","heading":"8.1.2 Monotonicity of the EM algorithm","text":"prove algorithm (weakly) increases log-likelihood every step,\nthus descent algorithm negative log-likelihood \\(H = - \\ell\\).holds great generality conditional distribution \\(Y\\) given \\(X = x\\)\ndensity\\[\\begin{equation}\nh(y \\mid x, \\theta) = \\frac{f(y \\mid \\theta)}{g(x \\mid \\theta)}\n\\tag{8.1}\n\\end{equation}\\]w.r.t. measure \\(\\mu_x\\) (depend upon \\(\\theta\\)), \n\\(g\\) density marginal distribution.can verified quite easily discrete distributions \n\\(Y = (Z, X)\\) joint density w.r.t. product measure \\(\\mu \\otimes \\nu\\) \ndepend upon \\(\\theta\\). latter case, \\(f(y \\mid \\theta) = f(z, x \\mid \\theta)\\) \n\\[g(x \\mid \\theta) = \\int f(z, x \\mid \\theta) \\ \\mu(\\mathrm{d} z)\\]\nmarginal density w.r.t. \\(\\nu\\).Whenever (8.1) holds follows \\[\\ell(\\theta) = \\log g(x \\mid \\theta) = \\log f(y \\mid \\theta) - \\log h(y \\mid x, \\theta),\\]\n\\(\\ell(\\theta)\\) log-likelihood.Theorem 8.1  \\(\\log f(Y \\mid \\theta)\\) well \\(\\log h(Y \\mid x, \\theta)\\) \nfinite \\(\\theta'\\)-conditional expectation given \\(M(Y) = x\\) \n\\[Q(\\theta \\mid \\theta') > Q(\\theta' \\mid \\theta') \\quad \\Rightarrow \\quad  \\ell(\\theta) > \\ell(\\theta').\\]Proof. Since \\(\\ell(\\theta)\\) depends \\(y\\) \\(M(y) = x\\),\\[\\begin{align*}\n\\ell(\\theta) & = E_{\\theta'} ( \\ell(\\theta) \\mid X = x) \\\\\n& =  \\underbrace{E_{\\theta'} ( \\log f(Y \\mid \\theta) \\mid X = x)}_{Q(\\theta \\mid \\theta')} +  \\underbrace{ E_{\\theta'} ( - \\log h(Y \\mid x, \\theta) \\mid X = x)}_{H(\\theta \\mid \\theta')} \\\\\n& = Q(\\theta \\mid \\theta') + H(\\theta \\mid \\theta'). \n\\end{align*}\\]Now second term find, using Jensen’s inequality\nconvex function \\(-\\log\\), \\[\\begin{align*}\nH(\\theta \\mid \\theta') & = \\int - \\log(h(y \\mid x, \\theta)) h(y \\mid x, \\theta') \\mu_x(\\mathrm{d}y) \\\\\n& = \\int - \\log\\left(\\frac{h(y \\mid x, \\theta)}{ h(y \\mid x, \\theta')}\\right) h(y \\mid x, \\theta') \\mu_x(\\mathrm{d}y) \\\\ \n& \\quad + \\int - \\log(h(y \\mid x, \\theta')) h(y \\mid x, \\theta') \\mu_x(\\mathrm{d}y) \\\\\n& \\geq  -\\log \\left( \\int \\frac{h(y \\mid x, \\theta)}{ h(y \\mid x, \\theta')} h(y \\mid x, \\theta') \\mu_x(\\mathrm{d}y) \\right) + H(\\theta' \\mid \\theta') \\\\\n& = -\\log\\Big(\\underbrace{ \\int h(y \\mid x, \\theta) \\mu_x(\\mathrm{d}y)}_{=1}\\Big)  + H(\\theta' \\mid \\theta') \\\\\n& =  H(\\theta' \\mid \\theta').\n\\end{align*}\\]see \\[\\ell(\\theta) \\geq  Q(\\theta \\mid \\theta') + H(\\theta' \\mid \\theta')\\]\\(\\theta\\) right hand side -called minorant log-likelihood.\nObserving \\[\\ell(\\theta') = Q(\\theta' \\mid \\theta') + H(\\theta' \\mid \\theta')\\]completes proof theorem.Note proof can also given referring Gibbs’ inequality information theory\nstating Kullback-Leibler divergence positive, equivalently\ncross-entropy \\(H(\\theta \\mid \\theta')\\) smaller \nentropy \\(H(\\theta' \\mid \\theta')\\), proof , , \nconsequence Jensen’s inequality just .follows Theorem 8.1 \\(\\theta_n\\) computed\niteratively starting \\(\\theta_0\\) \n\\[Q(\\theta_{n+1} \\mid \\theta_{n}) > Q(\\theta_{n} \\mid \\theta_{n}),\\]\n\n\\[H(\\theta_0) > H(\\theta_1) > H(\\theta_2) > \\ldots.\\]\nproves EM algorithm strict descent algorithm negative\nlog-likelihood long possible iteration find\n\\(\\theta\\) \\(Q(\\theta \\mid \\theta_{n}) > Q(\\theta_{n} \\mid \\theta_{n}).\\)term EM algorithm reserved specific algorithm maximizes\n\\(Q(\\cdot \\mid \\theta_n)\\) M-step, \nreason insist M-step maximization. choice\nascent direction \\(Q(\\cdot \\mid \\theta_n)\\) step-length\nguaranteeing sufficient descent \\(H\\) (sufficient ascent \\(Q(\\cdot \\mid \\theta_n)\\))\nenough give descent\nalgorithm. variation usually termed generalized EM algorithm.imagine minorant useful lower bound \ndifficult--compute log-likelihood. additive constant \\(H(\\theta' \\mid \\theta')\\) minorant\n, however, going computable general either, clear \nway use bound quantitatively.","code":""},{"path":"em.html","id":"peppered-moths","chapter":"8 Expectation maximization algorithms","heading":"8.1.3 Peppered moths","text":"return section peppered moths implementation \nEM algorithm multinomial cell collapsing.EM algorithm can implemented two simple functions compute\nconditional expectations (E-step) maximization\ncomplete observation log-likelihood.MLE complete log-likelihood linear estimator,\ncase many examples explicit MLEs.EStep0 MStep0 functions abstract implementations. require\nspecification arguments group X, respectively, become concrete.M-step implemented case complete-data MLE \nlinear estimator, , linear map complete data vector \\(y\\)\ncan expressed terms matrix \\(\\mathbf{X}\\).EM algorithm finally implemented iterative, alternating call\nEStep MStep convergence measured terms relative\nchange iteration iteration sufficiently small.check going step EM algorithm.Note log-axis. EM-algorithm converges linearly (terminology,\nsee Algorithms Convergence). log-rate convergence can estimated\nleast-squares.rate small case implying fast convergence. always case.\nlog-likelihood flat, EM-algorithm can become quite slow \nrate close 1.","code":"\nEStep0 <- function(p, x, group) {\n  x[group] * p / M(p, group)[group]\n}\nMStep0 <- function(n, X)\n  as.vector(X %*% n / (sum(n)))\nEStep <- function(par, x)\n  EStep0(prob(par), x, c(1, 1, 1, 2, 2, 3))\n\nMStep <- function(n) {\n  X <- matrix(\n  c(2, 1, 1, 0, 0, 0,\n    0, 1, 0, 2, 1, 0) / 2,\n  2, 6, byrow = TRUE)\n  \n  MStep0(n, X)\n}\nEM <- function(par, x, epsilon = 1e-6, trace = NULL) {\n  repeat{\n    par0 <- par\n    par <- MStep(EStep(par, x))\n    if(!is.null(trace)) trace()\n    if(sum((par - par0)^2) <= epsilon * (sum(par^2) + epsilon))\n      break\n  } \n  par  ## Remember to return the parameter estimate\n}\n  \nphat <- EM(c(0.3, 0.3), c(85, 196, 341))\nphat## [1] 0.07083693 0.18877365\nEM_tracer <- tracer(\"par\")\nEM(c(0.3, 0.3), c(85, 196, 341), trace = EM_tracer$tracer)## n = 1: par = 0.08038585, 0.22464192; \n## n = 2: par = 0.07118928, 0.19546961; \n## n = 3: par = 0.07084985, 0.18993393; \n## n = 4: par = 0.07083738, 0.18894757; \n## n = 5: par = 0.07083693, 0.18877365;## [1] 0.07083693 0.18877365\nEM_tracer <- tracer(c(\"par0\", \"par\"), N = 0)\nphat <- EM(c(0.3, 0.3), c(85, 196, 341), epsilon = 1e-20, \n           trace = EM_tracer$tracer)\nEM_trace <- summary(EM_tracer)\n  EM_trace <- transform(\n  EM_trace, \n  n = 1:nrow(EM_trace),\n  par_norm_diff = sqrt((par0.1 - par.1)^2 + (par0.2 - par.2)^2)\n)\nqplot(n, log(par_norm_diff), data = EM_trace)\nlog_rate_fit <- lm(log(par_norm_diff) ~ n,  data = EM_trace)\nexp(coefficients(log_rate_fit)[\"n\"])##         n \n## 0.1750251"},{"path":"em.html","id":"EM-exp","chapter":"8 Expectation maximization algorithms","heading":"8.2 Exponential families","text":"consider section special case model \\(\\mathbf{y}\\)\ngiven exponential family Bayesian network Section 6.1.2\n\\(x = M(\\mathbf{y})\\) observed transformation.complete data log-likelihood \n\\[\\theta \\mapsto \\theta^T t(\\mathbf{y}) - \\kappa(\\theta)  = \\theta^T \\sum_{j=1}^m t_j(y_j) -  \\kappa(\\theta),\\]\nfind \n\\[Q(\\theta \\mid \\theta') = \\theta^T \\sum_{j=1}^m E_{\\theta'}(t_j(Y_j) \\mid X = x)  - \nE_{\\theta'}( \\kappa(\\theta) \\mid X = x).\\]maximize \\(Q\\) differentiate \\(Q\\) equate derivative equal zero. \nfind resulting equation \n\\[\\sum_{j=1}^m E_{\\theta'}(t_j(Y_j) \\mid X = x) = E_{\\theta'}( \\nabla \\kappa(\\theta) \\mid X = x).\\]Alternatively, one may also note following general equation finding\nmaximum \\(Q(\\cdot \\mid \\theta')\\)\n\\[\\sum_{j=1}^m E_{\\theta'}(t_j(Y_j) \\mid X = x) = \\sum_{j=1}^m E_{\\theta'}(E_{\\theta}(t_j(Y_j) \\mid y_1, \\ldots, y_{j-1}) \\mid X = x),\\]\nsince\n\\[E_{\\theta'}(\\nabla \\kappa(\\theta)\\mid X = x) = \n\\sum_{j=1}^m E_{\\theta'}(\\nabla \\log \\varphi_j(\\theta) \\mid X = x) = \n\\sum_{j=1}^m E_{\\theta'}(E_{\\theta}(t_j(Y_j) \\mid y_1, \\ldots, y_{j-1}) \\mid X = x) \\]Example 8.1  Continuing Example 6.4 \\(M\\) projection map\n\\[(\\mathbf{y}, \\mathbf{z}) \\mapsto \\mathbf{y}\\]\nsee \\(Q\\) maximized \\(\\theta\\) solving\n\\[\\sum_{,j} E_{\\theta'}(t(Y_{ij} \\mid Z_i) \\mid \\mathbf{Y} = \\mathbf{y}) = \n  \\sum_{} m_i E_{\\theta'}(\\nabla \\kappa(\\theta \\mid Z_i) \\mid \\mathbf{Y} = \\mathbf{y}).\\]using Example 6.2 see \n\\[\\kappa(\\theta \\mid Z_i) = \\frac{(\\theta_1 + \\theta_3 Z_i)^2}{4\\theta_2} - \\frac{1}{2}\\log \\theta_2,\\]\nhence\\[\\nabla \\kappa(\\theta \\mid Z_i) = \\frac{1}{2\\theta_2} \\left(\\begin{array}{cc} \\theta_1 + \\theta_3 Z_i \\\\ \n- \\frac{(\\theta_1 + \\theta_3 Z_i)^2}{2\\theta_2} - 1 \\\\ \\theta_1 Z_i + \\theta_3 Z_i^2 \\end{array}\\right)\n= \\left(\\begin{array}{cc} \\beta_0 + \\nu Z_i \\\\ \n- (\\beta_0 + \\nu Z_i)^2 - \\sigma^2 \\\\ \\beta_0 Z_i + \\nu Z_i^2 \\end{array}\\right).\\]Therefore, \\(Q\\) maximized solving equation\\[\\sum_{,j} \\left(\\begin{array}{cc}  y_{ij} \\\\ -  y_{ij}^2 \\\\ E_{\\theta'}(Z_i \\mid \\mathbf{Y} = \\mathbf{y}) y_{ij} \\end{array}\\right) = \\sum_{}  m_i \\left(\\begin{array}{cc} \\beta_0 + \\nu E_{\\theta'}(Z_i \\mid \\mathbf{Y}_i = \\mathbf{y}_i) \\\\ \n- E_{\\theta'}((\\beta_0 + \\nu Z_i)^2 \\mid \\mathbf{Y} = \\mathbf{y}) - \\sigma^2 \\\\ \\beta_0 E_{\\theta'}(Z_i \\mid \\mathbf{Y} = \\mathbf{y}) + \\nu E_{\\theta'}(Z_i^2 \\mid \\mathbf{Y} = \\mathbf{y}) \\end{array}\\right).\\]\nIntroducing first \\(\\xi_i = E_{\\theta'}(Z_i \\mid \\mathbf{Y} = \\mathbf{y})\\) \n\\(\\zeta_i = E_{\\theta'}(Z_i^2 \\mid \\mathbf{Y} = \\mathbf{y})\\) can rewrite \nfirst last three equations linear equation\n\\[ \\left(\\begin{array}{cc} \\sum_{} m_i& \\sum_{} m_i\\xi_i \\\\ \\sum_{} m_i\\xi_i & \\sum_{} m_i\\zeta_i \\end{array}\\right) \n\\left(\\begin{array}{c} \\beta_0 \\\\  \\nu \\end{array}\\right) = \\left(\\begin{array}{cc}  \\sum_{,j} y_{ij} \\\\ \\sum_{,j} \\xi_i y_{ij} \\end{array}\\right).\\]\nPlugging solution \\(\\beta_0\\) \\(\\nu\\) second equation \nfind\n\\[\\sigma^2 = \\frac{1}{\\sum_{} m_i}\\left(\\sum_{ij} y_{ij}^2 - \\sum_{} m_i(\\beta_0^2 + \\nu^2 \\zeta_i + 2 \\beta_0 \\nu \\xi_i)\\right).\\]solves M-step EM algorithm mixed effects model. \nremains E-step amounts computation \\(\\xi_i\\) \\(\\zeta_i\\).\nknow joint distribution \\(\\mathbf{Y}\\) \\(\\mathbf{Z}\\) Gaussian,\ncan easily compute variances covariances:\n\\[\\mathrm{cov}(Z_i, Z_j) = \\delta_{ij}\\]\\[\\mathrm{cov}(Y_{ij}, Y_{kl}) = \\left\\{ \\begin{array}{ll}  \\nu^2 + \\sigma^2 & \\quad \\text{} = k, j = l \\\\\n\\nu^2 & \\quad \\text{} = k, j \\neq l  \\\\\n0 & \\quad \\text{otherwise } \\end{array} \\right.\\]\\[\\mathrm{cov}(Z_i, Y_{kl}) = \\left\\{ \\begin{array}{ll}  \\nu  & \\quad \\text{} = k \\\\\n0 & \\quad \\text{otherwise } \\end{array} \\right.\\]gives joint Gaussian distribution\n\\[\\left( \\begin{array}{c} \\mathbf{Z} \\\\ \\mathbf{Y} \\end{array} \\right)  \\sim \\mathcal{N}\\left(\n\\left(\\begin{array}{c} \\mathbf{0} \\\\ \\beta_0 \\mathbf{1}\\end{array} \\right), \n\\left(\\begin{array}{cc}  \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{array}\\right)\\right).\\]general formulas computing conditional distributions\nmultivariate Gaussian distribution:\n\\[\\mathbf{Z} \\mid \\mathbf{Y} \\sim \\mathcal{N}\\left( \\Sigma_{12} \\Sigma_{22}^{-1}(\\mathbf{Y} - \\beta_0 \\mathbf{1}), \n\\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21} \\right).\\]\nconditional means, \\(\\xi_i\\), thus coordinates \\(\\Sigma_{12} \\Sigma_{22}^{-1}(\\mathbf{Y} - \\beta_0 \\mathbf{1})\\). conditional second moments, \\(\\zeta_i\\), can found diagonal\nelements conditional covariance matrix plus \\(\\xi_i^2\\).","code":""},{"path":"em.html","id":"fisher-information","chapter":"8 Expectation maximization algorithms","heading":"8.3 Fisher information","text":"statistics relying classical asymptotic theory\nneed estimate Fisher information, e.g. observed Fisher information\n(Hessian negative log-likelihood observed data). numerical\noptimization \\(Q\\) variants\nEM algorithm (like EM gradient acceleration methods) gradient Hessian \\(Q\\)\ncan useful. However, directly inform us Fisher information.\nsection show interesting useful relations \nderivatives log-likelihood observed data derivatives \n\\(Q\\) primary purpose estimating Fisher information.First look peppered moth example, note \\(p = p(\\theta)\\)\nparametrization cell probabilities,\n\\[Q(\\theta \\mid \\theta') = \\sum_{k=1}^K \\frac{x_{j(k)} p_k(\\theta')}{M(p(\\theta'))_{j(k)}} \\log p_k(\\theta),\\]\n\\(j(k)\\) defined \\(k \\A_{j(k)}\\). gradient \\(Q\\) w.r.t.\n\\(\\theta\\) therefore\\[\\nabla_{\\theta} Q(\\theta \\mid \\theta') = \n\\sum_{k = 1}^K \\frac{x_{j(k)} p_k(\\theta')}{M(p(\\theta'))_{j(k)} p_k(\\theta)} \\nabla_{\\theta} p_k(\\theta').\\]recognize previous computations Section 7.2.4\nevaluate \\(\\nabla_{\\theta} Q(\\theta \\mid \\theta')\\) \\(\\theta = \\theta'\\)\nget\\[\\nabla_{\\theta} Q(\\theta' \\mid \\theta') = \\sum_{= 1}^K \\frac{x_{j()} }{M(p(\\theta'))_{j()}} \\nabla_{\\theta} p_i(\\theta') = \\nabla_{\\theta} \\ell(\\theta'),\\]thus gradient \\(\\ell\\) \\(\\theta'\\) actually\nidentical gradient \\(Q(\\cdot \\mid \\theta')\\) \\(\\theta'\\). \ncoincidence, holds generally \n\\[\\nabla_{\\theta} Q(\\theta' \\mid \\theta') = \\nabla_{\\theta} \\ell(\\theta').\\]\nfollows fact derived proof Theorem 8.1\n\\(\\theta'\\) minimizes\\[\\theta \\mapsto \\ell(\\theta) - Q(\\theta \\mid \\theta').\\]Another way phrase minorant \\(\\ell(\\theta)\\) touches\n\\(\\ell\\) tangentially \\(\\theta'\\).case observation \\(\\mathbf{y}\\) consists \\(n\\) ..d. observations\nmodel parameter \\(\\theta_0\\), \\(\\ell\\) well \\(Q(\\cdot \\mid \\theta')\\) sums terms \ngradient identity holds term. particular,\n\\[\\nabla_{\\theta} \\ell(\\theta_0) = \\sum_{=1}^n \\nabla_{\\theta} \\ell_i(\\theta_0) = \\sum_{=1}^n \\nabla_{\\theta} Q_i(\\theta_0 \\mid \\theta_0),\\]\nusing second Bartlett identity\\[\\mathcal{}(\\theta_0) = V_{\\theta_0}(\\nabla_{\\theta} \\ell(\\theta_0))\\]see \\[\\hat{\\mathcal{}}(\\theta_0) =  \\sum_{=1}^n \\big(\\nabla_{\\theta} Q_i(\\theta_0 \\mid \\theta_0) - n^{-1} \\nabla_{\\theta} \\ell(\\theta_0)\\big)\\big(\\nabla_{\\theta} Q_i(\\theta_0 \\mid \\theta_0) - n^{-1} \\nabla_{\\theta} \\ell(\\theta_0)\\big)^T\\]almost unbiased estimator \nFisher information. mean \\(\\mathcal{}(\\theta_0)\\), \nestimator \\(\\theta_0\\) known. Using plug--estimator,\n\\(\\hat{\\theta}\\), \\(\\theta_0\\) get real estimator\\[\\hat{\\mathcal{}} = \\hat{\\mathcal{}}(\\hat{\\theta}) =  \\sum_{=1}^n \\big(\\nabla_{\\theta} Q_i(\\hat{\\theta} \\mid \\hat{\\theta}) - n^{-1} \\nabla_{\\theta} \\ell(\\hat{\\theta})\\big)\\big(\\nabla_{\\theta} Q_i(\\hat{\\theta} \\mid \\hat{\\theta}) - n^{-1} \\nabla_{\\theta} \\ell(\\hat{\\theta})\\big)^T,\\]though \\(\\hat{\\mathcal{}}\\) longer necessarily unbiased.refer \\(\\hat{\\mathcal{}}\\) empirical Fisher information given \nestimator \\(\\hat{\\theta}\\). cases, \\(\\hat{\\theta}\\) maximum-likelihood\nestimator, case \\(\\nabla_{\\theta} \\ell(\\hat{\\theta}) = 0\\) empirical\nFisher information simplifies \n\\[\\hat{\\mathcal{}} = \\sum_{=1}^n \\nabla_{\\theta} Q_i(\\hat{\\theta} \\mid \\hat{\\theta}) \\nabla_{\\theta} Q_i(\\hat{\\theta} \\mid \\hat{\\theta})^T.\\]\nHowever, \\(\\nabla_{\\theta} \\ell(\\hat{\\theta})\\) practice approximately\nequal zero, unclear dropped.peppered moths, data collected ..d. samples \\(n\\)\nindividual specimens tabulated according phenotype, implement\nempirical Fisher information optional possibility centering\ngradients computing information estimate. note \nthree different observations phenotype possible, giving rise \nthree different possible terms sum. implementation\nworks directly tabulated data computing three possible\nterms forming weighted sum according number times \nterm present.test implementation without centering compare\nresult numerically computed hessian using optimHess (\npossible get optim compute Hessian numerically minimizer\nfinal step, optimHess computation separately).Note numerically computed Hessian (observed Fisher information)\nempirical Fisher information different\nestimates quantity. Thus supposed identical \ngiven data set, supposed estimates thing\nthus similar.alternative empirical Fisher information direct computation \nobserved Fisher information supplemented EM (SEM). general method\ncomputing observed Fisher\ninformation relies EM steps numerical differentiation scheme.\nDefine EM map \\(\\Phi : \\Theta \\mapsto \\Theta\\) \\[\\Phi(\\theta') = \\textrm{arg max}_{\\theta} \\ Q(\\theta \\mid \\theta').\\]global maximum likelihood fixed point \\(\\Phi\\), \nEM algorithm searches fixed point \\(\\Phi\\), , solution \\[\\Phi(\\theta) = \\theta.\\]Variations EM-algorithm can often seen ways \nfind fixed point \\(\\Phi\\). \n\\[\\ell(\\theta) = Q(\\theta \\mid \\theta') + H(\\theta \\mid \\theta')\\]\nfollows observed Fisher information equals\\[\\hat{}_X := - D^2_{\\theta} \\ell(\\hat{\\theta}) = \n\\underbrace{-D^2_{\\theta} Q(\\hat{\\theta} \\mid \\theta')}_{= \\hat{}_Y(\\theta')} - D\n\\underbrace{^2_{\\theta} H(\\hat{\\theta} \\mid \\theta')}_{= \\hat{}_{Y \\mid X}(\\theta')}.\\]possible compute \\(\\hat{}_Y := \\hat{}_Y(\\hat{\\theta})\\).\npeppered moths (exponential families)\ndifficult computing Fisher information complete observations.want compute \\(\\hat{}_X\\) \\(\\hat{}_{Y \\mid X} := \\hat{}_{Y \\mid X}(\\hat{\\theta})\\)\ncomputable either. can, however, shown \\[D_{\\theta} \\Phi(\\hat{\\theta})^T = \\hat{}_{Y\\mid X} \\left(\\hat{}_Y\\right)^{-1}.\\]Hence\n\\[\\begin{align}\n\\hat{}_X & = \\left(- \\hat{}_{Y\\mid X} \\left(\\hat{}_Y\\right)^{-1}\\right) \\hat{}_Y \\\\\n& = \\left(- D_{\\theta} \\Phi(\\hat{\\theta})^T\\right) \\hat{}_Y.\n\\end{align}\\]Though EM map \\(\\Phi\\) might simple analytic expression,\nJacobian, \\(D_{\\theta} \\Phi(\\hat{\\theta})\\), can computed via numerical\ndifferentiation implemented \\(\\Phi\\). also need \nhessian map \\(Q\\), implement R function well.R package numDeriv contains functions compute numerical derivatives.Hessian \\(Q\\) can computed using package.Supplemented EM can implemented computing Jacobian \n\\(\\Phi\\) using numDeriv well.statistics, actually need inverse Fisher information, can\ncomputed inverting \\(\\hat{}_X\\), also following\ninteresting identity\\[\\begin{align}\n\\hat{}_X^{-1} & = \\hat{}_Y^{-1} \\left(- D_{\\theta} \\Phi(\\hat{\\theta})^T\\right)^{-1} \\\\\n & = \\hat{}_Y^{-1} \\left(+ \\sum_{n=1}^{\\infty} \\left(D_{\\theta} \\Phi(\\hat{\\theta})^T\\right)^n \\right) \\\\\n & = \\hat{}_Y^{-1} + \\hat{}_Y^{-1} D_{\\theta} \\Phi(\\hat{\\theta})^T \\left(- D_{\\theta} \\Phi(\\hat{\\theta})^T\\right)^{-1}\n\\end{align}\\]second identity follows \nNeumann series.last formula explicitly gives asymptotic variance incomplete\nobservation \\(X\\) asymptotic variance complete observation \\(Y\\) plus\ncorrection term.SEM implementation relies hessian jacobian functions \nnumDeriv package numerical differentiation.possible implement computation hessian \\(Q\\) analytically\npeppered moths, illustrate functionality numDeriv package\nimplemented computation numerically .Variants strategy computing \\(D_{\\theta} \\Phi(\\hat{\\theta})\\) via\nnumerical differentiation suggested literature, specifically\nusing difference quotient approximations along \nsequence EM steps. going work well standard numerical\ndifferentiation since method ignores numerical errors, algorithm\ngets sufficiently close MLE, numerical errors dominate \ndifference quotients.","code":"\nempFisher <- function(par, x, grad, center = FALSE) {\n  grad_MLE <- 0 ## is supposed to be 0 in the MLE\n  if (center) \n     grad_MLE <-  grad(par, x) / sum(x)\n   grad1 <- grad(par, c(1, 0, 0)) - grad_MLE\n   grad2 <- grad(par, c(0, 1, 0)) - grad_MLE\n   grad3 <- grad(par, c(0, 0, 1)) - grad_MLE\n   x[1] * t(grad1) %*% grad1 + \n     x[2] * t(grad2) %*% grad2 + \n     x[3] * t(grad3) %*% grad3 \n}\n## The gradient of Q (equivalently the log-likelihood) was \n## implemented earlier as 'grad_loglik'.\ngrad <- function(par, x) grad_loglik(par, x, prob, Dprob, c(1, 1, 1, 2, 2, 3))\nempFisher(phat, c(85, 196, 341), grad)##           [,1]     [,2]\n## [1,] 18487.558 1384.626\n## [2,]  1384.626 6816.612\nempFisher(phat, c(85, 196, 341), grad, center = TRUE)##           [,1]     [,2]\n## [1,] 18487.558 1384.626\n## [2,]  1384.626 6816.612\noptimHess(phat, loglik, grad_loglik, x = c(85, 196, 341), \n          prob = prob, Dprob = Dprob, group = c(1, 1, 1, 2, 2, 3))##           [,1]     [,2]\n## [1,] 18490.938 1384.629\n## [2,]  1384.629 6816.769\nQ <- function(p, pp, x = c(85, 196, 341), group) {\n  p[3] <- 1 - p[1] - p[2]\n  pp[3] <- 1 - pp[1] - pp[2]\n  - (x[group] * prob(pp) / M(prob(pp), group)[group]) %*% log(prob(p))\n}\nlibrary(numDeriv)\niY <- hessian(Q, phat, pp = phat, group = c(1, 1, 1, 2, 2, 3))\nPhi <- function(pp) MStep(EStep(pp, x = c(85, 196, 341)))\nDPhi <- jacobian(Phi, phat)  ## Using numDeriv function 'jacobian'\niX <- (diag(1, 2) - t(DPhi)) %*% iY\niX##           [,1]     [,2]\n## [1,] 18487.558 1384.626\n## [2,]  1384.626 6816.612\niYinv <- solve(iY)\niYinv + iYinv %*% t(solve(diag(1, 2) - DPhi, DPhi))##               [,1]          [,2]\n## [1,]  5.492602e-05 -1.115686e-05\n## [2,] -1.115686e-05  1.489667e-04\nsolve(iX) ## SEM-based, but different use of inversion##               [,1]          [,2]\n## [1,]  5.492602e-05 -1.115686e-05\n## [2,] -1.115686e-05  1.489667e-04"},{"path":"em.html","id":"revisiting-gaussian-mixtures","chapter":"8 Expectation maximization algorithms","heading":"8.4 Revisiting Gaussian mixtures","text":"two-component Gaussian mixture model marginal density \ndistribution \\(Y\\) \n\\[ f(y) = p \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} e^{-\\frac{(y - \\mu_1)^2}{2 \\sigma_1^2}} + \n(1 - p)\\frac{1}{\\sqrt{2 \\pi \\sigma_2^2}}e^{-\\frac{(y - \\mu_2)^2}{2 \\sigma_2^2}}.\\]\nfollowing simulation data mixture model.implement log-likelihood assuming variances known. Note\nimplementation takes just one single parameter argument, \nsupposed vector parameters model. Internally \nfunction one decide entry parameter vector \nparameter model corresponds .Without implementations, optim can find \nmaximum-likelihood estimate sensible initial parameter guess.\ncase use true parameters, can used \nalgorithms tested, , course, available \nreal applications.However, initialize optimization badly, find maximum\nlocal maximum instead.implement EM algorithm Gaussian mixture model \nimplementing E-step M-step function. know Section\n6.4.1 complete log-likelihood looks, E-step\nbecomes matter computing\n\\[p_i(\\mathbf{y}) = E(1(Z_i = 1) \\mid \\mathbf{Y} = \\mathbf{y}) = P(Z_i = 1 \\mid  \\mathbf{Y} = \\mathbf{y}).\\]\nM-step becomes identical MLE, can found explicitly,\nindicators \\(1(Z_i = 1)\\) \\(1(Z_i = 2) = 1 - 1(Z_i = 1)\\) \nreplaced conditional probabilities \\(p_i(\\mathbf{y})\\) \n\\(1 - p_i(\\mathbf{y})\\), respectively.EM algorithm may, just optimization algorithm,\nend local maximum, started wrongly.","code":"\nsigma1 <- 1\nsigma2 <- 2\nmu1 <- -0.5\nmu2 <- 4\np <- 0.5\nn <- 1000\nz <- sample(c(TRUE, FALSE), n, replace = TRUE, prob = c(p, 1 - p))\ny <- numeric(n)\nn1 <- sum(z)\ny[z] <- rnorm(n1, mu1, sigma1)\ny[!z] <- rnorm(n - n1, mu2, sigma2)\nloglik <- function(par, y) {\n  p <- par[1]\n  if(p < 0 || p > 1)\n    return(Inf)\n  \n  mu1 <- par[2]\n  mu2 <- par[3]\n  -sum(log(p * exp(-(y - mu1)^2 / (2 * sigma1^2)) / sigma1 + \n             (1 - p) * exp(-(y - mu2)^2 / (2 * sigma2^2)) / sigma2))\n}\noptim(c(0.5, -0.5, 4), loglik, y = y)[c(1, 2)]## $par\n## [1]  0.4934452 -0.5495679  4.0979106\n## \n## $value\n## [1] 1384.334\noptim(c(0.9, 3, 1), loglik, y = y)[c(1, 2)]## $par\n## [1] 0.2334382 5.6763596 0.6255516\n## \n## $value\n## [1] 1509.86\nEStep <- function(par, y) {\n  p <- par[1]\n  mu1 <- par[2]\n  mu2 <- par[3]\n  a <- p * exp(- (y - mu1)^2 / (2 * sigma1^2)) / sigma1 \n  b <- (1 - p) * exp(- (y - mu2)^2 / (2 * sigma2^2)) / sigma2\n  b / (a + b)\n}\n\nMStep <- function(y, pz) {\n  n <- length(y)\n  N2 <- sum(pz)\n  N1 <- n - N2\n  c(N1 / n, sum((1 - pz) * y) / N1, sum(pz * y) / N2)\n}\n\nEM <- function(par, y, epsilon = 1e-12) {\n  repeat{\n    par0 <- par\n    par <- MStep(y, EStep(par, y))\n    if(sum((par - par0)^2) <= epsilon * (sum(par^2) + epsilon))\n      break\n  } \n  par  ## Remember to return the parameter estimate\n}\n\nEM(c(0.5, -0.5, 4), y)## [1]  0.4934443 -0.5497060  4.0982383\nEM(c(0.9, 3, 1), y)## [1] 0.2334722 5.6759456 0.6256279"},{"path":"StochOpt.html","id":"StochOpt","chapter":"9 Stochastic Optimization","heading":"9 Stochastic Optimization","text":"Numerical optimization involves different tradeoffs \nexploration-exploitation tradeoff.\none hand, objective function must thoroughly explored\nbuild adequate model . hand, model \nexploited find minimum quickly. Another tradeoff \naccuracy model time takes compute .optimization algorithms considered Chapters 7 8\nwork available data take deterministic steps iteration.\nmodels based accurate local computations derivatives can \ngreedily exploit local model obtained derivatives, \nlittle exploration.including randomness optimization algorithms \npossible lower computational costs make algorithms \nexploratory. can done various ways. Examples stochastic\noptimization algorithms include simulated annealing evolutionary algorithms \nincorporate randomness iterative steps purpose exploring\nobjective function better deterministic algorithm able . \nparticular, avoid getting stuck saddle points escape local minima.\nStochastic gradient algorithms form another example, descent\ndirections approximated gradients random subsets data.literature stochastic optimization huge, chapter \ncover examples particular relevance statistics machine\nlearning. prominent applications large scale optimization,\nstochastic gradient algorithms \nbecome standard solution. dimension optimization problem becomes\nlarge, second order methods become prohibitively slow, \nnumber observations also large, even one computation gradient\nentire data batch becomes time consuming. cases,\nstochastic gradient algorithms, originate online learning, can make\nprogress quickly still using entire batch data.","code":""},{"path":"StochOpt.html","id":"SG-alg","chapter":"9 Stochastic Optimization","heading":"9.1 Stochastic gradient algorithms","text":"Stochastic gradient algorithms origin online learning\nframework, data arrives sequentially stream data points \nobjective function expected loss. Robbins Monro (1951) introduced \nseminal paper variant online stochastic gradient algorithm \ncalled stochastic approximation method, established \nfirst convergence result algorithms. understand stochastic\ngradient algorithms supposed optimize, introduce \ngeneral framework population model give conditions ensure\nbasic online algorithm converges. Subsequently, basic online\nalgorithm turned algorithm batch data, algorithm \nprimary interest. following sections, various beneficial extensions \nbasic batch algorithm explored.","code":""},{"path":"StochOpt.html","id":"Pop-model","chapter":"9 Stochastic Optimization","heading":"9.1.1 Population models and loss functions","text":"consider observations sample\nspace \\(\\mathcal{X}\\), interested estimating parameters\nparameter space \\(\\Theta\\). loss function\n\\[L : \\mathcal{X} \\times \\Theta \\\\mathbb{R}\\]\nfixed throughout, want minimize expected\nloss also known risk. , want minimize\n\\[H(\\theta) = E(L(X, \\theta)) = \\int L(x, \\theta) \\mathrm{\\mu}(dx)\\]\n\\(X \\\\mathcal{X}\\) distribution \\(\\mu\\).\ncourse, implicitly understood expectation well\ndefined \\(\\theta\\).Example 9.1  Suppose \\(X = (Y, Z)\\) \\(Y\\) real valued random variable, \n\\(\\mu(z, \\theta)\\) denotes parametrized mean value conditionally \\(Z = z\\).\nsquared error loss,\n\\[L((y,z), \\theta) = \\frac{1}{2} (y - \\mu(z, \\theta))^2,\\]\nrisk mean squared error\n\\[\\mathrm{MSE}(\\theta) = \\frac{1}{2} E (Y - \\mu(Z, \\theta))^2.\\]\ndefinition \\(\\mathrm{MSE}(\\theta)\\) see \n\\[2 \\mathrm{MSE}(\\theta) =  E(Y - E(Y \\mid Z))^2 + E (E(Y\\mid Z) - \\mu(Z, \\theta))^2,\\]\nfirst term depend upon \\(\\theta\\). Thus minimizing \nmean squared error finding \\(\\theta_0\\) \\(\\mu(z, \\theta_0)\\)\noptimal approximation \\(E(Y \\mid Z = z)\\) squared error sense.Note link distribution \\(X\\) parameter defined\nexample choice loss function. upfront\nassumption \\(E(Y\\mid Z = z) = \\mu(z, \\theta_0)\\) \\(\\theta_0 \\\\Theta\\),\n\\(\\theta_0\\), clearly minimizer. general, \noptimal \\(\\theta_0\\) simply \\(\\theta\\) minimizes risk.alternative squared error loss log-likelihood loss,\ncan used parametrized family distributions.Example 9.2  Suppose \\(f_{\\theta}\\) denotes density \\(\\mathcal{X}\\) parametrized \n\\(\\theta\\). log-likelihood loss \n\\[L(x, \\theta) = - \\log f_{\\theta}(x).\\]\ncorresponding risk,\n\\[H(\\theta) = - E \\log(f_{\\theta}(X)),\\]\nknown cross-entropy. distribution \\(X\\) density \\(f^0\\) \n\\[\\begin{align*}\nH(\\theta) & = - E \\log(f^0(X)) - E \\log(f_{\\theta}(X)/f^0(X)) \\\\\n& = H(f^0) + D(f^0 \\ || \\ f_{\\theta})\n\\end{align*}\\]\nfirst term entropy \\(f^0\\), second \nKullback-Leibler divergence \\(f_{\\theta}\\) \\(f^0\\).\nentropy depend upon \\(\\theta\\) minimizing\ncross-entropy thus finding \\(\\theta\\) \\(f_{\\theta}\\)\noptimal approximation \\(f^0\\) Kullback-Leibler sense.consider now regression setup Example 9.1\n\\(X = (Y, Z)\\), let\n\\[f_{\\theta}(y | z) = e^{- \\mu(z, \\theta)} \\frac{y^{\\mu(z, \\theta)}}{y!}\\]\ndenote Poisson point probabilities Poisson distribution mean\n\\(\\mu(z, \\theta)\\) conditionally \\(Z = z\\). log-likelihood loss \n\\[- \\log f_{\\theta}(y | z) = \\mu(z, \\theta) - y \\log(\\mu(z, \\theta))\\]\nadditive constant depending \\(\\theta\\), cross-entropy \n\\[H(\\theta) = E\\big(\\mu(Z, \\theta) - E(Y \\mid Z) \\log(\\mu(Z, \\theta))\\big),\\]\nadditive constant. risk function quantifies \\(\\mu(Z, \\theta)\\)\ndeviates \\(E(Y \\mid Z)\\) different way risk based \nsquared error loss. However, \\(E(Y \\mid Z = z) = \\mu(z, \\theta_0)\\) \n\\(\\theta_0\\), still true \\(\\theta_0\\) minimizer, cf. \nExercise 9.1.log-likelihood loss appropriate loss function \nparametrized family distributions fits data well.\nGaussian (conditional) mean value model log-likelihood loss gives \nrisk squared error loss – squared error loss can\nalso suitable even data Gaussian. can suitable loss\nwhenever just want fit model conditional\nmean \\(Y\\). want fit (conditional) median instead, \ncan use absolute deviation\n\\[L((y,z), \\theta) = |y - \\mu(z, \\theta)|.\\]\nspecial case check loss functions used \nquantile regression.\nThus choosing loss function decide aspects \ndistribution model, , whether want good global fit \nlog-likelihood loss, fit mean value squared error loss, \nfit median another quantile check loss.","code":""},{"path":"StochOpt.html","id":"online-sg","chapter":"9 Stochastic Optimization","heading":"9.1.2 Online stochastic gradient algorithm","text":"classical stochastic gradient algorithm example online\nlearning algorithm.\nbased simple observation can\ninterchange differentiation expectation \n\\[\\nabla H(\\theta) = E \\left( \\nabla_{\\theta} L(X, \\theta) \\right),\\]\nthus \\(X_1, X_2, \\ldots\\) form ..d. sequence \\(\\nabla_{\\theta} L(X_i, \\theta)\\)\nunbiased estimate gradient \\(H\\) \\(\\theta\\) \\(\\).\ninspiration gradient descent algorithms natural suggest\nstochastic parameter updates form\n\\[\\theta_{n + 1} = \\theta_n - \\gamma_n \\nabla_{\\theta} L(X_{n+1}, \\theta_n)\\]\nstarting initial value \\(\\theta_0\\). direction,\n\\(\\nabla_{\\theta} L(X_{n+1}, \\theta_n)\\), , however, guaranteed \ndescent direction \\(H\\), even , \\(\\gamma_n\\) typically \nchosen guarantee descent.sequence step size parameters \\(\\gamma_n \\geq 0\\) known collectively\nlearning rate. can deterministic sequence, \\(\\gamma_n\\) may\nalso depend \\(X_1, \\ldots, X_{n}\\) \\(\\theta_0, \\ldots, \\theta_{n}\\).\nstochastic gradient algorithms, convergence can shown global\nconditions decay learning rate rather local conditions\nindividual step lengths.Theorem 9.1  Suppose \\(H\\) strongly convex \n\\[E(\\|\\nabla_{\\theta} L(X, \\theta))\\|^2) \\leq + B \\|\\theta\\|^2.\\]\n\\(\\theta^*\\) global minimizer \\(H\\) \\(\\theta_n\\) converges almost surely\ntoward \\(\\theta^*\\) \n\\[\\begin{equation}\n\\sum_{n=1}^{\\infty} \\gamma_n^2 < \\infty \\quad \\text{} \\quad \n\\sum_{n=1}^{\\infty} \\gamma_n = \\infty. \\tag{9.1}\n\\end{equation}\\]result, convergence algorithm\nguaranteed learning rate, \\(\\gamma_n\\), converges 0 \nsufficiently slowly. Though formulated slightly different way,\nRobbins Monro (1951) first demonstrate convergence online\nlearning algorithm conditions learning rate.\nFollowing terminology, much since written online\nlearning adaptive control theory name stochastic approximation,\nLai (2003).precise way learning\nrate decays known decay schedule, flexible\nthree-parameter power law family decay schedules given \n\\[\\gamma_n = \\frac{\\gamma_0  K}{K + n^} = \\frac{\\gamma_0 }{1 + K^{-1} n^{}}\\]\ninitial learning rate \\(\\gamma_0 > 0\\) constants \\(K, > 0\\). \n\\(\\(0.5, 1]\\) resulting learning rate satisfies convergence conditions\n(9.1).\nFigure 9.1: Power law decay schedules function \\(n\\) \\(\\gamma_0 = 1\\) different choices \\(K\\) \\(.\\) left figure shows decay schedules \\(\\) chosen convergence conditions fulfilled, whereas right figure shows decay schedules convergence conditions fulfilled.\nparameter \\(\\gamma_0\\) determines initial baseline rate, Figure 9.1\nillustrates effect parameters \\(K\\) \\(\\) decay. parameter\n\\(\\) asymptotic exponent \\(\\gamma_n \\sim \\gamma_0 K n^{-}\\), \\(K\\)\ndetermines quickly rate turn pure power law decay. Moreover,\ntarget rate, \\(\\gamma_{1}\\), want hit \\(n_{1}\\)\niterations, fix exponent \\(\\), can also solve \\(K\\) find\n\\[K = \\frac{n_1^\\gamma_1}{\\gamma_0 - \\gamma_1}.\\]\ngives us decay schedule interpolates \\(\\gamma_0\\)\n\\(\\gamma_1\\) range \\(0, \\ldots, n_1\\) iterations.implement decay_scheduler() function returns particular\ndecay schedule, possibility determine \\(K\\) automatically \ntarget rate.following example online Poisson regression illustrates general ideas.Example 9.3  example \\(Y_i \\mid Z_i = z_i \\sim \\mathrm{Pois}(\\varphi(\\beta_0 + \\beta_1 z_i))\\)\n\\(\\beta = (\\beta_0, \\beta_1)^T\\) parameter vector \n\\(\\varphi: \\mathbb{R} \\(0,\\infty)\\) continuously differentiable function.\nlet \\(Z_i\\)-s uniformly distributed \\((-1, 1)\\), choice \nparticularly important. conditional mean \\(Y_i\\) given \\(Z_i = z_i\\) \n\\[\\mu(z_i, \\beta) = \\varphi(\\beta_0 + \\beta_1 z_i)\\]\nfirst consider squared error loss. end, observe \n\\[\\nabla_{\\beta}  \\mu(z_i, \\beta) =  \\varphi'(\\beta_0 + \\beta_1 z_i) \n\\left( \\begin{array}{c} 1 \\\\ z_i \\end{array} \\right),\\]\nsquared error loss results gradient\n\\[\\nabla_{\\beta} \\frac{1}{2} (y_i - \\mu(z_i, \\beta) )^2 = \n  \\varphi'(\\beta_0 + \\beta_1 z_i) (\\mu(z_i, \\beta) - y_i) \\left( \\begin{array}{c} 1 \\\\ z_i \\end{array} \\right).\\]simulate data explore learning algorithm special case\n\\(\\varphi = \\exp\\). clearly emulate online nature algorithm,\nimplementation generates observations sequentially loop.log-likelihood loss instead find gradient\n\\[\\nabla_{\\beta} \\big( \\mu(z_i, \\beta) - y_i \\log(\\mu(z_i, \\beta)) \\big) = \n  \\frac{\\varphi'(\\beta_0 + \\beta_1 z_i)}{\\mu(z_i, \\beta)} (\\mu(z_i, \\beta) - y_i) \n\\left( \\begin{array}{c} 1 \\\\ z_i \\end{array} \\right),\\]\nleads slightly different equally valid algorithm. special\ncase \\(\\varphi = \\exp\\), derivative \\(\\varphi'(\\beta_0 + \\beta_1 z_i) = \\mu(z_i, \\beta)\\),\n\\[\\nabla_{\\beta} \\big( \\mu(z_i, \\beta) - y_i \\log(\\mu(z_i, \\beta)) \\big) = \n  (\\mu(z_i, \\beta) - y_i) \n\\left( \\begin{array}{c} 1 \\\\ z_i \\end{array} \\right),\\]\nlog-likelihood gradient differs squared error gradient \nlacking factor \\(\\mu(z_i, \\beta)\\).\n\\(Z\\) uniformly distributed \\((-1, 1\\)), distribution \n\\(\\mu(Z, (2, 3))\\) range \\(e^{-1} \\simeq 0.3679\\)\n\\(e^5 \\simeq 148.4\\) right skewed, , concentrated toward \nsmaller values long right tail. median \\(e^2 \\simeq 7.389\\),\nmean \\((e^5 - e) / 6 \\simeq 24.67\\).squared error gradient typically longer log-likelihood\ngradient due factor \\(\\mu(z_i, \\beta)\\) — sometimes large factor.\nimplementation \ngradient log-likelihood therefore choose \\(\\gamma_0\\)\nfactor 25 larger \\(\\gamma_0 = 0.0004\\) used \nsquared error gradient.\nFigure 9.2: Estimated parameter values two parameters \\(\\beta_0\\) (true value \\(2\\)) \\(\\beta_1\\) (true value \\(3\\)) Poisson regression model function number data points online stochastic gradient algorithm.\nFigure 9.2 shows estimates \\(\\beta_0\\) \\(\\beta_1\\)\nconverge toward true values two different choices loss functions.\n\\(\\varphi = \\exp\\) either loss function, risk, \\(H(\\beta)\\), \nstrongly convex attains unique minimum \\(\\beta^* = (2, 3)^T\\).\nTheorem 9.1 suggests convergence appropriate learning\nrates – except growth condition gradient fulfilled\n\\(\\varphi = \\exp\\). growth condition fulfilled replace\nexponential function softplus, \\(\\varphi(w) = \\log(1 + e^w)\\), \nsmall values \\(w\\) behaves like exponential function, grows linearly\n\\(w\\) \\(w \\\\infty\\).gradient squared error loss resulted slower convergence \njagged sample path gradient log-likelihood. \nexplained random factor \\(\\mu(z_i, \\beta)\\) squared error gradient,\nmakes step sizes somewhat irregular data Poisson\ndistribution. also makes choosing \\(\\gamma_0\\) delicate balance.\nsmall increase \\(\\gamma_0\\) make algorithm unstable, \ndecreasing \\(\\gamma_0\\) make convergence even slower, though\nfluctuations also damped. Making right choice – even suitable choice –\ndecay schedule depends heavily problem considered \ngradient used. problem specific challenge find good schedule –\neven just choose three parameters use power law schedule.example illustrates loss function determines \nmodel, also well learning algorithm works. loss\nfunctions appropriate, since data Poisson distribution,\nlog-likelihood loss leads faster convergence. Exercise\n9.2 explores opposite situation data\nGaussian distribution.","code":"\ndecay_scheduler <- function(gamma0 = 1, a = 1, K = 1, gamma1, n1) {\n  force(a)\n  if (!missing(gamma1) && !missing(n1))\n    K <- n1^a * gamma1 / (gamma0 - gamma1)\n  b <- gamma0 * K\n  function(n) b / (K + n^a)\n}\nN <- 5000\nbeta_true = c(2, 3)\nmu <- function(z, beta) exp(beta[1] + beta[2] * z)\nbeta <- vector(\"list\", N)\n\nrate <- decay_scheduler(gamma0 = 0.0004, K = 100) \nbeta[[1]] <- c(beta0 = 1, beta1 = 1)\n\nfor(i in 2:N) {\n  # Simulating a new data point\n  z <- runif(1, -1, 1)\n  y <- rpois(1, mu(z, beta_true))\n  # Update via squared error gradient \n  mu_old <- mu(z, beta[[i - 1]])\n  beta[[i]] <- beta[[i - 1]]  - rate(i) * mu_old * (mu_old - y) * c(1, z)\n}\nbeta[[N]]  # This is close to beta_true; the algorithm works!##    beta0    beta1 \n## 2.037990 2.987941\nrate <- decay_scheduler(gamma0 = 0.01, K = 100) \nbeta[[1]] <- c(beta0 = 1, beta1 = 1)\n\nfor(i in 2:N) {\n  # Simulating a new data point\n  z <- runif(1, -1, 1)\n  y <- rpois(1, mu(z, beta_true))\n  # Update via log-likelihood gradient \n  mu_old <- mu(z, beta[[i - 1]])\n  beta[[i]] <- beta[[i - 1]]  - rate(i) * (mu_old - y) * c(1, z)\n}\nbeta[[N]]  # This is close to beta_true; this algorithm also works!##    beta0    beta1 \n## 2.008452 2.987035"},{"path":"StochOpt.html","id":"batch-stochastic-gradient-algorithms","chapter":"9 Stochastic Optimization","heading":"9.1.3 Batch stochastic gradient algorithms","text":"online algorithm store data, data point used \nforgotten. online algorithm working context data arrives \nstream data points model updated continually. statistics,\nfrequently encounter batch algorithms, entire batch \ndata points stored processed algorithm, data point\nbatch can accessed many times like. However, batch\nsufficiently large, many standard batch algorithms slow, \nideas online algorithms can beneficially transferred batch\nprocessing data.Within population model framework Section 9.1.1\nobjective minimize population risk, \\(H(\\theta)\\), defined\nexpected loss w.r.t. probability distribution \\(\\mu\\). online\nalgorithms imagine endless stream data points \\(\\mu\\), can \nused ultimately minimize \\(H(\\theta)\\). Batch algorithms replace\npopulation quantity empirical surrogate — average loss batch\n\\[H_N(\\theta) = \\frac{1}{N} \\sum_{=1}^N L(x_i, \\theta).\\]\nMinimizing \\(H_N\\) surrogate minimizing \\(H\\) known \nempirical risk minimization.data ..d. standard deviation \\(H_N(\\theta)\\) decays \\(N^{-1/2}\\) \\(N\\),\nrun time computation\nincreases \\(N\\). Thus invest computation time using \ndata get diminishing returns; doubling run time lower \nprecision \\(H_N\\) approximation \\(H\\) factor \\(1 / \\sqrt{2} \\simeq 0.7\\).\n, course,\ntrue look gradient, \\(\\nabla H_N(\\theta)\\), higher derivatives\n\\(H_N\\).obvious computational costs using entire data set\ncompute gradient, say, worth effort compared using \nfraction data. Thus ask better tradeoff \nrun time precision using fraction data points time\ncompute gradient? stochastic gradient algorithm one\nalgorithm “cycles data” uses random\nfraction data points computation. basic version presented\nfirst, uses single data point iteration, \nreally algorithm presented Section 9.1.2\nexcept population risk replaced empirical risk defined \nterms batch data.\\(\\hat{\\mu}_N = \\frac{1}{N}\\sum_{=1}^N \\delta_{x_i}\\) denoting \nempirical distribution batch data set, see \n\\[H_N(\\theta) = \\int L(x, \\theta) \\mathrm{\\mu}_N(dx),\\]\n, empirical risk simply expected loss respect \nempirical distribution. Thus minimize \\(H_N\\) can use online\napproach sampling observations \\(\\hat{\\mu}_N\\). leads \nfollowing basic version stochastic gradient algorithm applied \ndata batch.initial parameter value, \\(\\theta_0\\), iteratively compute\nparameters follows: given \\(\\theta_{n}\\)sample index \\(\\) uniformly \\(\\{1, \\ldots, N\\}\\)compute \\(\\rho_n = \\nabla_{\\theta} L(x_i, \\theta_{n})\\)update parameter \\(\\theta_{n+1} = \\theta_{n} - \\gamma_n \\rho_n.\\)Note sampling index \\(\\) equivalent sampling observation\n\\(\\hat{\\mu}_N\\), turn nonparametric bootstrapping.\nJust online setting, sequence learning rates, \\(\\gamma_n\\),\ntuning parameter algorithm.implement basic stochastic gradient algorithm ,\nallowing user defined decay schedule learning rate. However,\ninstead implementing one long loop, divide iterations \nepochs epoch consisting \\(N\\) iterations. implementation,\nmaximal number iterations also given terms epochs, \ndecay schedule applied per epoch basis.also introduce small twist sampling empirical distribution;\ninstead sampling replacement (bootstrapping) sample without\nreplacement. Sampling \\(N\\) indices \\(\\{1, \\ldots, N\\}\\) without replacement\nsampling permutation indices.One epoch algorithm exactly one pass entire batch \ndata points, random order. default value sampler = sample\nmeans resampling done without replacement. call SG() \nsampler = function(N) sample(N, replace = TRUE) get sampling\nreplacement, case epoch pass \n\\(N\\) data points sampled independently batch.\nSampling replacement feed stochastic gradient\nalgorithm ..d. samples empirical distribution. Sampling without\nreplacement introduces dependence. Curiously, sampling without\nreplacement turned empirically superior sampling\nreplacement, recent theoretical\nresults, Gürbüzbalaban, Ozdaglar, Parrilo (2019), support leads faster rate convergence.may ask sampling actually matters, whether just leave \npart algorithm? practice, data sets may come “bad order,”\ninstance unfortunate ordering according \none variables, cycling data points \nordering can easily lead algorithm astray. therefore important \nalways randomize order data points somehow. minimal amount randomization\ncommon use just one initial random permutation, corresponding\nmoving samp <- sampler(N) outside outer -loop . may\nenough randomization algorithm work cases, link\nconvergence result online algorithm lost.Example 9.4  continuation Example 9.3 consider batch version\nPoisson regression. use log-likelihood gradient, \nfirst simulate small data set \\(N = 50\\) data points.Using grad_pois() function , run stochastic gradient\nalgorithm 1000 epochs decay schedule \ninterpolates \\(\\gamma_0 = 0.02\\) \\(\\gamma_1 = 0.001\\).resulting parameter estimate compared maximum-likelihood\nestimate ordinary Poisson regression.batch version stochastic gradient\ndescent converges toward minimizer, \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\),\nempirical risk. contrary \nonline version converges toward minimizer \ntheoretical risk, case \\((\\beta_0, \\beta_1) = (2, 3)\\).\nlarger batch size, \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\) come closer\n\\((2, 3)\\). Figure 9.3\nshows clearly algorithms converge toward limit depends \nbatch size, \\(N = 500\\), limit much closer theoretical\nminimizer.\nFigure 9.3: Estimated parameter values two parameters \\(\\beta_0 =2\\) \\(\\beta_1 = 3\\) Poisson regression model function number iterations stochastic gradient algorithm. batch size \\(N = 50\\), algorithm converges parameter clearly different theoretically optimal one (gray dashed lines), batch size \\(N = 500\\) limit closer \\((2, 3).\\)\nExample 9.4 Figure 9.3, \nparticular, illustrate data set relatively small, algorithm quickly\nattains precision smaller statistical uncertainty, \noptimization therefore futile. However, larger data sets, optimization\ngreater precision can beneficial.","code":"\nSG <- function(\n  par, \n  grad,              # Function of parameter and observation index\n  N,                 # Sample size\n  gamma,             # Decay schedule or a fixed learning rate\n  maxiter = 100,     # Max epoch iterations\n  sampler = sample,  # How data is resampled. Default is a random permutation\n  cb = NULL, \n  ...\n) {\n  gamma <- if (is.function(gamma)) gamma(1:maxiter) else rep(gamma, maxiter) \n  for(k in 1:maxiter) {\n    if(!is.null(cb)) cb()\n    samp <- sampler(N)   \n    for(j in 1:N) {\n      i <-  samp[j]\n      par <- par - gamma[k] * grad(par, i, ...)\n    }\n  }\n  par\n}\nN <- 50\nz <- runif(N, -1, 1)\ny <- rpois(N, mu(z, beta_true))\ngrad_pois <- function(par, i) (mu(z[i], par) - y[i]) * c(1, z[i])\npois_SG_tracer <- tracer(\"par\", N = 0)\nSG(\n  c(0, 0), \n  grad_pois, \n  N = N, \n  gamma = decay_scheduler(gamma0 = 0.02, gamma1 = 0.001, n1 = 1000), \n  maxiter = 1000, \n  cb = pois_SG_tracer$tracer\n)## [1] 1.905394 3.162559\nbeta_hat <- coefficients(glm(y ~ z, family = poisson))## beta_hat: 1.898305 3.15986\nN <- 500\nz <- runif(N, -1, 1)\ny <- rpois(N, mu(z, beta_true))\npois_SG_tracer_2 <- tracer(\"par\", N = 0)\nSG(\n  par = c(0, 0), \n  grad = grad_pois, \n  N = N, \n  gamma = decay_scheduler(gamma0 = 0.02, gamma1 = 0.001, n1 = 100), \n  cb = pois_SG_tracer_2$tracer\n)## [1] 1.994688 3.022137\nbeta_hat_2 <- coefficients(glm(y ~ z, family = poisson))## beta_hat_2: 1.98893 3.027715"},{"path":"StochOpt.html","id":"news","chapter":"9 Stochastic Optimization","heading":"9.1.4 Predicting news article sharing on social media","text":"section illustrate use basic stochastic gradient\nalgorithm learning model predicts \nmany times news article shared social media. data subjected \ntransformations normalizations make use linear model \nsquared error loss reasonable.basic stochastic gradient algorithm linear model introduced\nearly machine learning community 1960 via ADALINE (Adaptive Linear Neuron)\nBernard Widrow Ted Hoff. ADALINE \nimplemented physical device\ncapable learning patterns\nvia stochastic gradient updates. math today, implementation\nfortunately become somewhat easier.linear model squared error loss,\n\\(L((y, x), \\beta) = \\frac{1}{2} (y - \\beta^T x)^2\\),\ngradient becomes\n\\[\\nabla_{\\beta} L((y, x), \\beta) = - x (y - \\beta^T x) = x (\\beta^T x - y),\\]\nresults updates form\n\\[\\beta_{n+1} = \\beta_n - \\gamma_n x_i (\\beta_n^T x_i - y_i).\\]\n, parameter moves direction \\(x_i\\) \\(\\beta_n^T x_i < y_i\\)\ndirection \\(-x_i\\) \\(\\beta_n^T x_i > y_i\\). amount \nmoves controlled partly learning rate, \\(\\gamma_n\\), partly \nsize residual, \\(y_i - \\beta_n^T x_i\\). larger residual gives\nlarger move.following function factory linear models takes model\nmatrix response vector complete data batch arguments implements squared error\nloss function entire batch well gradient single\nobservation. returned list also contains parameter vector \ncorrect dimension, can used initialization optimization\nalgorithms.data, originally collected Fernandes, Vinagre, Cortez (2015), obtained \nUCI Machine Learning Repository\ncontains 39,644 observations 61 variables. One variable integer\nvalued shares, target variable predictions.Two variables, timedelta url, relevant predictors, \nis_weekend redundant given weekday variables, exclude\nthree variables. predictors also highly correlated,\nexclude four additional predictors model matrix constructed.data set current standards large data set – dense model\nmatrix takes 20 MB memory – \nlinear model (target variable shares log-transformed)\ncan easily fitted simply solving least squares problem.\ntakes 0.2 seconds standard laptop compute solution.\nresidual plot Figure 9.4 shows \nmodel actually poor fit data, though considerable\nunexplained residual variance.\nFigure 9.4: Residual plot linear model logarithm news article shares.\nOptimization squared error loss using data set used \nillustrate number stochastic gradient algorithms even though can\ncompute optimizer easily means. real practical benefit \nstochastic gradient algorithms comes applied large scale problems difficult \ntreat textbook examples. using toy problem makes easier \nunderstand detail different algorithms behave.standardize columns \\(X\\) norm. Specifically\nnon-central second moment 1.\nchange optimization problem corresponds \nreparametrization parameters rescaled. rescaling brings\nparameters comparable scale, typically good idea \noptimization algorithms based gradients , see also Exercise\n9.3. rescaling \ninitialize linear model call ls_model() refit \nmodel using new parametrization.first run stochastic gradient algorithm fixed learning rate \n\\(\\gamma = 10^{-5}\\) 50 epochs tracer computes \nstores value objective function epoch.Using trace last epochs, can compare objective function\nvalues minimum found using lm.fit() . minimum \nreached completely 50 epochs took time compute.use profiling investigate time spent\nstochastic gradient algorithm.profiling result shows, unsurprisingly, \ncomputation gradient update parameter vector takes \nrun time. look closer implementation gradient, see\ninnocuously looking subsetting X[, ] \\(\\)-th row actually\nresponsible half run time. also see substantial\nallocation deallocation memory associated line. \nbottleneck R implementation slicing row bigger\nmatrix done without creating copy row, \nparticular line takes much time.investigate convergence basic stochastic gradient algorithm\nrun larger\nlearning rate \\(\\gamma = 5 \\times 10^{-5}\\) power law decay\nschedule, interpolates initial learning rate \\(10^{-3}\\) \nlearning rate \\(10^{-5}\\) 50 epochs.compare convergence three stochastic gradient algorithms \nconvergence gradient descent backtracking. gradient descent choose\n\\(\\gamma = 8 \\times 10^{-2}\\), results initial backtracking\nsteps subsequent steps use step length\n\\(\\gamma = 8 \\times 10^{-2}\\). Choosing larger \\(\\gamma\\) particular\noptimization resulted backtracking step length around \\(8 \\times 10^{-2}\\) \nreached, thus choice \\(\\gamma\\) use minimal amount time \nbacktracking step algorithm.Figure 9.5 shows four algorithms converge.\ngradient descent algorithm converges faster stochastic gradient\nalgorithm low learning rate \\(\\gamma = 10^{-5}\\), \nhigh learning rate \\(\\gamma = 5 \\times 10^{-5}\\) power law decay schedule\nstochastic gradient algorithm converges fast gradient descent.\nparticular run, power law decay schedule shows marginally\nfaster convergence fixed high learning rate, ensured \ntuning decay schedule parameters. Despite\ntheoretical guarantees convergence power law decay schedule,\npractical choice suitable learning rate decay schedule really\nempirical art. large data, tuning\nlearning rate parameters can done small subset\ndata algorithms run using full data set.\nFigure 9.5: Convergence squared error loss news article data set four algorithms: gradient descent (gd) three basic stochastic gradient algorithms low learning rate, high learning rate power law decay schedule.\ncomparison purposes typically better monitor convergence \noptimization algorithms function real time iterations,\ncomparing algorithms terms \nreal time admittedly comparing specific implementations.\nR implementation stochastic gradient algorithm shortcomings\nshared gradient descent algorithm. One\nepoch stochastic gradient algorithm computationally\ndemanding one iteration gradient descent \ncompute gradient data point exactly add .\nvectorized batch gradient computation fairly efficient R, iterative\nlooping data stochastic gradient algorithm , \ninefficiency compounded inefficiency matrix slicing\nresults data copying profiling revealed. Thus \ncomparisons Figure 9.5 arguably \nentirely fair stochastic gradient algorithm – \nspecific R implementation.subsequent section see alternatives basic stochastic\ngradient algorithm, diminish shortcomings pure R\nimplementation somewhat. Another way circumvent shortcomings \nR implementation rewrite algorithm using Rcpp. pursue\nRcpp implementations Section 9.3, first consider\nbeneficial modifications basic algorithm.","code":"\nls_model <- function(X, y) {\n  N <- length(y)\n  X <- unname(X) # Strips X of names\n  list(\n    # Initial parameter value\n    par0 = rep(0, ncol(X)),\n    # Objective function\n    H = function(beta) \n      drop(crossprod(y - X %*% beta)) / (2 * N),\n    # Gradient in a single observation\n    grad = function(beta, i) {  \n      xi <- X[i, ]\n      xi * drop(xi %*% beta - y[i])\n    }\n  )\n}\nNews <- readr::read_csv(\"data/OnlineNewsPopularity.csv\")\nNews <- dplyr::select(\n  News,\n  - url, \n  - timedelta,\n  - is_weekend,\n  - n_non_stop_words,\n  - n_non_stop_unique_tokens,\n  - self_reference_max_shares,\n  - kw_min_max\n)\n# The model matrix without an explicit intercept is constructed from all\n# variables remaining in the data set but the target variable 'shares' \nX <- model.matrix(shares ~ . - 1, data = News)  \nX_raw <- X\n# Standardization and log-transforming the target variable\nX <- scale(X, center = FALSE)\ny <- log(News$shares)\n# The '%<-%' destructure assignment operator is from the zeallot package\nc(par0, H, ls_grad) %<-% ls_model(X, y)\n# Fitting the model using standard linear model computations\nlm_News <- lm.fit(X, y)\npar_hat <- lm_News$coefficients  # Will be used below for comparisons\nSG_tracer <- tracer(\"value\", expr = quote(value <- H(par)))\nSG(\n  par = par0, \n  grad = ls_grad, \n  N = nrow(X), \n  gamma = 1e-5, \n  maxiter = 50, \n  cb = SG_tracer$tracer\n)\nSG_trace_low <- summary(SG_tracer)\ntail(SG_trace_low)##        value    .time\n## 45 0.4268483 17.26148\n## 46 0.4262983 17.82874\n## 47 0.4259705 18.21194\n## 48 0.4251088 18.61303\n## 49 0.4245424 18.99180\n## 50 0.4240064 19.37994\nH(par_hat)## [1] 0.3781595\nprofvis(\n  SG(\n    par = par0, \n    grad = ls_grad, \n    N = nrow(X), \n    gamma = 1e-5,\n    maxiter = 50\n  )\n)\nSG_tracer$clear()\nSG(\n  par = par0, \n  grad = ls_grad, \n  N = nrow(X), \n  gamma = 5e-5, \n  maxiter= 50, \n  cb = SG_tracer$tracer\n)\nSG_trace_high <- summary(SG_tracer)\nSG_tracer$clear()\nSG(\n  par = par0, \n  grad = ls_grad, \n  N = nrow(X),\n  gamma = decay_scheduler(gamma0 = 7e-5, gamma1 = 4e-5, a = 0.5, n1 = 50), \n  maxiter= 50, \n  cb = SG_tracer$tracer\n)\nSG_trace_decay <- summary(SG_tracer)\ngrad_H <- function(beta) crossprod(X, X %*% beta - y) / nrow(X)\nGD_tracer <- tracer(\"value\", N = 10)\nGD(\n  par = par0, \n  H = H, \n  gr = grad_H, \n  gamma = 8e-2, \n  maxiter = 800, \n  cb = GD_tracer$tracer\n)\nGD_trace <- summary(GD_tracer)"},{"path":"StochOpt.html","id":"beyond-basic-stochastic-gradient-algorithms","chapter":"9 Stochastic Optimization","heading":"9.2 Beyond basic stochastic gradient algorithms","text":"gradient \\(\\nabla_{\\theta} L(x_i, \\theta)\\) single random data point quickly\ncomputed, though unbiased estimate \\(\\nabla_{\\theta} H_N(\\theta)\\)\nlarge variance.\naffects basic stochastic gradient algorithm negatively directions\nupdate can oscillate quite wildly iteration iteration. section\ncovers techniques yield better tradeoff \nrun time variability.obvious technique use one observation per computation\ngradient, gives us mini-batch stochastic gradient algorithms.\nsecond technique incorporate memory previous directions \nmovements algorithms — spirit conjugate gradient\nalgorithm uses previous gradient modify descent direction.literature deep learning recently exploded variations\nstochastic gradient algorithm. Performance mostly\nstudied empirically applied practice highly non-convex\noptimization problem learning neural network. comprehensive coverage\ndifferent ideas attempted, three \nsolidified variations treated. use mini-batches ubiquitous,\nmomentum introduced illustrate variation memory. Finally,\nAdam algorithm uses memory combination adaptive learning rates\nachieve speed robustness.","code":""},{"path":"StochOpt.html","id":"mini-batches","chapter":"9 Stochastic Optimization","heading":"9.2.1 Mini-batches","text":"three steps mini-batch stochastic gradient algorithm\nmini-batch size \\(m\\) : given \\(\\theta_{n}\\)sample \\(m\\) indices, \\(I_n = \\{i_1, \\ldots, i_m\\}\\), \\(\\{1, \\ldots, N\\}\\)compute \\(\\rho_n = \\frac{1}{m} \\sum_{\\I_n} \\nabla_{\\theta} L(x_i, \\theta_{n})\\)update parameter \\(\\theta_{n+1} = \\theta_{n} - \\gamma_n \\rho_n.\\)course, mini-batch algorithm \\(m = 1\\) basic stochastic gradient\nalgorithm. basic algorithm, implement variation\nsample partition\n\\[I_1 \\cup \\ldots \\cup I_M \\subseteq \\{1, \\ldots, N\\}\\]\n\\(M = \\lfloor N / m \\rfloor\\) one epoch loop mini-batches\n\\(I_1, \\ldots, I_M\\).following sections develop couple modifications basic\nstochastic gradient algorithm, therefore implement generic\nversion algorithm. common modifications \ndiffer details epoch loop, thus take loop separate\nfunction.implementation uses batch() default update function,\nimplement function . uses \nrandom permutation generate \\(M\\) mini-batches, contains \nloop mini-batches containing call grad()\ncomputation average gradient mini-batch loop.\nNote grad argument batch() captured passed \ncall SG() via ... argument.grad() function implemented ls_model() Section 9.1.4 assumes\nindex argument \\(\\) single number vector. computes\nvector \\(\\) matrix containing \ndifferent gradients columns. therefore reimplement\nls_model() grad() computes average gradients\nsupposed mini-batch.initialize linear model using new implementation ls_model().increased flexibility algorithms comes tuning parameters,\nmaking good choice becomes increasingly difficult. \nintroducing mini-batches need choose mini-batch size \naddition learning rate, good choice learning rate decay schedule\ndepend size mini-batch. simplify matters, mini-batch\nsize 1000 fixed learning rate used subsequent applications.\nlearning rate generally chosen large \npossible without making algorithms diverge, mini-batch size 1000\npossible run algorithm learning rate \\(\\gamma = 0.05\\).\nFigure 9.6: Convergence squared error loss news article data set three algorithms: basic stochastic gradient power law decay schedule (decay), gradient descent (gd), mini-batch stochastic gradient batch size 1000 fixed learning rate (mini).\nFigure 9.6 shows implementation \nmini-batch stochastic gradient\nalgorithm converges faster basic stochastic gradient algorithm \npower law decay schedule\nwell gradient descent algorithm. Eventually, begins \nfluctuate due fixed learning rate, quickly gets close \nminimum. jump conclusions assessment. \nreally shows improvement specific R implementation\nusing mini-batch algorithm, implementation clearly benefits\nvectorized computations mini-batches size 1000.\nreturn discussion Section 9.3.1.","code":"\nSG <- function(\n  par, \n  N,                 # Sample size\n  gamma,             # Decay schedule or a fixed learning rate\n  epoch = batch,     # Epoch update function\n  ...,               # Other arguments passed to epoch updates\n  maxiter = 100,     # Max epoch iterations\n  sampler = sample,  # How data is resampled. Default is a random permutation\n  cb = NULL\n) {\n  gamma <- if (is.function(gamma)) gamma(1:maxiter) else rep(gamma, maxiter) \n  for(k in 1:maxiter) {\n    if(!is.null(cb)) cb()\n    samp <- sampler(N)\n    par <- epoch(par, samp, gamma[k], ...)\n  }\n  par\n}\nbatch <- function(\n    par, \n    samp,\n    gamma,  \n    grad,              # Function of parameter and observation index\n    m = 50,            # Mini-batch size\n    ...\n  ) {\n    M <- floor(length(samp) / m)\n    for(j in 0:(M - 1)) {\n      i <- samp[(j * m + 1):(j * m + m)]\n      par <- par - gamma * grad(par, i, ...)\n    }\n    par\n}\nls_model <- function(X, y) {\n  N <- length(y)\n  X <- unname(X)\n  list(\n    par0 = rep(0, ncol(X)),\n    H = function(beta)\n      drop(crossprod(y - X %*% beta)) / (2 * N),\n    # Gradient that works for a mini-batch indexed by i\n    grad = function(beta, i) {               \n      xi <- X[i, , drop = FALSE]\n      drop(crossprod(xi, xi %*% beta - y[i])) / length(i)\n    }\n  )\n}\nc(par0, H, ls_grad) %<-% ls_model(X, y)\nSG_tracer$clear()\nSG(\n  par = par0, \n  N = nrow(X), \n  gamma = 5e-2, \n  grad = ls_grad, \n  m = 1000, \n  maxiter = 200, \n  cb = SG_tracer$tracer\n)"},{"path":"StochOpt.html","id":"momentum","chapter":"9 Stochastic Optimization","heading":"9.2.2 Momentum","text":"Mini-batches stabilize gradients, momentum. techniques\ncan used combination, momentum update \nmini-batch stochastic gradient algorithm follows: Given \\(\\theta_{n}\\)\nbatch \\(I_n \\subseteq \\{1, \\ldots, N\\}\\) \\(|I_n| = m\\)compute \\(g_n = \\frac{1}{m} \\sum_{\\I_n} \\nabla_{\\theta} L(x_i, \\theta_{n})\\)compute \\(\\rho_n = \\beta \\rho_{n-1} + (1-\\beta) g_n\\)update parameter \\(\\theta_{n+1} = \\theta_{n} - \\gamma_n \\rho_n.\\)memory algorithm second step, direction, \\(\\rho_{n}\\), \nupdated using convex combination previous direction, \\(\\rho_{n-1}\\),\nmini-batch gradient, \\(g_n\\). Usually, initial direction chosen\n\\(\\rho_0 = 0\\). parameter \\(\\beta \\[0,1)\\) tuning parameter determining\nlong memory . value like \\(\\beta = 0.9\\) \\(\\beta = 0.95\\) \noften recommended – otherwise memory algorithm rather short,\neffect using momentum small. choice \\(\\beta = 0\\)\ncorresponds mini-batch algorithm without memory.Contrary batch epoch function, momentum epoch function\nneeds store previous direction updates. immediately\nclear achieve two epochs using generic SG()\nimplementation, implementing momentum epochs using function factory,\ncan easily use enclosing environment epoch function\nstorage.calling SG() epoch = momentum(), evaluation\nfunction factory momentum() returns momentum epoch function\nlocal environment used store \\(\\rho\\). momentum, can\nincrease learning rate \\(\\gamma = 7 \\times 10^{-2}\\).\nFigure 9.7: Convergence squared error loss news article data set four algorithms: gradient descent (gd), basic stochastic gradient power law decay schedule (decay), mini-batch stochastic gradient batch size 1000 fixed learning rate either without momentum (mini) momentum (moment).\nFigure 9.7 shows momentum\nalgorithm converges little faster mini-batch algorithm without\nmomentum. can explained slightly larger learning rate made\npossible use momentum. enough memory, momentum dampens rapid fluctuations,\nallows larger choice learning rate speedier convergence.\nHowever, much memory results low frequency oscillations slow convergence.\nexcellent article Momentum Really Works\ncontains many details momentum algorithms beautiful illustrations.","code":"\nmomentum <- function() {\n  rho <- 0\n  function(\n    par, \n    samp,\n    gamma,  \n    grad,\n    m = 50,             # Mini-batch size\n    beta = 0.95,        # Momentum memory \n    ...\n  ) {\n    M <- floor(length(samp) / m)\n    for(j in 0:(M - 1)) {\n      i <- samp[(j * m + 1):(j * m + m)]\n      # Using '<<-' assigns the value to rho in the enclosing environment\n      rho <<- beta * rho + (1 - beta) * grad(par, i, ...)  \n      par <- par - gamma * rho\n    }\n    par\n  }\n}\nSG_tracer$clear()\nSG(\n  par = par0, \n  N = nrow(X), \n  gamma = 7e-2, \n  epoch = momentum(), \n  grad = ls_grad,\n  m = 1000, \n  maxiter = 150, \n  cb = SG_tracer$tracer\n)"},{"path":"StochOpt.html","id":"adaptive-learning-rates","chapter":"9 Stochastic Optimization","heading":"9.2.3 Adaptive learning rates","text":"One difficulty optimization algorithms based gradients\ngradients invariant reparametrizations. fact, using gradients\nimplicitly assumes parameters comparable scales. news article\nexample, standardized model matrix achieve , many \noptimization problems easy choose parametrization \nparameters comparable scales. even can reparametrize \nparameters common scale, common scale can change \nproblem problem making impossible recommend good\ndefault choice learning rate. practical implication considerable\namount tuning necessary, algorithms applied.Algorithms implement adaptive learning rates alternatives extensive\ntuning. include schemes adjusting learning rate specific\noptimization problem. Adapting learning rate equivalent scaling\ngradient adaptively, achieve form automatic standardization\nparameter scales, consider algorithms adaptively scale \ncoordinate gradient separately.gain intuition sensibly adapt scales gradient,\nanalyze typical scale mini-batch gradient linear\nmodel. Introduce first normalized squared column norms\n\\[\\zeta_j = \\frac{1}{N} \\sum_{=1}^N x_{ij}^2 = \\frac{1}{N} \\| x_{\\cdot j}\\|^2_2,\\]\nnote standardized \\(X\\), \\(\\zeta_j\\)-s equal. \n\\(\\hat{\\beta}\\) least squares estimate also \n\\[\\frac{1}{N} \\sum_{=1}^N x_{ij} (y_i - \\hat{\\beta}{}^Tx_i) = 0\\]\n\\(j = 1, \\ldots, p\\). Thus sample random index, \\(\\iota\\), \n\\(\\{1, \\ldots, N\\}\\) holds \n\\(E(x_{\\iota j} (y_{\\iota} - \\hat{\\beta}{}^Tx_{\\iota})) = 0\\), expectation w.r.t. \\(\\iota\\).\n\n\\[\\hat{\\sigma}^2 = \\frac{1}{N} \\sum_{=1}^N (y_i - \\hat{\\beta}{}^Tx_i)^2\\]\ndenoting residual variance, also \\[\nV(x_{\\iota j} (y_{\\iota} - \\hat{\\beta}{}^Tx_{\\iota})) = \nE(x_{\\iota j}^2 (y_{\\iota} - \\hat{\\beta}{}^Tx_{\\iota})^2) \n\\simeq \\zeta_j \\hat{\\sigma}^2\n\\]approximation hold, squared residual, \\((y_{\\iota} - \\hat{\\beta}{}^Tx_{\\iota})^2\\),\nmust roughly independent \\(x_{\\iota}\\), guaranteed least\nsquares fit alone, holds approximately data population \nhomogeneous residual variance.\\(\\subseteq \\{1,\\ldots,N\\}\\) random subset size \\(m\\) sampled \nreplacement, averaged gradient\n\\[g = - \\frac{1}{m} \\sum_{\\} x_{} (y_i - \\hat{\\beta}{}^T x_i)\\]\naverage \\(m\\) ..d. random variables mean 0, thus\n\\[E(g_j^2) = V(g_j) \\simeq \\zeta_j \\frac{\\hat{\\sigma}^2}{m}.\\]\n\\(\\odot\\) denotes coordinate wise product vectors (aka Hadamard product),\ncan also written \n\\[E(g \\odot g) \\simeq  \\zeta \\frac{\\hat{\\sigma}^2}{m}.\\]computations suggest estimating \\(v = E(g \\odot g)\\) mini-batch\ngradient evaluated \\(\\beta = \\hat{\\beta},\\) fact estimating\n\\(\\zeta\\) scale factor. extrapolate insight general case\nstandardize \\(j\\)-th coordinate descent direction \nestimate \\(1 / \\sqrt{v_j}\\)\nbring coordinates () common scale. implement adaptive\nestimation \\(v\\) using similar update scheme momentum, \nestimate iteration \\(n\\) updated convex combination current\nvalue \\(g_n \\odot g_n\\) previous estimate \\(v\\).Given \\(\\theta_{n}\\) batch \\(I_n \\subseteq \\{1, \\ldots, N\\}\\) \\(|I_n| = m\\)\nupdate consists following stepscompute \\(g_n = \\frac{1}{m} \\sum_{\\I_n} \\nabla_{\\theta} L(x_i, \\theta_{n})\\)compute \\(\\rho_n = \\beta_1 \\rho_{n-1} + (1-\\beta_1) g_n\\)compute \\(v_n = \\beta_2 v_{n-1} + (1-\\beta_2) g_n \\odot g_n\\)update parameter \\(\\theta_{n+1} = \\theta_{n} - \\gamma_n \\rho_n / (\\sqrt{v_n} + 10^{-8}).\\)vectors \\(\\rho_0\\) \\(v_0\\) usually initialized \\(0\\). tuning\nparameters \\(\\beta_1, \\beta_2 \\[0, 1)\\) control memory first \nsecond moments, respectively. \\(\\sqrt{v_n}\\) division last step\ncoordinate wise. constant \\(10^{-8}\\) , course, chosen differently,\njust safeguard division--zero.algorithm known Adam (adaptive moment estimation),\nintroduced analyzed Kingma Ba (2014). include -called\nbias-correction steps upscale \\(\\rho_n\\) \\(v_n\\) factors \\(1 / (1 - \\beta_1^n)\\)\n\\(1 / (1 - \\beta_2^n)\\), respectively. steps difficult\nimplement left implementation simplicity.\nalso possible replace \\(\\sqrt{v_n}\\) powers \\(v_n^q\\).\nchoice \\(q = 1\\) instead \\(q = 1/2\\) makes algorithm ()\ninvariant scale changes. , simplicity implement\nalgorithm \\(q = 1/2\\).adam() function function factory just momentum(), \nreturns function Adam epoch update loop enclosing\nenvironment used storage rho v.run stochastic gradient algorithm Adam epochs mini-batches \nsize 1000 fixed learning rate \\(0.01\\) power law decay\nschedule interpolating learning rate \\(0.5\\) \\(0.002\\).\ntheoretical results Kingma Ba (2014) support decay schedule proportional\n\\(1/\\sqrt{n}\\), thus take \\(= 0.5\\) .\nFigure 9.8: Convergence squared error loss news article data set six algorithms: basic stochastic gradient power law decay schedule (decay), gradient descent (gd), mini-batch stochastic gradient batch size 1000 fixed learning rate either without momentum (mini) momentum (moment), Adam algorithm either fixed learning rate (adam) power law decay schedule (adam_decay).\nFigure 9.8 shows runs Adam implementation\nconverge faster initially algorithms. Eventually \nlevel error around \\(10^{-3}\\) point \nfluctuate randomly. shown Adam also somewhat\nrobust changes learning rate rescaling parameters.\nThough Adam tuning parameters, easier find good values\nparameters lead fast convergence.","code":"\nadam <- function() {\n  rho <- v <- 0\n  function(\n    par, \n    samp,\n    gamma,   \n    grad,\n    m = 50,          # Mini-batch size\n    beta1 = 0.9,     # Momentum memory     \n    beta2 = 0.9,     # Second moment memory \n    ...\n  ) {\n    M <- floor(length(samp) / m)\n    for(j in 0:(M - 1)) {\n      i <- samp[(j * m + 1):(j * m + m)]\n      gr <- grad(par, i, ...) \n      rho <<- beta1 * rho + (1 - beta1) * gr \n      v <<- beta2 * v + (1 - beta2) * gr^2 \n      par <- par - gamma * (rho / (sqrt(v) + 1e-8))  \n    }\n    par\n  }\n}\nSG_tracer$clear()\nSG(\n  par = par0, \n  N = nrow(X), \n  gamma = 1e-2, \n  epoch = adam(), \n  grad = ls_grad,\n  m = 1000, \n  maxiter = 150, \n  cb = SG_tracer$tracer\n)\nSG_trace_adam <- summary(SG_tracer)\nSG_tracer$clear()\nSG(\n  par = par0, \n  N = nrow(X), \n  gamma = decay_scheduler(gamma0 = 0.5, gamma1 = 2e-3, a = 0.5, n1 = 150), \n  epoch = adam(), \n  grad = ls_grad,\n  m = 1000, \n  maxiter = 150, \n  cb = SG_tracer$tracer\n)\nSG_trace_adam_decay <- summary(SG_tracer)"},{"path":"StochOpt.html","id":"SG-Rcpp","chapter":"9 Stochastic Optimization","heading":"9.3 Stochastic gradient algorithms with Rcpp","text":"pointed toward end Section 9.1.4, implementations \nstochastic gradient algorithms R suffer shortcomings. \nsection explore either parts algorithms entire\nalgorithms can moved C++ via Rcpp.modularity SG() implementation makes easy replace \nimplementation either gradient computation entire epoch\nloop C++ implementation, retaining overall control algorithm \nresampling R. explored first consists mostly translating\nnumerical linear algebra gradient computations C++ code.\ncan easily test, compare benchmark implementations using R\nimplementation reference.second part section entire mini-batch stochastic gradient\nalgorithm translated C++. couple notable consequences. First,\nneed access sampler C++ can randomization. \nvarious C++ interfaces equivalent sample(), \nconsiderations need go appropriate choice. Second, \ngive tracing otherwise implemented. Though possible implement\ncallback R function C++ function, tracer \naccess calling environment R implementation.\nThus performance assessment rely benchmarking\nentire algorithm.C++ implementations need give abstractions\nR provides, though benefit Rcpp data types like\nNumericVector NumericMatrix. final implementation\nuse RcppArmadillo\nregain abstract approach numerical linear\nalgebra via C++ library Armadillo.","code":""},{"path":"StochOpt.html","id":"Rcpp-grad-epoch","chapter":"9 Stochastic Optimization","heading":"9.3.1 Gradients and epochs in Rcpp","text":"first implement computation mini-batch gradient Rcpp \nlinear model, thus replacing R implementation gradient \nls_model(). implemented function takes model matrix \\(X\\) \ntarget variable \\(y\\) arguments addition \nparameter subset indices representing mini-batch.\nmatrix-vector multiplications computing predicted values,\nresiduals ultimately gradient replaced two double -loops.Note clone(ii) used create copy index vector ii \nshifted one. R objects like vectors matrices passed\narguments C++ function, pointer data object\neffectively passed. modification resulting Rcpp object within\nbody C++ function reflected R – likely\nunwanted side effect. prevent can create \ncopy using clone() data object – -called deep copy.Since pointer matrix passed calling\nC++ function, notable cost associated passing entire\nmodel matrix argument. contrary, pass subset model\nmatrix C++ function R, data inevitably copied.\nsince slicing matrix C++ create copy data,\nimplementation lin_grad() avoids data copying problem \nmatrix slicing R incurred gradient computations.test lin_grad() comparing ls_grad() different\nchoices parameter vectors indices. simple way achieve \nrun mini-batch stochastic gradient algorithm either implementation\ngradient one epoch compare results. Note\nmake sure algorithms use mini-batches setting seed.differences 0, thus test indicates lin_grad() implements\ncomputation ls_grad().can also move entire epoch loop C++, thus replacing batch()\nR function Rcpp function lin_batch() . achieved simply \nadding one outer loop lin_grad() , loops mini-batches \nupdates parameter vector.can test implementation lin_batch() just lin_grad() \nrunning stochastic gradient algorithm one epoch. note \nlin_batch() function passed SG() epoch argument,\ngrad argument required. gradient computations hard\ncoded implementation epoch function.see differences also zero, test indicates \nlin_batch() implements computation batch() either\ntwo implementations gradient.investigate effect C++ implementation, mini-batch\nstochastic gradient algorithm run Rcpp implementations \neither gradient entire epoch using mini-batch size 1000\nconstant learning rate \\(\\gamma = 0.05\\), can compared \npure R implementation. addition, run algorithm \nmini-batch size one \\(\\gamma = 5 \\times 10^{-5}\\) using Rcpp\nimplementation epoch.\nFigure 9.9: Convergence squared error loss news article data set four algorithms: R implementation mini-batch stochastic gradient batch size 1000 fixed learning rate (mini), mini-batch algorithm gradient implemented Rcpp (Rcpp_grad_mini) entire epoch implemented Rcpp (Rcpp_epoch_mini), basic stochastic gradient algorithm entire epoch implemented Rcpp (Rcpp_epoch).\nFigure 9.9 shows, unsurprisingly, implementing \ngradient C++ function makes convergence faster compared pure\nR implementation, turning entire epoch C++ function results \nlittle improvement. However, figure also shows \nbasic stochastic gradient algorithm (mini-batch size equal one) converges\nrate comparable mini-batch algorithm mini-batch\nsize 1000 entire epoch C++. Thus benefit mini-batch\nalgorithm mini-batch size 1000 seems largely due \ninefficiency basic algorithm pure R. Experiments using \nlin_batch() epoch implementation (shown) suggest mini-batch\nalgorithm real edge basic algorithm, much smaller\nmini-batch sizes, e.g. \\(m = 20\\), cf Exercise 9.4.","code":"// [[Rcpp::export]]\nNumericVector lin_grad(\n    NumericVector beta, \n    IntegerVector ii, \n    NumericMatrix X, \n    NumericVector y\n) {\n  int m = ii.length(), p = beta.length();\n  NumericVector grad(p), yhat(m);\n  // Shift indices one down due to zero-indexing in C++\n  IntegerVector iii = clone(ii) - 1;  \n  \n  for(int i = 0; i < m; ++i) {\n    for(int j = 0; j < p; ++j) {\n      yhat[i] += X(iii[i], j) * beta[j];\n    }\n  }\n  for(int i = 0; i < m; ++i) {\n    for(int j = 0; j < p; ++j) {\n      grad[j] += X(iii[i], j) * (yhat[i]- y[iii[i]]);\n    }\n  }\n  return grad / m;\n}\nset.seed(10)\npar1 <- SG(\n  par = par0,\n  grad = ls_grad,\n  N = nrow(X),\n  gamma = 0.01,\n  maxiter = 1\n)\nset.seed(10)\npar2 <- SG(\n  par = par0, \n  grad = lin_grad,\n  N = nrow(X),\n  gamma = 0.01,\n  maxiter = 1, \n  X = X,\n  y = y\n)\nrange(par1 - par2)## [1] 0 0// [[Rcpp::export]]\nNumericVector lin_batch(\n    NumericVector par, \n    IntegerVector ii, \n    double gamma, \n    NumericMatrix X, \n    NumericVector y, \n    int m = 50\n) {\n  int p = par.length(), N = ii.length();\n  int M = floor(N / m);\n  NumericVector grad(p), yhat(N), beta = clone(par);\n  IntegerVector iii = clone(ii) - 1;  \n  \n  for(int j = 0; j < M; ++j) {\n    for(int i = j * m; i < (j + 1) * m; ++i) {\n      for(int k = 0; k < p; ++k) {\n        yhat[i] += X(iii[i], k) * beta[k];\n      }\n    }\n    for(int k = 0; k < p; ++k) {\n      grad[k] = 0;\n      for(int i = j * m; i < (j + 1) * m; ++i) {\n        grad[k] += X(iii[i], k) * (yhat[i] - y[iii[i]]);\n      }\n    }\n    beta = beta - gamma * (grad / m);\n  }\n  return beta;\n}\nset.seed(10)\npar1 <- SG(\n  par = par0,\n  grad = ls_grad,\n  N = nrow(X),\n  gamma = 0.01,\n  maxiter = 1\n)\nset.seed(10)\npar2 <- SG(\n  par = par0, \n  epoch = lin_batch,   \n  N = nrow(X),\n  gamma = 0.01,\n  maxiter = 1, \n  X = X,\n  y = y\n)\nrange(par1 - par2)## [1] 0 0"},{"path":"StochOpt.html","id":"full-rcpp-implementations","chapter":"9 Stochastic Optimization","heading":"9.3.2 Full Rcpp implementations","text":"section convert entire stochastic gradient algorithm \nlinear model C++ function. Two considerations come \n:sample without replacement C++?possible C++ implement gradient computations using\nnumerical linear algebra instead two double -loops?deal first question first, , sample finite\nset size \\(d\\) C++ sample() R. Though may seem\nlike rather trivial problem, entirely trivial implement\nsampling general distribution finite set size \\(d\\) \ncorrect efficient function \\(d\\). non-exhaustive list \nways sample discrete sets using Rcpp :R function sample() can retrieved via\nFunction sample(\"sample\"); Rcpp, makes function callable \nC++ code. , however, overhead\nassociated calling R functions C++, prefer avoid.Rcpp sugar implementation sample(). generates different samples \nR function sample().RcppArmadilloExtension implementation sample(). See also blog post Sampling Integers. requires R\npackage RcppArmadillo, also generates different samples R function sample().function dqsample.int() dqrng package\ndealt Section 4.1.2. requires R package dqrng \nruns independent stream pseudo random numbers R’s RNG.Rcpp sugar Rcpp Armadillo versions sample() touched \nRcpp introduction vignette, documented detail. use R’s stream pseudo random numbers, \nnevertheless generate sequences differ sample().\nreproduce results R, write interface C++\nfunctions use . Since dqrng package provides interface\ndqsample.int() R well C++, use sampler\nimplementations.want generate random permutations, couple additional\nalternatives. randperm() Armadillo library, also\nstd::random_shuffle() C++ Standard Library.\nimplementation std::random_shuffle() , however, specified\nstandard can compiler dependent. alternatives\nalso run independent stream pseudo random numbers \nR’s RNG, considered .full Rcpp implementation stochastic gradient algorithm now\nsimple extension lin_batch() one additional outer loop \nepochs single line calling dqsample_int() sample \nrandom permutation.test implementation comparing pure R implementation, \nusing dqsample_int() / dqsample.int() sample permutations cases.two notable overall differences SG() SG_Rcpp() \nSG_Rcpp() low level problem specific, SG() high level\ngeneric. \nsurely possible refactor C++ code write generic stochastic\ngradient algorithm, rely functions computing\ne.g. gradient. However, retain performance functions need \nimplemented C++ well.achieve high level implementation, can use RcppArmadillo\npackage, provides interface numerical linear\nalgebra C++ library Armadillo.\nway can use matrix-vector style implementation similar \nR implementation without excessive data copying. , however,\nnecessary use various data types Armadillo library, e.g. \nmatrices column vectors, also necessary type\ncast sequence integers dqsample_int() uvec data type.function returns column vector, R represents \\(p \\times 1\\)\nmatrix, whereas implementations return vector. besides \ndifference, test show SG_Rcpp() computes updates \nimplementations dqsample_int() used.benchmark compare five implementations mini-batch\nstochastic gradient algorithm running 10 epoch iterations\nfixed learning rate \\(\\gamma = 10^{-4}\\) default mini-batch size\n\\(m = 50\\).pure R implementation (SG) , unsurprisingly, slowest, full\nRcpp implementation (SG_Rcpp) less factor 3 faster. \nArmadillo based implementation somewhat slower, though still faster \npure R implementation.rerun algorithms, time mini-batch size \\(m = 1\\). \nSG() allocates lot memory, \ndefault memory tracking mark() become prohibitively slow,\nthus memory tracking disabled.reveals bigger differences. fixed number epoch iterations,\ntotal amount computations change substantially \nchanging mini-batch size 50 1, run times SG()\nSG_lin_grad() increase notably. C++\nimplementations SG_lin_batch() SG_Rcpp mostly retain run time\n\\(m = 50\\). runtime Armadillo implementation \naffected much either, still slower two \nC++ based implementations now factor 2.summary, mini-batches small, improvements runtime\nusing Rcpp relatively small – factor 2 3. However, \nmini-batch size small even one, runtime factor 10\nsmaller using either full Rcpp implementation Rcpp implementation\nentire epoch update. Using lin_batch() epoch updates strikes\ngood compromise particular example, runtime competitive\nmain control structures algorithm R.","code":"// [[Rcpp::depends(dqrng)]]\n// [[Rcpp::export]]\nNumericVector SG_Rcpp(\n    NumericVector par, \n    int N, \n    NumericVector gamma,\n    NumericMatrix X, \n    NumericVector y,\n    int m = 50, \n    int maxiter = 100\n) {\n  int p = par.length(), M = floor(N / m);\n  NumericVector grad(p), yhat(N), beta = clone(par);\n  IntegerVector ii;\n  \n  for(int l = 0; l < maxiter; ++l) {\n    // Note that dqsample_int samples from {0, 1, ..., N - 1}\n    ii = dqrng::dqsample_int(N, N); \n    for(int j = 0; j < M; ++j) {\n      for(int i = j * m; i < (j + 1) * m; ++i) {\n        yhat[i] = 0;\n        for(int k = 0; k < p; ++k) {\n          yhat[i] += X(ii[i], k) * beta[k];\n        }\n      }\n      for(int k = 0; k < p; ++k) {\n        grad[k] = 0;\n        for(int i = j * m; i < (j + 1) * m; ++i) {\n          grad[k] += X(ii[i], k) * (yhat[i] - y[ii[i]]);\n        }\n      }\n      beta = beta - gamma[l] * (grad / m);\n    }\n  }\n  return beta;\n}\ndqset.seed(10) \npar1 <- SG(\n  par = par0, \n  grad = ls_grad,\n  N = nrow(X), \n  gamma = 1e-3, \n  maxiter = 10,\n  sampler = dqrng::dqsample.int\n)\ndqset.seed(10) \npar2 <- SG_Rcpp(\n  par = par0, \n  N = nrow(X),\n  gamma = rep(1e-3, 10),\n  X = X, \n  y = y,\n  maxiter = 10\n)\nrange(par1 - par2)## [1] 0 0// [[Rcpp::depends(RcppArmadillo)]]\n// [[Rcpp::export]]\narma::colvec SG_arma(\n    NumericVector par, \n    int N, \n    NumericVector gamma,\n    const arma::mat& X, \n    const arma::colvec& y, \n    int m = 50, \n    int maxiter = 100\n) {\n  int p = par.length(), M = floor(N / m);\n  arma::colvec grad(p), yhat(N), beta = clone(par);\n  uvec ii, iii;\n\n  for(int l = 0; l < maxiter; ++l) {\n    ii = as<arma::uvec>(dqrng::dqsample_int(N, N));\n    for(int j = 0; j < M; ++j) {\n      iii = ii.subvec(j * m, (j + 1) * m - 1);\n      beta = beta - gamma[l] * (X.rows(iii).t() * (X.rows(iii) * beta - y(iii)) / m); \n    }\n  }\n  return beta;\n}\nbench::mark(\n  SG = SG(par0, nrow(X), 1e-4, maxiter = 10, grad = ls_grad),\n  SG_lin_grad = SG(par0, nrow(X), 1e-4, maxiter = 10, grad = lin_grad, X = X, y = y),\n  SG_lin_batch = SG(par0, nrow(X), 1e-4, lin_batch, maxiter = 10, X = X, y = y),\n  SG_Rcpp = SG_Rcpp(par0, nrow(X), rep(1e-4, 10), X = X, y = y, maxiter = 10),\n  SG_arma = SG_arma(par0, nrow(X), rep(1e-4, 10), X = X, y = y, maxiter = 10), \n  check = FALSE, iterations = 5\n)## # A tibble: 5 × 6\n##   expression        min   median `itr/sec` mem_alloc `gc/sec`\n##   <bch:expr>   <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n## 1 SG              897ms    991ms      1.01  183.18MB    5.07 \n## 2 SG_lin_grad     461ms    493ms      2.03    43.9MB    2.44 \n## 3 SG_lin_batch    345ms    364ms      2.74    9.14MB    0.549\n## 4 SG_Rcpp         311ms    326ms      3.08    1.82MB    0    \n## 5 SG_arma         567ms    587ms      1.64    4.54MB    0.328\nbench::mark(\n  SG = SG(par0, nrow(X), 1e-4, maxiter = 10, m = 1, grad = ls_grad),\n  SG_lin_grad = SG(par0, nrow(X), 1e-4, maxiter = 10, m = 1, grad = lin_grad, X = X, y = y),\n  SG_lin_batch = SG(par0, nrow(X), 1e-4, lin_batch, maxiter = 10, m = 1, X = X, y = y),\n  SG_Rcpp = SG_Rcpp(par0, nrow(X), rep(1e-4, 10), X = X, y = y, maxiter = 10, m = 1),\n  SG_arma = SG_arma(par0, nrow(X), rep(1e-4, 10), X = X, y = y, maxiter = 10, m = 1), \n  check = FALSE, memory = FALSE, iterations = 5\n)## # A tibble: 5 × 6\n##   expression        min   median `itr/sec` mem_alloc `gc/sec`\n##   <bch:expr>   <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n## 1 SG              5.49s    5.83s     0.169        NA    5.11 \n## 2 SG_lin_grad      2.3s    2.46s     0.401        NA    6.89 \n## 3 SG_lin_batch  418.5ms 421.59ms     2.35         NA    0.469\n## 4 SG_Rcpp      396.32ms 424.52ms     2.38         NA    0    \n## 5 SG_arma      748.43ms 802.69ms     1.22         NA    0"},{"path":"StochOpt.html","id":"stochopt:ex","chapter":"9 Stochastic Optimization","heading":"9.4 Exercises","text":"Exercise 9.1  Consider loss\n\\[L((y, z), \\theta) = \\mu(z, \\theta) - y \\log (\\mu(z, \\theta))\\]\nmodel, \\(\\mu(z, \\theta)\\), conditional mean, \\(E(Y \\mid Z = z)\\), \n\\(Y\\) given \\(Z = z\\). See also Example 9.2.Show function\n\\[\\mapsto - b \\log()\\]\nfixed \\(b\\) attains unique minimum \\((0, \\infty)\\) \\(= b\\).show lower bound risk\n\\[H(\\theta) = E(L((Y, Z), \\theta)) \\geq E\\Big( E(Y \\mid Z)  - E(Y \\mid Z) \\log (E(Y \\mid Z))\\Big),\\]\nconclude \\(E(Y \\mid Z = z) = \\mu(z, \\theta_0)\\) \\(\\theta_0\\),\n\\(\\theta_0\\) global minimizer risk.Exercise 9.2  Consider online setup Example 9.3 \n\\[Y_i \\mid Z_i = z_i \\sim \\mathcal{N}(e^{\\beta_0 + \\beta_1 z_i}, 5)\\]\ninstead. , conditional mean value model still log-linear, \ndistribution Gaussian instead Poisson.Replicate online learning simulation Example 9.3 using\nPoisson log-likelihood gradient squared error gradient. may want\nexperiment different learning rates. change \ndistribution affect convergence algorithms?Exercise 9.3  Show linear model squared error loss, hessian \nobjective function \n\\[D^2 H_N(\\beta) = X^T X\\]\n\\(X\\) model matrix. Compute eigenvalues matrix using\nnews data Section 9.1.4 standardization. \nconditioning numbers matrices, values\nrelate convergence gradient based methods? See also Section 7.1.1.Exercise 9.4  Make sure working implementation mini-batch stochastic\ngradient algorithm news data Section 9.1.4 \nlin_batch() Rcpp implementation epoch updates. Set experiment\nrun algorithm grid mini-batch sizes learning rates.\nRun algorithm absolute deviation minimum \nempirical squared error loss less 0.01 determine optimal\nchoice tuning parameters.","code":""},{"path":"the-stochastic-em-algorithm.html","id":"the-stochastic-em-algorithm","chapter":"10 The stochastic EM algorithm","heading":"10 The stochastic EM algorithm","text":"","code":""},{"path":"app-R.html","id":"app-R","chapter":"A R programming","heading":"A R programming","text":"appendix R programming brief overview important\nprogramming constructs concepts R used book.\ndetailed much \nextensive coverage R programming language reader referred \nbook Advanced R.Depending background, appendix can serve different purposes.\nalready experience programming R, appendix can serve\nbrush basic aspects R language used throughout\nbook. experience programming languages R, \ncan serve introduction typical R programming techniques,\nmay differ know languages. little\nprior experience programming, appendix can teach important\nthings getting started book, encouraged follow \nadditional detailed material like Advanced R.everybody, appendix covers specific topics relevant reading book.\ntopics includedata types, comparisons numerical precisionvectorized computationsfunctions, environments function factoriesperformance assessment improvementS3 objects methodsSeveral important topics, \nS4 \nR6 objects,\nexpressions, \ndata wrangling, covered \ndetail. book implementations correct efficient\nnumerical algorithms used statistics, reflected \ntopics covered appendix.","code":""},{"path":"app-R.html","id":"app-data-structures","chapter":"A R programming","heading":"A.1 Data structures","text":"fundamental data structure R vector. Even variables look \nbehave like single numbers vectors length one. Vectors come two flavors:\natomic vectors lists.atomic vector indexed collection data elements \ntype, e.g.integersfloating point numberslogical valuescharacter stringsA list indexed collection elements without type restrictions \nindividual elements. element list can, instance, list .","code":""},{"path":"app-R.html","id":"atomic-vectors","chapter":"A R programming","heading":"A.1.1 Atomic vectors","text":"can construct vector R simply typing elements, e.g. constructed vector contains numbers 1, 2 3. use classical\nassignment operator <- throughout, R supports using = prefer.\nc() used right hand side assignment short combine\n(concatenate), also used combine two vectors one.several convenient techniques R constructing vectors various\nregular nature, e.g. sequences. following example shows construct \nvector containing integers 1 10. type vector integer\nindicating elements vector stored integers.can access individual elements well subsets vector\nindices using square brackets.function seq generalizes colon operator, 1:10, way \ngenerate regular sequences. following example shows generate\nsequence, double_vector, 0.1 1.0 increments size 0.1.\ntype resulting vector double, indicates elements\ndouble_vector stored doubles. , numbers stored \nfloating point numbers using 64 bits storage precision \njust 16 digits.Integers often stored coerced doubles automatically. supposedly\ninteger vector, first_vector, actually type double.R, numerical data either type integer double collectively referred \nnumerics mode (class) numeric. confusing, particular\n“numeric” used also pseudonym type double, \nrarely practical problem. vector type integer double often just\nsaid numeric, function .numeric() reflects .possible insist integers actually stored integers \nappending L integer, e.g.sequences generated using colon operator endpoints \nintegers, 1:10, result vector type integer. \nexception. Apparent integers usually – silently – converted \ndoubles explicitly marked integers L.Vectors length can created generic function vector(), \ntype specific functions numeric() creates vectors type\ndouble.vectors type double length 10 initialized \nelements 0.logical vector another example useful atomic vector. default\ntype vector created vector() logical, elements FALSE.Logical vectors encountered compare elements one vector\nanother vector number.logical vector type stored efficiently ,\nbehaves many ways numeric vector FALSE equivalent\n0 TRUE equivalent 1. want compute relative\nfrequency elements integer_vector (strictly) larger 4,\nsay, can simply take mean logical_vector.mean logical vector behaves logical values coerced \nzeros ones mean computed.final example atomic vector character vector. example,\nvector combined 6 individual strings form vector length\n6. Combining strings vector paste strings together – \nforms vector, whose elements individual strings.type vector character. Elements vector type character\nstrings. Note numeric value 6 construction vector \nautomatically coerced string \"6\".possible paste together strings character vector.likewise possible split string according pattern. instance\nindividual characters.Various additional string operations available – see ?character \ninformation.summary, atomic vectors primitive data structures used R,\nelements accessible via indices (random access).\nTypical vectors contain numbers, logical values strings. \ndeclarations data types – inferred data\ncomputations. flexible type system many operations R\nsilently coercing elements vector one type another.","code":"\nfirst_vector <- c(1, 2, 3)  # Note the 'c'\nfirst_vector## [1] 1 2 3\nsecond_vector <- c(4, 5, 6)\nc(first_vector, second_vector)## [1] 1 2 3 4 5 6\ninteger_vector <- 1:10\ninteger_vector##  [1]  1  2  3  4  5  6  7  8  9 10\ntypeof(integer_vector)  # `typeof` reveals the internal storage mode## [1] \"integer\"\ninteger_vector[3]## [1] 3\ninteger_vector[first_vector]  # The first three elements## [1] 1 2 3\ndouble_vector <- seq(0.1, 1, by = 0.1)\ndouble_vector##  [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\ntypeof(double_vector)## [1] \"double\"\ntypeof(first_vector)## [1] \"double\"\ntypeof(c(1L, 2L, 3L))## [1] \"integer\"\nvector(\"numeric\", length = 10) ##  [1] 0 0 0 0 0 0 0 0 0 0\nnumeric(10)##  [1] 0 0 0 0 0 0 0 0 0 0\nvector(length = 10)##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\nlogical_vector <- integer_vector > 4\nlogical_vector##  [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\ntypeof(logical_vector)## [1] \"logical\"\nmean(logical_vector)## [1] 0.6\ncharacter_vector <- c(\"A\", \"vector\", \"of\", \"length\", 6, \".\")\ncharacter_vector## [1] \"A\"      \"vector\" \"of\"     \"length\" \"6\"      \".\"\ntypeof(character_vector)## [1] \"character\"\npaste(character_vector, collapse = \" \")  # Now a character vector of length 1!## [1] \"A vector of length 6 .\"\nstrsplit(character_vector[2], split = \"\")  # Split \"vector\" into characters## [[1]]\n## [1] \"v\" \"e\" \"c\" \"t\" \"o\" \"r\""},{"path":"app-R.html","id":"comparisons-and-numerical-precision","chapter":"A R programming","heading":"A.1.2 Comparisons and numerical precision","text":"can compare vectors using equality operator ==, compares\ntwo vectors element--element. result following comparison might\nsurprise first sight.result comparison vector length 2 containing logical\nvalues TRUE FALSE, double_vector[2:3] appears\nequal c(0.2, 0.3). difference shows increase number\nprinted digits default (7) 20.0.3 produced seq computed 0.1 + 0.1 + 0.1, \n0.3 vector c(0.2, 0.3) converted directly double precision\nnumber. difference arises neither 0.1 0.3 exactly representable\nbinary numeral system, arithmetic operations induce rounding\nerrors. function numToBits can reveal exact difference three\nleast significant bits.Differences least significant bits tolerable numerical\ncomputations can nuisance equality testing. comparing vectors\ncontaining doubles therefore often interested testing approximate\nequality instead using e.g. function .equal tolerance argument controlling numerical\ndifferences regarded actual differences. Another way comparing\nnumerical vectors computing range differenceThis shows largest positive negative difference. Usually,\nsize differences assessed relative magnitudes \nnumbers compared. \nnumbers approximately magnitude 1, differences\norder \\(2^{-52} \\simeq 2.22 \\times 10^{-16}\\) (“machine epsilon”) can\nsafely regarded rounding errors. default (relative) tolerance\n.equal permissive number \\(\\sqrt{2^{-53}} \\simeq 1.5 \\times 10^{-8}\\).\nnumber ad hoc commonly used tolerance. See ?.equal details.","code":"\ndouble_vector[2:3]## [1] 0.2 0.3\ndouble_vector[2:3] == c(0.2, 0.3)## [1]  TRUE FALSE\nc(0.2, 0.3)## [1] 0.2000000000000000111 0.2999999999999999889\ndouble_vector[2:3]## [1] 0.20000000000000001110 0.30000000000000004441\nnumToBits(0.3)[1:3] ## [1] 01 01 00\nnumToBits(0.1 + 0.1 + 0.1)[1:3] ## [1] 00 00 01\nall.equal(double_vector, c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0))## [1] TRUE\nrange(double_vector - c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0))## [1] 0.000000e+00 1.110223e-16"},{"path":"app-R.html","id":"lists","chapter":"A R programming","heading":"A.1.3 Lists","text":"","code":""},{"path":"app-R.html","id":"other-data-structures","chapter":"A R programming","heading":"A.1.4 Other data structures","text":"Factors, dates, data frames","code":""},{"path":"app-R.html","id":"functions","chapter":"A R programming","heading":"A.2 Functions","text":"write R script source , different R expressions \nscript evaluated sequentially. Intermediate results may stored variables\nused script. may obvious, expressions\nscript , fact, function calls. script relies functions\nimplemented either core R packages installed packages.possible use R scripting language without ever writing \nfunctions, writing new R functions extend language, \nmodularize reuse code efficient way. first\nexample, implement Gaussian kernel bandwidth \\(h\\) (see Section 2.2)\n\\[K_h(x) = \\frac{1}{h \\sqrt{2 \\pi}} e^{- \\frac{x^2}{2h^2}}.\\]\ncall function gauss() remind us Gaussian kernel.body function R expression exp(- x^2 / (2 * h^2)) / (h * sqrt(2 * pi)).\nfunction called specific numeric values two formal arguments\nx h, body evaluated formal arguments replaced \nvalues. value (last) evaluated expression body returned \nfunction. bandwidth argument h given default value 1,\nargument specified function called, gets \nvalue 1.Gaussian kernel bandwidth \\(h\\) Gaussian density standard\ndeviation \\(h\\). Thus can compare implementation dnorm() R\ncomputes Gaussian density.two cases functions compute (least, printed\nprecision). Note formal argument sd given value 0.1 \nsecond call dnorm(). argument sd third argument dnorm(),\ndon’t need specify second, dnorm(0.1, 0, 0.1), \nspecify third. can name, case second argument\ndnorm() gets default value 0.several alternative ways implement Gaussian kernel \nillustrate functions can written R.following small test shows two cases implementations compute \n.function always tested. test comparison return value\nfunction expected value specific argument(s). \nexpected value can either computed hand another implementation –\ncomparisons gauss() dnorm(). tests prove\nimplementation correct, discrepancies can help catch \ncorrect bugs. Remember testing numerical computations use floating\npoint numbers expect exact equality. Thus tests reveal\nreturn values within acceptable tolerance expected results.Larger pieces software – entire R package – include\nnumber tests function implements. known \nunit testing (unit, , function, tested), \npackages supporting systematic development\nunit tests R package development. comprehensive set unit tests\nalso helps functions rewritten e.g. improve performance extend\nfunctionality. rewritten function passes tests, chances \ndidn’t break anything rewriting function.good practice write functions one well defined computation\nkeep body function relatively small. easier reason\nfunction easier comprehensively test .\nComplex behavior achieved composing small well tested functions.summarize:body enclosed curly brackets {}, may left \nbody one line.function returns value last expression body except \nbody contains explicit return statement.Formal arguments can given default values function implemented,\narguments can passed function position well name.Functions single well defined computation well tested.","code":"\ngauss <- function(x, h = 1) {\n  exp(- x^2 / (2 * h^2)) / (h * sqrt(2 * pi))\n}\nc(gauss(1), dnorm(1))## [1] 0.2419707 0.2419707\nc(gauss(0.1, 0.1), dnorm(0.1, sd = 0.1))  # Bandwidth changed to 0.1## [1] 2.419707 2.419707\n# A one-liner without curly brackets\ngauss_one_liner <- function(x, h = 1) \n  exp(- x^2 / (2 * h^2)) / (h * sqrt(2 * pi)) \n\n# A stepwise implementation computing the exponent first \ngauss_step <- function(x, h = 1) {\n  exponent <- (x / h)^2 / 2\n  exp(- exponent) / (h * sqrt(2 * pi))\n}\n\n# A stepwise implementation with an explicit return statement\ngauss_step_return <- function(x, h = 1) {\n  exponent <- (x / h)^2 / 2\n  value <- exp(- exponent) / (h * sqrt(2 * pi))\n  return(value)\n}\nc(gauss(1), gauss_one_liner(1), gauss_step(1), gauss_step_return(1))## [1] 0.2419707 0.2419707 0.2419707 0.2419707\nc(\n  gauss(0.1, 0.1), \n  gauss_one_liner(0.1, 0.1), \n  gauss_step(0.1, 0.1), \n  gauss_step_return(0.1, 0.1)\n)## [1] 2.419707 2.419707 2.419707 2.419707"},{"path":"app-R.html","id":"vectorization","chapter":"A R programming","heading":"A.2.1 Vectorization","text":"Section .1 shown comparison operators\nwork vectorized way. R, comparing vector another vector \nnumber leads element--element comparisons logical vector result.\none example many operations function evaluations R \nnatively vectorized, means function evaluated \nvector argument function body effectively evaluated entry \nvector.gauss() function another example function automatically\nworks vectorized function.works expected vector input functions body\ngauss() vectorized, , arithmetic operators vectorized,\nsquare root vectorized exponential function vectorized.good practice write R programs use vectorized computations whenever\npossible. alternative -loop can much slower. Several examples \nbook illustrate computational benefits vectorized implementations. may,\nhowever, always obvious correctly implement vectorized function.Suppose want implement following function\n\\[\\overline{f}_h(x) =  \\frac{1}{n} \\sum_{=1}^n K_h(x - x_i)\\]\ndata set \\(x_1, \\ldots, x_n\\) \\(K_h\\) Gaussian kernel. \nGaussian kernel density estimator considered Section 2.4. \nstraight forward implementation isThis implementation works correctly x h single numbers\ne.g. x vector, function returns number (vector)\nunrelated \\(\\overline{f}_h(0)\\) \\(\\overline{f}_h(1)\\).quick fix following explicit vectorizationThe R function Vectorize() \nfunction operator, \ntakes function argument returns function. case vectorized\nversion input function. , f_bar_vec() can applied vector,\nresults applying f_bar() element vector. test f_bar_vec()\nworks correctly – first second argument given \nvector.function Vectorize() basically works looping \nelements vector argument(s) applying function element.\nprototyping quick implementations can convenient,\nshortcut efficient vectorized computations.Vectorize() used Section 2.4 implement \\(\\overline{f}_h\\) based \ndnorm(). purpose implementation able compute\n\\(\\overline{f}_h(x)\\) arbitrary \\(x\\) vectorized way, function\ncan used together curve() likelihood computations \nsection. implementations Section 2.2.1 differ \ncomputing returning\n\\[\\overline{f}_h(\\tilde{x}_1), \\ldots, \\overline{f}_h(\\tilde{x}_m).\\]\n, implementations return evaluations \\(\\overline{f}_h\\) \nequidistant grid \\(\\tilde{x}_1, \\ldots, \\tilde{x}_m\\) \\(\\overline{f}_h\\)\n.","code":"\ngauss(c(1, 0.1), c(1, 0.1))## [1] 0.2419707 2.4197072\nxs <- rnorm(10)  # A data set with 10 observations\nf_bar <- function(x, h) \n  mean(gauss(x - xs, h))\nc(f_bar(0, 1), f_bar(1, 1))## [1] 0.2289101 0.2434550\nf_bar(c(0, 1), 1) # Computation is not vectorized## [1] 0.2201532\nf_bar_vec <- Vectorize(f_bar)\nf_bar_vec(c(0, 1), 1)## [1] 0.2289101 0.2434550\nc(f_bar(0, 1), f_bar(1, 1)) # Same result as the vectorized computation## [1] 0.2289101 0.2434550\nc(f_bar(1, 1), f_bar(1, 0.1))## [1] 0.2434550 0.4022502\nf_bar_vec(1, c(1, 0.1)) # Same result as the vectorized computation## [1] 0.2434550 0.4022502"},{"path":"app-R.html","id":"environments","chapter":"A R programming","heading":"A.2.2 Environments","text":"SomethingUse xs example.","code":""},{"path":"app-R.html","id":"function-factories","chapter":"A R programming","heading":"A.2.3 Function factories","text":"SomethingThe random number streams Chap. 4. Include implementation package.","code":""},{"path":"app-R.html","id":"performance","chapter":"A R programming","heading":"A.3 Performance","text":"SomethingMention parallel computations","code":""},{"path":"app-R.html","id":"tracing","chapter":"A R programming","heading":"A.3.1 Tracing","text":"","code":""},{"path":"app-R.html","id":"rcpp","chapter":"A R programming","heading":"A.3.2 Rcpp","text":"Use mean_numeric mean_complex examples.","code":""},{"path":"app-R.html","id":"objects-and-methods","chapter":"A R programming","heading":"A.4 Objects and methods","text":"Something","code":""},{"path":"app-R.html","id":"app-ex","chapter":"A R programming","heading":"A.5 Exercises","text":"","code":""},{"path":"app-R.html","id":"functions-1","chapter":"A R programming","heading":"Functions","text":"Exercise .2  Write function takes numeric vector x threshold value h\narguments returns vector values x greater h.\nTest function seq(0, 1, 0.1) threshold 0.3. example\nExercise .1 mind.Exercise .3  Investigate function Exercise .2\ntreats missing values (NA), infinite values\n(Inf -Inf) special value “Number” (NaN). Rewrite \nfunction (necessary) exclude values x.Hint: functions .na, .nan .finite useful.","code":"\n(0.1 + 0.1 + 0.1) > 0.3## [1] TRUE"},{"path":"app-R.html","id":"histograms-with-non-equidistant-breaks","chapter":"A R programming","heading":"Histograms with non-equidistant breaks","text":"following three exercises use data set consisting measurements\ninfrared emissions objects outside galax. focus variable\nF12, total 12 micron band flux density.purpose exercise two-fold. First, get familiar \ndata see different choices visualizations using histograms can affect\ninterpretation data. Second, learn write\nfunctions R gain better understanding work.Exercise .4  Plot histogram log(F12) using default value argument breaks. Experiment alternative values breaks.Exercise .5  Write function, called my_breaks, takes two arguments, x (vector) h (positive integer). Let h default value 5. function first sort\nx increasing order return vector : starts smallest entry x;\ncontains every \\(h\\)th unique entry sorted x; ends largest entry x.example, h = 2 x = c(1, 3, 2, 5, 10, 11, 1, 1, 3) function return c(1, 3, 10, 11). see , first sort x, gives vector c(1, 1, 1, 2, 3, 3, 5, 10, 11), whose unique\nvalues c(1, 2, 3, 5, 10, 11). Every second unique entry c(1, 3, 10), largest entry 11 concatenated.Hint: functions sort unique can useful.Use function construct breakpoints histogram different values h, compare histograms obtained Exercise .4.Exercise .6  ties data set, function produce breakpoints\nh observations interval two consecutive breakpoints\n(except last two perhaps). ties, function construction\nreturn unique breakpoints, may \nh observations intervals.intention now rewrite my_breaks possible interval\ncontains h observations.Modify my_breaks function intention \nfollowing properties:breakpoints must unique.range breakpoints must cover range x.two subsequent breakpoints, \\(\\) \\(b\\), must least h observations interval \\((,b],\\) provided h < length(x). (exception first two breakpoints, interval \\([,b].\\))","code":"\ninfrared <- read.table(\"data/infrared.txt\", header = TRUE)\nF12 <- infrared$F12"},{"path":"app-R.html","id":"functions-and-objects","chapter":"A R programming","heading":"Functions and objects","text":"following exercises build implemented function \ncomputes breakpoints histogram either \nExercise .5 Exercise .6.Exercise .8  Modify my_hist function returns object class my_histogram,\nplotted. Write print method objects class,\nprints just number cells.Hint: can useful know function cat.","code":"\nmy_hist()\nmy_hist(h = 5, freq = TRUE)\nmy_hist(h = 0)"},{"path":"app-R.html","id":"functions-and-environments","chapter":"A R programming","heading":"Functions and environments","text":"following exercises assume implemented my_hist function\nExercise .7.Exercise .11  happens remove data call my_hist subsequently?\nenvironment my_hist? Change new environment, assign\n(using function assign) data \nvariable appropriate name environment. done,\ncheck now happens calling my_hist \ndata removed global environment.Exercise .12  Write function takes argument x (data) \nreturns function, returned function\ntakes argument h (just my_hist) plots histogram (just my_hist).\nreturn value function, may refer function\nfunction factory.environment function created function factory? \nenvironment?\neffect calling function whether data altered\nremoved global environment?Exercise .13  Evaluate following function call:type class tmp? happens plot(tmp, col = \"red\")\nexecuted? can find help plot \nobject class? Specifically, find documentation \nargument col, argument plot?","code":"\ntmp <- my_hist(10, plot = FALSE)"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]

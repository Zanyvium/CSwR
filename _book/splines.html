<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Statistics with R</title>
  <meta name="description" content="Lecture notes providing an introduction to computational statistics using the R programming language.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Statistics with R" />
  
  <meta name="twitter:description" content="Lecture notes providing an introduction to computational statistics using the R programming language." />
  

<meta name="author" content="Niels Richard Hansen">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="onb.html">
<link rel="next" href="gaussian-processes.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro-smooth.html"><a href="intro-smooth.html"><i class="fa fa-check"></i><b>1.1</b> Smoothing</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro-smooth.html"><a href="intro-smooth.html#intro-angles"><i class="fa fa-check"></i><b>1.1.1</b> Angle distributions in proteins</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-smooth.html"><a href="intro-smooth.html#using-ggplot2"><i class="fa fa-check"></i><b>1.1.2</b> Using ggplot2</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-smooth.html"><a href="intro-smooth.html#changing-the-defaults"><i class="fa fa-check"></i><b>1.1.3</b> Changing the defaults</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-smooth.html"><a href="intro-smooth.html#large-scale-smoothing"><i class="fa fa-check"></i><b>1.1.4</b> Large scale smoothing</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html"><i class="fa fa-check"></i><b>1.2</b> Monte Carlo Methods</a><ul>
<li class="chapter" data-level="1.2.1" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#vM"><i class="fa fa-check"></i><b>1.2.1</b> Univariate von Mises distributions</a></li>
<li class="chapter" data-level="1.2.2" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#mixtures-of-von-mises-distributions"><i class="fa fa-check"></i><b>1.2.2</b> Mixtures of von Mises distributions</a></li>
<li class="chapter" data-level="1.2.3" data-path="monte-carlo-methods.html"><a href="monte-carlo-methods.html#large-scale-simulation"><i class="fa fa-check"></i><b>1.2.3</b> Large scale simulation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="optimization.html"><a href="optimization.html"><i class="fa fa-check"></i><b>1.3</b> Optimization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="optimization.html"><a href="optimization.html#the-em-algorithm"><i class="fa fa-check"></i><b>1.3.1</b> The EM-algorithm</a></li>
<li class="chapter" data-level="1.3.2" data-path="optimization.html"><a href="optimization.html#large-scale-optimization"><i class="fa fa-check"></i><b>1.3.2</b> Large scale optimization</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.4</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#r-training-exercises"><i class="fa fa-check"></i>R training exercises</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#histograms-with-non-equidistant-breaks"><i class="fa fa-check"></i>Histograms with non-equidistant breaks</a></li>
<li class="chapter" data-level="" data-path="exercises.html"><a href="exercises.html#functions-and-functional-programming"><i class="fa fa-check"></i>Functions and functional programming</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part I: Smoothing</b></span></li>
<li class="chapter" data-level="2" data-path="density.html"><a href="density.html"><i class="fa fa-check"></i><b>2</b> Density estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="unidens.html"><a href="unidens.html"><i class="fa fa-check"></i><b>2.1</b> Univariate density estimation</a><ul>
<li class="chapter" data-level="2.1.1" data-path="unidens.html"><a href="unidens.html#likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Likelihood considerations</a></li>
<li class="chapter" data-level="2.1.2" data-path="unidens.html"><a href="unidens.html#sieves"><i class="fa fa-check"></i><b>2.1.2</b> Method of sieves</a></li>
<li class="chapter" data-level="2.1.3" data-path="unidens.html"><a href="unidens.html#basis-density"><i class="fa fa-check"></i><b>2.1.3</b> Basis expansions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="kernel-density.html"><a href="kernel-density.html"><i class="fa fa-check"></i><b>2.2</b> Kernel methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="kernel-density.html"><a href="kernel-density.html#implementation"><i class="fa fa-check"></i><b>2.2.1</b> Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="kernel-density.html"><a href="kernel-density.html#benchmarking"><i class="fa fa-check"></i><b>2.2.2</b> Benchmarking</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bandwidth.html"><a href="bandwidth.html"><i class="fa fa-check"></i><b>2.3</b> Bandwidth selection</a><ul>
<li class="chapter" data-level="2.3.1" data-path="bandwidth.html"><a href="bandwidth.html#rectangular"><i class="fa fa-check"></i><b>2.3.1</b> Revisiting the rectangular kernel</a></li>
<li class="chapter" data-level="2.3.2" data-path="bandwidth.html"><a href="bandwidth.html#ise-mise-and-mse-for-kernel-estimators"><i class="fa fa-check"></i><b>2.3.2</b> ISE, MISE and MSE for kernel estimators</a></li>
<li class="chapter" data-level="2.3.3" data-path="bandwidth.html"><a href="bandwidth.html#plug-in-estimation-of-the-oracle-bandwidth"><i class="fa fa-check"></i><b>2.3.3</b> Plug-in estimation of the oracle bandwidth</a></li>
<li class="chapter" data-level="2.3.4" data-path="bandwidth.html"><a href="bandwidth.html#cross-validation"><i class="fa fa-check"></i><b>2.3.4</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multivariate-smoothing.html"><a href="multivariate-smoothing.html"><i class="fa fa-check"></i><b>2.4</b> Multivariate methods</a></li>
<li class="chapter" data-level="2.5" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#kernel-density-estimation"><i class="fa fa-check"></i>Kernel density estimation</a></li>
<li class="chapter" data-level="" data-path="exercises-1.html"><a href="exercises-1.html#benchmarking-1"><i class="fa fa-check"></i>Benchmarking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i><b>3</b> Bivariate smoothing</a><ul>
<li class="chapter" data-level="3.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html"><i class="fa fa-check"></i><b>3.1</b> Nearest neighbor smoothers</a><ul>
<li class="chapter" data-level="3.1.1" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#linear-smoothers"><i class="fa fa-check"></i><b>3.1.1</b> Linear smoothers</a></li>
<li class="chapter" data-level="3.1.2" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#implementing-the-running-mean"><i class="fa fa-check"></i><b>3.1.2</b> Implementing the running mean</a></li>
<li class="chapter" data-level="3.1.3" data-path="nearest-neighbor-smoothers.html"><a href="nearest-neighbor-smoothers.html#choose-k-by-cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Choose <span class="math inline">\(k\)</span> by cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="kernel-methods.html"><a href="kernel-methods.html"><i class="fa fa-check"></i><b>3.2</b> Kernel methods</a></li>
<li class="chapter" data-level="3.3" data-path="sparse-linear-algebra.html"><a href="sparse-linear-algebra.html"><i class="fa fa-check"></i><b>3.3</b> Sparse linear algebra</a></li>
<li class="chapter" data-level="3.4" data-path="onb.html"><a href="onb.html"><i class="fa fa-check"></i><b>3.4</b> Orthogonal basis expansions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="onb.html"><a href="onb.html#polynomial-expansions"><i class="fa fa-check"></i><b>3.4.1</b> Polynomial expansions</a></li>
<li class="chapter" data-level="3.4.2" data-path="onb.html"><a href="onb.html#fourier-expansions"><i class="fa fa-check"></i><b>3.4.2</b> Fourier expansions</a></li>
<li class="chapter" data-level="3.4.3" data-path="onb.html"><a href="onb.html#wavelets"><i class="fa fa-check"></i><b>3.4.3</b> Wavelets</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>3.5</b> Splines</a><ul>
<li class="chapter" data-level="3.5.1" data-path="splines.html"><a href="splines.html#smoothing-splines"><i class="fa fa-check"></i><b>3.5.1</b> Smoothing splines</a></li>
<li class="chapter" data-level="3.5.2" data-path="splines.html"><a href="splines.html#splines-in-r"><i class="fa fa-check"></i><b>3.5.2</b> Splines in R</a></li>
<li class="chapter" data-level="3.5.3" data-path="splines.html"><a href="splines.html#efficient-computation-with-splines"><i class="fa fa-check"></i><b>3.5.3</b> Efficient computation with splines</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="gaussian-processes.html"><a href="gaussian-processes.html"><i class="fa fa-check"></i><b>3.6</b> Gaussian processes</a></li>
<li class="chapter" data-level="3.7" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html"><i class="fa fa-check"></i><b>3.7</b> The Kalman filter</a><ul>
<li class="chapter" data-level="3.7.1" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#ar1-example"><i class="fa fa-check"></i><b>3.7.1</b> AR(1)-example</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-smoother"><i class="fa fa-check"></i><b>3.7.2</b> The Kalman smoother</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#implementation-1"><i class="fa fa-check"></i><b>3.7.3</b> Implementation</a></li>
<li class="chapter" data-level="3.7.4" data-path="the-kalman-filter.html"><a href="the-kalman-filter.html#the-kalman-filter-1"><i class="fa fa-check"></i><b>3.7.4</b> The Kalman filter</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Part II: Monte Carlo Methods</b></span></li>
<li class="chapter" data-level="4" data-path="univariate-random-variables.html"><a href="univariate-random-variables.html"><i class="fa fa-check"></i><b>4</b> Univariate random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="pseudo-random-numbers.html"><a href="pseudo-random-numbers.html"><i class="fa fa-check"></i><b>4.1</b> Pseudo random numbers</a></li>
<li class="chapter" data-level="4.2" data-path="transformation-techniques.html"><a href="transformation-techniques.html"><i class="fa fa-check"></i><b>4.2</b> Transformation techniques</a><ul>
<li class="chapter" data-level="4.2.1" data-path="transformation-techniques.html"><a href="transformation-techniques.html#sampling-from-a-t-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Sampling from a <span class="math inline">\(t\)</span>-distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="reject-samp.html"><a href="reject-samp.html"><i class="fa fa-check"></i><b>4.3</b> Rejection sampling</a><ul>
<li class="chapter" data-level="4.3.1" data-path="reject-samp.html"><a href="reject-samp.html#vMsim"><i class="fa fa-check"></i><b>4.3.1</b> von Mises distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="reject-samp.html"><a href="reject-samp.html#gamma-distribution"><i class="fa fa-check"></i><b>4.3.2</b> Gamma distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="adaptive.html"><a href="adaptive.html"><i class="fa fa-check"></i><b>4.4</b> Adaptive envelopes</a><ul>
<li class="chapter" data-level="4.4.1" data-path="adaptive.html"><a href="adaptive.html#beta-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Beta distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="adaptive.html"><a href="adaptive.html#von-mises-distribution"><i class="fa fa-check"></i><b>4.4.2</b> von Mises distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="exercises-2.html"><a href="exercises-2.html#rejection-sampling-of-gaussian-random-variables"><i class="fa fa-check"></i>Rejection sampling of Gaussian random variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mci.html"><a href="mci.html"><i class="fa fa-check"></i><b>5</b> Monte Carlo integration</a><ul>
<li class="chapter" data-level="5.1" data-path="assessment.html"><a href="assessment.html"><i class="fa fa-check"></i><b>5.1</b> Assessment</a><ul>
<li class="chapter" data-level="5.1.1" data-path="assessment.html"><a href="assessment.html#using-the-central-limit-theorem"><i class="fa fa-check"></i><b>5.1.1</b> Using the central limit theorem</a></li>
<li class="chapter" data-level="5.1.2" data-path="assessment.html"><a href="assessment.html#concentration-inequalities"><i class="fa fa-check"></i><b>5.1.2</b> Concentration inequalities</a></li>
<li class="chapter" data-level="5.1.3" data-path="assessment.html"><a href="assessment.html#exponential-tail-bound-for-gamma-distributed-variables"><i class="fa fa-check"></i><b>5.1.3</b> Exponential tail bound for Gamma distributed variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>5.2</b> Importance sampling</a><ul>
<li class="chapter" data-level="5.2.1" data-path="importance-sampling.html"><a href="importance-sampling.html#unknown-normalization-constants"><i class="fa fa-check"></i><b>5.2.1</b> Unknown normalization constants</a></li>
<li class="chapter" data-level="5.2.2" data-path="importance-sampling.html"><a href="importance-sampling.html#computing-a-high-dimensional-integral"><i class="fa fa-check"></i><b>5.2.2</b> Computing a high-dimensional integral</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="network-failure.html"><a href="network-failure.html"><i class="fa fa-check"></i><b>5.3</b> Network failure</a><ul>
<li class="chapter" data-level="5.3.1" data-path="network-failure.html"><a href="network-failure.html#object-oriented-implementation"><i class="fa fa-check"></i><b>5.3.1</b> Object oriented implementation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="design-of-experiments.html"><a href="design-of-experiments.html"><i class="fa fa-check"></i><b>5.4</b> Design of experiments</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-random-variables.html"><a href="multivariate-random-variables.html"><i class="fa fa-check"></i><b>6</b> Multivariate random variables</a><ul>
<li class="chapter" data-level="6.1" data-path="sequential-simulation.html"><a href="sequential-simulation.html"><i class="fa fa-check"></i><b>6.1</b> Sequential simulation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="sequential-simulation.html"><a href="sequential-simulation.html#sequential-mc-for-the-ar1-process"><i class="fa fa-check"></i><b>6.1.1</b> Sequential MC for the AR(1)-process</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="gaussian-random-variables.html"><a href="gaussian-random-variables.html"><i class="fa fa-check"></i><b>6.2</b> Gaussian random variables</a></li>
</ul></li>
<li class="part"><span><b>Part III: Optimization</b></span></li>
<li class="chapter" data-level="7" data-path="five-examples.html"><a href="five-examples.html"><i class="fa fa-check"></i><b>7</b> Five Examples</a><ul>
<li class="chapter" data-level="7.1" data-path="exp-fam.html"><a href="exp-fam.html"><i class="fa fa-check"></i><b>7.1</b> Exponential families</a><ul>
<li class="chapter" data-level="7.1.1" data-path="exp-fam.html"><a href="exp-fam.html#exp-fam"><i class="fa fa-check"></i><b>7.1.1</b> Full exponential families</a></li>
<li class="chapter" data-level="7.1.2" data-path="exp-fam.html"><a href="exp-fam.html#bayes-net"><i class="fa fa-check"></i><b>7.1.2</b> Exponential family Bayesian networks</a></li>
<li class="chapter" data-level="7.1.3" data-path="exp-fam.html"><a href="exp-fam.html#exp-fam-deriv"><i class="fa fa-check"></i><b>7.1.3</b> Likelihood computations</a></li>
<li class="chapter" data-level="7.1.4" data-path="exp-fam.html"><a href="exp-fam.html#curved-exponential-families"><i class="fa fa-check"></i><b>7.1.4</b> Curved exponential families</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="multinomial-models.html"><a href="multinomial-models.html"><i class="fa fa-check"></i><b>7.2</b> Multinomial models</a><ul>
<li class="chapter" data-level="7.2.1" data-path="multinomial-models.html"><a href="multinomial-models.html#peppered-moths"><i class="fa fa-check"></i><b>7.2.1</b> Peppered Moths</a></li>
<li class="chapter" data-level="7.2.2" data-path="multinomial-models.html"><a href="multinomial-models.html#multinomial-cell-collapsing"><i class="fa fa-check"></i><b>7.2.2</b> Multinomial cell collapsing</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>7.3</b> Regression models</a></li>
<li class="chapter" data-level="7.4" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html"><i class="fa fa-check"></i><b>7.4</b> Finite mixture models</a><ul>
<li class="chapter" data-level="7.4.1" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#gaussian-mixtures"><i class="fa fa-check"></i><b>7.4.1</b> Gaussian mixtures</a></li>
<li class="chapter" data-level="7.4.2" data-path="finite-mixture-models.html"><a href="finite-mixture-models.html#von-mises-mixtures"><i class="fa fa-check"></i><b>7.4.2</b> von Mises mixtures</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="mixed-models.html"><a href="mixed-models.html"><i class="fa fa-check"></i><b>7.5</b> Mixed models</a></li>
<li class="chapter" data-level="7.6" data-path="state-space-models.html"><a href="state-space-models.html"><i class="fa fa-check"></i><b>7.6</b> State space models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="numopt.html"><a href="numopt.html"><i class="fa fa-check"></i><b>8</b> Numerical optimization</a><ul>
<li class="chapter" data-level="8.1" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html"><i class="fa fa-check"></i><b>8.1</b> Algorithms and convergence</a><ul>
<li class="chapter" data-level="8.1.1" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#descent-algorithms"><i class="fa fa-check"></i><b>8.1.1</b> Descent algorithms</a></li>
<li class="chapter" data-level="8.1.2" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#maps-and-fixed-points"><i class="fa fa-check"></i><b>8.1.2</b> Maps and fixed points</a></li>
<li class="chapter" data-level="8.1.3" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#convergence-rate"><i class="fa fa-check"></i><b>8.1.3</b> Convergence rate</a></li>
<li class="chapter" data-level="8.1.4" data-path="algorithms-and-convergence.html"><a href="algorithms-and-convergence.html#stopping-criteria"><i class="fa fa-check"></i><b>8.1.4</b> Stopping criteria</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html"><i class="fa fa-check"></i><b>8.2</b> Descent direction algorithms</a><ul>
<li class="chapter" data-level="8.2.1" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#line-search"><i class="fa fa-check"></i><b>8.2.1</b> Line search</a></li>
<li class="chapter" data-level="8.2.2" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#gradient-descent"><i class="fa fa-check"></i><b>8.2.2</b> Gradient descent</a></li>
<li class="chapter" data-level="8.2.3" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#conjugate-gradients"><i class="fa fa-check"></i><b>8.2.3</b> Conjugate gradients</a></li>
<li class="chapter" data-level="8.2.4" data-path="descent-direction-algorithms.html"><a href="descent-direction-algorithms.html#peppered-moths-1"><i class="fa fa-check"></i><b>8.2.4</b> Peppered Moths</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html"><i class="fa fa-check"></i><b>8.3</b> Newton-type algorithms</a><ul>
<li class="chapter" data-level="8.3.1" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#poisson-regression"><i class="fa fa-check"></i><b>8.3.1</b> Poisson regression</a></li>
<li class="chapter" data-level="8.3.2" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#quasi-newton-algorithms"><i class="fa fa-check"></i><b>8.3.2</b> Quasi-Newton algorithms</a></li>
<li class="chapter" data-level="8.3.3" data-path="newton-type-algorithms.html"><a href="newton-type-algorithms.html#sparsity"><i class="fa fa-check"></i><b>8.3.3</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="misc-.html"><a href="misc-.html"><i class="fa fa-check"></i><b>8.4</b> Misc.</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="em.html"><a href="em.html"><i class="fa fa-check"></i><b>9</b> Expectation Maximization algorithms</a><ul>
<li class="chapter" data-level="9.1" data-path="basic-properties.html"><a href="basic-properties.html"><i class="fa fa-check"></i><b>9.1</b> Basic properties</a><ul>
<li class="chapter" data-level="9.1.1" data-path="basic-properties.html"><a href="basic-properties.html#incomplete-data-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Incomplete data likelihood</a></li>
<li class="chapter" data-level="9.1.2" data-path="basic-properties.html"><a href="basic-properties.html#the-em-algorithm-is-ascending"><i class="fa fa-check"></i><b>9.1.2</b> The EM-algorithm is ascending</a></li>
<li class="chapter" data-level="9.1.3" data-path="basic-properties.html"><a href="basic-properties.html#multinomial-cell-collapsing-1"><i class="fa fa-check"></i><b>9.1.3</b> Multinomial cell collapsing</a></li>
<li class="chapter" data-level="9.1.4" data-path="basic-properties.html"><a href="basic-properties.html#peppered-moths-e--and-m-steps"><i class="fa fa-check"></i><b>9.1.4</b> Peppered Moths E- and M-steps</a></li>
<li class="chapter" data-level="9.1.5" data-path="basic-properties.html"><a href="basic-properties.html#inside-the-em"><i class="fa fa-check"></i><b>9.1.5</b> Inside the EM</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="EM-exp.html"><a href="EM-exp.html"><i class="fa fa-check"></i><b>9.2</b> Exponential families</a></li>
<li class="chapter" data-level="9.3" data-path="fisher-information.html"><a href="fisher-information.html"><i class="fa fa-check"></i><b>9.3</b> Fisher information</a></li>
<li class="chapter" data-level="9.4" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html"><i class="fa fa-check"></i><b>9.4</b> Two examples revisited</a><ul>
<li class="chapter" data-level="9.4.1" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html#gaussian-mixtures-1"><i class="fa fa-check"></i><b>9.4.1</b> Gaussian mixtures</a></li>
<li class="chapter" data-level="9.4.2" data-path="two-examples-revisited.html"><a href="two-examples-revisited.html#gaussian-state-space"><i class="fa fa-check"></i><b>9.4.2</b> Gaussian state space</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="stochopt.html"><a href="stochopt.html"><i class="fa fa-check"></i><b>10</b> Stochastic Optimization</a><ul>
<li class="chapter" data-level="10.1" data-path="stochastic-gradient.html"><a href="stochastic-gradient.html"><i class="fa fa-check"></i><b>10.1</b> Stochastic gradient</a></li>
<li class="chapter" data-level="10.2" data-path="stochastic-em.html"><a href="stochastic-em.html"><i class="fa fa-check"></i><b>10.2</b> Stochastic EM</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="splines" class="section level2">
<h2><span class="header-section-number">3.5</span> Splines</h2>
<p>In <a href="onb.html#onb">the previous section</a> orthogonality of basis functions played an important role for computing basis function expansions efficiently as well as for the statistical assessment of estimated coefficients. This section will deal with bivariate smoothing via basis functions that are not necessarily orthogonal.</p>
<p>Though some of the material of this section will apply to any choice of basis, we restrict attention to splines and consider almost exclusively the widely used B-splines (the “B” is for basis).</p>
<div id="smoothing-splines" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Smoothing splines</h3>
To motivate splines we briefly consider the following penalized least squares criterion for finding a smooth approximation to bivariate data: minimize
<span class="math display" id="eq:spline-criterion">\[\begin{equation}
L(f) = \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \|f&#39;&#39;\|_2^2
\tag{3.1}
\end{equation}\]</span>
<p>over all twice differentiable functions <span class="math inline">\(f\)</span>. The first term is the standard squared error, and we can easily find a smooth function interpolating the <span class="math inline">\(y\)</span>-values (if all the <span class="math inline">\(x\)</span>-values are different), which will thus drive the squared error to 0. The squared 2-norm regularizes the minimization problem so that the minimizer finds a balance between interpolation and having a small second derivative (note that <span class="math inline">\(\|f&#39;&#39;\|_2 = 0\)</span> if and only if <span class="math inline">\(f\)</span> is an affine function). The tuning parameter <span class="math inline">\(\lambda\)</span> controls this balance.</p>
<p>It is possible to show that the minimizer of <a href="splines.html#eq:spline-criterion">(3.1)</a> is a <a href="https://en.wikipedia.org/wiki/Spline_(mathematics)">natural cubic spline</a> with <em>knots</em> in the data points <span class="math inline">\(x_i\)</span>. That is, the spline is a <span class="math inline">\(C^2\)</span>-function that equals a third degree polynomial in between the knots. At the knots, the two polynomials that meet fit together up to the second derivative, but they may differ on the third derivative. That the solution is <em>natural</em> means that it has zero second and third derivative at and beyond the two boundary knots.</p>
<p>It is not particularly difficult to show that the space of natural cubic splines is a vector space of dimension <span class="math inline">\(n\)</span> if all the <span class="math inline">\(x\)</span>-values are different. It is therefore possible to find a basis of splines, <span class="math inline">\(\varphi_1, \ldots, \varphi_n\)</span>, such that the <span class="math inline">\(f\)</span> that minimizes <a href="splines.html#eq:spline-criterion">(3.1)</a> is of the form <span class="math display">\[f = \sum_{i=1}^n \beta_i \varphi_i.\]</span> What is remarkable about this is that the basis (and the finite dimensional vector space it spans) doesn’t depend upon the <span class="math inline">\(y\)</span>-values. Though the optimization is over an infinite dimensional space, the penalization ensures that the minimizer is always in the same finite dimensional space nomatter what <span class="math inline">\(y_1, \ldots, y_n\)</span> are. Moreover, since <a href="splines.html#eq:spline-criterion">(3.1)</a> is a quite natural criterion to minimize to find a smooth function fitting the bivariate data, splines appear as good candidates for producing such smooth fits. On top of that, splines have several computational advantages and are widely used.</p>
<p>If we let <span class="math inline">\(\hat{f}_i = \hat{f}(x_i)\)</span> with <span class="math inline">\(\hat{f}\)</span> the minimizer of <a href="splines.html#eq:spline-criterion">(3.1)</a>, we have in vector notation that <span class="math display">\[\hat{\mathbf{f}} = \boldsymbol{\Phi}\hat{\beta}\]</span> with <span class="math inline">\(\boldsymbol{\Phi}_{ij} = \varphi_j(x_i)\)</span>. The minimizer can be found by observing that</p>
<span class="math display">\[\begin{align}
 L(\mathbf{f}) &amp; = (\mathbf{y} - \mathbf{f})^T (\mathbf{y} - \mathbf{f}) + \lambda \| f&#39;&#39; \|_2^2 \\
&amp; = ( \mathbf{y} -  \boldsymbol{\Phi}\beta)^T (\mathbf{y} -  \boldsymbol{\Phi}\beta) + \lambda \beta^T \mathbf{\Omega} \beta
\end{align}\]</span>
<p>where <span class="math display">\[\mathbf{\Omega}_{ij} = \langle \varphi_i&#39;&#39;, \varphi_j&#39;&#39; \rangle = 
\int  \varphi_i&#39;&#39;(z) \varphi_j&#39;&#39;(z) \mathrm{d}z.\]</span> The matrix <span class="math inline">\(\mathbf{\Omega}\)</span> is positive semidefinite by construction, and we refer to it as the <em>penalty matrix</em>. It induces a seminorm on <span class="math inline">\(\mathbb{R}^n\)</span> so that we can express the seminorm, <span class="math inline">\(\|f&#39;&#39;\|_2\)</span>, of <span class="math inline">\(f\)</span> in terms of the parameters in the basis expansion using <span class="math inline">\(\varphi_i\)</span>.</p>
<p>This is a standard penalized least squares problem, whose solution is <span class="math display">\[\hat{\beta} = (\boldsymbol{\Phi}^T \boldsymbol{\Phi} + \lambda \mathbf{\Omega})^{-1}\boldsymbol{\Phi}^T  \mathbf{y}\]</span> and with resulting smoother <span class="math display">\[\hat{\mathbf{f}} = \underbrace{\boldsymbol{\Phi} ((\boldsymbol{\Phi}^T \boldsymbol{\Phi} + \lambda \mathbf{\Omega})^{-1}\boldsymbol{\Phi}^T}_{\mathbf{S}_{\lambda}} \mathbf{y}.\]</span> This linear smoother with smoothing matrix <span class="math inline">\(\mathbf{S}_{\lambda}\)</span> based on natural cubic splines gives what is known as a <a href="https://en.wikipedia.org/wiki/Smoothing_spline">smoothing spline</a> that minimizes <a href="splines.html#eq:spline-criterion">(3.1)</a>. We will pursue spline based smoothing by minimizing <a href="splines.html#eq:spline-criterion">(3.1)</a> but using various B-spline bases that may have more or less than <span class="math inline">\(n\)</span> elements. For the linear algebra, it doesn’t matter what basis we choose as long as <span class="math inline">\(\boldsymbol{\Phi}_{ij} = \varphi_j(x_i)\)</span> and <span class="math inline">\(\mathbf{\Omega}\)</span> is given in terms of <span class="math inline">\(\varphi&#39;&#39;_i\)</span> as above.</p>
</div>
<div id="splines-in-r" class="section level3">
<h3><span class="header-section-number">3.5.2</span> Splines in R</h3>
<p>The splines package in R implements some of the basic functions needed to work with splines. In particular, the <code>splineDesign</code> function that computes evaluations of B-splines and their derivatives.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(splines)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Note the specification of repeated boundary knots
knots &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.2</span>), <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)
xx &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.005</span>)
B_splines &lt;-<span class="st"> </span><span class="kw">splineDesign</span>(knots, xx)
<span class="kw">matplot</span>(xx, B_splines, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">lty =</span> <span class="dv">1</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:splines"></span>
<img src="CSwR_files/figure-html/splines-1.png" alt="B-spline basis as computed by `splineDesign`." width="70%" />
<p class="caption">
Figure 3.9: B-spline basis as computed by <code>splineDesign</code>.
</p>
</div>
<p>The basis shown in Figure <a href="splines.html#fig:splines">3.9</a> is an example of a cubic B-spline basis with 11 inner knots in <span class="math inline">\(0, 0.1, \ldots, 0.9, 1\)</span>. The repeated boundary knots control how the spline basis behaves close to the boundaries of the interval. This basis has 13 basis functions, not 11, and spans a larger space than the space of <em>natural</em> cubic splines. It is possible to compute a basis based on B-splines for the natural cubic splines using the function <code>ns</code>, but for all practical purposes this is not important, and we will work exclusively with the B-spline basis itself.</p>
<p>The computation of the penalty matrix <span class="math inline">\(\mathbf{\Omega}\)</span> constitutes a practical problem, but observing that <span class="math inline">\(\varphi&#39;&#39;_i\)</span> is an affine function in between knots leads to a simple way of computing <span class="math inline">\(\mathbf{\Omega}_{ij}\)</span>. Letting <span class="math inline">\(g_{ij} = \varphi&#39;&#39;_i \varphi&#39;&#39;_j\)</span> it holds that <span class="math inline">\(g_{ij}\)</span> is quadratic between two consecutive knots <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, in which case <span class="math display">\[\int_a^b g_{ij}(z) \mathrm{d}z = \frac{b - a}{6}\left(g_{ij}(a) + 4 g_{ij}\left(\frac{b-a}{2}\right) + g_{ij}(b)\right).\]</span> This identity is behind <a href="https://en.wikipedia.org/wiki/Simpson%27s_rule">Simpson’s rule</a> for numerical integration, and the fact that this is an identity for quadratic polynomials, and not an approximation, means that Simpson’s rule applied appropriately leads to exact computation of <span class="math inline">\(\mathbf{\Omega}_{ij}\)</span>. All we need is the ability to evaluate <span class="math inline">\(\varphi&#39;&#39;_i\)</span> at certain points, and <code>splineDesign</code> can be used for that.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pen_mat &lt;-<span class="st"> </span><span class="cf">function</span>(inner_knots) {
  knots &lt;-<span class="st"> </span><span class="kw">sort</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="kw">range</span>(inner_knots), <span class="dv">3</span>), inner_knots))
  d &lt;-<span class="st"> </span><span class="kw">diff</span>(inner_knots)  ## the vector of knot differences; b - a 
  g_ab &lt;-<span class="st"> </span><span class="kw">splineDesign</span>(knots, inner_knots, <span class="dt">derivs =</span> <span class="dv">2</span>) 
  knots_mid &lt;-<span class="st"> </span>inner_knots[<span class="op">-</span><span class="kw">length</span>(inner_knots)] <span class="op">+</span><span class="st"> </span>d <span class="op">/</span><span class="st"> </span><span class="dv">2</span>
  g_ab_mid &lt;-<span class="st"> </span><span class="kw">splineDesign</span>(knots, knots_mid, <span class="dt">derivs =</span> <span class="dv">2</span>)
  g_a &lt;-<span class="st"> </span>g_ab[<span class="op">-</span><span class="kw">nrow</span>(g_ab), ]
  g_b &lt;-<span class="st"> </span>g_ab[<span class="op">-</span><span class="dv">1</span>, ]
  (<span class="kw">crossprod</span>(d <span class="op">*</span><span class="st"> </span>g_a,  g_a) <span class="op">+</span><span class="st"> </span>
<span class="st">      </span><span class="dv">4</span> <span class="op">*</span><span class="st"> </span><span class="kw">crossprod</span>(d <span class="op">*</span><span class="st"> </span>g_ab_mid, g_ab_mid) <span class="op">+</span><span class="st"> </span>
<span class="st">      </span><span class="kw">crossprod</span>(d <span class="op">*</span><span class="st"> </span>g_b, g_b)) <span class="op">/</span><span class="st"> </span><span class="dv">6</span> 
}</code></pre></div>
<p>It is laborious to write good tests of <code>pen_mat</code>. We would have to work out a set of example matrices by other means, e.g. by hand. Alternatively, we can compare to a simpler numerical integration technique using Riemann sums.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tmp_deriv &lt;-<span class="st"> </span><span class="kw">splineDesign</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>), 
                          <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">1e-5</span>), <span class="dt">derivs =</span> <span class="dv">2</span>)
Omega_numeric &lt;-<span class="st"> </span><span class="kw">crossprod</span>(tmp_deriv[<span class="op">-</span><span class="dv">1</span>, ]) <span class="op">*</span><span class="st"> </span><span class="fl">1e-5</span>  ## Right Riemann sums
Omega &lt;-<span class="st"> </span><span class="kw">pen_mat</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">1</span>))
Omega_numeric <span class="op">/</span><span class="st"> </span>Omega</code></pre></div>
<pre><code>##           [,1]      [,2]     [,3]     [,4]     [,5]
## [1,] 0.9999700 0.9999673 0.999940 1.000000      NaN
## [2,] 0.9999673 0.9999663 0.999955 1.000000 1.000000
## [3,] 0.9999400 0.9999550 1.000000 1.000045 1.000060
## [4,] 1.0000000 1.0000000 1.000045 1.000034 1.000033
## [5,]       NaN 1.0000000 1.000060 1.000033 1.000030</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">range</span>((Omega_numeric <span class="op">-</span><span class="st"> </span>Omega) <span class="op">/</span><span class="st"> </span>(Omega <span class="op">+</span><span class="st"> </span><span class="fl">0.001</span>))  ## Relative error</code></pre></div>
<pre><code>## [1] -5.99967e-05  5.99983e-05</code></pre>
<p>And we should also test an example with non-equidistant knots.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tmp_deriv &lt;-<span class="st"> </span><span class="kw">splineDesign</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.65</span>, <span class="fl">0.7</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>), 
                          <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">1e-5</span>), <span class="dt">derivs =</span> <span class="dv">2</span>)
Omega_numeric &lt;-<span class="st"> </span><span class="kw">crossprod</span>(tmp_deriv[<span class="op">-</span><span class="dv">1</span>, ]) <span class="op">*</span><span class="st"> </span><span class="fl">1e-5</span>  ## Right Riemann sums
Omega &lt;-<span class="st"> </span><span class="kw">pen_mat</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.65</span>, <span class="fl">0.7</span>, <span class="dv">1</span>))
<span class="kw">range</span>((Omega_numeric <span class="op">-</span><span class="st"> </span>Omega) <span class="op">/</span><span class="st"> </span>(Omega <span class="op">+</span><span class="st"> </span><span class="fl">0.001</span>)) ## Relative error</code></pre></div>
<pre><code>## [1] -0.0001607084  0.0002545494</code></pre>
<p>These examples indicate that <code>pen_mat</code> computes <span class="math inline">\(\mathbf{\Omega}\)</span> correctly, in particular as increasing the Riemann sum precision by lowering the number <span class="math inline">\(10^{-5}\)</span> will decrease the relative error (not shown). Of course, correctness ultimately depends on <code>splineDesign</code> computing the correct second derivatives, which hasn’t been tested here.</p>
<p>We can also test how our implementation of smoothing splines works on data. We do this here by implementing the matrix-algebra directly for computing <span class="math inline">\(\mathbf{S}_{\lambda} \mathbf{y}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">inner_knots &lt;-<span class="st"> </span>Nuuk_year<span class="op">$</span>Year
Phi &lt;-<span class="st"> </span><span class="kw">splineDesign</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="kw">range</span>(inner_knots), <span class="dv">3</span>), inner_knots), inner_knots)
Omega &lt;-<span class="st"> </span><span class="kw">pen_mat</span>(inner_knots)
smoother &lt;-<span class="st"> </span><span class="cf">function</span>(lambda) 
  Phi <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(<span class="kw">crossprod</span>(Phi) <span class="op">+</span><span class="st"> </span>lambda <span class="op">*</span><span class="st"> </span>Omega, <span class="kw">t</span>(Phi) <span class="op">%*%</span><span class="st"> </span>Nuuk_year<span class="op">$</span>Temperature)

p_Nuuk <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">smoother</span>(<span class="fl">0.1</span>)), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span><span class="st">   </span>## Undersmooth
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">smoother</span>(<span class="dv">10</span>)), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span><span class="st">     </span>## Smooth
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">smoother</span>(<span class="dv">1000</span>)), <span class="dt">color =</span> <span class="st">&quot;purple&quot;</span>)  ## Oversmooth</code></pre></div>
<p><img src="CSwR_files/figure-html/Nuuk-smooth-spline-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Smoothing splines can be computed using the R function <code>smooth.spline</code> from the stats package. It uses generalized cross validation (GCV) by default for automatically choosing the tuning parameter <span class="math inline">\(\lambda\)</span>, but one can also manually specify the amount of smoothing using one of the arguments <code>lambda</code>, <code>spar</code> or <code>df</code> (the latter being the trace of the smoother matrix). Note, however, that due to internal differences from the <code>splineDesign</code> basis above, the <code>lambda</code> argument to <code>smooth.spline</code> does not match the <span class="math inline">\(\lambda\)</span> parameter above. To compare our results to <code>smooth.spline</code> we therefore optimize the GCV criterion. First we implement a function that computes GCV for a fixed value of <span class="math inline">\(\lambda\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gcv &lt;-<span class="st"> </span><span class="cf">function</span>(lambda, y) {
  S &lt;-<span class="st"> </span>Phi <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(<span class="kw">crossprod</span>(Phi) <span class="op">+</span><span class="st"> </span>lambda <span class="op">*</span><span class="st"> </span>Omega, <span class="kw">t</span>(Phi))
  df &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">diag</span>(S))  ## The trace of the smoother matrix
  <span class="kw">sum</span>(((y <span class="op">-</span><span class="st"> </span>S <span class="op">%*%</span><span class="st"> </span>y) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>df <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(y)))<span class="op">^</span><span class="dv">2</span>, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>) 
}</code></pre></div>
<p>Then we apply this function to a grid of <span class="math inline">\(\lambda\)</span>-values and choose the value of <span class="math inline">\(\lambda\)</span> that minimizes GCV.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lambda &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">50</span>, <span class="dv">250</span>, <span class="dv">2</span>)
GCV &lt;-<span class="st"> </span><span class="kw">sapply</span>(lambda, gcv, <span class="dt">y =</span> Nuuk_year<span class="op">$</span>Temperature)
lambda_opt &lt;-<span class="st"> </span>lambda[<span class="kw">which.min</span>(GCV)]
<span class="kw">qplot</span>(lambda, GCV) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> lambda_opt, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:Nuuk-spline-gcv"></span>
<img src="CSwR_files/figure-html/Nuuk-spline-gcv-1.png" alt="The generalized cross-validation criterion for smoothing splines as a function of the tuning parameter $\lambda$." width="70%" />
<p class="caption">
Figure 3.10: The generalized cross-validation criterion for smoothing splines as a function of the tuning parameter <span class="math inline">\(\lambda\)</span>.
</p>
</div>
<p>Finally, we can visualize the resulting smoothing spline.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">temp_smooth_opt &lt;-<span class="st"> </span>Phi <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(<span class="kw">crossprod</span>(Phi) <span class="op">+</span><span class="st"> </span>lambda_opt <span class="op">*</span><span class="st"> </span>Omega, 
                                 <span class="kw">t</span>(Phi) <span class="op">%*%</span><span class="st"> </span>Nuuk_year<span class="op">$</span>Temperature)
p_Nuuk <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> temp_smooth_opt), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:Nuuk-spline-opt"></span>
<img src="CSwR_files/figure-html/Nuuk-spline-opt-1.png" alt="The smoothing spline that minimizes GCV over the tuning parameter $\lambda$" width="70%" />
<p class="caption">
Figure 3.11: The smoothing spline that minimizes GCV over the tuning parameter <span class="math inline">\(\lambda\)</span>
</p>
</div>
<p>The smoothing spline that we found by minimizing GCV can be compared to the smoothing spline that <code>smooth.spline</code> computes by minimizing GCV as well.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">temp_smooth_splines &lt;-<span class="st"> </span><span class="kw">smooth.spline</span>(Nuuk_year<span class="op">$</span>Year, Nuuk_year<span class="op">$</span>Temperature, 
                                    <span class="dt">all.knots =</span> <span class="ot">TRUE</span>)  ## Don&#39;t use fast heuristic
<span class="kw">range</span>(temp_smooth_splines<span class="op">$</span>y <span class="op">-</span><span class="st"> </span>temp_smooth_opt)</code></pre></div>
<pre><code>## [1] -0.000775662  0.001072587</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p_Nuuk <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> temp_smooth_splines<span class="op">$</span>y), <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:Nuuk-spline-opt2"></span>
<img src="CSwR_files/figure-html/Nuuk-spline-opt2-1.png" alt="The smoothing spline that minimizes GCV as computed by `smooth.spline`." width="70%" />
<p class="caption">
Figure 3.12: The smoothing spline that minimizes GCV as computed by <code>smooth.spline</code>.
</p>
</div>
<p>The differences between the smoothing spline computed by our implementation and by <code>smooth.spline</code> is hardly detectable visually, and they are at most of the order <span class="math inline">\(10^{-3}\)</span> as computed above. It is possible to further decrease the differences by finding the optimal value of <span class="math inline">\(\lambda\)</span> with a higher precision, but we will not pursue this here.</p>
</div>
<div id="efficient-computation-with-splines" class="section level3">
<h3><span class="header-section-number">3.5.3</span> Efficient computation with splines</h3>
<p>Using of the full B-spline basis with knots in every observation is computationally heavy and from a practical viewpoint often unnecessary. Smoothing using B-splines is therefore often done using a knot-selection heuristic that selects much fewer knots than <span class="math inline">\(n\)</span>, in particular if <span class="math inline">\(n\)</span> is large. This is also what <code>smooth.spline</code> does unless <code>all.knots = TRUE</code>. The heuristic for selecting the number of knots is a bit complicated, but it is implemented in the function <code>.nknots.smspl</code>, which can be inspected for details. Once the number of knots gets above 200 it grows extremely slowly with <span class="math inline">\(n\)</span>. With the number of knots selected, a common heuristic for selecting their position is to use the quantiles of the distribution of the <span class="math inline">\(x\)</span>-values. That is, with 9 knots, say, the knots are positioned in the deciles (0.1-quantile, 0.2-quantile etc.). This is effectively also what <code>smooth.spline</code> does, and this heuristic places most of the knots where we have most of the data points.</p>
<p>Having implemented a knot-selection heuristic that results in <span class="math inline">\(p\)</span> B-spline basis functions, the matrix <span class="math inline">\(\Phi\)</span> will be <span class="math inline">\(n \times p\)</span>, typically with <span class="math inline">\(p &lt; n\)</span> and with <span class="math inline">\(\Phi\)</span> of full rank <span class="math inline">\(p\)</span>. In this case we derive a way of computing the smoothing spline that is computationally more efficient and numerically more stable than relying on the matrix-algebraic solution above. This is particularly so when we need to compute the smoother for many different <span class="math inline">\(\lambda\)</span>s to optimize the smoother. As we will show, we are effectively computing a simultaneous diagonalization of the (symmetric) smoother matrix <span class="math inline">\(\mathbf{S}_{\lambda}\)</span> for all values of <span class="math inline">\(\lambda\)</span>.</p>
<p>The matrix <span class="math inline">\(\Phi\)</span> has a singular value decomposition <span class="math display">\[\Phi = \mathbf{U} D \mathbf{V}^T\]</span> where <span class="math inline">\(D\)</span> is diagonal with entries <span class="math inline">\(d_1 \geq d_2 \geq \ldots \geq d_p &gt; 0\)</span>, <span class="math inline">\(\mathbf{U}\)</span> is <span class="math inline">\(n \times p\)</span>, <span class="math inline">\(\mathbf{V}\)</span> is <span class="math inline">\(p \times p\)</span> and both are orthogonal matrices. This means that <span class="math display">\[\mathbf{U}^T \mathbf{U} = \mathbf{V}^T \mathbf{V} = \mathbf{V} \mathbf{V}^T =  I\]</span> is the <span class="math inline">\(p \times p\)</span> dimensional identity matrix. We find that</p>
<span class="math display">\[\begin{align}
\mathbf{S}_{\lambda} &amp; =  \mathbf{U}D\mathbf{V}^T(\mathbf{V}D^2\mathbf{V}^T + \lambda \mathbf{\Omega})^{-1}
\mathbf{V}D\mathbf{U}^T \\
&amp; = \mathbf{U}D (D^2 + \lambda  \mathbf{V}^T \mathbf{\Omega} \mathbf{V})^{-1} D \mathbf{U}^T \\
&amp; = \mathbf{U} (I + \lambda D^{-1} \mathbf{V}^T \mathbf{\Omega} \mathbf{V} D^{-1})^{-1} \mathbf{U}^T \\
&amp; = \mathbf{U}(I + \lambda  \widetilde{\mathbf{\Omega}})^{-1} \mathbf{U}^T,
\end{align}\]</span>
<p>where <span class="math inline">\(\widetilde{\mathbf{\Omega}} = D^{-1} \mathbf{V}^T \mathbf{\Omega} \mathbf{V} D^{-1}\)</span> is a positive semidefinite <span class="math inline">\(p \times p\)</span> matrix. By diagonalization, <span class="math display">\[\widetilde{\mathbf{\Omega}} = \mathbf{W} \Gamma \mathbf{W}^T,\]</span> where <span class="math inline">\(\mathbf{W}\)</span> is orthogonal and <span class="math inline">\(\Gamma\)</span> is a diagonal matrix with nonnegative values in the diagonal, we find that</p>
<span class="math display">\[\begin{align}
\mathbf{S}_{\lambda} &amp; = \mathbf{U} \mathbf{W} (I + \lambda  \Gamma)^{-1}  \mathbf{W}^T  \mathbf{U}^T \\
&amp; = \widetilde{\mathbf{U}}  (I + \lambda  \Gamma)^{-1} \widetilde{\mathbf{U}}^T
\end{align}\]</span>
<p>where <span class="math inline">\(\widetilde{\mathbf{U}} = \mathbf{U} \mathbf{W}\)</span> is an orthogonal <span class="math inline">\(n \times p\)</span> matrix.</p>
<p>The interpretation of this representation is as follows.</p>
<ul>
<li>First, the coefficients, <span class="math inline">\(\hat{\beta} = \widetilde{\mathbf{U}}^Ty\)</span>, are computed for expanding <span class="math inline">\(y\)</span> in the basis given by the columns of <span class="math inline">\(\mathbf{U}\)</span>.</li>
<li>Second, the <span class="math inline">\(i\)</span>th coefficient is shrunk towards 0, <span class="math display">\[\hat{\beta}_i(\lambda) = \frac{\hat{\beta}_i}{1 + \lambda \gamma_i}.\]</span></li>
<li>Third, the smoothed values, <span class="math inline">\(\widetilde{\mathbf{U}} \hat{\beta}(\lambda)\)</span>, are computed as an expansion using the shrunken coefficients.</li>
</ul>
<p>Thus the smoother works by shrinking the coefficients in the orthonormal basis given by <span class="math inline">\(\widetilde{\mathbf{U}}\)</span> toward zero. The coefficients corresponding to the largest eigenvalues <span class="math inline">\(\gamma_i\)</span> are shrunk relatively more toward zero than those corresponding to the small eigenvalues.</p>
<p>We implement the computation of the diagonalization for the Nuuk temperature data using <span class="math inline">\(p = 20\)</span> basis functions (18 inner knots) equidistantly distributed over the range of the years for which we have data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">inner_knots &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1867</span>, <span class="dv">2013</span>, <span class="dt">length.out =</span> <span class="dv">18</span>)
Phi &lt;-<span class="st"> </span><span class="kw">splineDesign</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="kw">range</span>(inner_knots), <span class="dv">3</span>), inner_knots), Nuuk_year<span class="op">$</span>Year)
Omega &lt;-<span class="st"> </span><span class="kw">pen_mat</span>(inner_knots)
Phi_svd &lt;-<span class="st"> </span><span class="kw">svd</span>(Phi)
Omega_tilde &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">t</span>(<span class="kw">crossprod</span>(Phi_svd<span class="op">$</span>v, Omega <span class="op">%*%</span><span class="st"> </span>Phi_svd<span class="op">$</span>v)) <span class="op">/</span><span class="st"> </span>Phi_svd<span class="op">$</span>d) <span class="op">/</span><span class="st"> </span>Phi_svd<span class="op">$</span>d
## It is safer to use the numerical singular value decomposition (&#39;svd&#39;)
## for diagonalizing a positive semidefinite matrix than to use a 
## more general numerical diagonalization implementation such as &#39;eigen&#39;. 
Omega_tilde_svd &lt;-<span class="st"> </span><span class="kw">svd</span>(Omega_tilde)  
U_tilde &lt;-<span class="st"> </span>Phi_svd<span class="op">$</span>u <span class="op">%*%</span><span class="st"> </span>Omega_tilde_svd<span class="op">$</span>u</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:spline-gamma"></span>
<img src="CSwR_files/figure-html/spline-gamma-1.png" alt="The eigenvalues $\gamma_i$ that determine how much the different basis coefficients in the orthonormal spline expansion are shrunk toward zero. Left plot shows the eigenvalues untransformed, while the right plot shows the eigenvalues log-transformed." width="49%" /><img src="CSwR_files/figure-html/spline-gamma-2.png" alt="The eigenvalues $\gamma_i$ that determine how much the different basis coefficients in the orthonormal spline expansion are shrunk toward zero. Left plot shows the eigenvalues untransformed, while the right plot shows the eigenvalues log-transformed." width="49%" />
<p class="caption">
Figure 3.13: The eigenvalues <span class="math inline">\(\gamma_i\)</span> that determine how much the different basis coefficients in the orthonormal spline expansion are shrunk toward zero. Left plot shows the eigenvalues untransformed, while the right plot shows the eigenvalues log-transformed.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:spline-basis"></span>
<img src="CSwR_files/figure-html/spline-basis-1.png" alt="The columns of $\widetilde{\mathbf{U}}$  that consitute an orthonormal basis for computing the spline based smoother." width="100%" />
<p class="caption">
Figure 3.14: The columns of <span class="math inline">\(\widetilde{\mathbf{U}}\)</span> that consitute an orthonormal basis for computing the spline based smoother.
</p>
</div>
<p>We observe from Figures <a href="splines.html#fig:spline-gamma">3.13</a> and <a href="splines.html#fig:spline-basis">3.14</a> that there are two relatively large eigenvalues corresponding to the two basis functions with erratic behavior close to the boundaries, and there are two eigenvalues that are effectively zero corresponding to the two affine basis functions. In addition, the more oscillating the basis function is, the larger is the corresponding eigenvalue, and the more is the corresponding coefficient shrunk toward zero by the spline smoother.</p>
<p>Observe also that <span class="math display">\[\mathrm{df}(\lambda) = \mathrm{trace}(\mathbf{S}_\lambda) 
= \sum_{i=1}^n \frac{1}{1 + \lambda \gamma_i},\]</span> which makes it possible to implement GCV without even computing the diagonal entries of <span class="math inline">\(\mathbf{S}_{\lambda}.\)</span></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="onb.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="gaussian-processes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

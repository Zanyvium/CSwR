<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 2 Density estimation | Computational Statistics with R</title>
<meta name="author" content="Niels Richard Hansen">
<meta name="description" content="This chapter is on nonparametric density estimation. A classical nonparametric estimator of a density is the histogram, which provides discontinuous and piecewise constant estimates. The focus in...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 2 Density estimation | Computational Statistics with R">
<meta property="og:type" content="book">
<meta property="og:description" content="This chapter is on nonparametric density estimation. A classical nonparametric estimator of a density is the histogram, which provides discontinuous and piecewise constant estimates. The focus in...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 2 Density estimation | Computational Statistics with R">
<meta name="twitter:description" content="This chapter is on nonparametric density estimation. A classical nonparametric estimator of a density is the histogram, which provides discontinuous and piecewise constant estimates. The focus in...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/_Roboto%20Slab-0.4.0/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.3.0/transition.js"></script><script src="libs/bs3compat-0.3.0/tabs.js"></script><script src="libs/bs3compat-0.3.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Computational Statistics with R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li class="book-part">Part I: Smoothing</li>
<li><a class="active" href="density.html"><span class="header-section-number">2</span> Density estimation</a></li>
<li><a class="" href="bivariate.html"><span class="header-section-number">3</span> Bivariate smoothing</a></li>
<li class="book-part">Part II: Monte Carlo Methods</li>
<li><a class="" href="univariate-random-variables.html"><span class="header-section-number">4</span> Univariate random variables</a></li>
<li><a class="" href="mci.html"><span class="header-section-number">5</span> Monte Carlo integration</a></li>
<li class="book-part">Part III: Optimization</li>
<li><a class="" href="four-examples.html"><span class="header-section-number">6</span> Four Examples</a></li>
<li><a class="" href="numopt.html"><span class="header-section-number">7</span> Numerical optimization</a></li>
<li><a class="" href="em.html"><span class="header-section-number">8</span> Expectation maximization algorithms</a></li>
<li><a class="" href="StochOpt.html"><span class="header-section-number">9</span> Stochastic Optimization</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="app-R.html"><span class="header-section-number">A</span> R programming</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/nielsrhansen/CSwR">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="density" class="section level1" number="2">
<h1>
<span class="header-section-number">2</span> Density estimation<a class="anchor" aria-label="anchor" href="#density"><i class="fas fa-link"></i></a>
</h1>
<p>This chapter is on nonparametric density estimation. A classical
nonparametric estimator of a density is the histogram, which provides
discontinuous and piecewise constant estimates. The focus in this chapter is on
some of the alternatives that provide continuous or even smooth estimates
instead.</p>
<p><em>Kernel methods</em> form an important class of smooth density estimators as
implemented by the R function <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code>. These estimators are essentially
just locally weighted averages, and their computation is relatively
straightforward in theory. In practice, different choices of how to implement
the computations can, however, have a big effect on the actual computation time,
and the implementation of kernel density estimators will illustrate three points:</p>
<ul>
<li>if possible, choose vectorized implementations in R,</li>
<li>if a small loss in accuracy is acceptable, an approximate solution
can be orders of magnitudes faster than a literal implementation,</li>
<li>the time it takes to numerically evaluate different
<a href="https://en.wikipedia.org/wiki/Elementary_function">elementary functions</a>
can depend a lot on the function and how you implement the computation.</li>
</ul>
<p>The first point is emphasized because it results in implementations that
are short, expressive and easier to understand just as much as it typically results
in computationally more efficient implementations. Note also that not every
computation can be vectorized in a beneficial way, and one should never go
through hoops to vectorize a computation.</p>
<p>Kernel methods rely on one or more <em>regularization parameters</em> that must be
selected to achieve the right balance of adapting
to data without adapting too much to the random variation in the data.
Choosing the right amount of regularization is just as important as choosing
the method to use in the first place. It may, in fact, be more important.
We actually do not have a complete implementation of a nonparametric estimator
until we have implemented a data driven and automatic way of choosing the
amount of regularization. Implementing only the computations for
evaluating a kernel estimator, say, and leaving it completely
to the user to choose the bandwidth is a job half done. Methods and implementations
for choosing the bandwidth are therefore treated in some detail in this chapter.</p>
<p>In the final section a likelihood analysis is carried out.
This is done to further clarify why regularized estimators are needed
to avoid overfitting to the data, and why there is in general no nonparametric
maximum likelihood estimator of a density. Regularization of the likelihood
can be achieved by constraining the density estimates to belong to a family
of increasingly flexible parametric densities that are fitted to data. This is
known as the <em>method of sieves</em>. Another approach is based on basis expansions,
but in either case, automatic selection of the amount regularization is just as
important as for kernel methods.</p>
<div id="unidens" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> Univariate density estimation<a class="anchor" aria-label="anchor" href="#unidens"><i class="fas fa-link"></i></a>
</h2>
<p>Recall the data on <a href="intro.html#intro-angles"><span class="math inline">\(\phi\)</span>- and <span class="math inline">\(\psi\)</span>-angles</a> in
polypeptide backbone structures, as considered in Section <a href="intro.html#intro-angles">1.1.1</a>.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:phipsiDens2"></span>
<img src="CSwR_files/figure-html/phipsiDens2-1.png" alt="Histograms equipped with a rug plot of the distribution of \(\phi\)-angles (left) and \(\psi\)-angles (right) of the peptide planes in the protein human protein 1HMP." width="49%"><img src="CSwR_files/figure-html/phipsiDens2-2.png" alt="Histograms equipped with a rug plot of the distribution of \(\phi\)-angles (left) and \(\psi\)-angles (right) of the peptide planes in the protein human protein 1HMP." width="49%"><p class="caption">
Figure 2.1: Histograms equipped with a rug plot of the distribution of <span class="math inline">\(\phi\)</span>-angles (left) and <span class="math inline">\(\psi\)</span>-angles (right) of the peptide planes in the protein human protein <a href="https://www.rcsb.org/structure/1HMP">1HMP</a>.
</p>
</div>
<p>We will in this section treat methods for smooth density
estimation for univariate data such as data on either the <span class="math inline">\(\phi\)</span>- or the
<span class="math inline">\(\psi\)</span>-angle.</p>
<p>We let <span class="math inline">\(f_0\)</span> denote the unknown density that we want to estimate.
That is, we imagine that the data points <span class="math inline">\(x_1, \ldots, x_n\)</span> are
all observations drawn from the probability measure with
density <span class="math inline">\(f_0\)</span> w.r.t. Lebesgue measure on <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p>Suppose first that <span class="math inline">\(f_0\)</span> belongs to a parametrized statistical model
<span class="math inline">\((f_{\theta})_{\theta}\)</span>, where <span class="math inline">\(f_{\theta}\)</span> is a density w.r.t.
Lebesgue measure on <span class="math inline">\(\mathbb{R}\)</span>. If <span class="math inline">\(\hat{\theta}\)</span> is an estimate of
the parameter, <span class="math inline">\(f_{\hat{\theta}}\)</span> is an estimate of the unknown
density <span class="math inline">\(f_0\)</span>. For a parametric family we can always try to use the MLE
<span class="math display">\[\hat{\theta} = \text{arg max}_{\theta} \sum_{j=1}^n \log f_{\theta}(x_j)\]</span>
as an estimate of <span class="math inline">\(\theta\)</span>. Likewise, we might compute the empirical mean and variance
for the data and plug those numbers into the density for the Gaussian
distribution, and in this way obtain a Gaussian density estimate of <span class="math inline">\(f_0\)</span>.</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">psi_mean</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">psi</span><span class="op">)</span>
<span class="va">psi_sd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">psi</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">psi</span>, prob <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/rug.html">rug</a></span><span class="op">(</span><span class="va">psi</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">psi_mean</span>, <span class="va">psi_sd</span><span class="op">)</span>, add <span class="op">=</span> <span class="cn">TRUE</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:psi-gauss"></span>
<img src="CSwR_files/figure-html/psi-gauss-1.png" alt="Gaussian density (red) fitted to the $\psi$-angles." width="70%"><p class="caption">
Figure 2.2: Gaussian density (red) fitted to the <span class="math inline">\(\psi\)</span>-angles.
</p>
</div>
<p>As Figure <a href="density.html#fig:psi-gauss">2.2</a> shows, if we fit a Gaussian distribution to the <span class="math inline">\(\psi\)</span>-angle
data we get a density estimate that clearly does not match the
histogram. The Gaussian density matches the data on the first and second moments,
but the histogram shows a clear bimodality that the Gaussian distribution by
definition cannot match. Thus we need a more flexible parametric model than
the Gaussian if we want to fit a density to this data set.</p>
<p>In nonparametric density estimating we want to estimate the target density, <span class="math inline">\(f_0\)</span>,
without assuming that it belongs to a particular parametrized family of densities.</p>
</div>
<div id="kernel-density" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Kernel methods<a class="anchor" aria-label="anchor" href="#kernel-density"><i class="fas fa-link"></i></a>
</h2>
<p>A simple approach to nonparametric density estimation relies on the approximation
<span class="math display">\[P(X \in (x-h, x+h)) = \int_{x-h}^{x+h} f_0(z) \ dz \simeq f_0(x) 2h,\]</span>
which is valid for any continuous density <span class="math inline">\(f_0\)</span>. Inverting this approximation
and using the law of large numbers,</p>
<p><span class="math display">\[\begin{align*}
f_0(x) &amp; \simeq \frac{1}{2h}P(X \in (x-h, x+h)) \\
&amp; \simeq \frac{1}{2hn} \sum_{j=1}^n 1_{(x-h, x+h)}(x_j) \\
&amp; =  \underbrace{\frac{1}{2hn} \sum_{j=1}^n 1_{(-h, h)}(x - x_j)}_{\hat{f}_h(x)}
\end{align*}\]</span></p>
<p>for i.i.d. observations <span class="math inline">\(x_1, \ldots, x_n\)</span> from the distribution <span class="math inline">\(f_0 \cdot m\)</span>.
The function <span class="math inline">\(\hat{f}_h\)</span> defined as above is an example of a kernel
density estimator with a rectangular kernel. We immediately note that <span class="math inline">\(h\)</span> has
to be chosen appropriately. If <span class="math inline">\(h\)</span> is large, <span class="math inline">\(\hat{f}_h\)</span> will be flat and close to
a constant. If <span class="math inline">\(h\)</span> is small, <span class="math inline">\(\hat{f}_h\)</span> will make large jumps close to the observations.</p>
<p>What do we then mean by an “appropriate” choice of <span class="math inline">\(h\)</span> above? To answer this we
must have some prior assumptions about what we expect
<span class="math inline">\(f_0\)</span> to look like. Typically, we expect <span class="math inline">\(f_0\)</span> to have few oscillations and to be
fairly smooth, and we want <span class="math inline">\(\hat{f}_h\)</span> to reflect that. A too large <span class="math inline">\(h\)</span> will oversmooth
the data relative to <span class="math inline">\(f_0\)</span> by effectively ignoring the data, while a too small <span class="math inline">\(h\)</span> will undersmooth
the data relative to <span class="math inline">\(f_0\)</span> by allowing individual data points to have large local effects that
make the estimate wiggly. More formally, we can look at the mean and variance
of <span class="math inline">\(\hat{f}_h\)</span>. Letting <span class="math inline">\(p(x, h) = P(X \in (x-h, x+h))\)</span>, it follows that
<span class="math inline">\(f_h(x) = E(\hat{f}_h(x)) = p(x, h) / (2h)\)</span> while</p>
<p><span class="math display" id="eq:varRect">\[\begin{equation} 
V(\hat{f}_h(x)) = \frac{p(x, h) (1 - p(x, h))}{4h^2 n} \simeq f_h(x) \frac{1}{2hn}.
\tag{2.1}
\end{equation}\]</span></p>
<p>We see from these computations that for <span class="math inline">\(\hat{f}_h(x)\)</span> to be approximately unbiased for any <span class="math inline">\(x\)</span>
we need <span class="math inline">\(h\)</span> to be small – ideally letting <span class="math inline">\(h \to 0\)</span> since then <span class="math inline">\(f_h(x) \to f_0(x)\)</span>.
However, this will make the variance blow up, and to minimize variance we should
instead choose <span class="math inline">\(h\)</span> as large as possible. One way to define “appropriate” is
then to strike a balance between the bias and the variance as a function
of <span class="math inline">\(h\)</span> so as to minimize the mean squared error of <span class="math inline">\(\hat{f}_h(x)\)</span>.</p>
<p>We will find the optimal tradeoff for the rectangular kernel in Section <a href="density.html#bandwidth">2.3</a>
on <a href="density.html#bandwidth">bandwidth selection</a>. It’s not difficult, and you are encouraged
to try finding it yourself at this point. In this section we will focus on
computational aspects of kernel density estimation, but first we will generalize
the estimator by allowing for other kernels.</p>
<p>The estimate <span class="math inline">\(\hat{f}_h(x)\)</span> will be unbiased if <span class="math inline">\(f_0\)</span> is constantly equal to <span class="math inline">\(f_0(x)\)</span>
in the entire interval <span class="math inline">\((x-h, x+h)\)</span>. This is atypical and can only happen for all <span class="math inline">\(x\)</span>
if <span class="math inline">\(f_0\)</span> is constant. We expect the typical situation to be that
<span class="math inline">\(f_0\)</span> deviates the most from <span class="math inline">\(f_0(x)\)</span> close to <span class="math inline">\(x \pm h\)</span>, and
that this causes a bias of <span class="math inline">\(\hat{f}_h(x).\)</span>
Observations falling close to <span class="math inline">\(x + h\)</span>, say, should thus count less
than observations falling close to <span class="math inline">\(x\)</span>? The rectangular kernel makes a sharp
cut; either a data point is in or it is out. If we use a smooth weighting
function instead of a sharp cut, we might be able to include more
data points and lower the variance while keeping the bias small. This is
precisely the idea of <em>kernel estimators</em>, defined generally as</p>
<p><span class="math display" id="eq:kernel-def">\[\begin{equation}
\hat{f}_h(x) = \frac{1}{hn} \sum_{j=1}^n K\left(\frac{x - x_j}{h}\right)
\tag{2.2}
\end{equation}\]</span></p>
<p>for a kernel <span class="math inline">\(K : \mathbb{R} \to \mathbb{R}\)</span>. The parameter <span class="math inline">\(h &gt; 0\)</span> is known
as the <em>bandwidth</em>. Examples of kernels include the <em>uniform</em> or <em>rectangular kernel</em>
<span class="math display">\[K(x) = \frac{1}{2} 1_{(-1,1)}(x),\]</span>
and the <em>Gaussian kernel</em>
<span class="math display">\[K(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}.\]</span></p>
<p>One direct benefit of considering other kernels than the rectangular is that
<span class="math inline">\(\hat{f}_h\)</span> inherits all smoothness properties from <span class="math inline">\(K\)</span>. Whereas the rectangular
kernel is not even continuous, the Gaussian kernel is <span class="math inline">\(C^{\infty}\)</span> and so is
the resulting kernel density estimate.</p>
<div id="dens-implement" class="section level3" number="2.2.1">
<h3>
<span class="header-section-number">2.2.1</span> Implementation<a class="anchor" aria-label="anchor" href="#dens-implement"><i class="fas fa-link"></i></a>
</h3>
<p>What should be computed to compute a kernel density estimate? That is, in fact,
a good question, because the definition actually just specifies how to evaluate
<span class="math inline">\(\hat{f}_h\)</span> in any given point <span class="math inline">\(x\)</span>, but there is really not anything to compute
until we need to evaluate <span class="math inline">\(\hat{f}_h\)</span>. Thus when we implement kernel density
estimation we really implement algorithms for evaluating a density estimate
in a finite number of points.</p>
<p>Our first implementation is a fairly low-level implementation that returns
the evaluation of the density estimate in a given number of equidistant
points. The function mimics some of the defaults of <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code> so that it
actually evaluates the estimate in the same points as <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code>.</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># This is an implementation of the function 'kern_dens' that computes </span>
<span class="co"># evaluations of Gaussian kernel density estimates in a grid of points.</span>
<span class="co">#</span>
<span class="co"># The function has three formal arguments: 'x' is the numeric vector of data </span>
<span class="co"># points, 'h' is the bandwidth and 'm' is the number of grid points. </span>
<span class="co"># The default value of 512 is chosen to match the default of 'density()'. </span>
<span class="va">kern_dens</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">h</span>, <span class="va">m</span> <span class="op">=</span> <span class="fl">512</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">rg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
  <span class="co"># xx is equivalent to grid points in 'density()'</span>
  <span class="va">xx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="va">rg</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="fl">3</span> <span class="op">*</span> <span class="va">h</span>, <span class="va">rg</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="fl">3</span> <span class="op">*</span> <span class="va">h</span>, length.out <span class="op">=</span> <span class="va">m</span><span class="op">)</span>
  <span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span> <span class="co"># The evaluations, initialized as a vector of zeros</span>
  <span class="co"># The actual computation is done using nested for-loops. The outer loop</span>
  <span class="co"># is over the grid points, and the inner loop is over the data points.</span>
  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span><span class="op">(</span><span class="va">xx</span><span class="op">)</span><span class="op">)</span>
    <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span>
      <span class="va">y</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">y</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span> <span class="op">(</span><span class="va">xx</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="va">j</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">h</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span>  
  <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">y</span> <span class="op">/</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span><span class="op">)</span> <span class="op">*</span> <span class="va">h</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span>
  <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">xx</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>Note that the function returns a list containing the grid points (<code>x</code>) where
the density estimate is evaluated as well as the estimated density
evaluations (<code>y</code>). Note also that the argument <code>m</code> above sets the number of
grid points, whereas <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code> uses the argument <code>n</code> for that. The
latter can be a bit confusing as <span class="math inline">\(n\)</span> is often used to denote the
number of data points.</p>
<p>We will immediately test if the implementation works as expected – in this
case by comparing it to our reference implementation <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code>.</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">f_hat</span> <span class="op">&lt;-</span> <span class="fu">kern_dens</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span>
<span class="va">f_hat_dens</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">f_hat</span>, type <span class="op">=</span> <span class="st">"l"</span>, lwd <span class="op">=</span> <span class="fl">4</span>, xlab <span class="op">=</span> <span class="st">"x"</span>, ylab <span class="op">=</span> <span class="st">"Density"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">f_hat_dens</span>, col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">f_hat</span><span class="op">$</span><span class="va">x</span>, <span class="va">f_hat</span><span class="op">$</span><span class="va">y</span> <span class="op">-</span> <span class="va">f_hat_dens</span><span class="op">$</span><span class="va">y</span>, 
  type <span class="op">=</span> <span class="st">"l"</span>, 
  lwd <span class="op">=</span> <span class="fl">2</span>,
  xlab <span class="op">=</span> <span class="st">"x"</span>, 
  ylab <span class="op">=</span> <span class="st">"Difference"</span>
<span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:kern-dens-fig"></span>
<img src="CSwR_files/figure-html/kern-dens-fig-1.png" alt="Kernel density estimates with the Gaussian kernel (left) using R's implementation (black) and our implementation (red) together with differences of the estimates (right)." width="49%"><img src="CSwR_files/figure-html/kern-dens-fig-2.png" alt="Kernel density estimates with the Gaussian kernel (left) using R's implementation (black) and our implementation (red) together with differences of the estimates (right)." width="49%"><p class="caption">
Figure 2.3: Kernel density estimates with the Gaussian kernel (left) using R’s implementation (black) and our implementation (red) together with differences of the estimates (right).
</p>
</div>
<p>Figure <a href="density.html#fig:kern-dens-fig">2.3</a> suggests that the estimates computed by our
implementation and by <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code> are the same when we just visually compare the
plotted densities. However, if we look at the differences instead,
we see that they are as large as <span class="math inline">\(4 \times 10^{-4}\)</span> in absolute value. This is
way above what we should expect from rounding errors alone when using
double precision arithmetic. Thus the two implementations only compute
<em>approximately</em> the same, which is, in fact, because <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code> relies
on certain approximations for run time efficiency.</p>
<p>In R we can often beneficially implement computations in a vectorized way
instead of using an explicit loop. It is fairly easy to change the implementation
to be more vectorized by computing each evaluation in one single line using
the <code><a href="https://rdrr.io/r/base/sum.html">sum()</a></code> function and the fact that <code><a href="https://rdrr.io/r/base/Log.html">exp()</a></code> and squaring are vectorized.</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">kern_dens_vec</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">h</span>, <span class="va">m</span> <span class="op">=</span> <span class="fl">512</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">rg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
  <span class="va">xx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="va">rg</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="fl">3</span> <span class="op">*</span> <span class="va">h</span>, <span class="va">rg</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="fl">3</span> <span class="op">*</span> <span class="va">h</span>, length.out <span class="op">=</span> <span class="va">m</span><span class="op">)</span>
  <span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span> 
  <span class="co"># The inner loop from 'kern_dens' has been vectorized, and only the </span>
  <span class="co"># outer loop over the grid points remains. </span>
  <span class="va">const</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span><span class="op">)</span> <span class="op">*</span> <span class="va">h</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span>
  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span><span class="op">(</span><span class="va">xx</span><span class="op">)</span><span class="op">)</span>
      <span class="va">y</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">xx</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">-</span> <span class="va">x</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">h</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="va">const</span>
  <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">xx</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>We test this new implementation by comparing it to our previous
implementation.</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="fu">kern_dens</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">$</span><span class="va">y</span> <span class="op">-</span> <span class="fu">kern_dens_vec</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">$</span><span class="va">y</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] -5.551115e-16  3.885781e-16</code></pre>
<p>The magnitude of the differences are of order at most <span class="math inline">\(10^{-16}\)</span>, which is what
can be expected due to rounding errors. Thus we conclude that up to rounding
errors, <code>kern_dens()</code> and <code>kern_dens_vec()</code> return the same on this data set. This is,
of course, not a comprehensive test, but it is an example of one among a
number of tests that should be considered.</p>
<p>There are several ways to get completely rid of the explicit loops and write
an entirely vectorized implementation in R. One of the solutions will use the
<code><a href="https://rdrr.io/r/base/lapply.html">sapply()</a></code> function, which belongs to the <a href="http://adv-r.had.co.nz/Functionals.html#functionals-loop">family of <code>*apply()</code> functions</a>
that apply a function to each element in a vector or a list. In the
parlance of functional programming the <code>*apply()</code> functions are variations
of the functional, or higher-order-function, known as <a href="https://en.wikipedia.org/wiki/Map_(higher-order_function)">map</a>.</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">kern_dens_apply</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">h</span>, <span class="va">m</span> <span class="op">=</span> <span class="fl">512</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">rg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
  <span class="va">xx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="va">rg</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="fl">3</span> <span class="op">*</span> <span class="va">h</span>, <span class="va">rg</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="fl">3</span> <span class="op">*</span> <span class="va">h</span>, length.out <span class="op">=</span> <span class="va">m</span><span class="op">)</span>
  <span class="va">const</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span><span class="op">)</span> <span class="op">*</span> <span class="va">h</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
  <span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">xx</span>, <span class="kw">function</span><span class="op">(</span><span class="va">z</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">z</span> <span class="op">-</span> <span class="va">x</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">h</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="va">const</span><span class="op">)</span>
  <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">xx</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>The <code><a href="https://rdrr.io/r/base/lapply.html">sapply()</a></code> call above will apply the function <code>function(z) sum(dnorm(...</code>
to every element in the vector <code>xx</code> and return the result as a vector. The
function is an example of an <em>anonymous function</em> that does not get a name and
exists only during the <code><a href="https://rdrr.io/r/base/lapply.html">sapply()</a></code> evaluation. Instead of <code><a href="https://rdrr.io/r/base/lapply.html">sapply()</a></code> it is possible
to use <code><a href="https://rdrr.io/r/base/lapply.html">lapply()</a></code> that returns a list. In fact, <code><a href="https://rdrr.io/r/base/lapply.html">sapply()</a></code> is a
simple wrapper around <code><a href="https://rdrr.io/r/base/lapply.html">lapply()</a></code> that attempts to “simplify” the result from
a list to an array (and in this case to a vector).</p>
<p>An alternative, and also completely vectorized, solution can be based on
the functions <code><a href="https://rdrr.io/r/base/outer.html">outer()</a></code> and <code><a href="https://rdrr.io/r/base/colSums.html">rowMeans()</a></code>.</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">kern_dens_outer</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">h</span>, <span class="va">m</span> <span class="op">=</span> <span class="fl">512</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">rg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
  <span class="va">xx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="va">rg</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">-</span> <span class="fl">3</span> <span class="op">*</span> <span class="va">h</span>, <span class="va">rg</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="fl">3</span> <span class="op">*</span> <span class="va">h</span>, length.out <span class="op">=</span> <span class="va">m</span><span class="op">)</span>
  <span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/outer.html">outer</a></span><span class="op">(</span><span class="va">xx</span>, <span class="va">x</span>, <span class="kw">function</span><span class="op">(</span><span class="va">zz</span>, <span class="va">z</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="op">-</span><span class="op">(</span><span class="va">zz</span> <span class="op">-</span> <span class="va">z</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">h</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
  <span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">rowMeans</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span><span class="op">)</span> <span class="op">*</span> <span class="va">h</span><span class="op">)</span>
  <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">xx</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>The <code><a href="https://rdrr.io/r/base/outer.html">outer()</a></code> function evaluates the kernel in all combinations
of the grid and data points and returns a matrix of
dimensions <span class="math inline">\(m \times n\)</span>. The function <code><a href="https://rdrr.io/r/base/colSums.html">rowMeans()</a></code> computes
the means of each row and returns a vector of length <span class="math inline">\(m\)</span>.</p>
<p>We should, of course, also remember to test these two last implementations.</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="fu">kern_dens</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">$</span><span class="va">y</span> <span class="op">-</span> <span class="fu">kern_dens_apply</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">$</span><span class="va">y</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] -5.551115e-16  3.885781e-16</code></pre>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="fu">kern_dens</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">$</span><span class="va">y</span> <span class="op">-</span> <span class="fu">kern_dens_outer</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">$</span><span class="va">y</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] -4.996004e-16  3.885781e-16</code></pre>
<p>The natural question is then how to choose between the different implementations?
Besides being correct it is important that the code is easy to read
and understand. Which of the four implementations above that is best in
this respect may depend a lot on the background of the reader. If you strip
the implementations for comments, all four are arguably quite readable, but
<code>kern_dens()</code> with the double loop might appeal a bit more to people
with a background in imperative programming, while <code>kern_dens_apply()</code> might
appeal more to people with a preference for functional programming.
This functional and vectorized solution is also
a bit closer to the mathematical notation with
e.g. the sum sign <span class="math inline">\(\Sigma\)</span> being mapped directly to the <code><a href="https://rdrr.io/r/base/sum.html">sum()</a></code> function
instead of the incremental addition in the for-loop. For
these specific implementations these differences are mostly
aesthetic nuances and preferences may be more subjective than substantial.</p>
<p>To make a qualified choice between the implementations we should
investigate if they differ in terms of run time and memory consumption.</p>
</div>
<div id="benchmarking" class="section level3" number="2.2.2">
<h3>
<span class="header-section-number">2.2.2</span> Benchmarking<a class="anchor" aria-label="anchor" href="#benchmarking"><i class="fas fa-link"></i></a>
</h3>
<p>Benchmarking is about measuring and comparing performance. For software
this often means measuring run time and memory usage, though there are clearly
many other aspects of software that should be benchmarked in general. This
includes user experience, energy consumption and implementation and maintenance
time. In this section we focus on benchmarking run time.</p>
<p>The function <code>system.time</code> in R provides a simple way of benchmarking run time
measured in seconds.</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/system.time.html">system.time</a></span><span class="op">(</span><span class="fu">kern_dens</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/system.time.html">system.time</a></span><span class="op">(</span><span class="fu">kern_dens_vec</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/system.time.html">system.time</a></span><span class="op">(</span><span class="fu">kern_dens_apply</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/system.time.html">system.time</a></span><span class="op">(</span><span class="fu">kern_dens_outer</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>## kern_dens:
##    user  system elapsed 
##   0.044   0.033   0.082 
## kern_dens_vec:
##    user  system elapsed 
##   0.004   0.000   0.005 
## kern_dens_apply:
##    user  system elapsed 
##   0.005   0.001   0.006 
## kern_dens_outer:
##    user  system elapsed 
##   0.005   0.002   0.007</code></pre>
<p>The “elapsed” time is the total run time as experienced, while the “user” and
“system” times are how long the CPU spent on executing your code and
operating system code on behalf of your code, respectively.</p>
<p>From this simple benchmark, <code>kern_dens()</code> is clearly substantially slower
than the three other implementations. For more systematic benchmarking of run
time, the R package microbenchmark is useful.</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/joshuaulrich/microbenchmark/">microbenchmark</a></span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">kern_bench</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html">microbenchmark</a></span><span class="op">(</span>
  <span class="fu">kern_dens</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span>,
  <span class="fu">kern_dens_vec</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span>,
  <span class="fu">kern_dens_apply</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span>,
  <span class="fu">kern_dens_outer</span><span class="op">(</span><span class="va">psi</span>, <span class="fl">0.2</span><span class="op">)</span>
<span class="op">)</span></code></pre></div>
<p>The result stored in <code>kern_bench()</code> is a data frame with two columns. The first
contains the R expressions evaluated, and the second is the evaluation
time measured in nanoseconds. Each of the four expressions were evaluated 100 times,
and the data frame thus has 400 rows. The <code>times</code> argument to <code><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html">microbenchmark()</a></code>
can be used to change the number of evaluations per expression if needed.</p>
<p>It may not be immediately obvious that <code>kern_bench()</code>
is a data frame, because printing will automatically summarize the
data, but the actual data structure is revealed by the R function <code><a href="https://rdrr.io/r/utils/str.html">str()</a></code>.</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">kern_bench</span><span class="op">)</span></code></pre></div>
<pre><code>## Classes 'microbenchmark' and 'data.frame':   400 obs. of  2 variables:
##  $ expr: Factor w/ 4 levels "kern_dens(psi, 0.2)",..: 3 3 3 4 3 2 1 1 2 2 ...
##  $ time: num  53000885 9967802 5594794 7379961 4926126 ...</code></pre>
<p>A total of 400 evaluations were done for the above benchmark, and <code><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html">microbenchmark()</a></code><br>
does the evaluations in a random order by default. Measuring evaluation time on a
complex system like a modern computer is an empirical science, and
the order of evaluation can potentially affect the results as the
conditions for the evaluation change over time. The purpose of the
randomization is to avoid that the ordering causes systematically
misleading results.</p>
<p>The microbenchmark package implements some methods for summarizing and
printing the results such as the following summary table with times in
milliseconds.</p>
<pre><code>## Unit: milliseconds
##                       expr   min    lq  mean median    uq    max neval
##        kern_dens(psi, 0.2) 27.90 29.46 36.92  31.72 33.82 499.20   100
##    kern_dens_vec(psi, 0.2)  2.81  3.31  5.10   3.60  3.94  79.36   100
##  kern_dens_apply(psi, 0.2)  3.24  3.83  4.93   4.34  4.86  53.00   100
##  kern_dens_outer(psi, 0.2)  3.47  5.30  5.79   5.59  6.31   9.06   100</code></pre>
<p>The summary table shows some key statistics like median and mean evaluation
times but also extremes and upper and lower quartiles. The distributions
of run times can be investigated further using the <code><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">autoplot()</a></code> function, which is
based on ggplot2 and thus easy to modify.</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">autoplot</a></span><span class="op">(</span><span class="va">kern_bench</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_jitter.html">geom_jitter</a></span><span class="op">(</span>position <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/position_jitter.html">position_jitter</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0</span><span class="op">)</span>, 
              <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>color <span class="op">=</span> <span class="va">expr</span><span class="op">)</span>, alpha <span class="op">=</span> <span class="fl">0.4</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>fill <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span><span class="op">(</span><span class="st">"gray"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="CSwR_files/figure-html/kern-bench-autoplot-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>This more refined benchmark study does not change our initial impression from
using <code><a href="https://rdrr.io/r/base/system.time.html">system.time()</a></code> substantially. The function <code>kern_dens()</code> is notably
slower than the three vectorized implementations, but we are now able to
more clearly see the minor differences among them. For instance, <code>kern_dens_vec()</code>
and <code>kern_dens_apply()</code> have very similar run time distributions, while
<code>kern_dens_outer()</code> clearly has a larger median run time and also a
run time distribution that is more spread out to the right.</p>
<p>In many cases when we benchmark run time it is of interest
to investigate how run time depends on various parameters. This is so
for kernel density estimation, where we want to understand how changes in
the number of data points, <span class="math inline">\(n\)</span>, and the number of grid points, <span class="math inline">\(m\)</span>, affect
run time. We can still use <code><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html">microbenchmark()</a></code> for running the benchmark
experiment, but we will typically process and plot the benchmark data afterwards
in a customized way.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:kern-bench-fig"></span>
<img src="CSwR_files/figure-html/kern-bench-fig-1.png" alt="Median run times for the four different implementations of kernel density estimation. The dashed gray line is a reference line with slope 1." width="100%"><p class="caption">
Figure 2.4: Median run times for the four different implementations of kernel density estimation. The dashed gray line is a reference line with slope 1.
</p>
</div>
<p>Figure <a href="density.html#fig:kern-bench-fig">2.4</a> shows median run times for an experiment with
28 combinations of parameters for each of the four different implementations
yielding a total of 112 different R expressions being benchmarked. The number of replications
for each expression was set to 40. The results confirm that <code>kern_dens()</code>
is substantially slower than the vectorized implementations for all combinations
of <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span>. However, Figure <a href="density.html#fig:kern-bench-fig">2.4</a> also reveals a new
pattern; <code>kern_dens_outer()</code> appears to scale with <span class="math inline">\(n\)</span> in a slightly different way
than the two other vectorized implementations for small <span class="math inline">\(n\)</span>. It is comparable to
or even a bit faster than
<code>kern_dens_vec()</code> and <code>kern_dens_apply()</code> for very small data sets, while it becomes
slower for the larger data sets.</p>
<p>Run time for many algorithms have to a good approximation a dominating
power law behavior as a function of typical size parameters, that is, the
run time will scale approximately like <span class="math inline">\(n \mapsto C n^a\)</span> for constants <span class="math inline">\(C\)</span> and <span class="math inline">\(a\)</span>
and with <span class="math inline">\(n\)</span> denoting a generic size parameter. Therefore it is
beneficial to plot run time using log-log scales and to design benchmark studies
with size parameters being equidistant on a log-scale. With approximate power
law scaling, the log run time behaves like
<span class="math display">\[\log(C) + a \log(n),\]</span>
that is, on a log-log scale we see approximate straight lines. The slope
reveals the exponent <span class="math inline">\(a\)</span>, and two different algorithms for solving the
same problem might have different exponents and thus different slopes
on the log-log-scale. Two different implementations of the same
algorithm should have approximately the same slope
but may differ in the constant <span class="math inline">\(C\)</span> depending upon how efficient the
particular implementation is in the particular programming language used.
Differences in <span class="math inline">\(C\)</span> correspond to vertical translations on the log-log scale.</p>
<p>In practice, we will see some deviations from straight lines on the log-log plot
for a number of reasons. Writing the run time as <span class="math inline">\(C n^a + R(n)\)</span>,
the residual term <span class="math inline">\(R(n)\)</span> will often be noticeable or even dominating and positive
for small <span class="math inline">\(n\)</span>. It is only for large enough <span class="math inline">\(n\)</span>
that the power law term, <span class="math inline">\(C n^a\)</span>, will dominate. In addition, run time
can be affected by hardware constraints such as cache and memory sizes, which
can cause abrupt jumps in run time.</p>
<p>Using <code><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html">microbenchmark()</a></code> over <code><a href="https://rdrr.io/r/base/system.time.html">system.time()</a></code> has two main benefits. First,
it handles the replication and randomization automatically, which is
convenient. Second, it attempts to provide more accurate timings. The latter
is mostly important when we benchmark very fast computations.</p>
<p>It can be <a href="https://radfordneal.wordpress.com/2014/02/02/inaccurate-results-from-microbenchmark/">debated if a median summary of randomly ordered evaluations</a>
is the best way to summarize run time. This is due to the way R does memory
management. R allocates and deallocates memory automatically
and uses <a href="https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)">garbage collection</a>
for the deallocation. This means that computations occasionally, and in a somewhat
unpredictable manner, trigger the garbage collector, and as a result a small
fraction of the evaluations may take substantially longer time than the
rest. The median will typically be almost unaffected, and memory deallocation
is thus effectively (and wrongly) disregarded from run time when the median
summary is used. This is an argument for using the mean instead of the median,
but due to the randomization the computation that triggered
the garbage collector might not be the one that caused the memory allocation
in the first place. Using the mean instead of the median will therefore smear
out the garbage collection run time on all benchmarked expressions. Setting
the argument <code>control = list(order = "block")</code> for <code><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html">microbenchmark()</a></code> will
evaluate the expressions in blocks, which in combination with a mean
summary more correctly accounts for memory allocation and deallocation in the
run time. The downside is that without the randomization the results might
suffer from other artefacts. This book will use randomization
and median summaries throughout, but we keep in mind that this
could underestimate actual average run time depending upon how much
memory a given computation requires. Memory usage and how it affects
run time by triggering garbage collection will be dealt with via
code profiling tools instead.</p>
</div>
</div>
<div id="bandwidth" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Bandwidth selection<a class="anchor" aria-label="anchor" href="#bandwidth"><i class="fas fa-link"></i></a>
</h2>
<div id="rectangular" class="section level3" number="2.3.1">
<h3>
<span class="header-section-number">2.3.1</span> Revisiting the rectangular kernel<a class="anchor" aria-label="anchor" href="#rectangular"><i class="fas fa-link"></i></a>
</h3>
<p>We return to the rectangular kernel and compute the mean squared error. In the
analysis it may be helpful to think about <span class="math inline">\(n\)</span> large and <span class="math inline">\(h\)</span> small. Indeed,
we will eventually choose <span class="math inline">\(h = h_n\)</span> as a function of <span class="math inline">\(n\)</span> such
that as <span class="math inline">\(n \to \infty\)</span> we have <span class="math inline">\(h_n \to 0\)</span>. We should also note the <span class="math inline">\(f_h(x) = E (\hat{f}_h(x))\)</span>
is a density, thus <span class="math inline">\(\int f_h(x) \mathrm{d}x = 1\)</span>.</p>
<p>We will assume that <span class="math inline">\(f_0\)</span> is sufficiently differentiable
and use a Taylor expansion of the distribution function <span class="math inline">\(F_0\)</span> to get that</p>
<p><span class="math display">\[\begin{align*}
f_h(x) &amp; = \frac{1}{2h}\left(F_0(x + h) - F_0(x - h)\right) \\
&amp; = \frac{1}{2h}\left(2h f_0(x) + \frac{h^3}{3} f_0''(x) + R_0(x,h) \right) \\
&amp; = f_0(x) + \frac{h^2}{6} f_0''(x) + R_1(x,h) 
\end{align*}\]</span></p>
<p>where <span class="math inline">\(R_1(x, h) = o(h^2)\)</span>. One should note how the quadratic terms in <span class="math inline">\(h\)</span>
in the Taylor expansion canceled. This gives the following formula for
the squared bias of <span class="math inline">\(\hat{f}_h\)</span>.</p>
<p><span class="math display">\[\begin{align*}
\mathrm{bias}(\hat{f}_h(x))^2 &amp; = (f_h(x) - f_0(x))^2 \\
&amp; = \left(\frac{h^2}{6} f_0''(x) + R_1(x,h) \right)^2 \\
&amp; = \frac{h^4}{36} f_0''(x)^2 + R(x,h)
\end{align*}\]</span></p>
<p>where <span class="math inline">\(R(x,h) = o(h^4)\)</span>. For the variance we see from <a href="density.html#eq:varRect">(2.1)</a>
that
<span class="math display">\[V(\hat{f}_h(x)) = f_h(x)\frac{1}{2hn} - f_h(x)^2 \frac{1}{n}.\]</span>
Integrating the sum of the bias and the variance over <span class="math inline">\(x\)</span> gives the
integrated mean squared error</p>
<p><span class="math display">\[\begin{align*}
\mathrm{MISE}(h) &amp; = \int \mathrm{bias}(\hat{f}_h(x))^2 + V(\hat{f}_h(x)) \mathrm{d}x \\
&amp; = \frac{h^4}{36} \|f_0''\|^2_2 + \frac{1}{2hn} + \int R(x,h) \mathrm{d} x - 
\frac{1}{n} \int f_h(x)^2 \mathrm{d} x.
\end{align*}\]</span></p>
<p>If <span class="math inline">\(f_h(x) \leq C\)</span> (which happens if <span class="math inline">\(f_0\)</span> is bounded),
<span class="math display">\[\int f_h(x)^2 \mathrm{d} x \leq C \int f_h(x) \mathrm{d} x = C,\]</span>
and the last term is <span class="math inline">\(o((nh)^{-1})\)</span>. The second last term is <span class="math inline">\(o(h^4)\)</span>
if we can interchange the limit and integration order. It is conceivable
that we can do so under suitable assumptions on <span class="math inline">\(f_0\)</span>, but we will not pursue
those at this place. The sum of the two remaining and asymptotically dominating terms in
the formula for MISE is<br><span class="math display">\[\mathrm{AMISE}(h) = \frac{h^4}{36} \|f_0''\|^2_2 + \frac{1}{2hn},\]</span>
which is known as the asymptotic mean integrated squared error. Clearly,
for this to be a useful formula, we must assume <span class="math inline">\(\|f_0''\|_2^2 &lt; \infty\)</span>.
In this case the formula for AMISE can be used to find the asymptotic
optimal tradeoff between (integrated) bias and variance. Differentiating w.r.t. <span class="math inline">\(h\)</span> we
find that
<span class="math display">\[\mathrm{AMISE}'(h) = \frac{h^3}{9} \|f_0''\|^2_2 - \frac{1}{2h^2n},\]</span>
and solving for <span class="math inline">\(\mathrm{AMISE}'(h) = 0\)</span> yields
<span class="math display">\[h_n = \left(\frac{9}{2 \|f_0''\|_2^2}\right)^{1/5} n^{-1/5}.\]</span>
When AMISE is regarded as a function of <span class="math inline">\(h\)</span> we observe that
it tends to <span class="math inline">\(\infty\)</span> for <span class="math inline">\(h \to 0\)</span> as well as for <span class="math inline">\(h \to \infty\)</span>, thus the unique
stationary point <span class="math inline">\(h_n\)</span> is a unique global minimizer. Choosing the bandwidth to
be <span class="math inline">\(h_n\)</span> will therefore minimize the asympototic mean integrated squared error, and it
is in this sense an optimal choice of bandwidth.</p>
<p>We see how “wiggliness” of <span class="math inline">\(f_0\)</span> enters into the formula for the optimal
bandwidth <span class="math inline">\(h_n\)</span> via <span class="math inline">\(\|f_0''\|_2\)</span>. This norm of the second derivative
is precisely a quantification of how much <span class="math inline">\(f_0\)</span> oscillates. A large value,
indicating a wiggly <span class="math inline">\(f_0\)</span>, will drive the optimal bandwidth down whereas
a small value will drive the optimal bandwidth up.</p>
<p>We should also observe that if we plug the optimal bandwidth into the formula
for AMISE, we get
<span class="math display">\[\begin{align*}
\mathrm{AMISE}(h_n) &amp; = \frac{h_n^4}{36} \|f_0''\|^2_2 + \frac{1}{2h_n n} \\
&amp; = C n^{-4/5},
\end{align*}\]</span>
which indicates that in terms of integrated mean squared error the rate
at which we can nonparametrically estimate
<span class="math inline">\(f_0\)</span> is <span class="math inline">\(n^{-4/5}\)</span>. This should be contrasted to the common parametric
rate of <span class="math inline">\(n^{-1}\)</span> for mean squared error.</p>
<p>From a practical viewpoint there is one major problem with the optimal
bandwidth <span class="math inline">\(h_n\)</span>; it depends via <span class="math inline">\(\|f_0''\|^2_2\)</span> upon the unknown <span class="math inline">\(f_0\)</span>
that we are trying to estimate. We therefore refer to <span class="math inline">\(h_n\)</span> as an <em>oracle</em>
bandwidth – it is the bandwidth that an oracle that knows <span class="math inline">\(f_0\)</span> would
tell us to use. In practice, we will have to come up with an estimate
of <span class="math inline">\(\|f_0''\|^2_2\)</span> and plug that estimate into the formula for <span class="math inline">\(h_n\)</span>.
We pursue a couple of different options for doing so for general kernel
density estimators below together with methods that do not rely on the
AMISE formula.</p>
</div>
<div id="ise-mise-and-mse-for-kernel-estimators" class="section level3" number="2.3.2">
<h3>
<span class="header-section-number">2.3.2</span> ISE, MISE and MSE for kernel estimators<a class="anchor" aria-label="anchor" href="#ise-mise-and-mse-for-kernel-estimators"><i class="fas fa-link"></i></a>
</h3>
<p>Bandwidth selection for general kernel estimators can be studied
asymptotically just as above. To this end it is useful to formalize
how we quantify the <em>quality</em> of an estimate <span class="math inline">\(\hat{f}_h\)</span>. One natural
quantification is the <em>integrated squared error</em>,
<span class="math display">\[\mathrm{ISE}(\hat{f}_h) = \int (\hat{f}_h(x) - f_0(x))^2 \ \mathrm{d}x = \|\hat{f}_h - f_0\|_2^2.\]</span></p>
<p>The quality of the estimation procedure producing <span class="math inline">\(\hat{f}_h\)</span> from data
can then be quantified by taking the mean ISE,
<span class="math display">\[\mathrm{MISE}(h) = E(\mathrm{ISE}(\hat{f}_h)),\]</span>
where the expectation integral is over the data. Using Tonelli’s theorem
we may interchange the expectation and the integration over <span class="math inline">\(x\)</span> to
get
<span class="math display">\[\mathrm{MISE}(h) = \int \mathrm{MSE}_x(h) \ \mathrm{d}x\]</span>
where
<span class="math display">\[\mathrm{MSE}_h(x) = \mathrm{var}(\hat{f}_h(x)) + \mathrm{bias}(\hat{f}_h(x))^2.\]</span>
is the pointwise mean squared error.</p>
<p>Using the same kind of Taylor expansion argument as above we can show that if
<span class="math inline">\(K\)</span> is a square integrable probability density with mean 0 and
<span class="math display">\[\sigma_K^2 = \int z^2 K(z) \ \mathrm{d}z &gt; 0,\]</span>
then
<span class="math display">\[\mathrm{MISE}(h) = \mathrm{AMISE}(h) + o((nh)^{-1} + h^4)\]</span>
where the <em>asymptotic mean integrated squared error</em> is
<span class="math display">\[\mathrm{AMISE}(h) = \frac{\|K\|_2^2}{nh} + \frac{h^4 \sigma^4_K \|f_0''\|_2^2}{4}\]</span>
with
<span class="math display">\[\|g\|_2^2 = \int g(z)^2 \ \mathrm{d}z  \quad (\mathrm{squared } \ L_2\mathrm{-norm}).\]</span>
Some regularity assumptions on <span class="math inline">\(f_0\)</span> are necessary, and from the result
we clearly need to require that <span class="math inline">\(f_0''\)</span> is meaningful and square integrable.
However, that is also enough. See Proposition A.1 in <span class="citation"><a href="references.html#ref-Tsybakov:2009" role="doc-biblioref">Tsybakov</a> (<a href="references.html#ref-Tsybakov:2009" role="doc-biblioref">2009</a>)</span> for a rigorous
proof.</p>
<p>By minimizing <span class="math inline">\(\mathrm{AMISE}(h)\)</span> we derive the optimal oracle bandwidth</p>
<p><span class="math display" id="eq:oracle">\[\begin{equation}
\tag{2.3}
h_n = \left( \frac{\|K\|_2^2}{ \|f_0''\|^2_2  \sigma_K^4} \right)^{1/5} n^{-1/5}.
\end{equation}\]</span></p>
<p>If we plug this formula into the formula for AMISE we arrive at the asymptotic
error rate <span class="math inline">\(\mathrm{AMISE}(h_n) = C n^{-4/5}\)</span> with a constant <span class="math inline">\(C\)</span> depending on
<span class="math inline">\(f_0''\)</span> and the kernel. It is noteworthy that the asymptotic analysis can be carried
out even if <span class="math inline">\(K\)</span> is allowed to take negative values, though the resulting estimate
may not be a valid density as it is. <span class="citation"><a href="references.html#ref-Tsybakov:2009" role="doc-biblioref">Tsybakov</a> (<a href="references.html#ref-Tsybakov:2009" role="doc-biblioref">2009</a>)</span> demonstrates how to improve on
the rate <span class="math inline">\(n^{-4/5}\)</span> by allowing for kernels whose moments of order two or above vanish.
Necessarily, such kernels must take negative values.</p>
<p>We observe that for the rectangular kernel,
<span class="math display">\[\sigma_K^4 = \left(\frac{1}{2} \int_{-1}^1 z^2 \ \mathrm{d} z\right)^2 = \frac{1}{9}\]</span>
and
<span class="math display">\[\|K\|_2^2 = \frac{1}{2^2} \int_{-1}^1 \ \mathrm{d} z = \frac{1}{2}.\]</span>
Plugging these numbers into <a href="density.html#eq:oracle">(2.3)</a> we find the oracle bandwidth for
the rectangular kernel as derived in Section <a href="density.html#rectangular">2.3.1</a>. For the Gaussian
kernel we find that <span class="math inline">\(\sigma_K^4 = 1\)</span>, while
<span class="math display">\[\|K\|_2^2 = \frac{1}{2 \pi} \int e^{-x^2} \ \mathrm{d} x = \frac{1}{2 \sqrt{\pi}}.\]</span></p>
</div>
<div id="plug-in-estimation-of-the-oracle-bandwidth" class="section level3" number="2.3.3">
<h3>
<span class="header-section-number">2.3.3</span> Plug-in estimation of the oracle bandwidth<a class="anchor" aria-label="anchor" href="#plug-in-estimation-of-the-oracle-bandwidth"><i class="fas fa-link"></i></a>
</h3>
<p>To compute <span class="math inline">\(\|f_0''\|^2_2\)</span> that enters into the formula for the asymptotically optimal
bandwidth we have to know <span class="math inline">\(f_0\)</span> that we are trying to estimate in the first place.
To resolve the circularity we will make a first guess of what <span class="math inline">\(f_0\)</span> is and plug that
guess into the formula for the oracle bandwidth.</p>
<p>Our first guess is that <span class="math inline">\(f_0\)</span> is Gaussian with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>.
Then</p>
<p><span class="math display">\[\begin{align*}
\|f_0''\|^2_2 &amp; = \frac{1}{2 \pi \sigma^2} \int \left(\frac{x^2}{\sigma^4} -  \frac{1}{\sigma^2}\right)^2 e^{-x^2/\sigma^2} \ \mathrm{d}x \\
&amp; = \frac{1}{2 \sigma^9 \sqrt{\pi}} \frac{1}{\sqrt{\pi \sigma^2}} \int (x^4 - 2 \sigma^2 x^2 + \sigma^4) e^{-x^2/\sigma^2} \ \mathrm{d}x \\ 
&amp; = \frac{1}{2 \sigma^9 \sqrt{\pi}} (\frac{3}{4} \sigma^4 - \sigma^4 + \sigma^4) \\
&amp; = \frac{3}{8 \sigma^5 \sqrt{\pi}}.
\end{align*}\]</span></p>
<p>Plugging this expression for the squared 2-norm of the second derivative of the density
into the formula for the oracle bandwidth gives</p>
<p><span class="math display" id="eq:oracle-silverman">\[\begin{equation}
\tag{2.4}
h_n = \left( \frac{8 \sqrt{\pi} \|K\|_2^2}{3 \sigma_K^4} \right)^{1/5} \sigma n^{-1/5},
\end{equation}\]</span></p>
<p>with <span class="math inline">\(\sigma\)</span> the only quantity depending on the unknown density <span class="math inline">\(f_0\)</span>. We can now
simply estimate <span class="math inline">\(\sigma\)</span>, using e.g. the empirical standard
deviation <span class="math inline">\(\hat{\sigma}\)</span>, and plug this estimate into the formula above to get an
estimate of the oracle bandwidth.</p>
<p>It is well known that the empirical standard deviation is sensitive to outliers,
and if <span class="math inline">\(\sigma\)</span> is overestimated for that reason, the bandwidth will be too large
and the resulting estimate will be oversmoothed. To
get more robust bandwidth selection, <span class="citation"><a href="references.html#ref-Silverman:1986" role="doc-biblioref">Silverman</a> (<a href="references.html#ref-Silverman:1986" role="doc-biblioref">1986</a>)</span> suggested using
the interquartile range to estimate <span class="math inline">\(\sigma\)</span>. In fact, he suggested
estimating <span class="math inline">\(\sigma\)</span> by
<span class="math display">\[\tilde{\sigma} = \min\{\hat{\sigma}, \mathrm{IQR} / 1.34\}.\]</span>
In this estimator, IQR denotes the empirical interquartile range, and
<span class="math inline">\(1.34\)</span> is approximately the interquartile range, <span class="math inline">\(\Phi^{-1}(0.75) - \Phi^{-1}(0.25)\)</span>,
of the standard Gaussian distribution. Curiously, the interquartile range for
the standard Gaussian distribution is <span class="math inline">\(1.35\)</span> to two decimals accuracy, but the
use of <span class="math inline">\(1.34\)</span> in the estimator <span class="math inline">\(\tilde{\sigma}\)</span>
has prevailed. Silverman, moreover, suggested to reduce the kernel-dependent
constant in the formula <a href="density.html#eq:oracle-silverman">(2.4)</a> for <span class="math inline">\(h_n\)</span> to further reduce
oversmoothing.</p>
<p>If we specialize to the Gaussian kernel, formula <a href="density.html#eq:oracle-silverman">(2.4)</a>
simplifies to
<span class="math display">\[\hat{h}_n = \left(\frac{4}{3}\right)^{1/5} \tilde{\sigma} n^{-1/5},\]</span>
with <span class="math inline">\(\tilde{\sigma}\)</span> plugged in as a robust estimate of <span class="math inline">\(\sigma\)</span>. <span class="citation"><a href="references.html#ref-Silverman:1986" role="doc-biblioref">Silverman</a> (<a href="references.html#ref-Silverman:1986" role="doc-biblioref">1986</a>)</span>
made the ad hoc suggestion to reduce the factor <span class="math inline">\((4/3)^{1/5} \simeq 1.06\)</span> to <span class="math inline">\(0.9\)</span>.
This results in the bandwidth estimate
<span class="math display">\[\hat{h}_n = 0.9 \tilde{\sigma} n^{-1/5},\]</span>
which has become known as <em>Silverman’s rule of thumb</em>.</p>
<p>Silverman’s rule of thumb is the default bandwidth estimator for <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code>
as implemented by the function <code><a href="https://rdrr.io/r/stats/bandwidth.html">bw.nrd0()</a></code>. The <code><a href="https://rdrr.io/r/stats/bandwidth.html">bw.nrd()</a></code> function implements
bandwidth selection using the factor <span class="math inline">\(1.06\)</span> instead of <span class="math inline">\(0.9\)</span>. Though the theoretical
derivations behind these implementations assume that <span class="math inline">\(f_0\)</span>
is Gaussian, either will give reasonable bandwidth selection for a range of
unimodal distributions. If <span class="math inline">\(f_0\)</span> is multimodal, Silverman’s
rule of thumb is known to oversmooth the density estimate.</p>
<p>Instead of computing <span class="math inline">\(\|f_0''\|^2_2\)</span> assuming that the distribution is Gaussian,
we can compute the norm for a pilot estimate, <span class="math inline">\(\tilde{f}\)</span>, and plug the result
into the formula for <span class="math inline">\(h_n\)</span>. If the pilot estimate is a kernel estimate with
kernel <span class="math inline">\(H\)</span> and bandwidth <span class="math inline">\(r\)</span> we get
<span class="math display">\[\|\tilde{f}''\|^2_2 = \frac{1}{n^2r^6} \sum_{i = 1}^n \sum_{j=1}^n 
\int H''\left( \frac{x - x_i}{r} \right) H''\left( \frac{x - x_j}{r} \right) \mathrm{d} x.\]</span>
The problem is, of course, that now we have to choose the pilot bandwidth
<span class="math inline">\(r\)</span>. But doing so using a simple method like Silverman’s rule of thumb at this
stage is typically not too bad an idea. Thus we arrive at the following
plug-in procedure using the Gaussian kernel for the pilot estimate:</p>
<ul>
<li>Compute an estimate, <span class="math inline">\(\hat{r}\)</span>, of the pilot bandwidth using Silverman’s
rule of thumb.</li>
<li>Compute <span class="math inline">\(\|\tilde{f}''\|^2_2\)</span> using the Gaussian kernel as pilot kernel <span class="math inline">\(H\)</span> and
using the estimated pilot bandwidth <span class="math inline">\(\hat{r}\)</span>.</li>
<li>Plug <span class="math inline">\(\|\tilde{f}''\|^2_2\)</span> into the oracle bandwidth formula <a href="density.html#eq:oracle">(2.3)</a>
to compute <span class="math inline">\(\hat{h}_n\)</span> for the kernel <span class="math inline">\(K\)</span>.</li>
</ul>
<p>Note that to use a pilot kernel different from the Gaussian we have
to adjust the constant 0.9 (or 1.06) in Silverman’s rule of thumb accordingly
by computing <span class="math inline">\(\|H\|_2^2\)</span> and <span class="math inline">\(\sigma_H^4\)</span> and using <a href="density.html#eq:oracle-silverman">(2.4)</a>.</p>
<p><span class="citation"><a href="references.html#ref-Sheather:1991" role="doc-biblioref">Sheather and Jones</a> (<a href="references.html#ref-Sheather:1991" role="doc-biblioref">1991</a>)</span> took these plug-in ideas a step further and analyzed in detail
how to choose the pilot bandwidth in a good and data adaptive way. The resulting
method is somewhat complicated but implementable. We skip the details but simply
observe that their method is implemented in R in the function <code><a href="https://rdrr.io/r/stats/bandwidth.html">bw.SJ()</a></code>, and
it can be selected when using <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code> by setting the argument <code>bw = "SJ"</code>.
This plug-in method is regarded as a solid default that performs well for
many different data generating densities <span class="math inline">\(f_0\)</span>.</p>
</div>
<div id="cv" class="section level3" number="2.3.4">
<h3>
<span class="header-section-number">2.3.4</span> Cross-validation<a class="anchor" aria-label="anchor" href="#cv"><i class="fas fa-link"></i></a>
</h3>
<p>An alternative to relying on the asymptotic optimality arguments for
integrated mean squared error and the corresponding plug-in estimates
of the bandwidth is known as <em>cross-validation</em>. The method mimics
the idea of setting aside a subset of the data set, which is then <em>not</em>
used for computing an estimate but only for validating the estimator’s
performance. The benefit of cross-validation over simply setting
aside a validation data set is that we do not “waste” any of the data points
on validation only. All data points are used for the ultimate computation
of the estimate. The deficit is that cross-validation is usually
computationally more demanding.</p>
<p>Suppose that <span class="math inline">\(I_1, \ldots, I_k\)</span> form a partition of the index set <span class="math inline">\(\{1, \ldots, n\}\)</span>
and define
<span class="math display">\[I^{-i} = \bigcup_{l: i \not \in I_l} I_l.\]</span>
That is, <span class="math inline">\(I^{-i}\)</span> contains all indices but those that belong to the set <span class="math inline">\(I_l\)</span>
containing <span class="math inline">\(i\)</span>. In particular, <span class="math inline">\(i \not \in I^{-i}\)</span>. Define also <span class="math inline">\(n_i = |I^{-i}|\)</span>
and
<span class="math display">\[\hat{f}^{-i}_h = \frac{1}{h n_i} \sum_{j \in I^{-i}} K\left(\frac{x_i - x_j}{h}\right).\]</span>
That is, <span class="math inline">\(\hat{f}^{-i}_h\)</span> is the kernel density estimate based on data with
indices in <span class="math inline">\(I^{-i}\)</span> and evaluated in <span class="math inline">\(x_i\)</span>. Since the density
estimate evaluated in <span class="math inline">\(x_i\)</span> is not based on <span class="math inline">\(x_i\)</span>, the quantity <span class="math inline">\(\hat{f}^{-i}_h\)</span>
can be used to assess how well the density estimate computed using a bandwidth
<span class="math inline">\(h\)</span> concur with the data point <span class="math inline">\(x_i\)</span>. This can be summarized using the
log-likelihood
<span class="math display">\[\ell_{\mathrm{CV}}(h) = \sum_{i=1}^n \log (\hat{f}^{-i}_h),\]</span>
that we will refer to as the cross-validated log-likelihood, and we define
the bandwidth estimate as
<span class="math display">\[\hat{h}_{\mathrm{CV}} = \textrm{arg max}_h \ \ \ell_{\mathrm{CV}}(h).\]</span>
This cross-validation based bandwidth can then be used for computing kernel
density estimates using the entire data set.</p>
<p>If the partition of indices consists of <span class="math inline">\(k\)</span> subsets we usually talk about
<span class="math inline">\(k\)</span>-fold cross-validation. If <span class="math inline">\(k = n\)</span> so that all subsets consist of just a single
index we talk about leave-one-out cross-validation. For leave-one-out
cross-validation there is only one possible partition, while for <span class="math inline">\(k &lt; n\)</span>
there are many possible partitions. Which should be chosen then? In practice,
we choose the partition by sampling indices randomly without replacement
into <span class="math inline">\(k\)</span> sets of size roughly <span class="math inline">\(n / k\)</span>.</p>
<p>It is also possible to use cross-validation in combination with MISE. Rewriting
we find that
<span class="math display">\[\mathrm{MISE}(h) = E (\| \hat{f}_h\|_2^2) - 2 E (\hat{f}_h(X)) + E(\|f_0^2\|_2^2)\]</span>
for <span class="math inline">\(X\)</span> a random variable independent of the data and with distribution having
density <span class="math inline">\(f_0\)</span>. The last term does not depend upon <span class="math inline">\(h\)</span> and we can ignore it from
the point of view of minimizing MISE. For the first term we have an unbiased
estimate in <span class="math inline">\(\| \hat{f}_h\|_2^2\)</span>. The middle term can be estimated
without bias by
<span class="math display">\[\frac{2}{n} \sum_{i=1}^n \hat{f}^{-i}_h,\]</span>
and this leads to the statistic
<span class="math display">\[\mathrm{UCV}(h) = \| \hat{f}_h\|_2^2 - \frac{2}{n} \sum_{i=1}^n \hat{f}^{-i}_h\]</span>
known as the unbiased cross-validation criterion. The corresponding
bandwidth estimate is
<span class="math display">\[\hat{h}_{\mathrm{UCV}} = \textrm{arg min}_h \ \ \mathrm{UCV}(h).\]</span>
Contrary to the log-likelihood based criterion, this criterion requires
the computation of <span class="math inline">\(\| \hat{f}_h\|_2^2\)</span>. Bandwidth selection using UCV
is implemented in R in the function <code><a href="https://rdrr.io/r/stats/bandwidth.html">bw.ucv()</a></code> and can be
used with <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code> by setting the argument <code>bw = "ucv"</code>.</p>
</div>
</div>
<div id="likelihood" class="section level2" number="2.4">
<h2>
<span class="header-section-number">2.4</span> Likelihood considerations<a class="anchor" aria-label="anchor" href="#likelihood"><i class="fas fa-link"></i></a>
</h2>
<p>In Section <a href="density.html#cv">2.3.4</a> the cross-validated likelihood was used for bandwidth selection.
It is natural to ask why we did not go all-in and simply maximized the
likelihood over all possible densities to find a maximum likelihood estimator
instead of using the ad hoc idea behind kernel density estimation.</p>
<p>The log-likelihood
<span class="math display">\[\ell(f) = \sum_{i=j}^n \log f(x_j)\]</span>
is well defined as a function of the density <span class="math inline">\(f\)</span> – even when <span class="math inline">\(f\)</span> is not restricted
to belong to a finite-dimensional parametric model.
To investigate if a nonparametric MLE is meaningful we consider how the likelihood
behaves for the densities
<span class="math display">\[\overline{f}_h(x) = \frac{1}{nh \sqrt{2 \pi}} \sum_{j=1}^n e^{- \frac{(x - x_j)^2}{2 h^2} }\]</span>
for different choices of <span class="math inline">\(h\)</span>. Note that <span class="math inline">\(\overline{f}_h\)</span> is simply the
kernel density estimator with the Gaussian kernel and bandwidth <span class="math inline">\(h\)</span>.</p>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## The densities can easily be implemented using the density implementation</span>
<span class="co">## of the Gaussian density in R</span>
<span class="va">f_h</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">h</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">psi</span>, <span class="va">h</span><span class="op">)</span><span class="op">)</span>
<span class="co">## This function does not work as a vectorized function as it is, but there is </span>
<span class="co">## a convenience function, 'Vectorize', in R that turns the function into a </span>
<span class="co">## function that can actually be applied correctly to a vector. </span>
<span class="va">f_h</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Vectorize.html">Vectorize</a></span><span class="op">(</span><span class="va">f_h</span><span class="op">)</span></code></pre></div>
<p>Figure <a href="density.html#fig:gausKern">2.5</a> shows what some of
these densities look like compared to the histogram of the <span class="math inline">\(\psi\)</span>-angle data.
Clearly, for large <span class="math inline">\(h\)</span> these densities are smooth and slowly oscillating, while
as <span class="math inline">\(h\)</span> gets smaller the densities become more and more wiggly. As <span class="math inline">\(h \to 0\)</span>
the densities become increasingly dominated by tall narrow peaks
around the individual data points.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:gausKern"></span>
<img src="CSwR_files/figure-html/gausKern-1.png" alt="The densities \(\overline{f}_h\) for different choices of \(h\)." width="49%"><img src="CSwR_files/figure-html/gausKern-2.png" alt="The densities \(\overline{f}_h\) for different choices of \(h\)." width="49%"><img src="CSwR_files/figure-html/gausKern-3.png" alt="The densities \(\overline{f}_h\) for different choices of \(h\)." width="49%"><img src="CSwR_files/figure-html/gausKern-4.png" alt="The densities \(\overline{f}_h\) for different choices of \(h\)." width="49%"><img src="CSwR_files/figure-html/gausKern-5.png" alt="The densities \(\overline{f}_h\) for different choices of \(h\)." width="49%"><img src="CSwR_files/figure-html/gausKern-6.png" alt="The densities \(\overline{f}_h\) for different choices of \(h\)." width="49%"><p class="caption">
Figure 2.5: The densities <span class="math inline">\(\overline{f}_h\)</span> for different choices of <span class="math inline">\(h\)</span>.
</p>
</div>
<p>The way that these densities adapt to the data points is reflected in the
log-likelihood as well.</p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># To plot the log-likelihood we need to evaluate it in a grid of h-values.</span>
<span class="va">hseq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.001</span>, <span class="op">-</span><span class="fl">0.001</span><span class="op">)</span>
<span class="co"># For the following computation of the log-likelihood it is necessary </span>
<span class="co"># that f_h is vectorized. There are other ways to implement this computation</span>
<span class="co"># in R, and some are more efficient, but the computation of the log-likelihood </span>
<span class="co"># for each h scales as O(n^2) with n the number of data points. </span>
<span class="va">ll</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">hseq</span>, <span class="kw">function</span><span class="op">(</span><span class="va">h</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu">f_h</span><span class="op">(</span><span class="va">psi</span>, <span class="va">h</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/qplot.html">qplot</a></span><span class="op">(</span><span class="va">hseq</span>, <span class="va">ll</span>, geom <span class="op">=</span> <span class="st">"line"</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"h"</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"Log-likelihood"</span><span class="op">)</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/qplot.html">qplot</a></span><span class="op">(</span><span class="va">hseq</span>, <span class="va">ll</span>, geom <span class="op">=</span> <span class="st">"line"</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_x_log10</a></span><span class="op">(</span><span class="st">"h"</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">""</span><span class="op">)</span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:LogLike"></span>
<img src="CSwR_files/figure-html/LogLike-1.png" alt="Log-likelihood, $\ell(\overline{f}_h)$, for the densities $\overline{f}_h$ as a function of $h$. Note the log-scale on the right." width="49%"><img src="CSwR_files/figure-html/LogLike-2.png" alt="Log-likelihood, $\ell(\overline{f}_h)$, for the densities $\overline{f}_h$ as a function of $h$. Note the log-scale on the right." width="49%"><p class="caption">
Figure 2.6: Log-likelihood, <span class="math inline">\(\ell(\overline{f}_h)\)</span>, for the densities <span class="math inline">\(\overline{f}_h\)</span> as a function of <span class="math inline">\(h\)</span>. Note the log-scale on the right.
</p>
</div>
<p>From Figure <a href="density.html#fig:LogLike">2.6</a> it is clear that the likelihood is decreasing in
<span class="math inline">\(h\)</span>, and it appears that it is unbounded as <span class="math inline">\(h \to 0\)</span>. This is most clearly
seen on the figure when <span class="math inline">\(h\)</span> is plotted on the log-scale because then it appears
that the log-likelihood approximately behaves as <span class="math inline">\(-\log(h)\)</span> for <span class="math inline">\(h \to 0\)</span>.</p>
<p>We can show that that is, indeed, the case. If <span class="math inline">\(x_i \neq x_j\)</span> when <span class="math inline">\(i \neq j\)</span></p>
<p><span class="math display">\[\begin{align*}
\ell(\overline{f}_h) &amp; = \sum_{i} \log\left(1 + \sum_{j \neq i} e^{-(x_i - x_j)^2 / (2 h^2)} \right) - 
n \log(nh\sqrt{2 \pi}) \\
&amp; \sim - n \log(nh\sqrt{2 \pi})
\end{align*}\]</span></p>
<p>for <span class="math inline">\(h \to 0\)</span>. Hence, <span class="math inline">\(\ell(\overline{f}_h) \to \infty\)</span> for <span class="math inline">\(h \to 0\)</span>. This demonstrates that the MLE
of the density does not exist in the set of all distributions with densities.</p>
<p>In the sense of <a href="https://en.wikipedia.org/wiki/Convergence_of_measures#Weak_convergence_of_measures">weak convergence</a> it actually holds that
<span class="math display">\[\overline{f}_h \cdot m \overset{\mathrm{wk}}{\longrightarrow} 
\varepsilon_n = \frac{1}{n} \sum_{j=1}^n \delta_{x_j}\]</span>
for <span class="math inline">\(h \to 0\)</span>. The <em>empirical measure</em> <span class="math inline">\(\varepsilon_n\)</span> can sensibly be regarded as
the nonparametric MLE of the distribution, but the empirical measure does not have
a density. We conclude that we cannot directly define a sensible
density estimator as a maximum-likelihood estimator.</p>
<div id="sieves" class="section level3" number="2.4.1">
<h3>
<span class="header-section-number">2.4.1</span> Method of sieves<a class="anchor" aria-label="anchor" href="#sieves"><i class="fas fa-link"></i></a>
</h3>
<p>A sieve is a family of models, <span class="math inline">\(\Theta_{h}\)</span>, indexed by
a real valued parameter, <span class="math inline">\(h \in \mathbb{R}\)</span>, such that <span class="math inline">\(\Theta_{h_1} \subseteq \Theta_{h_2}\)</span> for
<span class="math inline">\(h_1 \leq h_2\)</span>. In this chapter <span class="math inline">\(\Theta_{h}\)</span> will denote a set of probability densities.
If the increasing family of models is chosen in a sensible way, we
may be able to compute the MLE
<span class="math display">\[\hat{f}_h = \text{arg max}_{f \in \Theta_h} \ell(f),\]</span>
and we may even be able to choose <span class="math inline">\(h = h_n\)</span> as a function of the sample size <span class="math inline">\(n\)</span>
such that <span class="math inline">\(\hat{f}_{h_n}\)</span> becomes a consistent estimator of <span class="math inline">\(f_0\)</span>.</p>
<p>It is possible to take
<span class="math display">\[\Theta_h = \{ \overline{f}_{h'} \mid h' \leq h \}\]</span>
with <span class="math inline">\(\overline{f}_{h'}\)</span> as defined above, in which case <span class="math inline">\(\hat{f}_h = \overline{f}_h\)</span>.
We will see in the following section that this is simply a kernel estimator.</p>
<p>A more interesting example is obtained by letting
<span class="math display">\[\Theta_h = \left\{ x \mapsto  \frac{1}{h \sqrt{2 \pi}} \int  e^{- \frac{(x - z)^2}{2 h^2} } \mathrm{d}\mu(z) \Biggm| \mu \textrm{ a probability measure}   \right\},\]</span>
which is known as the convolution sieve. We note that <span class="math inline">\(\overline{f}_h \in \Theta_h\)</span> by
taking <span class="math inline">\(\mu = \varepsilon_n\)</span>, but generally <span class="math inline">\(\hat{f}_h\)</span> will be
different from <span class="math inline">\(\overline{f}_h\)</span>.</p>
<p>We will not pursue the general theory of sieve estimators, but refer
to the paper <a href="https://projecteuclid.org/euclid.aos/1176345782">Nonparametric Maximum Likelihood Estimation by the Method of Sieves</a> by Geman and Hwang.
In the following section we will work out some more practical details
for a particular sieve estimator based on a basis expansion of the
log-density.</p>
</div>
<div id="basis-density" class="section level3" number="2.4.2">
<h3>
<span class="header-section-number">2.4.2</span> Basis expansions<a class="anchor" aria-label="anchor" href="#basis-density"><i class="fas fa-link"></i></a>
</h3>
<p>We suppose in this section that the data points are all contained in the
interval <span class="math inline">\([a,b]\)</span> for <span class="math inline">\(a, b \in \mathbb{R}\)</span>. This is true for the angle
data with <span class="math inline">\(a = -\pi\)</span> and <span class="math inline">\(b = \pi\)</span> no matter the size of the data set,
but if <span class="math inline">\(f_0\)</span> does not have a bounded support it may be necessary to let <span class="math inline">\(a\)</span>
and <span class="math inline">\(b\)</span> change with the data. However, for any fixed data set we can choose
some sufficiently large <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>In this section the sieve will be indexed by integers,
and for <span class="math inline">\(h \in \mathbb{N}\)</span> we suppose that we have chosen continuous
functions <span class="math inline">\(b_1, \ldots, b_h : [a,b] \to \mathbb{R}\)</span>. These will be
called <em>basis functions</em>. We then define
<span class="math display">\[\Theta_h = \left\{ x \mapsto  \varphi(\boldsymbol{\beta})^{-1} 
\exp\left(\sum_{k=1}^h \beta_k b_k(x)\right) \Biggm| \boldsymbol{\beta} = (\beta_1, \ldots, \beta_h)^T \in \mathbb{R}^h \right\},\]</span>
where
<span class="math display">\[\varphi(\boldsymbol{\beta}) = \int_a^b \exp\left(\sum_{k=1}^h \beta_k b_k(x)\right) \mathrm{d} x.\]</span></p>
<p>The MLE over <span class="math inline">\(\Theta_h\)</span> is then given as
<span class="math display">\[\hat{f}_h(x) =  \varphi(\hat{\boldsymbol{\beta}})^{-1} 
\exp\left(\sum_{k=1}^h \hat{\beta}_k b_k(x)\right),\]</span>
where</p>
<p><span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}} &amp; = 
\text{arg max}_{\boldsymbol{\beta} \in \mathbb{R}^h} \ \sum_{j=1}^n \sum_{k=1}^h \beta_k b_k(x_j) - \log \varphi(\boldsymbol{\beta}) \\
&amp; = \text{arg max}_{\boldsymbol{\beta} \in \mathbb{R}^h} \ \ \mathbf{1}^T \mathbf{B} \boldsymbol{\beta} - \log \varphi(\boldsymbol{\beta}).
\end{align*}\]</span></p>
<p>Here <span class="math inline">\(\mathbf{B}\)</span> is the <span class="math inline">\(n \times h\)</span> matrix with <span class="math inline">\(B_{jk} = b_k(x_j)\)</span>. Thus for
any fixed <span class="math inline">\(h\)</span> the model is, in fact, just an ordinary parametric exponential
family, though it may not be entirely straightforward how to compute
<span class="math inline">\(\varphi(\boldsymbol{\beta})\)</span>.</p>
<p>Many basis functions are possible. Polynomials may be used, but splines are
often preferred. An alternative is a selection of trigonometric functions,
for instance<br><span class="math display">\[b_1(x) = \cos(x), b_2(x) = \sin(x), \ldots, b_{2h-1}(x) = \cos(hx),  b_{2h}(x) = \sin(hx)\]</span>
on the interval <span class="math inline">\([-\pi, \pi]\)</span>. In Section <a href="intro.html#vM">1.2.1</a> a simple special case
was actually treated corresponding to <span class="math inline">\(h = 2\)</span>, where the normalization constant
was identified in terms of a modified Bessel function.</p>
<p>It is worth remembering the following:</p>
<blockquote>
<p>“With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.”</p>
<p>— John von Neumann</p>
</blockquote>
<p>The Normal-inverse Gaussian distribution has four parameters and
the generalized hyperbolic distribution is an extension with five,
but von Neumann was probably thinking more in terms of a
spline or a polynomial expansion as above with four or five suitably
chosen basis functions.</p>
<p>The quote is not a mathematical statement but an empirical observation.
With a handful of parameters you already have a quite flexible class of
densities that will fit many real data sets well. But remember
that a reasonably good fit does not mean
that you have found the “true” data generating model. Though data is
in some situations not as scarce a resource today as when von Neumann made
elephants wiggle their trunks, the quote still suggests that <span class="math inline">\(h\)</span> should grow
rather slowly with <span class="math inline">\(n\)</span> to avoid overfitting.</p>
</div>
</div>
<div id="density:ex" class="section level2" number="2.5">
<h2>
<span class="header-section-number">2.5</span> Exercises<a class="anchor" aria-label="anchor" href="#density:ex"><i class="fas fa-link"></i></a>
</h2>
<div id="kernel-density-estimation" class="section level3 unnumbered">
<h3>Kernel density estimation<a class="anchor" aria-label="anchor" href="#kernel-density-estimation"><i class="fas fa-link"></i></a>
</h3>
<div class="exercise">
<p><span id="exr:epanechnikov" class="exercise"><strong>Exercise 2.1  </strong></span>The Epanechnikov kernel is given by
<span class="math display">\[K(x) = \frac{3}{4}(1 - x^2)\]</span>
for <span class="math inline">\(x \in [-1, 1]\)</span> and 0 elsewhere. Show that this is a probability density
with mean zero and compute <span class="math inline">\(\sigma_K^2\)</span> as well as <span class="math inline">\(\|K\|_2^2\)</span>.</p>
</div>
<p>For the following exercises use the log(F12) variable as considered in
Exercise <a href="app-R.html#exr:plotHist">A.4</a>.</p>
<div class="exercise">
<p><span id="exr:epanechnikov-density" class="exercise"><strong>Exercise 2.2  </strong></span>Use <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code> to compute the kernel density estimate with the Epanechnikov kernel
to the log(F12) data. Try different bandwidths.</p>
</div>
<div class="exercise">
<p><span id="exr:epanechnikov-implement" class="exercise"><strong>Exercise 2.3  </strong></span>Implement kernel density estimation yourself using the Epanechnikov kernel.
Test your implementation by comparing it to <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code> using the log(F12) data.</p>
</div>
</div>
<div id="benchmarking-1" class="section level3 unnumbered">
<h3>Benchmarking<a class="anchor" aria-label="anchor" href="#benchmarking-1"><i class="fas fa-link"></i></a>
</h3>
<div class="exercise">
<p><span id="exr:benchmark-epanechnikov" class="exercise"><strong>Exercise 2.4  </strong></span>Construct the following vector</p>
</div>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">2</span><span class="op">^</span><span class="fl">13</span><span class="op">)</span></code></pre></div>
<p>Then use <code>microbenchmark</code> to benchmark the computation of</p>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">k</span><span class="op">]</span>, <span class="fl">0.2</span><span class="op">)</span></code></pre></div>
<p>for <code>k</code> ranging from <span class="math inline">\(2^5\)</span> to <span class="math inline">\(2^{13}\)</span>. Summarize the benchmarking results.</p>
<div class="exercise">
<p><span id="exr:benchmark-own-epanechnikov" class="exercise"><strong>Exercise 2.5  </strong></span>Benchmark your own implementation of kernel density estimation using the Epanechnikov
kernel. Compare the results to those obtained for <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code>.</p>
</div>
<div class="exercise">
<p><span id="exr:benchmark-kernel" class="exercise"><strong>Exercise 2.6  </strong></span>Experiment with different implementations of kernel evaluation in R using
the Gaussian kernel and the Epanechnikov kernel. Use <code><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html">microbenchmark()</a></code> to compare
the different implementations.</p>
</div>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="intro.html"><span class="header-section-number">1</span> Introduction</a></div>
<div class="next"><a href="bivariate.html"><span class="header-section-number">3</span> Bivariate smoothing</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#density"><span class="header-section-number">2</span> Density estimation</a></li>
<li><a class="nav-link" href="#unidens"><span class="header-section-number">2.1</span> Univariate density estimation</a></li>
<li>
<a class="nav-link" href="#kernel-density"><span class="header-section-number">2.2</span> Kernel methods</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#dens-implement"><span class="header-section-number">2.2.1</span> Implementation</a></li>
<li><a class="nav-link" href="#benchmarking"><span class="header-section-number">2.2.2</span> Benchmarking</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#bandwidth"><span class="header-section-number">2.3</span> Bandwidth selection</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#rectangular"><span class="header-section-number">2.3.1</span> Revisiting the rectangular kernel</a></li>
<li><a class="nav-link" href="#ise-mise-and-mse-for-kernel-estimators"><span class="header-section-number">2.3.2</span> ISE, MISE and MSE for kernel estimators</a></li>
<li><a class="nav-link" href="#plug-in-estimation-of-the-oracle-bandwidth"><span class="header-section-number">2.3.3</span> Plug-in estimation of the oracle bandwidth</a></li>
<li><a class="nav-link" href="#cv"><span class="header-section-number">2.3.4</span> Cross-validation</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#likelihood"><span class="header-section-number">2.4</span> Likelihood considerations</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sieves"><span class="header-section-number">2.4.1</span> Method of sieves</a></li>
<li><a class="nav-link" href="#basis-density"><span class="header-section-number">2.4.2</span> Basis expansions</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#density:ex"><span class="header-section-number">2.5</span> Exercises</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#kernel-density-estimation">Kernel density estimation</a></li>
<li><a class="nav-link" href="#benchmarking-1">Benchmarking</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/nielsrhansen/CSwR/blob/master/05-Density.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/nielsrhansen/CSwR/edit/master/05-Density.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Computational Statistics with R</strong>" was written by Niels Richard Hansen. It was last built on 2021-10-20, Git version: 86c4ccf.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
